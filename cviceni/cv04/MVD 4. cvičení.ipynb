{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVD 4. cvičení\n",
    "\n",
    "## 1. část - Načtení dat\n",
    "\n",
    "Po rozbalení archive.zip uvidíte articles csv soubor. Tento soubor pochází z [Kaggle datasetů](https://www.kaggle.com/hsankesara/medium-articles) a obsahuje malé množství Medium článků k tématům ML, AI a data science. K úloze dnešního cvičení bude stačit využítí dat s názvy a obsahy článků (title a text).\n",
    "\n",
    "\n",
    "### Příprava dat\n",
    "\n",
    "Pro přípravu dat se použivá různá sekvence kroků. Je doporučeno na následující kroky vytvořit samostatnou funkci, aby bylo možné zpracovat i vyhledávaný výraz při testování. Dnešní cvičení by mělo obsahovat následující kroky:\n",
    "\n",
    "1. Převést všechen text na lower case\n",
    "2. Odstranění interpunkce a všech speciálních znaků (apostrof, ...)\n",
    "3. Aplikace lemmatizátoru\n",
    "\n",
    "Pozn.: Jedná se pouze o jednoduchý preprocessing, v praxi je často potřeba použití více kroků. Tato aplikace by měla například problém s čísly (desetinná čísla, čísla vyhledávaná slovně). \n",
    "\n",
    "Pro lemmatizaci použijte knihovnu spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalace spaCy z Jupyter Notebooku\n",
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "\n",
    "# Stažení modelu pro angličtinu\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "lemmatizer = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # NLTK\n",
    "# Lemmatizace textu př.:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "def filetoLower(inf:str, outf:str, remove = False):\n",
    "    if (not os.path.exists(outf)) or remove:\n",
    "        with open(inf,\"r\",encoding=\"utf-8\") as file, open(outf,\"w\",encoding=\"utf-8\") as outFile:\n",
    "            reader = csv.reader(file)\n",
    "            writer = csv.writer(outFile)\n",
    "            writer.writerow(next(reader))\n",
    "            for line in reader:\n",
    "                outFile.write(line.lower())\n",
    "    else:\n",
    "        print(\"file already exsists\")\n",
    "filetoLower(\"articles.csv\",\"articlesLC.csv\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "disabledChars = [',','.','\\'','\"','’','‘','(',')','“','”','?','/','\\\\','!','[',']','–','/','\\\\',':','-']\n",
    "def filetoRemInter(inf:str, outf:str, remove = False):\n",
    "    if (not os.path.exists(outf)) or remove:\n",
    "        with open(inf,\"r\",encoding=\"utf-8\") as file, open(outf,\"w\",encoding=\"utf-8\") as outFile:\n",
    "            for line in file:\n",
    "                for ch in disabledChars:\n",
    "                    line = line.replace(ch,' ')\n",
    "                line = line.replace('  ',' ')\n",
    "                outFile.write(line)\n",
    "    else:\n",
    "        print(\"file already exsists\")\n",
    "filetoRemInter(\"articlesLC.csv\",\"articlesLC_RI.csv\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileLematize(inf:str, outf:str, remove = False):\n",
    "    if (not os.path.exists(outf)) or remove:\n",
    "        with open(inf,\"r\",encoding=\"utf-8\") as file, open(outf,\"w\",encoding=\"utf-8\") as outFile:\n",
    "            for line in file:\n",
    "                line = \" \".join([token.lemma_ for token in lemmatizer(line)])\n",
    "                outFile.write(line)\n",
    "    else:\n",
    "        print(\"file already exsists\")\n",
    "fileLematize(\"articlesLC_RI.csv\",\"articlesLEMMA.csv\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. část - Vytvoření invertovaného indexu\n",
    "\n",
    "Před další prací s textem je potřeba vytvořit invertovaný index, který poté usnadní práci. Invertovaný index bude slovník, kde klíčem bude slovo a hodnotou bude list s id dokumentů (index), které dané slovo obsahují.\n",
    "\n",
    "Pozn.: Je potřeba vytvořit dva invertované indexy - jeden pro title a druhý pro text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. část - Implementace TF-IDF\n",
    "\n",
    "Připravení funkce pro výpočet TF-IDF po příchodu dotazu. Funkce *tf_idf* by měla pracovat s dotazem, jedním invertovaným indexem a s danými dokumenty. Vrátit by měla list obsahující skóre pro každý dokument.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "$\n",
    "score(q,d) = TF\\_IDF(q,d) = \\sum\\limits_{w \\in q \\cap d} c(w, q) c(w, d) log(\\frac{M+1}{df(w)})\n",
    "$\n",
    "</center>\n",
    "\n",
    "$q$ ... dotaz<br>\n",
    "$d$ ... dokument<br>\n",
    "$c(w, q)$ ... kolikrát je slovo *w* v dotazu *q*<br>\n",
    "$M$ ... celkový počet dokumentů<br>\n",
    "$df(w)$ ... počet dokumentů, ve kterých se nachází slovo *w*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. část - Použití a testování TF-IDF\n",
    "\n",
    "Nyní lze získat skóre pro titulky nebo text. Následujícím krokem je sjednocení výsledného skóre pro ohodnocení celého dokumentu. V případě dvou hodnot si vystačíme s parametrem $\\alpha$, který nám určuje jakou váhu má titulek a jakou samotný text dokumentu. <br>\n",
    "\n",
    "<center>\n",
    "$\n",
    "score(q,d) = \\alpha \\; TF\\_IDF\\_title(q,d) + (1-\\alpha) \\; TF\\_IDF\\_text(q,d)\n",
    "$\n",
    "</center>\n",
    "\n",
    "Při nastavení parametru $\\alpha$ na hodnotu 0.7 a vyhledávání dotazu \"coursera vs udacity machine learning\" by výsledky měly vypadat následovně:\n",
    "\n",
    "![output](sample_output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b1f2b33e866b0bf2409397e5f58ba9cdf170d3b7f64c8f359c79998e2f88ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
