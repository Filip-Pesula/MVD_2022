{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVD 9. cvičení\n",
    "\n",
    "Dnešní cvičení nebude až tak obtížné. Cílem je seznámit se s HuggingFace a vyzkoušet si základní práci s BERT modelem.\n",
    "\n",
    "## 1. část - Seznámení s HuggingFace a modelem BERT\n",
    "\n",
    "Nainstalujte si Python knihovnu `transformers` a podívejte se na předtrénovaný [BERT model](https://huggingface.co/bert-base-uncased). Vyzkoušejte si unmasker s různými vstupy.\n",
    "\n",
    "<br>\n",
    "Pozn.: Použití BERT modelu vyžaduje zároveň PyTorch - postačí i cpu verze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0733572468161583,\n",
       "  'token': 23566,\n",
       "  'token_str': 'vegetarian',\n",
       "  'sequence': 'ananas on pizza is a vegetarian.'},\n",
       " {'score': 0.05517164617776871,\n",
       "  'token': 26136,\n",
       "  'token_str': 'pun',\n",
       "  'sequence': 'ananas on pizza is a pun.'},\n",
       " {'score': 0.044163379818201065,\n",
       "  'token': 12354,\n",
       "  'token_str': 'parody',\n",
       "  'sequence': 'ananas on pizza is a parody.'},\n",
       " {'score': 0.04168140888214111,\n",
       "  'token': 8257,\n",
       "  'token_str': 'joke',\n",
       "  'sequence': 'ananas on pizza is a joke.'},\n",
       " {'score': 0.01876555196940899,\n",
       "  'token': 5440,\n",
       "  'token_str': 'favorite',\n",
       "  'sequence': 'ananas on pizza is a favorite.'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")\n",
    "\n",
    "\n",
    "unmasker(\"Ananas on pizza is a [MASK].\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. část - BERT contextualized word embeddings\n",
    "\n",
    "BERT dokumentace obsahuje také návod jak použít tento model pro získání word embeddingů. Vyzkoušejte použití stejného slova v různém kontextu a podívejte se, jak se mění kosinova podobnost embeddingů v závislosti na kontextu daného slova.\n",
    "\n",
    "Podívejte se na výstup tokenizeru před vstupem do BERT modelu - kolik tokenů bylo vytvořeno pro větu \"Hello, this is Bert.\"? Zdůvodněte jejich počet.\n",
    "\n",
    "<br>\n",
    "Pozn.: Vyřešení předchozí otázky Vám pomůže zjistit, který vektor z výstupu pro cílové slovo použít."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from bertEmbedings import Embedings\n",
    "embedings = Embedings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ring']\n",
      "Shape is: 1 x 3072\n",
      "['can', 'you', 'ring', 'me']\n",
      "Shape is: 4 x 3072\n",
      "['did', 'you', 'hear', 'the', 'ring']\n",
      "Shape is: 5 x 3072\n",
      "['i', 'just', 'bought', 'a', 'new', 'ring']\n",
      "Shape is: 6 x 3072\n",
      "['ring', 'is', 'not', 'a', 'circle']\n",
      "Shape is: 5 x 3072\n",
      "ring \n",
      " [tensor(1.), tensor(0.4878), tensor(0.3521), tensor(0.3894), tensor(0.5676)]\n",
      "can you ring me \n",
      " [tensor(0.4878), tensor(1.), tensor(0.4527), tensor(0.5239), tensor(0.5082)]\n",
      "did you hear the ring \n",
      " [tensor(0.3521), tensor(0.4527), tensor(1.0000), tensor(0.5589), tensor(0.4818)]\n",
      "I just bought a new ring \n",
      " [tensor(0.3894), tensor(0.5239), tensor(0.5589), tensor(1.0000), tensor(0.5742)]\n",
      "ring is not a circle \n",
      " [tensor(0.5676), tensor(0.5082), tensor(0.4818), tensor(0.5742), tensor(1.)]\n"
     ]
    }
   ],
   "source": [
    "from bertEmbedings import Embedings\n",
    "\n",
    "\n",
    "embedings = Embedings()\n",
    "\n",
    "sentences = [\n",
    "    {\"sentence\":\"ring\",\"index\":0,\"embedings\":None,\"tokens\":None},\n",
    "    {\"sentence\":\"can you ring me\",\"index\":2,\"embedings\":None,\"tokens\":None},\n",
    "    {\"sentence\":\"did you hear the ring\",\"index\":3,\"embedings\":None,\"tokens\":None},\n",
    "    {\"sentence\":\"I just bought a new ring\",\"index\":5,\"embedings\":None,\"tokens\":None},\n",
    "    {\"sentence\":\"ring is not a circle\",\"index\":0,\"embedings\":None,\"tokens\":None},\n",
    "]\n",
    "embedings.processSentences(sentences)\n",
    "\n",
    "#print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, this is Bert.\n",
      "['hello', ',', 'this', 'is', 'bert', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"Hello, this is Bert.\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(text)\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Vizualizace slovních  embeddingů\n",
    "\n",
    "Vizualizujte slovní embeddingy - mění se jejich pozice v závislosti na kontextu tak, jak byste očekávali? Pokuste se vizualizovat i některá slova, ke kterým by se podle vás cílové slovo mělo po změně kontextu přiblížit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b1f2b33e866b0bf2409397e5f58ba9cdf170d3b7f64c8f359c79998e2f88ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
