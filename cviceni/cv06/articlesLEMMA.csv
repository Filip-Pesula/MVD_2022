author,claps,reading_time,link,title,text
Justin Lee,8.3K,11,https://medium.com/swlh/chatbots-were-the-next-big-thing-what-happened-5fc49dd6fa61?source=---------0----------------,chatbots be the next big thing what happen the startup medium,oh how the headline blare chatbot be the next big thing our hope be sky high bright eyed and bushy tail the industry be ripe for a new era of innovation it be time to start socialize with machine and why wouldn t they be all the road sign point towards insane success at the mobile world congress 2017 chatbot be the main headliner the conference organizer cite an overwhelming acceptance at the event of the inevitable shift of focus for brand and corporate to chatbot in fact the only significant question around chatbot be who would monopolize the field not whether chatbot would take off in the first place one year on we have an answer to that question no because there isn t even an ecosystem for a platform to dominate chatbot weren t the first technological development to be talk up in grandiose term and then slump spectacularly the age old hype cycle unfold in familiar fashion expectation build build and then it all kind of fizzle out the predict paradim shift didn t materialize and app be tellingly still alive and well we look back at our breathless optimism and turn to each other slightly baffled be that it that be the chatbot revolution we be promise digit s ethan bloch sum up the general consensus accord to dave feldman vice president of product design at heap chatbots didn t just take on one difficult problem and fail they take on several and fail all of they bot can interface with user in different way the big divide be text vs speech in the beginning of computer interface be the write word user have to type command manually into a machine to get anything do then graphical user interface guis come along and save the day we become entrance by windows mouse click icon and hey we eventually get color too meanwhile a bunch of research scientist be busily develop natural language nl interface to database instead of have to learn an arcane database query language another bunch of scientist be develop speech processing software so that you could just speak to your computer rather than have to type this turn out to be a whole lot more difficult than anyone originally realise the next item on the agenda be hold a two way dialog with a machine here s an example dialog date back to the 1990 with vcr setup system pretty cool right the system take turn in collaborative way and do a smart job of figure out what the user want it be carefully craft to deal with conversation involve vcrs and could only operate within strict limitation modern day bot whether they use type or speak input have to face all these challenge but also work in an efficient and scalable way on a variety of platform basically we re still try to achieve the same innovation we be 30 year ago here s where I think we re go wrong an oversized assumption have be that app be over and would be replace by bot by pit two such disparate concept against one another instead of see they as separate entity design to serve different purpose we discourage bot development you might remember a similar war cry when app first come onto the scene ten year ago but do you remember when app replace the internet it s say that a new product or service need to be two of the follow well cheap or fast be chatbot cheap or fast than app no — not yet at least whether they re well be subjective but I think it s fair to say that today s good bot isn t comparable to today s good app plus nobody think that use lyft be too complicated or that it s too hard to order food or buy a dress on an app what be too complicated be try to complete these task with a bot — and have the bot fail a great bot can be about as useful as an average app when it come to rich sophisticated multi layer app there s no competition that s because machine let we access vast and complex information system and the early graphical information system be a revolutionary leap forward in help we locate those system modern day app benefit from decade of research and experimentation why would we throw this away but if we swap the word replace with extend thing get much more interesting today s most successful bot experience take a hybrid approach incorporate chat into a broad strategy that encompass more traditional element the next wave will be multimodal app where you can say what you want like with siri and get back information as a map text or even a spoken response another problematic aspect of the sweeping nature of hype be that it tend to bypass essential question like these for plenty of company bot just aren t the right solution the past two year be litter with case of bot be blindly apply to problem where they aren t need build a bot for the sake of it let it loose and hope for the good will never end well the vast majority of bot be build use decision tree logic where the bot s can response rely on spot specific keyword in the user input the advantage of this approach be that it s pretty easy to list all the case that they be design to cover and that s precisely their disadvantage too that s because these bot be purely a reflection of the capability fastidiousness and patience of the person who create they ; and how many user need and input they be able to anticipate problem arise when life refuse to fit into those box accord to recent report 70 % of the 100 000 + bot on facebook messenger be fail to fulfil simple user request this be partly a result of developer fail to narrow their bot down to one strong area of focus when we be build growthbot we decide to make it specific to sale and marketer not an all rounder despite the temptation to get overexcite about potential capabiltie remember a bot that do one thing well be infinitely more helpful than a bot that do multiple thing poorly a competent developer can build a basic bot in minute — but one that can hold a conversation that s another story despite the constant hype around ai we re still a long way from achieve anything remotely human like in an ideal world the technology know as nlp natural language processing should allow a chatbot to understand the message it receive but nlp be only just emerge from research lab and be very much in its infancy some platform provide a bit of nlp but even the good be at toddler level capacity for example think about siri understand your word but not their meaning as matt asay outline this result in another issue failure to capture the attention and creativity of developer and conversation be complex they re not linear topic spin around each other take random turn restart or abruptly finish today s rule base dialogue system be too brittle to deal with this kind of unpredictability and statistical approach use machine learning be just as limit the level of ai require for human like conversation just isn t available yet and in the meantime there be few high quality example of trailblaze bot to lead the way as dave feldman remark once upon a time the only way to interact with computer be by type arcane command to the terminal visual interface use window icon or a mouse be a revolution in how we manipulate information there s a reason compute move from text base to graphical user interface guis on the input side it s easy and fast to click than it be to type tap or selecting be obviously preferable to type out a whole sentence even with predictive often error prone text on the output side the old adage that a picture be worth a thousand word be usually true we love optical display of information because we be highly visual creature it s no accident that kid love touch screen the pioneer who dream up graphical interface be inspire by cognitive psychology the study of how the brain deal with communication conversational uis be mean to replicate the way human prefer to communicate but they end up require extra cognitive effort essentially we re swap something simple for a more complex alternative sure there be some concept that we can only express use language show I all the way of get to a museum that give I 2000 step but don t take long than 35 minute but most task can be carry out more efficiently and intuitively with guis than with a conversational ui aim for a human dimension in business interaction make sense if there s one thing that s break about sale and market it s the lack of humanity brand hide behind ticket number feedback form do not reply email automate response and gate contact we form facebook s goal be that their bot should pass the so call ture test mean you can t tell whether you be talk to a bot or a human but a bot isn t the same as a human it never will be a conversation encompasse so much more than just text human can read between the line leverage contextual information and understand double layer like sarcasm bot quickly forget what they re talk about mean it s a bit like converse with someone who have little or no short term memory as hubspot team pinpoint people aren t easily fool and pretend a bot be a human be guarantee to diminish return not to mention the fact that you re lie to your user and even those rare bot that be power by state of the art nlp and excel at processing and produce content will fall short in comparison and here s the other thing conversational uis be build to replicate the way human prefer to communicate — with other human but be that how human prefer to interact with machine not necessarily at the end of the day no amount of witty quip or human like mannerism will save a bot from conversational failure in a way those early adopter weren t entirely wrong people be yell at google home to play their favorite song order pizza from the domino s bot and get makeup tip from sephora but in term of consumer response and developer involvement chatbots haven t live up to the hype generate circa 2015 16 not even close computer be good at be computer search for datum crunch number analyze opinion and condense that information computer aren t good at understand human emotion the state of nlp mean they still don t get what we re ask they never mind how we feel that s why it s still impossible to imagine effective customer support sale or marketing without the essential human touch empathy and emotional intelligence for now bot can continue to help we with automate repetitive low level task and query ; as cog in a large more complex system and we do they and ourselves a disservice by expect so much so soon but that s not the whole story yes our industry massively overestimate the initial impact chatbot would have emphasis on initial as bill gate once say the hype be over and that s a good thing now we can start examine the middle ground grey area instead of the hyper inflate frantic black and white zone I believe we re at the very beginning of explosive growth this sense of anti climax be completely normal for transformational technology messaging will continue to gain traction chatbot aren t go away nlp and ai be become more sophisticated every day developer app and platform will continue to experiment with and heavily invest in conversational marketing and I can t wait to see what happen next from a quick cheer to a stand ovation clap to show how much you enjoy this story head of growth for growthbot messaging & conversational strategy @hubspot medium s large publication for maker subscribe to receive our top story here → https goo gl zhclji
Conor Dewey,1.4K,7,https://towardsdatascience.com/python-for-data-science-8-concepts-you-may-have-forgotten-i-did-825966908393?source=---------1----------------,python for data science 8 concept you may have forget,if you ve ever find yourself look up the same question concept or syntax over and over again when program you re not alone I find myself do this constantly while it s not unnatural to look thing up on stackoverflow or other resource it do slow you down a good bit and raise question as to your complete understanding of the language we live in a world where there be a seemingly infinite amount of accessible free resource loom just one search away at all time however this can be both a blessing and a curse when not manage effectively an over reliance on these resource can build poor habit that will set you back long term personally I find myself pull code from similar discussion thread several time rather than take the time to learn and solidify the concept so that I can reproduce the code myself the next time this approach be lazy and while it may be the path of least resistance in the short term it will ultimately hurt your growth productivity and ability to recall syntax cough interview down the line recently I ve be work through an online data science course title python for datum science and machine learning on udemy oh god I sound like that guy on youtube over the early lecture in the series I be remind of some concept and syntax that I consistently overlook when perform datum analysis in python in the interest of solidify my understanding of these concept once and for all and save you guy a couple of stackoverflow search here s the stuff that I m always forget when work with python numpy and panda I ve include a short description and example for each however for your benefit I will also include link to video and other resource that explore each concept more in depth as well write out a for loop every time you need to define some sort of list be tedious luckily python have a build in way to address this problem in just one line of code the syntax can be a little hard to wrap your head around but once you get familiar with this technique you ll use it fairly often see the example above and below for how you would normally go about list comprehension with a for loop vs create your list with in one simple line with no loop necessary ever get tired of create function after function for limited use case lambda function to the rescue lambda function be use for create small one time and anonymous function object in python basically they let you create a function without create a function the basic syntax of lambda function be note that lambda function can do everything that regular function can do as long as there s just one expression check out the simple example below and the upcoming video to get a well feel for the power of lambda function once you have a grasp on lambda function learn to pair they with the map and filter function can be a powerful tool specifically map take in a list and transform it into a new list by perform some sort of operation on each element in this example it go through each element and map the result of itself time 2 to a new list note that the list function simply convert the output to list type the filter function take in a list and a rule much like map however it return a subset of the original list by compare each element against the boolean filtering rule for create quick and easy numpy array look no far than the arange and linspace function each one have their specific purpose but the appeal here instead of use range be that they output numpy array which be typically easy to work with for data science arange return evenly space value within a give interval along with a starting and stop point you can also define a step size or datum type if necessary note that the stopping point be a cut off value so it will not be include in the array output linspace be very similar but with a slight twist linspace return evenly space number over a specified interval so give a starting and stop point as well as a number of value linspace will evenly space they out for you in a numpy array this be especially helpful for datum visualization and declare axis when plot you may have run into this when drop a column in panda or sum value in numpy matrix if not then you surely will at some point let s use the example of drop a column for now I don t know how many time I write this line of code before I actually know why I be declare axis what I be as you can probably deduce from above set axis to 1 if you want to deal with column and set it to 0 if you want row but why be this my favorite reasoning or atleast how I remember this call the shape attribute from a pandas dataframe give we back a tuple with the first value represent the number of row and the second value represent the number of column if you think about how this be index in python row be at 0 and column be at 1 much like how we declare our axis value crazy right if you re familiar with sql then these concept will probably come a lot easy for you anyhow these function be essentially just way to combine dataframe in specific way it can be difficult to keep track of which be good to use at which time so let s review it concat allow the user to append one or more dataframe to each other either below or next to it depend on how you define the axis merge combine multiple dataframe on specific common column that serve as the primary key join much like merge combine two dataframe however it join they base on their index rather than some specify column check out the excellent panda documentation for specific syntax and more concrete example as well as some special case that you may run into think of apply as a map function but make for panda dataframe or more specifically for series if you re not as familiar series be pretty similar to numpy array for the most part apply send a function to every element along a column or row depend on what you specify you might imagine how useful this can be especially for format and manipulate value across a whole dataframe column without have to loop at all last but certainly not least be pivot table if you re familiar with microsoft excel then you ve probably hear of pivot table in some respect the panda build in pivot_table function create a spreadsheet style pivot table as a dataframe note that the level in the pivot table be store in multiindex object on the index and column of the result dataframe that s it for now I hope a couple of these overview have effectively jog your memory regard important yet somewhat tricky method function and concept you frequently encounter when use python for data science personally I know that even the act of write these out and try to explain they in simple term have help I out a ton if you re interested in receive my weekly rundown of interesting article and resource focus on data science machine learning and artificial intelligence then subscribe to self drive data science use the form below if you enjoy this post feel free to hit the clap button and if you re interested in post to come make sure to follow I on medium at the link below — I ll be write and shipping every day this month as part of a 30 day challenge this article be originally publish on conordewey com from a quick cheer to a stand ovation clap to show how much you enjoy this story data scientist & writer | www conordewey com sharing concept idea and code
William Koehrsen,2.8K,11,https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219?source=---------2----------------,automate feature engineering in python towards data science,machine learning be increasingly move from hand design model to automatically optimize pipeline use tool such as h20 tpot and auto sklearn these library along with method such as random search aim to simplify the model selection and tune part of machine learning by find the good model for a dataset with little to no manual intervention however feature engineer an arguably more valuable aspect of the machine learn pipeline remain almost entirely a human labor feature engineering also know as feature creation be the process of construct new feature from exist datum to train a machine learning model this step can be more important than the actual model use because a machine learn algorithm only learn from the datum we give it and create feature that be relevant to a task be absolutely crucial see the excellent paper a few useful thing to know about machine learning typically feature engineering be a draw out manual process rely on domain knowledge intuition and datum manipulation this process can be extremely tedious and the final feature will be limit both by human subjectivity and time automate feature engineering aim to help the data scientist by automatically create many candidate feature out of a dataset from which the good can be select and use for training in this article we will walk through an example of use automate feature engineering with the featuretools python library we will use an example dataset to show the basic stay tune for future post use real world datum the complete code for this article be available on github feature engineering mean build additional feature out of exist datum which be often spread across multiple relate table feature engineering require extract the relevant information from the datum and get it into a single table which can then be use to train a machine learning model the process of construct feature be very time consume because each new feature usually require several step to build especially when use information from more than one table we can group the operation of feature creation into two category transformation and aggregation let s look at a few example to see these concept in action a transformation act on a single table thinking in term of python a table be just a panda dataframe by create new feature out of one or more of the exist column as an example if we have the table of client below we can create feature by find the month of the joined column or take the natural log of the income column these be both transformation because they use information from only one table on the other hand aggregation be perform across table and use a one to many relationship to group observation and then calculate statistic for example if we have another table with information on the loan of client where each client may have multiple loan we can calculate statistic such as the average maximum and minimum of loan for each client this process involve group the loan table by the client calculate the aggregation and then merge the result datum into the client datum here s how we would do that in python use the language of panda these operation be not difficult by themselves but if we have hundred of variable spread across dozen of table this process be not feasible to do by hand ideally we want a solution that can automatically perform transformation and aggregation across multiple table and combine the result datum into a single table although panda be a great resource there s only so much datum manipulation we want to do by hand for more on manual feature engineering check out the excellent python data science handbook fortunately featuretool be exactly the solution we be look for this open source python library will automatically create many feature from a set of relate table featuretool be base on a method know as deep feature synthesis which sound a lot more imposing than it actually be the name come from stack multiple feature not because it use deep learn deep feature synthesis stack multiple transformation and aggregation operation which be call feature primitive in the vocab of featuretool to create feature from datum spread across many table like most idea in machine learn it s a complex method build on a foundation of simple concept by learn one building block at a time we can form a good understanding of this powerful method first let s take a look at our example datum we already see some of the dataset above and the complete collection of table be as follow if we have a machine learn task such as predict whether a client will repay a future loan we will want to combine all the information about client into a single table the table be relate through the client_id and the loan_id variable and we could use a series of transformation and aggregation to do this process by hand however we will shortly see that we can instead use featuretool to automate the process the first two concept of featuretool be entity and entityset an entity be simply a table or a dataframe if you think in panda an entityset be a collection of table and the relationship between they think of an entityset as just another python data structure with its own method and attribute we can create an empty entityset in featuretool use the following now we have to add entity each entity must have an index which be a column with all unique element that be each value in the index must appear in the table only once the index in the client dataframe be the client_idbecause each client have only one row in this dataframe we add an entity with an exist index to an entityset use the follow syntax the loan dataframe also have a unique index loan_id and the syntax to add this to the entityset be the same as for client however for the payment dataframe there be no unique index when we add this entity to the entityset we need to pass in the parameter make_index = true and specify the name of the index also although featuretool will automatically infer the data type of each column in an entity we can override this by pass in a dictionary of column type to the parameter variable_type for this dataframe even though miss be an integer this be not a numeric variable since it can only take on 2 discrete value so we tell featuretool to treat be as a categorical variable after add the dataframe to the entityset we inspect any of they the column type have be correctly infer with the modification we specify next we need to specify how the table in the entityset be relate the good way to think of a relationship between two table be the analogy of parent to child this be a one to many relationship each parent can have multiple child in the realm of table a parent table have one row for every parent but the child table may have multiple row corresponding to multiple child of the same parent for example in our dataset the client dataframe be a parent of the loan dataframe each client have only one row in client but may have multiple row in loan likewise loan be the parent of payment because each loan will have multiple payment the parent be link to their child by a share variable when we perform aggregation we group the child table by the parent variable and calculate statistic across the child of each parent to formalize a relationship in featuretool we only need to specify the variable that link two table together the client and the loan table be link via the client_id variable and loan and payment be link with the loan_id the syntax for create a relationship and add it to the entityset be show below the entityset now contain the three entity table and the relationship that link these entity together after add entity and formalizing relationship our entityset be complete and we be ready to make feature before we can quite get to deep feature synthesis we need to understand feature primitive we already know what these be but we have just be call they by different name these be simply the basic operation that we use to form new feature new feature be create in featuretool use these primitive either by themselves or stack multiple primitive below be a list of some of the feature primitive in featuretool we can also define custom primitive these primitive can be use by themselves or combine to create feature to make feature with specify primitive we use the ft dfs function stand for deep feature synthesis we pass in the entityset the target_entity which be the table where we want to add the feature the select trans_primitive transformation and agg_primitive aggregation the result be a dataframe of new feature for each client because we make client the target_entity for example we have the month each client join which be a transformation feature primitive we also have a number of aggregation primitive such as the average payment amount for each client even though we specify only a few feature primitive featuretool create many new feature by combine and stack these primitive the complete dataframe have 793 column of new feature we now have all the piece in place to understand deep feature synthesis dfs in fact we already perform dfs in the previous function call a deep feature be simply a feature make of stack multiple primitive and dfs be the name of process that make these feature the depth of a deep feature be the number of primitive require to make the feature for example the mean payment payment_amount column be a deep feature with a depth of 1 because it be create use a single aggregation a feature with a depth of two be last loan mean payment payment_amount this be make by stack two aggregation last most recent on top of mean this represent the average payment size of the most recent loan for each client we can stack feature to any depth we want but in practice I have never go beyond a depth of 2 after this point the feature be difficult to interpret but I encourage anyone interested to try go deep we do not have to manually specify the feature primitive but instead can let featuretool automatically choose feature for we to do this we use the same ft dfs function call but do not pass in any feature primitive featuretool have build many new feature for we to use while this process do automatically create new feature it will not replace the data scientist because we still have to figure out what to do with all these feature for example if our goal be to predict whether or not a client will repay a loan we could look for the feature most correlate with a specified outcome moreover if we have domain knowledge we can use that to choose specific feature primitive or seed deep feature synthesis with candidate feature automate feature engineering have solve one problem but create another too many feature although it s difficult to say before fit a model which of these feature will be important it s likely not all of they will be relevant to a task we want to train our model on moreover have too many feature can lead to poor model performance because the less useful feature drown out those that be more important the problem of too many feature be know as the curse of dimensionality as the number of feature increase the dimension of the data grow it become more and more difficult for a model to learn the mapping between feature and target in fact the amount of datum need for the model to perform well scale exponentially with the number of feature the curse of dimensionality be combat with feature reduction also know as feature selection the process of remove irrelevant feature this can take on many form principal component analysis pca selectkb use feature importance from a model or auto encoding use deep neural network however feature reduction be a different topic for another article for now we know that we can use featuretool to create numerous feature from many table with minimal effort like many topic in machine learning automate feature engineering with featuretool be a complicated concept build on simple idea use concept of entityset entity and relationship featuretool can perform deep feature synthesis to create new feature deep feature synthesis in turn stack feature primitive — aggregation which act across a one to many relationship between table and transformation function apply to one or more column in a single table — to build new feature from multiple table in future article I ll show how to use this technique on a real world problem the home credit default risk competition currently be host on kaggle stay tune for that post and in the meantime read this introduction to get start in the competition I hope that you can now use automate feature engineering as an aid in a data science pipeline our model be only as good as the datum we give they and automate feature engineering can help to make the feature creation process more efficient for more information on featuretool include advanced usage check out the online documentation to see how featuretool be use in practice read about the work of feature lab the company behind the open source library as always I welcome feedback and constructive criticism and can be reach on twitter @koehrsen_will from a quick cheer to a stand ovation clap to show how much you enjoy this story data scientist and master student datum science communicator and advocate sharing concept idea and code
Gant Laborde,1.3K,7,https://medium.freecodecamp.org/machine-learning-how-to-go-from-zero-to-hero-40e26f8aa6da?source=---------3----------------,machine learn how to go from zero to hero freecodecamp,if your understanding of a i and machine learning be a big question mark then this be the blog post for you here I gradually increase your awesomenessicitytm by glue inspirational video together with friendly text sit down and relax these video take time and if they don t inspire you to continue to the next section fair enough however if you find yourself at the bottom of this article you ve earn your well rounded knowledge and passion for this new world where you go from there be up to you a I be always cool from move a paddle in pong to light you up with combo in street fighter a I have always revolve around a programmer s functional guess at how something should behave fun but programmer aren t always gift in program a i as we often see just google epic game fail to see glitch in a i physics and sometimes even experience human player regardless a I have a new talent you can teach a computer to play video game understand language and even how to identify people or thing this tip of the iceberg new skill come from an old concept that only recently get the processing power to exist outside of theory I m talk about machine learn you don t need to come up with advanced algorithm anymore you just have to teach a computer to come up with its own advanced algorithm so how do something like that even work an algorithm isn t really write as much as it be sort of breed I m not use breeding as an analogy watch this short video which give excellent commentary and animation to the high level concept of create the a i wow right that s a crazy process now how be it that we can t even understand the algorithm when it s do one great visual be when the a I be write to beat mario game as a human we all understand how to play a side scroller but identify the predictive strategy of the result a I be insane impressed there s something amazing about this idea right the only problem be we don t know machine learning and we don t know how to hook it up to video game fortunately for you elon musk already provide a non profit company to do the latter yes in a dozen line of code you can hook up any a I you want to countless game task I have two good answer on why you should care firstly machine learning ml be make computer do thing that we ve never make computer do before if you want to do something new not just new to you but to the world you can do it with ml secondly if you don t influence the world the world will influence you right now significant company be invest in ml and we re already see it change the world think leader be warn that we can t let this new age of algorithm exist outside of the public eye imagine if a few corporate monolith control the internet if we don t take up arm the science win t be our I think christian heilmann say it well in his talk on ml the concept be useful and cool we understand it at a high level but what the heck be actually happen how do this work if you want to jump straight in I suggest you skip this section and move on to the next how do I get start section if you re motivated to be a doer in ml you win t need these video if you re still try to grasp how this could even be a thing the follow video be perfect for walk you through the logic use the classic ml problem of handwriting pretty cool huh that video show that each layer get simple rather than more complicated like the function be chew datum into small piece that end in an abstract concept you can get your hand dirty in interact with this process on this site by adam harley it s cool watch datum go through a train model but you can even watch your neural network get train one of the classic real world example of machine learning in action be the iris data set from 1936 in a presentation I attend by javafxpert s overview on machine learning I learn how you can use his tool to visualize the adjustment and back propagation of weight to neuron on a neural network you get to watch it train the neural model even if you re not a java buff the presentation jim give on all thing machine learning be a pretty cool 1 5 + hour introduction into ml concept which include more info on many of the example above these concept be exciting be you ready to be the einstein of this new era breakthrough be happen every day so get start now there be ton of resource available I ll be recommend two approach in this approach you ll understand machine learn down to the algorithm and the math I know this way sound tough but how cool would it be to really get into the detail and code this stuff from scratch if you want to be a force in ml and hold your own in deep conversation then this be the route for you I recommend that you try out brilliant org s app always great for any science lover and take the artificial neural network course this course have no time limit and help you learn ml while kill time in line on your phone this one cost money after level 1 combine the above with simultaneous enrollment in andrew ng s stanford course on machine learning in 11 week this be the course that jim weaver recommend in his video above I ve also have this course independently suggest to I by jen looper everyone provide a caveat that this course be tough for some of you that s a show stopper but for other that s why you re go to put yourself through it and collect a certificate say you do this course be 100 % free you only have to pay for a certificate if you want one with those two course you ll have a lot of work to do everyone should be impressed if you make it through because that s not simple but more so if you do make it through you ll have a deep understanding of the implementation of machine learning that will catapult you into successfully apply it in new and world change way if you re not interested in write the algorithm but you want to use they to create the next breathtaking website app you should jump into tensorflow and the crash course tensorflow be the de facto open source software library for machine learn it can be use in countless way and even with javascript here s a crash course plenty more information on available course and ranking can be find here if take a course be not your style you re still in luck you don t have to learn the nitty gritty of ml in order to use it today you can efficiently utilize ml as a service in many way with tech giant who have train model ready I would still caution you that there s no guarantee that your data be safe or even yours but the offering of service for ml be quite attractive use an ml service might be the good solution for you if you re excited and able to upload your datum to amazon microsoft google I like to think of these service as a gateway drug to advanced ml either way it s good to get start now I have to say thank you to all the aforementione people and video they be my inspiration to get start and though I m still a newb in the ml world I m happy to light the path for other as we embrace this awe inspire age we find ourselves in it s imperative to reach out and connect with people if you take up learn this craft without friendly face answer and sound board anything can be hard just be able to ask and get a response be a game changer add I and add the people mention above friendly people with friendly advice help see I hope this article have inspire you and those around you to learn ml from a quick cheer to a stand ovation clap to show how much you enjoy this story software consultant adjunct professor publish author award win speaker mentor organizer and immature nerd d — lately full of react native tech our community publish story worth read on development design and datum science
Emmanuel Ameisen,935,11,https://blog.insightdatascience.com/reinforcement-learning-from-scratch-819b65f074d8?source=---------4----------------,reinforcement learning from scratch insight datum,want to learn about apply artificial intelligence from lead practitioner in silicon valley new york or toronto learn more about the insight artificial intelligence fellow program be you a company work in ai and would like to get involve in the insight ai fellow program feel free to get in touch recently I give a talk at the o reilly ai conference in beijing about some of the interesting lesson we ve learn in the world of nlp while there I be lucky enough to attend a tutorial on deep reinforcement learning deep rl from scratch by unity technology I think that the session lead by arthur juliani be extremely informative and want to share some big takeaway below in our conversation with company we ve see a rise of interesting deep rl application tool and result in parallel the inner working and application of deep rl such as alphago picture above can often seem esoteric and hard to understand in this post I will give an overview of core aspect of the field that can be understand by anyone many of the visual be from the slide of the talk and some be new the explanation and opinion be mine if anything be unclear reach out to I here deep rl be a field that have see vast amount of research interest include learn to play atari game beat pro player at dota 2 and defeat go champion contrary to many classical deep learning problem that often focus on perception do this image contain a stop sign deep rl add the dimension of action that influence the environment what be the goal and how do I get there in dialog system for example classical deep learning aim to learn the right response for a give query on the other hand deep reinforcement learning focus on the right sequence of sentence that will lead to a positive outcome for example a happy customer this make deep rl particularly attractive for task that require planning and adaptation such as manufacturing or self drive however industry application have trail behind the rapidly advance result come out of the research community a major reason be that deep rl often require an agent to experiment million of time before learn anything useful the good way to do this rapidly be by use a simulation environment this tutorial will be use unity to create environment to train agent in for this workshop lead by arthur juliani and leon chen their goal be to get every participant to successfully train multiple deep rl algorithm in 4 hour a tall order below be a comprehensive overview of many of the main algorithm that power deep rl today for a more complete set of tutorial arthur juliani write an 8 part series start here deep rl can be use to best the top human player at go but to understand how that s do you first need to understand a few simple concept start with much easy problem 1 it all start with slot machine let s imagine you be face with 4 chest that you can pick from at each turn each of they have a different average payout and your goal be to maximize the total payout you receive after a fix number of turn this be a classic problem call multi armed bandit and be where we will start the crux of the problem be to balance exploration which help we learn about which state be good and exploitation where we now use what we know to pick the good slot machine here we will utilize a value function that map our action to an estimate reward call the q function first we ll initialize all q value at equal value then we ll update the q value of each action pick each chest base on how good the payout be after choose this action this allow we to learn a good value function we will approximate our q function use a neural network start with a very shallow one that learn a probability distribution by use a softmax over the 4 potential chest while the value function tell we how good we estimate each action to be the policy be the function that determine which action we end up take intuitively we might want to use a policy that pick the action with the high q value this perform poorly in practice as our q estimate will be very wrong at the start before we gather enough experience through trial and error this be why we need to add a mechanism to our policy to encourage exploration one way to do that be to use epsilon greedy which consist of take a random action with probability epsilon we start with epsilon be close to 1 always choose random action and low epsilon as we go along and learn more about which chest be good eventually we learn which chest be good in practice we might want to take a more subtle approach than either take the action we think be the good or a random action a popular method be boltzmann exploration which adjust probability base on our current estimate of how good each chest be add in a randomness factor 2 add different state the previous example be a world in which we be always in the same state wait to pick from the same 4 chest in front of we most real word problem consist of many different state that be what we will add to our environment next now the background behind chest alternate between 3 color at each turn change the average value of the chest this mean we need to learn a q function that depend not only on the action the chest we pick but the state what the color of the background be this version of the problem be call contextual multi armed bandit surprisingly we can use the same approach as before the only thing we need to add be an extra dense layer to our neural network that will take in as input a vector represent the current state of the world 3 learn about the consequence of our action there be another key factor that make our current problem simple than most in most environment such as in the maze depict above the action that we take have an impact on the state of the world if we move up on this grid we might receive a reward or we might receive nothing but the next turn we will be in a different state this be where we finally introduce a need for plan first we will define our q function as the immediate reward in our current state plus the discount reward we be expect by take all of our future action this solution work if our q estimate of state be accurate so how can we learn a good estimate we will use a method call temporal difference td learn to learn a good q function the idea be to only look at a limited number of step in the future td 1 for example only use the next 2 state to evaluate the reward surprisingly we can use td 0 which look at the current state and our estimate of the reward the next turn and get great result the structure of the network be the same but we need to go through one forward step before receive the error we then use this error to back propagate gradient like in traditional deep learning and update our value estimate 3 + introduce monte carlo another method to estimate the eventual success of our action be monte carlo estimate this consist of play out the entire episode with our current policy until we reach an end success by reach a green block or failure by reach a red block in the image above and use that result to update our value estimate for each traverse state this allow we to propagate value efficiently in one batch at the end of an episode instead of every time we make a move the cost be that we be introduce noise to our estimate since we attribute very distant reward to they 4 the world be rarely discrete the previous method be use neural network to approximate our value estimate by mapping from a discrete number of state and action to a value in the maze for example there be 49 state square and 4 action move in each adjacent direction in this environment we be try to learn how to balance a ball on a 2 dimensional paddle by decide at each time step whether we want to tilt the paddle leave or right here the state space become continuous the angle of the paddle and the position of the ball the good news be we can still use neural network to approximate this function a note about off policy vs on policy learn the method we use previously be off policy method mean we can generate datum with any strategy use epsilon greedy for example and learn from it on policy method can only learn from action that be take follow our policy remember a policy be the method we use to determine which action to take this constrain our learning process as we have to have an exploration strategy that be build in to the policy itself but allow we to tie result directly to our reasoning and enable we to learn more efficiently the approach we will use here be call policy gradient and be an on policy method previously we be first learn a value function q for each action in each state and then build a policy on top in vanilla policy gradient we still use monte carlo estimate but we learn our policy directly through a loss function that increase the probability of choose reward action since we be learn on policy we can not use method such as epsilon greedy which include random choice to get our agent to explore the environment the way that we encourage exploration be by use a method call entropy regularization which push our probability estimate to be wide and thus will encourage we to make risky choice to explore the space 4 + leverage deep learning for representation in practice many state of the art rl method require learn both a policy and value estimate the way we do this with deep learning be by have both be two separate output of the same backbone neural network which will make it easy for our neural network to learn good representation one method to do this be advantage actor critic a2c we learn our policy directly with policy gradient define above and learn a value function use something call advantage instead of update our value function base on reward we update it base on our advantage which measure how much well or bad an action be than our previous value function estimate it to be this help make learn more stable compare to simple q learning and vanilla policy gradient 5 learn directly from the screen there be an additional advantage to use deep learning for these method which be that deep neural network excel at perceptive task when a human play a game the information receive be not a list of state but an image usually of a screen or a board or the surround environment image base learning combine a convolutional neural network cnn with rl in this environment we pass in a raw image instead of feature and add a 2 layer cnn to our architecture without change anything else we can even inspect activation to see what the network pick up on to determine value and policy in the example below we can see that the network use the current score and distant obstacle to estimate the value of the current state while focus on nearby obstacle for determine action neat as a side note while toy around with the provide implementation I ve find that visual learning be very sensitive to hyperparameter change the discount rate slightly for example completely prevent the neural network from learn even on a toy application this be a widely know problem but it be interesting to see it first hand 6 nuanced action so far we ve play with environment with continuous and discrete state space however every environment we study have a discrete action space we could move in one of four direction or tilt the paddle to the left or right ideally for application such as self drive car we would like to learn continuous action such as turn the steering wheel between 0 and 360 degree in this environment call 3d ball world we can choose to tilt the paddle to any value on each of its axis this give we more control as to how we perform action but make the action space much large we can approach this by approximate our potential choice with gaussian distribution we learn a probability distribution over potential action by learn the mean and variance of a gaussian distribution and our policy we sample from that distribution simple in theory 7 next step for the brave there be a few concept that separate the algorithm describe above from state of the art approach it s interesting to see that conceptually the good robotic and game playing algorithm be not that far away from the one we just explore that s it for this overview I hope this have be informative and fun if you be look to dive deeply into the theory of rl give arthur s post a read or diving deeply by follow david silver s ucl course if you be look to learn more about the project we do at insight or how we work with company please check we out below or reach out to I here want to learn about apply artificial intelligence from lead practitioner in silicon valley new york or toronto learn more about the insight artificial intelligence fellow program be you a company work in ai and would like to get involve in the insight ai fellow program feel free to get in touch from a quick cheer to a stand ovation clap to show how much you enjoy this story ai lead at insight ai @emmanuelameisen insight fellow program your bridge to a career in datum
Irhum Shafkat,2K,15,https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1?source=---------5----------------,intuitively understand convolution for deep learning,the advent of powerful and versatile deep learning framework in recent year have make it possible to implement convolution layer into a deep learning model an extremely simple task often achievable in a single line of code however understand convolution especially for the first time can often feel a bit unnerving with term like kernels filter channel and so on all stack onto each other yet convolution as a concept be fascinatingly powerful and highly extensible and in this post we ll break down the mechanic of the convolution operation step by step relate it to the standard fully connect network and explore just how they build up a strong visual hierarchy make they powerful feature extractor for image the 2d convolution be a fairly simple operation at heart you start with a kernel which be simply a small matrix of weight this kernel slide over the 2d input datum perform an elementwise multiplication with the part of the input it be currently on and then sum up the result into a single output pixel the kernel repeat this process for every location it slide over convert a 2d matrix of feature into yet another 2d matrix of feature the output feature be essentially the weighted sum with the weight be the value of the kernel itself of the input feature locate roughly in the same location of the output pixel on the input layer whether or not an input feature fall within this roughly same location get determine directly by whether it s in the area of the kernel that produce the output or not this mean the size of the kernel directly determine how many or few input feature get combine in the production of a new output feature this be all in pretty stark contrast to a fully connect layer in the above example we have 5×5=25 input feature and 3×3=9 output feature if this be a standard fully connect layer you d have a weight matrix of 25×9 = 225 parameter with every output feature be the weighted sum of every single input feature convolution allow we to do this transformation with only 9 parameter with each output feature instead of look at every input feature only get to look at input feature come from roughly the same location do take note of this as it ll be critical to our later discussion before we move on it s definitely worth look into two technique that be commonplace in convolution layer padding and stride padding do something pretty clever to solve this pad the edge with extra fake pixel usually of value 0 hence the oft use term zero padding this way the kernel when slide can allow the original edge pixel to be at its center while extend into the fake pixel beyond the edge produce an output the same size as the input the idea of the stride be to skip some of the slide location of the kernel a stride of 1 mean to pick slide a pixel apart so basically every single slide act as a standard convolution a stride of 2 mean pick slide 2 pixel apart skip every other slide in the process downsize by roughly a factor of 2 a stride of 3 mean skip every 2 slide downsize roughly by factor 3 and so on more modern network such as the resnet architecture entirely forgo pooling layer in their internal layer in favor of strided convolution when need to reduce their output size of course the diagram above only deal with the case where the image have a single input channel in practicality most input image have 3 channel and that number only increase the deep you go into a network it s pretty easy to think of channel in general as be a view of the image as a whole emphasise some aspect de emphasise other so this be where a key distinction between term come in handy whereas in the 1 channel case where the term filter and kernel be interchangeable in the general case they re actually pretty different each filter actually happen to be a collection of kernel with there be one kernel for every single input channel to the layer and each kernel be unique each filter in a convolution layer produce one and only one output channel and they do it like so each of the kernel of the filter slide over their respective input channel produce a process version of each some kernel may have strong weight than other to give more emphasis to certain input channel than other eg a filter may have a red kernel channel with strong weight than other and hence respond more to difference in the red channel feature than the other each of the per channel process version be then sum together to form one channel the kernel of a filter each produce one version of each channel and the filter as a whole produce one overall output channel finally then there s the bias term the way the bias term work here be that each output filter have one bias term the bias get add to the output channel so far to produce the final output channel and with the single filter case down the case for any number of filter be identical each filter process the input with its own different set of kernel and a scalar bias with the process describe above produce a single output channel they be then concatenate together to produce the overall output with the number of output channel be the number of filter a nonlinearity be then usually apply before pass this as input to another convolution layer which then repeat this process even with the mechanic of the convolution layer down it can still be hard to relate it back to a standard feed forward network and it still doesn t explain why convolution scale to and work so much well for image datum suppose we have a 4×4 input and we want to transform it into a 2×2 grid if we be use a feedforward network we d reshape the 4×4 input into a vector of length 16 and pass it through a densely connect layer with 16 input and 4 output one could visualize the weight matrix w for a layer and although the convolution kernel operation may seem a bit strange at first it be still a linear transformation with an equivalent transformation matrix if we be to use a kernel k of size 3 on the reshaped 4×4 input to get a 2×2 output the equivalent transformation matrix would be note while the above matrix be an equivalent transformation matrix the actual operation be usually implement as a very different matrix multiplication 2 the convolution then as a whole be still a linear transformation but at the same time it s also a dramatically different kind of transformation for a matrix with 64 element there s just 9 parameter which themselves be reuse several time each output node only get to see a select number of input the one inside the kernel there be no interaction with any of the other input as the weight to they be set to 0 it s useful to see the convolution operation as a hard prior on the weight matrix in this context by prior I mean predefine network parameter for example when you use a pretraine model for image classification you use the pretraine network parameter as your prior as a feature extractor to your final densely connect layer in that sense there s a direct intuition between why both be so efficient compare to their alternative transfer learning be efficient by order of magnitude compare to random initialization because you only really need to optimize the parameter of the final fully connect layer which mean you can have fantastic performance with only a few dozen image per class here you don t need to optimize all 64 parameter because we set most of they to zero and they ll stay that way and the rest we convert to share parameter result in only 9 actual parameter to optimize this efficiency matter because when you move from the 784 input of mnist to real world 224×224×3 image that s over 150 000 input a dense layer attempt to halve the input to 75 000 input would still require over 10 billion parameter for comparison the entirety of resnet 50 have some 25 million parameter so fix some parameter to 0 and tie parameter increase efficiency but unlike the transfer learning case where we know the prior be good because it work on a large general set of image how do we know this be any good the answer lie in the feature combination the prior lead the parameter to learn early on in this article we discuss that so with backpropagation come in all the way from the classification node of the network the kernel have the interesting task of learn weight to produce feature only from a set of local input additionally because the kernel itself be apply across the entire image the feature the kernel learn must be general enough to come from any part of the image if this be any other kind of data eg categorical datum of app install this would ve be a disaster for just because your number of app install and app type column be next to each other doesn t mean they have any local share feature common with app install date and time use sure the four may have an underlie high level feature eg which app people want most that can be find but that give we no reason to believe the parameter for the first two be exactly the same as the parameter for the latter two the four could ve be in any consistent order and still be valid pixel however always appear in a consistent order and nearby pixel influence a pixel e g if all nearby pixel be red it s pretty likely the pixel be also red if there be deviation that s an interesting anomaly that could be convert into a feature and all this can be detect from compare a pixel with its neighbor with other pixel in its locality and this idea be really what a lot of early computer vision feature extraction method be base around for instance for edge detection one can use a sobel edge detection filter a kernel with fix parameter operate just like the standard one channel convolution for a non edge contain grid eg the background sky most of the pixel be the same value so the overall output of the kernel at that point be 0 for a grid with an vertical edge there be a difference between the pixel to the left and right of the edge and the kernel compute that difference to be non zero activating and reveal the edge the kernel only work only a 3×3 grid at a time detect anomaly on a local scale yet when apply across the entire image be enough to detect a certain feature on a global scale anywhere in the image so the key difference we make with deep learning be ask this question can useful kernel be learn for early layer operate on raw pixel we could reasonably expect feature detector of fairly low level feature like edge line etc there s an entire branch of deep learning research focus on make neural network model interpretable one of the most powerful tool to come out of that be feature visualization use optimization 3 the idea at core be simple optimize a image usually initialize with random noise to activate a filter as strongly as possible this do make intuitive sense if the optimize image be completely fill with edge that s strong evidence that s what the filter itself be look for and be activate by use this we can peek into the learn filter and the result be stunning one important thing to notice here be that convolve image be still image the output of a small grid of pixel from the top left of an image will still be on the top leave so you can run another convolution layer on top of another such as the two on the left to extract deep feature which we visualize yet however deep our feature detector get without any further change they ll still be operate on very small patch of the image no matter how deep your detector be you can t detect face from a 3×3 grid and this be where the idea of the receptive field come in a essential design choice of any cnn architecture be that the input size grow small and small from the start to the end of the network while the number of channel grow deep this as mention early be often do through stride or pool layer locality determine what input from the previous layer the output get to see the receptive field determine what area of the original input to the entire network the output get to see the idea of a strided convolution be that we only process slide a fix distance apart and skip the one in the middle from a different point of view we only keep output a fix distance apart and remove the rest 1 we then apply a nonlinearity to the output and per usual then stack another new convolution layer on top and this be where thing get interesting even if be we to apply a kernel of the same size 3×3 have the same local area to the output of the strided convolution the kernel would have a large effective receptive field this be because the output of the strided layer still do represent the same image it be not so much cropping as it be resize only thing be that each single pixel in the output be a representative of a large area of whose other pixel be discard from the same rough location from the original input so when the next layer s kernel operate on the output it s operate on pixel collect from a large area note if you re familiar with dilated convolution note that the above be not a dilated convolution both be method of increase the receptive field but dilated convolution be a single layer while this take place on a regular convolution follow a strided convolution with a nonlinearity inbetween this expansion of the receptive field allow the convolution layer to combine the low level feature line edge into high level feature curve texture as we see in the mixed3a layer follow by a pooling stride layer the network continue to create detector for even high level feature part pattern as we see for mixed4a the repeat reduction in image size across the network result in by the 5th block on convolution input size of just 7×7 compare to input of 224×224 at this point each single pixel represent a grid of 32×32 pixel which be huge compare to early layer where an activation mean detect an edge here an activation on the tiny 7×7 grid be one for a very high level feature such as for bird the network as a whole progress from a small number of filter 64 in case of googlenet detect low level feature to a very large number of filter 1024 in the final convolution each look for an extremely specific high level feature follow by a final pooling layer which collapse each 7×7 grid into a single pixel each channel be a feature detector with a receptive field equivalent to the entire image compare to what a standard feedforward network would have do the output here be really nothing short of awe inspire a standard feedforward network would have produce abstract feature vector from combination of every single pixel in the image require intractable amount of datum to train the cnn with the prior impose on it start by learn very low level feature detector and as across the layer as its receptive field be expand learn to combine those low level feature into progressively high level feature ; not an abstract combination of every single pixel but rather a strong visual hierarchy of concept by detect low level feature and use they to detect high level feature as it progress up its visual hierarchy it be eventually able to detect entire visual concept such as face bird tree etc and that s what make they such powerful yet efficient with image datum with the visual hierarchy cnn build it be pretty reasonable to assume that their vision system be similar to human and they re really great with real world image but they also fail in way that strongly suggest their vision system aren t entirely human like the most major problem adversarial example 4 example which have be specifically modify to fool the model adversarial example would be a non issue if the only tamper one that cause the model to fail be one that even human would notice the problem be the model be susceptible to attack by sample which have only be tamper with ever so slightly and would clearly not fool any human this open the door for model to silently fail which can be pretty dangerous for a wide range of application from self drive car to healthcare robustness against adversarial attack be currently a highly active area of research the subject of many paper and even competition and solution will certainly improve cnn architecture to become safe and more reliable cnn be the model that allow computer vision to scale from simple application to power sophisticated product and service range from face detection in your photo gallery to make well medical diagnosis they might be the key method in computer vision go forward or some other new breakthrough might just be around the corner regardless one thing be for sure they re nothing short of amazing at the heart of many present day innovative application and be most certainly worth deeply understanding hope you enjoy this article if you d like to stay connected you ll find I on twitter here if you have a question comment be welcome — I find they to be useful to my own learning process as well from a quick cheer to a stand ovation clap to show how much you enjoy this story curious programmer tinker around in python and deep learning sharing concept idea and code
Sam Drozdov,2.3K,6,https://uxdesign.cc/an-intro-to-machine-learning-for-designers-5c74ba100257?source=---------6----------------,an intro to machine learning for designer ux collective,there be an ongoing debate about whether or not designer should write code wherever you fall on this issue most people would agree that designer should know about code this help designer understand constraint and empathize with developer it also allow designer to think outside of the pixel perfect box when problem solve for the same reason designer should know about machine learning put simply machine learning be a field of study that give computer the ability to learn without be explicitly program arthur samuel 1959 even though arthur samuel coin the term over fifty year ago only recently have we see the most exciting application of machine learning — digital assistant autonomous driving and spam free email all exist thank to machine learning over the past decade new algorithm well hardware and more datum have make machine learn an order of magnitude more effective only in the past few year company like google amazon and apple have make some of their powerful machine learning tool available to developer now be the good time to learn about machine learning and apply it to the product you be build since machine learning be now more accessible than ever before designer today have the opportunity to think about how machine learning can be apply to improve their product designer should be able to talk with software developer about what be possible how to prepare and what outcome to expect below be a few example application that should serve as inspiration for these conversation machine learning can help create user centric product by personalize experience to the individual who use they this allow we to improve thing like recommendation search result notification and ad machine learning be effective at find abnormal content credit card company use this to detect fraud email provider use this to detect spam and social medium company use this to detect thing like hate speech machine learning have enable computer to begin to understand the thing we say natural language processing and the thing we see computer vision this allow siri to understand siri set a reminder google photo to create album of your dog and facebook to describe a photo to those visually impair machine learning be also helpful in understand how user be group this insight can then be use to look at analytic on a group by group basis from here different feature can be evaluate across group or be roll out to only a particular group of user machine learning allow we to make prediction about how a user might behave next know this we can help prepare for a user s next action for example if we can predict what content a user be plan on view we can preload that content so it s immediately ready when they want it depend on the application and what datum be available there be different type of machine learning algorithm to choose from I ll briefly cover each of the follow supervised learning allow we to make prediction use correctly label datum label datum be a group of example that have informative tag or output for example photo with associate hashtag or a house s feature eq number of bedroom location and its price by use supervised learning we can fit a line to the label datum that either split the datum into category or represent the trend of the datum use this line we be able to make prediction on new datum for example we can look at new photo and predict hashtag or look at a new house s feature and predict its price if the output we be try to predict be a list of tag or value we call it classification if the output we be try to predict be a number we call it regression unsupervised learning be helpful when we have unlabele datum or we be not exactly sure what output like an image s hashtag or a house s price be meaningful instead we can identify pattern among unlabeled datum for example we can identify related item on an e commerce website or recommend item to someone base on other who make similar purchase if the pattern be a group we call it a cluster if the pattern be a rule e q if this then that we call it an association reinforcement learning doesn t use an exist datum set instead we create an agent to collect its own datum through trial and error in an environment where it be reinforce with a reward for example an agent can learn to play mario by receive a positive reward for collect coin and a negative reward for walk into a goomba reinforcement learning be inspire by the way that human learn and have turn out to be an effective way to teach computer specifically reinforcement have be effective at training computer to play game like go and dota understand the problem you be try to solve and the available datum will constrain the type of machine learning you can use e q identify object in an image with supervised learning require a label datum set of image however constraint be the fruit of creativity in some case you can set out to collect datum that be not already available or consider other approach even though machine learning be a science it come with a margin of error it be important to consider how a user s experience might be impact by this margin of error for example when an autonomous car fail to recognize its surrounding people can get hurt even though machine learning have never be as accessible as it be today it still require additional resource developer and time to be integrate into a product this make it important to think about whether the result impact justify the amount of resource need to implement we have barely cover the tip of the iceberg but hopefully at this point you feel more comfortable thinking about how machine learning can be apply to your product if you be interested in learn more about machine learning here be some helpful resource thank for read chat with I on twitter @samueldrozdov from a quick cheer to a stand ovation clap to show how much you enjoy this story digital product designer samueldrozdov com curate story on user experience usability and product design by @fabriciot and @caioab
Conor Dewey,252,10,https://towardsdatascience.com/the-big-list-of-ds-ml-interview-resources-2db4f651bd63?source=---------7----------------,the big list of ds ml interview resource towards data science,data science interview certainly aren t easy I know this first hand I ve participate in over 50 individual interview and phone screen while apply for competitive internship over the last calendar year through this exciting and somewhat at time very painful process I ve accumulate a plethora of useful resource that help I prepare for and eventually pass data science interview long story short I ve decide to sort through all my bookmark and note in order to deliver a comprehensive list of data science resource with this list by your side you should have more than enough effective tool at your disposal next time you re preppe for a big interview it s worth note that many of these resource be naturally go to geared towards entry level and intern datum science position as that s where my expertise lie keep that in mind and enjoy here s some of the more general resource cover data science as a whole specifically I highly recommend check out the first two link regard 120 datum science interview question while the ebook itself be a couple buck out of pocket the answer themselves be free on quora these be some of my favorite full coverage question to practice with right before an interview even data scientist can not escape the dreaded algorithmic code interview in my experience this isn t the case 100 % of the time but chance be you ll be ask to work through something similar to an easy or medium question on leetcode or hackerrank as far as language go most company will let you use whatever language you want personally I do almost all of my algorithmic coding in java even though the position be target at python and r programmer if I have to recommend one thing it s to break out your wallet and invest in crack the code interview it absolutely live up to the hype I plan to continue use it for year to come once the interviewer know that you can think through problem and code effectively chance be that you ll move onto some more data science specific application depend on the interviewer and the position you will likely be able to choose between python and r as your tool of choice since I m partial to python my resource below will primarily focus on effectively use panda and numpy for datum analysis a data science interview typically isn t complete without check your knowledge of sql this can be do over the phone or through a live code question more likely the latter I ve find that the difficulty level of these question can vary a good bit range from be painfully easy to require complex join and obscure function our good friend statistic be still crucial for datum scientist and it s reflect as such in interview I have many interview begin by see if I can explain a common statistic or probability concept in simple and concise term as position get more experienced I suspect this happen less and less as traditional statistical question begin to take the more practical form of a b testing scenario cover later in the post you ll notice that I ve compile a few more resource here than in other section this isn t a mistake machine learning be a complex field that be a virtual guarantee in data science interview today the way that you ll be test on this be no guarantee however it may come up as a conceptual question regard cross validation or bias variance tradeoff or it may take the form of a take home assignment with a dataset attach I ve see both several time so you ve get to be prepare for anything specifically check out the machine learn flashcard below they re only a couple buck and be my by far my favorite way to quiz myself on any conceptual ml stuff this win t be cover in every single data science interview but it s certainly not uncommon most interview will have atleast one section solely dedicate to product thinking which often lend itself to a b testing of some sort make sure your familiar with the concept and statistical background necessary in order to be prepare when it come up if you have time to spare I take the free online course by udacity and overall I be pretty impressed lastly I want to call out all of the post relate to data science job and interview that I read over and over again to understand not only how to prepare but what to expect as well if you only check out one section here this be the one to focus on this be the layer that sit on top of all the technical skill and application don t overlook it I hope you find these resource useful during your next interview or job search I know I do truthfully I m just glad that I save these link somewhere lastly this post be part of an ongoing initiative to open source my experience apply and interview at data science position so if you enjoy this content then be sure to follow I for more stuff like this if you re interested in receive my weekly rundown of interesting article and resource focus on data science machine learning and artificial intelligence then subscribe to self drive data science use the form below if you enjoy this post feel free to hit the clap button and if you re interested in post to come make sure to follow I on medium at the link below — I ll be write and shipping every day this month as part of a 30 day challenge this article be originally publish on conordewey com from a quick cheer to a stand ovation clap to show how much you enjoy this story data scientist & writer | www conordewey com sharing concept idea and code
Abhishek Parbhakar,937,6,https://towardsdatascience.com/must-know-information-theory-concepts-in-deep-learning-ai-e54a5da9769d?source=---------8----------------,must know information theory concept in deep learning ai,information theory be an important field that have make significant contribution to deep learning and ai and yet be unknown to many information theory can be see as a sophisticated amalgamation of basic building block of deep learning calculus probability and statistic some example of concept in ai that come from information theory or related field in the early 20th century scientist and engineer be struggle with the question how to quantify the information be there a analytical way or a mathematical measure that can tell we about the information content for example consider below two sentence it be not difficult to tell that the second sentence give we more information since it also tell that bruno be big and brown in addition to be a dog how can we quantify the difference between two sentence can we have a mathematical measure that tell we how much more information second sentence have as compare to the first scientist be struggle with these question semantic domain and form of datum only add to the complexity of the problem then mathematician and engineer claude shannon come up with the idea of entropy that change our world forever and mark the beginning of digital information age shannon propose that the semantic aspect of datum be irrelevant and nature and meaning of data doesn t matter when it come to information content instead he quantify information in term of probability distribution and uncertainty shannon also introduce the term bit that he humbly credit to his colleague john tukey this revolutionary idea not only lay the foundation of information theory but also open new avenue for progress in field like artificial intelligence below we discuss four popular widely use and must know information theoretic concept in deep learning and datum science also call information entropy or shannon entropy entropy give a measure of uncertainty in an experiment let s consider two experiment if we compare the two experiment in exp 2 it be easy to predict the outcome as compare to exp 1 so we can say that exp 1 be inherently more uncertain unpredictable than exp 2 this uncertainty in the experiment be measure use entropy therefore if there be more inherent uncertainty in the experiment then it have high entropy or less the experiment be predictable more be the entropy the probability distribution of experiment be use to calculate the entropy a deterministic experiment which be completely predictable say toss a coin with p h = 1 have entropy zero an experiment which be completely random say roll fair dice be least predictable have maximum uncertainty and have the high entropy among such experiment another way to look at entropy be the average information gain when we observe outcome of an random experiment the information gain for a outcome of an experiment be define as a function of probability of occurrence of that outcome more the rarer be the outcome more be the information gain from observe it for example in an deterministic experiment we always know the outcome so no new information gain be here from observe the outcome and hence entropy be zero for a discrete random variable x with possible outcome state x_1 x_n the entropy in unit of bit be define as where p x_i be the probability of i^th outcome of x cross entropy be use to compare two probability distribution it tell we how similar two distribution be cross entropy between two probability distribution p and q define over same set of outcome be give by mutual information be a measure of mutual dependency between two probability distribution or random variable it tell we how much information about one variable be carry by the another variable mutual information capture dependency between random variable and be more generalized than vanilla correlation coefficient which capture only the linear relationship mutual information of two discrete random variable x and y be define as where p x y be the joint probability distribution of x and y and p x and p y be the marginal probability distribution of x and y respectively also call relative entropy kl divergence be another measure to find similarity between two probability distribution it measure how much one distribution diverge from the other suppose we have some datum and true distribution underlie it be p but we don t know this p so we choose a new distribution q to approximate this datum since q be just an approximation it win t be able to approximate the datum as good as p and some information loss will occur this information loss be give by kl divergence kl divergence between p and q tell we how much information we lose when we try to approximate datum give by p with q kl divergence of a probability distribution q from another probability distribution p be define as kl divergence be commonly use in unsupervised machine learning technique variational autoencoder information theory be originally formulate by mathematician and electrical engineer claude shannon in his seminal paper a mathematical theory of communication in 1948 note term experiment random variable & ai machine learn deep learning data science have be use loosely above but have technically different meaning in case you like the article do follow I abhishek parbhakar for more article relate to ai philosophy and economic from a quick cheer to a stand ovation clap to show how much you enjoy this story finding equilibria among ai philosophy and economic sharing concept idea and code
Aman Dalmia,2.3K,17,https://blog.usejournal.com/what-i-learned-from-interviewing-at-multiple-ai-companies-and-start-ups-a9620415e4cc?source=---------9----------------,what I learn from interview at multiple ai company and start up,over the past 8 month I ve be interview at various company like google s deepmind wadhwani institute of ai microsoft ola fractal analytic and a few other primarily for the role — data scientist software engineer & research engineer in the process not only do I get an opportunity to interact with many great mind but also have a peek at myself along with a sense of what people really look for when interview someone I believe that if I d have this knowledge before I could have avoid many mistake and have prepare in a much well manner which be what the motivation behind this post be to be able to help someone bag their dream place of work this post arise from a discussion with one of my junior on the lack of really fulfil job opportunity offer through campus placement for people work in ai also when I be prepare I notice people use a lot of resource but as per my experience over the past month I realise that one can do away with a few minimal one for most role in ai all of which I m go to mention at the end of the post I begin with how to get notice a k a the interview then I provide a list of company and start up to apply which be follow by how to ace that interview base on whatever experience I ve have I add a section on what we should strive to work for I conclude with minimal resource you need for preparation note for people who be sit for campus placement there be two thing I d like to add firstly most of what I m go to say except for the last one maybe be not go to be relevant to you for placement but and this be my second point as I mention before opportunity on campus be mostly in software engineering role have no intersection with ai so this post be specifically mean for people who want to work on solve interesting problem use ai also I want to add that I haven t clear all of these interview but I guess that s the essence of failure — it s the great teacher the thing that I mention here may not all be useful but these be thing that I do and there s no way for I to know what might have end up make my case strong to be honest this step be the most important one what make off campus placement so tough and exhausting be get the recruiter to actually go through your profile among the plethora of application that they get have a contact inside the organisation place a referral for you would make it quite easy but in general this part can be sub divide into three key step a do the regulatory preparation and do that well so with regulatory preparation I mean — a linkedin profile a github profile a portfolio website and a well polished cv firstly your cv should be really neat and concise follow this guide by udacity for clean up your cv — resume revamp it have everything that I intend to say and I ve be use it as a reference guide myself as for the cv template some of the in build format on overleaf be quite nice I personally use deedy resume here s a preview as it can be see a lot of content can be fit into one page however if you really do need more than that then the format link above would not work directly instead you can find a modified multi page format of the same here the next most important thing to mention be your github profile a lot of people underestimate the potential of this just because unlike linkedin it doesn t have a who view your profile option people do go through your github because that s the only way they have to validate what you have mention in your cv give that there s a lot of noise today with people associate all kind of buzzword with their profile especially for data science open source have a big role to play too with majority of the tool implementation of various algorithms list of learn resource all be open source I discuss the benefit of get involve in open source and how one can start from scratch in an early post here the bare minimum for now should be • create a github account if you don t already have one • create a repository for each of the project that you have do • add documentation with clear instruction on how to run the code• add documentation for each file mention the role of each function the meaning of each parameter proper format e g pep8 for python along with a script to automate the previous step optional move on the third step be what most people lack which be have a portfolio website demonstrate their experience and personal project make a portfolio indicate that you be really serious about get into the field and add a lot of point to the authenticity factor also you generally have space constraint on your cv and tend to miss out on a lot of detail you can use your portfolio to really delve deep into the detail if you want to and it s highly recommend to include some sort of visualisation or demonstration of the project idea it s really easy to create one too as there be a lot of free platform with drag and drop feature make the process really painless I personally use weebly which be a widely use tool it s well to have a reference to begin with there be a lot of awesome one out there but I refer to deshraj yadav s personal website to begin with make mine finally a lot of recruiter and start up have nowadays start use linkedin as their go to platform for hire a lot of good job get post there apart from recruiter the people work at influential position be quite active there as well so if you can grab their attention you have a good chance of get in too apart from that maintain a clean profile be necessary for people to have the will to connect with you an important part of linkedin be their search tool and for you to show up you must have the relevant keyword intersperse over your profile it take I a lot of iteration and re evaluation to finally have a decent one also you should definitely ask people with or under whom you ve work with to endorse you for your skill and add a recommendation talk about their experience of work with you all of this increase your chance of actually get notice I ll again point towards udacity s guide for linkedin and github profile all this might seem like a lot but remember that you don t need to do it in a single day or even a week or a month it s a process it never end set up everything at first would definitely take some effort but once it s there and you keep update it regularly as event around you keep happen you ll not only find it to be quite easy but also you ll be able to talk about yourself anywhere anytime without have to explicitly prepare for it because you become so aware about yourself b stay authentic I ve see a lot of people do this mistake of present themselves as per different job profile accord to I it s always well to first decide what actually interest you what would you be happy do and then search for relevant opportunity ; not the other way round the fact that the demand for ai talent surpass the supply for the same give you this opportunity spending time on your regulatory preparation mention above would give you an all around perspective on yourself and help make this decision easy also you win t need to prepare answer to various kind of question that you get ask during an interview most of they would come out naturally as you d be talk about something you really care about c network once you re do with a figure out b networking be what will actually help you get there if you don t talk to people you miss out on hear about many opportunity that you might have a good shot at it s important to keep connect with new people each day if not physically then on linkedin so that upon compound it after many day you have a large and strong network networking be not message people to place a referral for you when I be start off I do this mistake way too often until I stumble upon this excellent article by mark meloon where he talk about the importance of build a real connection with people by offer our help first another important step in networking be to get your content out for example if you re good at something blog about it and share that blog on facebook and linkedin not only do this help other it help you as well once you have a good enough network your visibility increase multi fold you never know how one person from your network like or comment on your post may help you reach out to a much broad audience include people who might be look for someone of your expertise I m present this list in alphabetical order to avoid the misinterpretation of any specific preference however I do place a * on the one that I d personally recommend this recommendation be base on either of the following mission statement people personal interaction or scope of learn more than 1 * be purely base on the 2nd and 3rd factor your interview begin the moment you have enter the room and a lot of thing can happen between that moment and the time when you re ask to introduce yourself — your body language and the fact that you re smile while greet they play a big role especially when you re interview for a start up as culture fit be something that they extremely care about you need to understand that as much as the interviewer be a stranger to you you re a stranger to he she too so they re probably just as nervous as you be it s important to view the interview as more of a conversation between yourself and the interviewer both of you be look for a mutual fit — you be look for an awesome place to work at and the interviewer be look for an awesome person like you to work with so make sure that you re feel good about yourself and that you take the charge of make the initial moment of your conversation pleasant for they and the easy way I know how to make that happen be to smile there be mostly two type of interview — one where the interviewer have come with come prepare set of question and be go to just ask you just that irrespective of your profile and the second where the interview be base on your cv I ll start with the second one this kind of interview generally begin with a can you tell I a bit about yourself at this point 2 thing be a big no — talk about your gpa in college and talk about your project in detail an ideal statement should be about a minute or two long should give a good idea on what have you be do till now and it s not restrict to academic you can talk about your hobby like read book play sport meditation etc — basically anything that contribute to define you the interviewer will then take something that you talk about here as a cue for his next question and then the technical part of the interview begin the motive of this kind of interview be to really check whether whatever you have write on your cv be true or not there would be a lot of question on what could be do differently or if x be use instead of y what would have happen at this point it s important to know the kind of trade off that be usually make during implementation for e g if the interviewer say that use a more complex model would have give well result then you might say that you actually have less datum to work with and that would have lead to overfitte in one of the interview I be give a case study to work on and it involve designing algorithm for a real world use case I ve notice that once I ve be give the green flag to talk about a project the interviewer really like it when I talk about it in the follow flow problem > 1 or 2 previous approach > our approach > result > intuition the other kind of interview be really just to test your basic knowledge don t expect those question to be too hard but they would definitely scratch every bit of the basic that you should be have mainly base around linear algebra probability statistic optimisation machine learning and or deep learn the resource mention in the minimal resource you need for preparation section should suffice but make sure that you don t miss out one bit among they the catch here be the amount of time you take to answer those question since these cover the basic they expect that you should be answer they almost instantly so do your preparation accordingly throughout the process it s important to be confident and honest about what you know and what you don t know if there s a question that you re certain you have no idea about say it upfront rather than make aah um sound if some concept be really important but you be struggle with answer it the interviewer would generally depend on how you do in the initial part be happy to give you a hint or guide you towards the right solution it s a big plus if you manage to pick their hint and arrive at the correct solution try to not get nervous and the good way to avoid that is by again smile now we come to the conclusion of the interview where the interviewer would ask you if you have any question for they it s really easy to think that your interview be do and just say that you have nothing to ask I know many people who get reject just because of fail at this last question as I mention before it s not only you who be be interview you be also look for a mutual fit with the company itself so it s quite obvious that if you really want to join a place you must have many question regard the work culture there or what kind of role be they see you in it can be as simple as be curious about the person interview you there s always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you re truly interested in be a part of their team a final question that I ve start ask all my interviewer be for a feedback on what they might want I to improve on this have help I tremendously and I still remember every feedback that I ve get which I ve incorporate into my daily life that s it base on my experience if you re just honest about yourself be competent truly care about the company you re interview for and have the right mindset you should have tick all the right box and should be get a congratulatory mail soon 😄 we live in an era full of opportunity and that apply to anything that you love you just need to strive to become the good at it and you will find a way to monetise it as gary vaynerchuk just follow he already say this be a great time to be work in ai and if you re truly passionate about it you have so much that you can do with ai you can empower so many people that have always be under represent we keep nagging about the problem surround we but there s be never such a time where common people like we can actually do something about those problem rather than just complain jeffrey hammerbacher founder cloudera have famously say we can do so much with ai than we can ever imagine there be many extremely challenging problem out there which require incredibly smart people like you to put your head down on and solve you can make many life well time to let go of what be cool or what would look good think and choose wisely any data science interview comprise of question mostly of a subset of the follow four category computer science math statistic and machine learning if you re not familiar with the math behind deep learning then you should consider go over my last post for resource to understand they however if you be comfortable I ve find that the chapter 2 3 and 4 of the deep learning book be enough to prepare revise for theoretical question during such interview I ve be prepare summary for a few chapter which you can refer to where I ve try to even explain a few concept that I find challenge to understand at first in case you be not willing to go through the entire chapter and if you ve already do a course on probability you should be comfortable answer a few numerical as well for stat cover these topic should be enough now the range of question here can vary depend on the type of position you be apply for if it s a more traditional machine learning base interview where they want to check your basic knowledge in ml you can complete any one of the follow course machine learning by andrew ng — cs 229 machine learning course by caltech professor yas abu mostafa important topic be supervise learn classification regression svm decision tree random forest logistic regression multi layer perceptron parameter estimation bayes decision rule unsupervise learn k mean cluster gaussian mixture model dimensionality reduction pca now if you re apply for a more advanced position there s a high chance that you might be question on deep learning in that case you should be very comfortable with convolutional neural network cnn and or depend upon what you ve work on recurrent neural network rnn and their variant and by be comfortable you must know what be the fundamental idea behind deep learning how cnns rnns actually work what kind of architecture have be propose and what have be the motivation behind those architectural change now there s no shortcut for this either you understand they or you put enough time to understand they for cnn the recommend resource be stanford s cs 231n and cs 224n for rnn I find this neural network class by hugo larochelle to be really enlighten too refer this for a quick refresher too udacity come to the aid here too by now you should have figure out that udacity be a really important place for an ml practitioner there be not a lot of place work on reinforcement learning rl in india and I too be not experience in rl as of now so that s one thing to add to this post sometime in the future get place off campus be a long journey of self realisation I realise that this have be another long post and I m again extremely grateful to you for value my thought I hope that this post find a way of be useful to you and that it help you in some way to prepare for your next datum science interview well if it do I request you to really think about what I talk about in what we should strive to work for I m very thankful to my friend from iit guwahati for their helpful feedback especially ameya godbole kothapalli vignesh and prabal jain a majority of what I mention here like view an interview as a conversation and seek feedback from our interviewer arise from multiple discussion with prabal who have be advise I constantly on how I can improve my interview skill this story be publish in noteworthy where thousand come every day to learn about the people & idea shape the product we love follow our publication to see more product & design story feature by the journal team from a quick cheer to a stand ovation clap to show how much you enjoy this story ai fanatic • math lover • dreamer the official journal blog
Sophia Arakelyan,7,4,https://buzzrobot.com/from-ballerina-to-ai-researcher-part-i-46fce67f809b?source=---------1----------------,from ballerina to ai researcher part I buzzrobot,last year I publish the article from ballerina to ai writer where I describe how I embrace the technical part of ai without a technical background but have love and passion for ai I educate myself and be able to build a neural net classifier and do project in deep rl recently I ve become a participant in the openai scholarship program openai be a non profit that gather top ai researcher to ensure the safety of ai to benefit humanity every week for the next three month I ll publish blog post share my story of transformation from a person dedicate to 15 year of professional dancing and then write about tech and ai to actually conduct ai research find your true calling — the key component of happiness my primary goal with the series of blog post from ballerina to ai researcher be to show that it s never too late to embrace a new field start over again and find your true calling find work you love be one of the most important component of happiness — something that you do every day and invest your time in to grow ; that make you feel fulfil give you energy ; something that be a refuge for your soul great thing never come easy we have to be able to fight to make great thing happen but you can t fight for something you don t believe in especially if you don t feel like it s really important for you and humanity find that thing be a real challenge I feel lucky that I find my true passion — ai to I the technology itself and the ai community — researcher scientist people who dedicate their life to build the most powerful technology of all time with the mission to benefit humanity and make it safe for we — be a great source of energy the structure of the blog post series today I m give an overall intro of what I m go to cover in my from ballerina to ai researcher series I ll dedicate the sequence of blog post during the openai scholar program to several aspect of ai technology I ll cover those area that concern I a lot like ai and automation bias in ml dual use of ai etc also the structure of my post will include some insight on what I m work on right now the final technical project will be available by the end of august and will be open source I feel very lucky to have alec radford an experienced researcher as my mentor who guide I in the nlp and nlu research area first week of my scholarship I ve dedicate my first week within the program to learn about the transformer architecture that perform much well on sequential datum compare to rnns lstms the novelty of the architecture be its multi head self attention mechanism accord to the original paper experiment with the transformer on two machine translation task show the model to be superior in quality while be more parallelizable and require significantly less time to train more concretely when rnns or cnn take a sequence as an input it go through sentence word by word which be a huge obstacle toward parallelization of the process take more time to train model moreover if sequence be too long the model tend to forget the content of distant position in sequence or mix it with the follow position content — this be the fundamental problem in deal with sequential datum the transformer architecture reduce this problem thank to the multi head self attention mechanism I digge into rnn lstm model to catch up with the background information to that end I ve find andrew ng s course on deep learning along with the paper extremely useful to develop insight regard the transformer I go through the follow resource the video by łukasz kaiser from google brain one of the model s creator ; a blog post with very well elaborate content re the model run the code tensor2tensor and the code use the pytorch framework from this paper to feel the difference between the tf and pytorch framework overall the goal within the program be to develop deep comprehension of the nlu research area challenge current state of the art ; and to formulate and test hypothesis that tackle the most important problem of the field I ll share more on what I m work on in my future article meanwhile if you have question feedback please leave a comment if you want to learn more about I here be my facebook and twitter account I d appreciate your feedback on my post such as what topic be most interesting to you that I should consider further coverage on from a quick cheer to a stand ovation clap to show how much you enjoy this story former ballerina turn ai writer fan of sci fi astrophysic consciousness be the key founder of buzzrobot com the publication aim to cover practical aspect of ai technology use case along with interview with notable people in the ai field
Dr. GP Pulipaka,2,6,https://medium.com/@gp_pulipaka/3-ways-to-apply-latent-semantic-analysis-on-large-corpus-text-on-macos-terminal-jupyterlab-colab-7b4dc3e1622?source=---------9----------------,3 way to apply latent semantic analysis on large corpus text on macos terminal jupyterlab and,latent semantic analysis work on large scale dataset to generate representation to discover the insight through natural language processing there be different approach to perform the latent semantic analysis at multiple level such as document level phrase level and sentence level primarily semantic analysis can be summarize into lexical semantic and the study of combine individual word into paragraph or sentence the lexical semantic classifie and decompose the lexical item apply lexical semantic structure have different context to identify the difference and similarity between the word a generic term in a paragraph or a sentence be hypernym and hyponymy provide the meaning of the relationship between instance of the hyponyms homonyms contain similar syntax or similar spelling with similar structuring with different meaning homonym be not relate to each other book be an example for homonym it can mean for someone to read something or an act of make a reservation with similar spelling form and syntax however the definition be different polysemy be another phenomenon of the word where a single word could be associate with multiple relate sense and distinct meaning the word polysemy be a greek word which mean many sign python provide nltk library to perform tokenization of the word by chop the word in large chunk into phrase or meaningful string processing word through tokenization produce token word lemmatization convert word from the current inflect form into the base form latent semantic analysis apply latent semantic analysis on large dataset of text and document represent the contextual meaning through mathematical and statistical computation method on large corpus of text many time latent semantic analysis overtake human score and subject matter test conduct by human the accuracy of latent semantic analysis be high as it read through machine readable document and text at a web scale latent semantic analysis be a technique that apply singular value decomposition and principal component analysis pca the document can be represent with z x y matrix a the row of the matrix represent the document in the collection the matrix a can represent numerous hundred thousand of row and column on a typical large corpus text document apply singular value decomposition develop a set of operation dub matrix decomposition natural language processing in python with nltk library apply a low rank approximation to the term document matrix later the low rank approximation aid in indexing and retrieve the document know as latent semantic indexing by cluster the number of word in the document brief overview of linear algebra the a with z x y matrix contain the real value entry with non negative value for the term document matrix determine the rank of the matrix come with the number of linearly independent column or row in the the matrix the rank of a ≤ { z y } a square c x c represent as diagonal matrix where off diagonal entry be zero examine the matrix if all the c diagonal matrix be one the identity matrix of the dimension c represent by ic for the square z x z matrix a with a vector k which contain not all zero for λ the matrix decomposition apply on the square matrix factor into the product of matrix from eigenvector this allow to reduce the dimensionality of the word from multi dimension to two dimension to view on the plot the dimensionality reduction technique with principal component analysis and singular value decomposition hold critical relevance in natural language process the zipfian nature of the frequency of the word in a document make it difficult to determine the similarity of the word in a static stage hence eigen decomposition be a by product of singular value decomposition as the input of the document be highly asymmetrical the latent semantic analysis be a particular technique in semantic space to parse through the document and identify the word with polysemy with nlkt library the resource such as punkt and wordnet have to be download from nltk deep learning at scale with google colab notebook train machine learning or deep learning model on cpu could take hour and could be pretty expensive in term of the programming language efficiency with time and energy of the computer resource google build colab notebooks environment for research and development purpose it run entirely on the cloud without require any additional hardware or software setup for each machine it s entirely equivalent of a jupyter notebook that aid the datum scientist to share the colab notebook by store on google drive just like any other google sheet or document in a collaborative environment there be no additional cost associate with enable gpu at runtime for acceleration on the runtime there be some challenge of upload the datum into colab unlike jupyter notebook that can access the datum directly from the local directory of the machine in colab there be multiple option to upload the file from the local file system or a drive can be mount to load the datum through drive fuse wrapper once this step be complete it show the follow log without error the next step would be generate the authentication token to authenticate the google credential for the drive and colab if it show successful retrieval of access token then colab be all set at this stage the drive be not mount yet it will show false when access the content of the text file once the drive be mount colab have access to the dataset from google drive once the file be accessible the python can be execute similar to execute in jupyter environment colab notebook also display the result similar to what we see on jupyter notebook pycharm ide the program can be run compile on pycharm ide environment and run on pycharm or can be execute from osx terminal result from osx terminal jupyter notebook on standalone machine jupyter notebook give a similar output run the latent semantic analysis on the local machine reference gorrell g 2006 generalize hebbian algorithm for incremental singular value decomposition in natural language processing retrieve from https www aclweb org anthology e06 1013 hardeniya n 2016 natural language processing python and nltk birmingham england packt publishing landauer t k foltz p w laham d & university of colorado at boulder 1998 an introduction to latent semantic analysis retrieve from http lsa colorado edu papers dp1 lsaintro pdf stackoverflow 2018 mount google drive on google colab retrieve from https stackoverflow com question 50168315 mount google drive on google colab stanford university 2009 matrix decomposition and latent semantic indexing retrieve from https nlp stanford edu ir book html htmledition matrix decomposition and latent semantic indexing 1 html from a quick cheer to a stand ovation clap to show how much you enjoy this story ganapathi pulipaka | founder and ceo @deepsingularity | bestselle author | big datum | iot | startup | sap | machinelearning | deeplearne | datascience
Scott Santens,7.3K,14,https://medium.com/basic-income/deep-learning-is-going-to-teach-us-all-the-lesson-of-our-lives-jobs-are-for-machines-7c6442e37a49?source=tag_archive---------0----------------,deep learning be go to teach we all the lesson of our life job be for machine,an alternate version of this article be originally publish in the boston globe on december 2nd 1942 a team of scientist lead by enrico fermi come back from lunch and watch as humanity create the first self sustain nuclear reaction inside a pile of brick and wood underneath a football field at the university of chicago know to history as chicago pile 1 it be celebrate in silence with a single bottle of chianti for those who be there understand exactly what it mean for humankind without any need for word now something new have occur that again quietly change the world forever like a whisper word in a foreign language it be quiet in that you may have hear it but its full meaning may not have be comprehend however it s vital we understand this new language and what it s increasingly tell we for the ramification be set to alter everything we take for grant about the way our globalize economy function and the way in which we as human exist within it the language be a new class of machine learning know as deep learning and the whisper word be a computer s use of it to seemingly out of nowhere defeat three time european go champion fan hui not once but five time in a row without defeat many who read this news consider that as impressive but in no way comparable to a match against lee se dol instead who many consider to be one of the world s good living go player if not the well imagining such a grand duel of man versus machine china s top go player predict that lee would not lose a single game and lee himself confidently expect to possibly lose one at the most what actually end up happen when they face off lee go on to lose all but one of their match s five game an ai name alphago be now a well go player than any human and have be grant the divine rank of 9 dan in other word its level of play border on godlike go have officially fall to machine just as jeopardy do before it to watson and chess before that to deep blue so what be go very simply think of go as super ultra mega chess this may still sound like a small accomplishment another feather in the cap of machine as they continue to prove themselves superior in the fun game we play but it be no small accomplishment and what s happen be no game alphago s historic victory be a clear signal that we ve go from linear to parabolic advance in technology be now so visibly exponential in nature that we can expect to see a lot more milestone be cross long before we would otherwise expect these exponential advance most notably in form of artificial intelligence limit to specific task we be entirely unprepared for as long as we continue to insist upon employment as our primary source of income this may all sound like exaggeration so let s take a few decade step back and look at what computer technology have be actively do to human employment so far let the above chart sink in do not be fool into think this conversation about the automation of labor be set in the future it s already here computer technology be already eat job and have be since 1990 all work can be divide into four type routine and nonroutine cognitive and manual routine work be the same stuff day in and day out while nonroutine work vary within these two variety be the work that require mostly our brain cognitive and the work that require mostly our body manual where once all four type see growth the stuff that be routine stagnate back in 1990 this happen because routine labor be easy for technology to shoulder rule can be write for work that doesn t change and that work can be well handle by machine distressingly it s exactly routine work that once form the basis of the american middle class it s routine manual work that henry ford transform by pay people middle class wage to perform and it s routine cognitive work that once fill we office space such job be now increasingly unavailable leave only two kind of job with rosy outlook job that require so little thought we pay people little to do they and job that require so much thought we pay people well to do they if we can now imagine our economy as a plane with four engine where it can still fly on only two of they as long as they both keep roar we can avoid concern ourselves with crashing but what happen when our two remain engine also fail that s what the advance field of robotic and ai represent to those final two engine because for the first time we be successfully teach machine to learn I m a writer at heart but my educational background happen to be in psychology and physic I m fascinate by both of they so my undergraduate focus end up be in the physics of the human brain otherwise know as cognitive neuroscience I think once you start to look into how the human brain work how our mass of interconnect neuron somehow result in what we describe as the mind everything change at least it do for I as a quick primer in the way our brain function they re a giant network of interconnect cell some of these connection be short and some be long some cell be only connect to one other and some be connect to many electrical signal then pass through these connection at various rate and subsequent neural firing happen in turn it s all kind of like fall domino but far fast large and more complex the result amazingly be we and what we ve be learn about how we work we ve now begin apply to the way machine work one of these application be the creation of deep neural network kind of like pare down virtual brain they provide an avenue to machine learning that s make incredible leap that be previously think to be much far down the road if even possible at all how it s not just the obvious grow capability of our computer and our expand knowledge in the neuroscience but the vastly grow expanse of our collective datum aka big datum big datum isn t just some buzzword it s information and when it come to information we re create more and more of it every day in fact we re create so much that a 2013 report by sintef estimate that 90 % of all information in the world have be create in the prior two year this incredible rate of data creation be even double every 1 5 year thank to the internet where in 2015 every minute we be like 4 2 million thing on facebook upload 300 hour of video to youtube and send 350 000 tweet everything we do be generate datum like never before and lot of datum be exactly what machine need in order to learn to learn why imagine program a computer to recognize a chair you d need to enter a ton of instruction and the result would still be a program detect chair that aren t and not detect chair that be so how do we learn to detect chair our parent point at a chair and say chair then we think we have that whole chair thing all figure out so we point at a table and say chair which be when our parent tell we that be table this be call reinforcement learn the label chair get connect to every chair we see such that certain neural pathway be weight and other aren t for chair to fire in our brain what we perceive have to be close enough to our previous chair encounter essentially our life be big datum filter through our brain the power of deep learning be that it s a way of use massive amount of datum to get machine to operate more like we do without give they explicit instruction instead of describe chairness to a computer we instead just plug it into the internet and feed it million of picture of chair it can then have a general idea of chairness next we test it with even more image where it s wrong we correct it which far improve its chairness detection repetition of this process result in a computer that know what a chair be when it see it for the most part as well as we can the important difference though be that unlike we it can then sort through million of image within a matter of second this combination of deep learning and big datum have result in astounding accomplishment just in the past year aside from the incredible accomplishment of alphago google s deepmind ai learn how to read and comprehend what it read through hundred of thousand of annotate news article deepmind also teach itself to play dozen of atari 2600 video game well than human just by look at the screen and its score and playing game repeatedly an ai name giraffe teach itself how to play chess in a similar manner use a dataset of 175 million chess position attain international master level status in just 72 hour by repeatedly play itself in 2015 an ai even pass a visual turing test by learn to learn in a way that enable it to be show an unknown character in a fictional alphabet then instantly reproduce that letter in a way that be entirely indistinguishable from a human give the same task these be all major milestone in ai however despite all these milestone when ask to estimate when a computer would defeat a prominent go player the answer even just month prior to the announcement by google of alphago s victory be by expert essentially maybe in another ten year a decade be consider a fair guess because go be a game so complex I ll just let ken jennings of jeopardy fame another former champion human defeat by ai describe it such confound complexity make impossible any brute force approach to scan every possible move to determine the next good move but deep neural network get around that barrier in the same way our own mind do by learn to estimate what feel like the good move we do this through observation and practice and so do alphago by analyze million of professional game and play itself million of time so the answer to when the game of go would fall to machine wasn t even close to ten year the correct answer end up be any time now any time now that s the new go to response in the 21st century for any question involve something new machine can do well than human and we need to try to wrap our head around it we need to recognize what it mean for exponential technological change to be enter the labor market space for nonroutine job for the first time ever machine that can learn mean nothing human do as a job be uniquely safe anymore from hamburger to healthcare machine can be create to successfully perform such task with no need or less need for human and at low cost than human amelia be just one ai out there currently be beta test in company right now create by ipsoft over the past 16 year she s learn how to perform the work of call center employee she can learn in second what take we month and she can do it in 20 language because she s able to learn she s able to do more over time in one company put she through the pace she successfully handle one of every ten call in the first week and by the end of the second month she could resolve six of ten call because of this it s be estimate that she can put 250 million people out of a job worldwide viv be an ai come soon from the creator of siri who ll be our own personal assistant she ll perform task online for we and even function as a facebook news feed on steroid by suggest we consume the medium she ll know we ll like well in do all of this for we we ll see far few ad and that mean the entire advertising industry — that industry the entire internet be build upon — stand to be hugely disrupt a world with amelia and viv — and the countless other ai counterpart come online soon — in combination with robot like boston dynamic next generation atlas portend be a world where machine can do all four type of job and that mean serious societal reconsideration if a machine can do a job instead of a human should any human be force at the threat of destitution to perform that job should income itself remain couple to employment such that have a job be the only way to obtain income when job for many be entirely unobtainable if machine be perform an increase percentage of our job for we and not get pay to do they where do that money go instead and what do it no long buy be it even possible that many of the job we re create don t need to exist at all and only do because of the income they provide these be question we need to start ask and fast fortunately people be begin to ask these question and there s an answer that s build up momentum the idea be to put machine to work for we but empower ourselves to seek out the form of remain work we as human find most valuable by simply provide everyone a monthly paycheck independent of work this paycheck would be grant to all citizen unconditionally and its name be universal basic income by adopt ubi aside from immunize against the negative effect of automation we d also be decrease the risk inherent in entrepreneurship and the size of bureaucracy necessary to boost income it s for these reason it have cros partisan support and be even now in the beginning stage of possible implementation in country like switzerland finland the netherlands and canada the future be a place of accelerate change it seem unwise to continue look at the future as if it be the past where just because new job have historically appear they always will the wef start 2016 off by estimate the creation by 2020 of 2 million new job alongside the elimination of 7 million that s a net loss not a net gain of 5 million job in a frequently cite paper an oxford study estimate the automation of about half of all exist job by 2033 meanwhile self drive vehicle again thank to machine learning have the capability of drastically impact all economy — especially the us economy as I write last year about automate truck driving — by eliminate million of job within a short span of time and now even the white house in a stunning report to congress have put the probability at 83 percent that a worker make less than $ 20 an hour in 2010 will eventually lose their job to a machine even worker make as much as $ 40 an hour face odd of 31 percent to ignore odd like these be tantamount to our now laughable duck and cover strategy for avoid nuclear blast during the cold war all of this be why it s those most knowledgeable in the ai field who be now actively sound the alarm for basic income during a panel discussion at the end of 2015 at singularity university prominent data scientist jeremy howard ask do you want half of people to starve because they literally can t add economic value or not before go on to suggest if the answer be not then the smart way to distribute the wealth be by implement a universal basic income ai pioneer chris eliasmith director of the centre for theoretical neuroscience warn about the immediate impact of ai on society in an interview with futurism ai be already have a big impact on our economy my suspicion be that more country will have to follow finland s lead in explore basic income guarantee for people moshe vardi express the same sentiment after speak at the 2016 annual meeting of the american association for the advancement of science about the emergence of intelligent machine we need to rethink the very basic structure of our economic system we may have to consider institute a basic income guarantee even baidu s chief scientist and founder of google s google brain deep learning project andrew ng during an onstage interview at this year s deep learning summit express the share notion that basic income must be seriously consider by government cite a high chance that ai will create massive labor displacement when those build the tool begin warn about the implication of their use shouldn t those wish to use those tool listen with the utmost attention especially when it s the very livelihood of million of people at stake if not then what about when nobel prize win economist begin agree with they in increase number no nation be yet ready for the change ahead high labor force non participation lead to social instability and a lack of consumer within consumer economy lead to economic instability so let s ask ourselves what s the purpose of the technology we re create what s the purpose of a car that can drive for we or artificial intelligence that can shoulder 60 % of our workload be it to allow we to work more hour for even less pay or be it to enable we to choose how we work and to decline any pay hour we deem insufficient because we re already earn the income that machine aren t what s the big lesson to learn in a century when machine can learn I offer it s that job be for machine and life be for people this article be write on a crowdfunde monthly basic income if you find value in this article you can support it along with all my advocacy for basic income with a monthly patron pledge of $ 1 + special thank to arjun banker steven grimm larry cohen topher hunt aaron marcus kubitza andrew stern keith davis albert wenger richard just chris smothers mark witham david ihnen danielle texeira katie doemland paul wicks jan smole joe esposito jack wagner joe ballou stuart matthews natalie foster chris mccoy michael honey gary aranovich kai wong john david hodge louise whitmore dan o sullivan harish venkatesan michiel dral gerald huff susanne berg cameron otten kian alavi gray scott kirk israel robert solovay jeff schulman andrew henderson robert f greene martin jordo victor lau shane gordon paolo narciso johan grahn tony destefano erhan altay bryan herdliska stephane boisvert dave shelton rise & shine pac luke sampson lee irving kris roadruck amy shaffer thomas welsh olli niinimäki casey young elizabeth balcar masud shah allen bauer all my other funder for their support and my amazing partner katie smith scott santens write about basic income on his blog you can also follow he here on medium on twitter on facebook or on reddit where he be a moderator for the r basicincome community of over 30 000 subscriber if you feel other would appreciate this article please click the green heart from a quick cheer to a stand ovation clap to show how much you enjoy this story new orleans writer focus on the potential for human civilization to get its act together in the 21st century moderator of r basicincome on reddit article discuss the concept of the universal basic income
Adam Geitgey,35K,15,https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471?source=tag_archive---------1----------------,machine learning be fun adam geitgey medium,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 日本語 português português alternate türkçe français 한국어 العَرَبِيَّة‎‎ español méxico español españa polski italiano 普通话 русский 한국어 tiếng việt or فارسی big update the content of this article be now available as a full length video course that walk you through every step of the code you can take the course for free and access everything else on lynda com free for 30 day if you sign up with this link have you hear people talk about machine learning but only have a fuzzy idea of what that mean be you tired of nod your way through conversation with co worker let s change that this guide be for anyone who be curious about machine learning but have no idea where to start I imagine there be a lot of people who try read the wikipedia article get frustrate and give up wish someone would just give they a high level explanation that s what this be the goal be be accessible to anyone — which mean that there s a lot of generalization but who care if this get anyone more interested in ml then mission accomplished machine learning be the idea that there be generic algorithm that can tell you something interesting about a set of datum without you have to write any custom code specific to the problem instead of write code you feed datum to the generic algorithm and it build its own logic base on the datum for example one kind of algorithm be a classification algorithm it can put datum into different group the same classification algorithm use to recognize handwritten number could also be use to classify email into spam and not spam without change a line of code it s the same algorithm but it s feed different training datum so it come up with different classification logic machine learning be an umbrella term cover lot of these kind of generic algorithm you can think of machine learning algorithm as fall into one of two main category — supervise learning and unsupervise learn the difference be simple but really important let s say you be a real estate agent your business be grow so you hire a bunch of new trainee agent to help you out but there s a problem — you can glance at a house and have a pretty good idea of what a house be worth but your trainee don t have your experience so they don t know how to price their house to help your trainee and maybe free yourself up for a vacation you decide to write a little app that can estimate the value of a house in your area base on it s size neighborhood etc and what similar house have sell for so you write down every time someone sell a house in your city for 3 month for each house you write down a bunch of detail — number of bedroom size in square foot neighborhood etc but most importantly you write down the final sale price use that training datum we want to create a program that can estimate how much any other house in your area be worth this be call supervised learning you know how much each house sell for so in other word you know the answer to the problem and could work backwards from there to figure out the logic to build your app you feed your training datum about each house into your machine learn algorithm the algorithm be try to figure out what kind of math need to be do to make the number work out this kind of like have the answer key to a math test with all the arithmetic symbol erase from this can you figure out what kind of math problem be on the test you know you be suppose to do something with the number on the left to get each answer on the right in supervised learning you be let the computer work out that relationship for you and once you know what math be require to solve this specific set of problem you could answer to any other problem of the same type let s go back to our original example with the real estate agent what if you didn t know the sale price for each house even if all you know be the size location etc of each house it turn out you can still do some really cool stuff this be call unsupervised learn this be kind of like someone give you a list of number on a sheet of paper and say I don t really know what these number mean but maybe you can figure out if there be a pattern or group or something — good luck so what could do with this datum for starter you could have an algorithm that automatically identify different market segment in your datum maybe you d find out that home buyer in the neighborhood near the local college really like small house with lot of bedroom but home buyer in the suburb prefer 3 bedroom house with lot of square footage know about these different kind of customer could help direct your marketing effort another cool thing you could do be automatically identify any outlier house that be way different than everything else maybe those outlier house be giant mansion and you can focus your good sale people on those area because they have big commission supervise learning be what we ll focus on for the rest of this post but that s not because unsupervised learning be any less useful or interesting in fact unsupervised learning be become increasingly important as the algorithm get well because it can be use without have to label the datum with the correct answer side note there be lot of other type of machine learning algorithm but this be a pretty good place to start as a human your brain can approach most any situation and learn how to deal with that situation without any explicit instruction if you sell house for a long time you will instinctively have a feel for the right price for a house the good way to market that house the kind of client who would be interested etc the goal of strong ai research be to be able to replicate this ability with computer but current machine learning algorithm aren t that good yet — they only work when focus a very specific limited problem maybe a well definition for learn in this case be figure out an equation to solve a specific problem base on some example datum unfortunately machine figure out an equation to solve a specific problem base on some example datum isn t really a great name so we end up with machine learning instead of course if you be read this 50 year in the future and we ve figure out the algorithm for strong ai then this whole post will all seem a little quaint maybe stop read and go tell your robot servant to go make you a sandwich future human so how would you write the program to estimate the value of a house like in our example above think about it for a second before you read far if you didn t know anything about machine learn you d probably try to write out some basic rule for estimate the price of a house like this if you fiddle with this for hour and hour you might end up with something that sort of work but your program will never be perfect and it will be hard to maintain as price change wouldn t it be well if the computer could just figure out how to implement this function for you who care what exactly the function do as long be it return the correct number one way to think about this problem be that the price be a delicious stew and the ingredient be the number of bedroom the square footage and the neighborhood if you could just figure out how much each ingredient impact the final price maybe there s an exact ratio of ingredient to stir in to make the final price that would reduce your original function with all those crazy if s and else s down to something really simple like this notice the magic number in bold — 841231951398213 1231 1231231 2 3242341421 and 201 23432095 these be our weight if we could just figure out the perfect weight to use that work for every house our function could predict house price a dumb way to figure out the good weight would be something like this start with each weight set to 1 0 run every house you know about through your function and see how far off the function be at guess the correct price for each house for example if the first house really sell for $ 250 000 but your function guess it sell for $ 178 000 you be off by $ 72 000 for that single house now add up the square amount you be off for each house you have in your data set let s say that you have 500 home sale in your datum set and the square of how much your function be off for each house be a grand total of $ 86 123 373 that s how wrong your function currently be now take that sum total and divide it by 500 to get an average of how far off you be for each house call this average error amount the cost of your function if you could get this cost to be zero by play with the weight your function would be perfect it would mean that in every case your function perfectly guess the price of the house base on the input datum so that s our goal — get this cost to be as low as possible by try different weight repeat step 2 over and over with every single possible combination of weight whichever combination of weight make the cost close to zero be what you use when you find the weight that work you ve solve the problem that s pretty simple right well think about what you just do you take some datum you feed it through three generic really simple step and you end up with a function that can guess the price of any house in your area watch out zillow but here s a few more fact that will blow your mind pretty crazy right ok of course you can t just try every combination of all possible weight to find the combo that work the good that would literally take forever since you d never run out of number to try to avoid that mathematician have figure out lot of clever way to quickly find good value for those weight without have to try very many here s one way first write a simple equation that represent step # 2 above now let s re write exactly the same equation but use a bunch of machine learn math jargon that you can ignore for now this equation represent how wrong our price estimating function be for the weight we currently have set if we graph this cost equation for all possible value of our weight for number_of_bedroom and sqft we d get a graph that might look something like this in this graph the low point in blue be where our cost be the low — thus our function be the least wrong the high point be where we be most wrong so if we can find the weight that get we to the low point on this graph we ll have our answer so we just need to adjust our weight so we be walk down hill on this graph towards the low point if we keep make small adjustment to our weight that be always move towards the low point we ll eventually get there without have to try too many different weight if you remember anything from calculus you might remember that if you take the derivative of a function it tell you the slope of the function s tangent at any point in other word it tell we which way be downhill for any give point on our graph we can use that knowledge to walk downhill so if we calculate a partial derivative of our cost function with respect to each of our weight then we can subtract that value from each weight that will walk we one step close to the bottom of the hill keep do that and eventually we ll reach the bottom of the hill and have the good possible value for our weight if that didn t make sense don t worry and keep read that s a high level summary of one way to find the good weight for your function call batch gradient descent don t be afraid to dig deeply if you be interested on learn the detail when you use a machine learning library to solve a real problem all of this will be do for you but it s still useful to have a good idea of what be happen the three step algorithm I describe be call multivariate linear regression you be estimate the equation for a line that fit through all of your house data point then you be use that equation to guess the sale price of house you ve never see before base where that house would appear on your line it s a really powerful idea and you can solve real problem with it but while the approach I show you might work in simple case it win t work in all case one reason be because house price aren t always simple enough to follow a continuous line but luckily there be lot of way to handle that there be plenty of other machine learning algorithm that can handle non linear datum like neural network or svms with kernel there be also way to use linear regression more cleverly that allow for more complicated line to be fit in all case the same basic idea of need to find the good weight still apply also I ignore the idea of overfitte it s easy to come up with a set of weight that always work perfectly for predict the price of the house in your original datum set but never actually work for any new house that weren t in your original datum set but there be way to deal with this like regularization and use a cross validation datum set learn how to deal with this issue be a key part of learn how to apply machine learning successfully in other word while the basic concept be pretty simple it take some skill and experience to apply machine learning and get useful result but it s a skill that any developer can learn once you start see how easily machine learning technique can be apply to problem that seem really hard like handwriting recognition you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough datum just feed in the datum and watch the computer magically figure out the equation that fit the datum but it s important to remember that machine learn only work if the problem be actually solvable with the datum that you have for example if you build a model that predict home price base on the type of pot plant in each house it s never go to work there just isn t any kind of relationship between the pot plant in each house and the home s sale price so no matter how hard it try the computer can never deduce a relationship between the two so remember if a human expert couldn t use the datum to solve the problem manually a computer probably win t be able to either instead focus on problem where a human could solve the problem but where it would be great if a computer could solve it much more quickly in my mind the big problem with machine learning right now be that it mostly live in the world of academia and commercial research group there isn t a lot of easy to understand material out there for people who would like to get a broad understanding without actually become expert but it s get a little well every day if you want to try out what you ve learn in this article I make a course that walk you through every step of this article include write all the code give it a try if you want to go deep andrew ng s free machine learning class on coursera be pretty amazing as a next step I highly recommend it it should be accessible to anyone who have a comp sci degree and who remember a very minimal amount of math also you can play around with ton of machine learning algorithm by download and instal scikit learn it s a python framework that have black box version of all the standard algorithm if you like this article please consider sign up for my machine learning be fun newsletter also please check out the full length course version of this article it cover everything in this article in more detail include write the actual code in python you can get a free 30 day trial to watch the course if you sign up with this link you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 2 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Adam Geitgey,14.2K,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=tag_archive---------2----------------,machine learning be fun part 3 deep learning and convolutional neural network,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 普通话 русский 한국어 português tiếng việt or italiano be you tired of read endless news story about deep learning and not really know what that mean let s change that this time we be go to learn how to write program that recognize object in image use deep learning in other word we re go to explain the black magic that allow google photo to search your photo base on what be in the picture just like part 1 and part 2 this guide be for anyone who be curious about machine learning but have no idea where to start the goal be be accessible to anyone — which mean that there s a lot of generalization and we skip lot of detail but who care if this get anyone more interested in ml then mission accomplish if you haven t already read part 1 and part 2 read they now you might have see this famous xkcd comic before the goof be base on the idea that any 3 year old child can recognize a photo of a bird but figure out how to make a computer recognize object have puzzle the very good computer scientist for over 50 year in the last few year we ve finally find a good approach to object recognition use deep convolutional neural network that sound like a a bunch of make up word from a william gibson sci fi novel but the idea be totally understandable if you break they down one by one so let s do it — let s write a program that can recognize bird before we learn how to recognize picture of bird let s learn how to recognize something much simple — the handwritten number 8 in part 2 we learn about how neural network can solve complex problem by chain together lot of simple neuron we create a small neural network to estimate the price of a house base on how many bedroom it have how big it be and which neighborhood it be in we also know that the idea of machine learning be that the same generic algorithm can be reuse with different datum to solve different problem so let s modify this same neural network to recognize handwritten text but to make the job really simple we ll only try to recognize one letter — the numeral 8 machine learning only work when you have datum — preferably a lot of datum so we need lot and lot of handwritten 8 s to get start luckily researcher create the mnist datum set of handwritten number for this very purpose mnist provide 60 000 image of handwritten digit each as an 18x18 image here be some 8 s from the datum set the neural network we make in part 2 only take in a three number as the input 3 bedroom 2000 sq foot etc but now we want to process image with our neural network how in the world do we feed image into a neural network instead of just number the answer be incredible simple a neural network take number as input to a computer an image be really just a grid of number that represent how dark each pixel be to feed an image into our neural network we simply treat the 18x18 pixel image as an array of 324 number the handle 324 input we ll just enlarge our neural network to have 324 input node notice that our neural network also have two output now instead of just one the first output will predict the likelihood that the image be an 8 and thee second output will predict the likelihood it isn t an 8 by have a separate output for each type of object we want to recognize we can use a neural network to classify object into group our neural network be a lot big than last time 324 input instead of 3 but any modern computer can handle a neural network with a few hundred node without blink this would even work fine on your cell phone all that s leave be to train the neural network with image of 8 s and not 8 s so it learn to tell they apart when we feed in an 8 we ll tell it the probability the image be an 8 be 100 % and the probability it s not an 8 be 0 % vice versa for the counter example image here s some of our training datum we can train this kind of neural network in a few minute on a modern laptop when it s do we ll have a neural network that can recognize picture of 8 s with a pretty high accuracy welcome to the world of late 1980 s era image recognition it s really neat that simply feed pixel into a neural network actually work to build image recognition machine learning be magic right well of course it s not that simple first the good news be that our 8 recognizer really do work well on simple image where the letter be right in the middle of the image but now the really bad news our 8 recognizer totally fail to work when the letter isn t perfectly center in the image just the slight position change ruin everything this be because our network only learn the pattern of a perfectly center 8 it have absolutely no idea what an off center 8 be it know exactly one pattern and one pattern only that s not very useful in the real world real world problem be never that clean and simple so we need to figure out how to make our neural network work in case where the 8 isn t perfectly center we already create a really good program for find an 8 center in an image what if we just scan all around the image for possible 8 s in small section one section at a time until we find one this approach call a slide window it s the brute force solution it work well in some limited case but it s really inefficient you have to check the same image over and over look for object of different size we can do well than this when we train our network we only show it 8 s that be perfectly center what if we train it with more datum include 8 s in all different position and size all around the image we don t even need to collect new training datum we can just write a script to generate new image with the 8 s in all kind of different position in the image use this technique we can easily create an endless supply of training datum more datum make the problem hard for our neural network to solve but we can compensate for that by make our network big and thus able to learn more complicated pattern to make the network big we just stack up layer upon layer of node we call this a deep neural network because it have more layer than a traditional neural network this idea have be around since the late 1960 but until recently train this large of a neural network be just too slow to be useful but once we figure out how to use 3d graphic card which be design to do matrix multiplication really fast instead of normal computer processor work with large neural network suddenly become practical in fact the exact same nvidia geforce gtx 1080 video card that you use to play overwatch can be use to train neural network incredibly quickly but even though we can make our neural network really big and train it quickly with a 3d graphic card that still isn t go to get we all the way to a solution we need to be smart about how we process image into our neural network think about it it doesn t make sense to train a network to recognize an 8 at the top of a picture separately from train it to recognize an 8 at the bottom of a picture as if those be two totally different object there should be some way to make the neural network smart enough to know that an 8 anywhere in the picture be the same thing without all that extra training luckily there be as a human you intuitively know that picture have a hierarchy or conceptual structure consider this picture as a human you instantly recognize the hierarchy in this picture most importantly we recognize the idea of a child no matter what surface the child be on we don t have to re learn the idea of child for every possible surface it could appear on but right now our neural network can t do this it think that an 8 in a different part of the image be an entirely different thing it doesn t understand that move an object around in the picture doesn t make it something different this mean it have to re learn the identify of each object in every possible position that suck we need to give our neural network understanding of translation invariance — an 8 be an 8 no matter where in the picture it show up we ll do this use a process call convolution the idea of convolution be inspire partly by computer science and partly by biology I e mad scientist literally poke cat brain with weird probe to figure out how cat process image instead of feed entire image into our neural network as one grid of number we re go to do something a lot smart that take advantage of the idea that an object be the same no matter where it appear in a picture here s how it s go to work step by step — similar to our slide window search above let s pass a slide window over the entire original image and save each result as a separate tiny picture tile by do this we turn our original image into 77 equally sized tiny image tile early we feed a single image into a neural network to see if it be an 8 we ll do the exact same thing here but we ll do it for each individual image tile however there s one big twist we ll keep the same neural network weight for every single tile in the same original image in other word we be treat every image tile equally if something interesting appear in any give tile we ll mark that tile as interesting we don t want to lose track of the arrangement of the original tile so we save the result from process each tile into a grid in the same arrangement as the original image it look like this in other word we ve start with a large image and we end with a slightly small array that record which section of our original image be the most interesting the result of step 3 be an array that map out which part of the original image be the most interesting but that array be still pretty big to reduce the size of the array we downsample it use an algorithm call max pooling it sound fancy but it isn t at all we ll just look at each 2x2 square of the array and keep the big number the idea here be that if we find something interesting in any of the four input tile that make up each 2x2 grid square we ll just keep the most interesting bit this reduce the size of our array while keep the most important bit so far we ve reduce a giant image down into a fairly small array guess what that array be just a bunch of number so we can use that small array as input into another neural network this final neural network will decide if the image be or isn t a match to differentiate it from the convolution step we call it a fully connect network so from start to finish our whole five step pipeline look like this our image processing pipeline be a series of step convolution max pooling and finally a fully connect network when solve problem in the real world these step can be combine and stack as many time as you want you can have two three or even ten convolution layer you can throw in max pooling wherever you want to reduce the size of your datum the basic idea be to start with a large image and continually boil it down step by step until you finally have a single result the more convolution step you have the more complicated feature your network will be able to learn to recognize for example the first convolution step might learn to recognize sharp edge the second convolution step might recognize beak use it s knowledge of sharp edge the third step might recognize entire bird use it s knowledge of beak etc here s what a more realistic deep convolutional network like you would find in a research paper look like in this case they start a 224 x 224 pixel image apply convolution and max pooling twice apply convolution 3 more time apply max pooling and then have two fully connect layer the end result be that the image be classify into one of 1000 category so how do you know which step you need to combine to make your image classifier work honestly you have to answer this by do a lot of experimentation and testing you might have to train 100 network before you find the optimal structure and parameter for the problem you be solve machine learning involve a lot of trial and error now finally we know enough to write a program that can decide if a picture be a bird or not as always we need some datum to get start the free cifar10 datum set contain 6 000 picture of bird and 52 000 picture of thing that be not bird but to get even more datum we ll also add in the caltech ucsd bird 200 2011 datum set that have another 12 000 bird pic here s a few of the bird from our combine datum set and here s some of the 52 000 non bird image this datum set will work fine for our purpose but 72 000 low re image be still pretty small for real world application if you want google level performance you need million of large image in machine learning have more datum be almost always more important that have well algorithm now you know why google be so happy to offer you unlimited photo storage they want your sweet sweet datum to build our classifier we ll use tflearn tflearn be a wrapper around google s tensorflow deep learning library that expose a simplified api it make build convolutional neural network as easy as write a few line of code to define the layer of our network here s the code to define and train the network if you be train with a good video card with enough ram like an nvidia geforce gtx 980 ti or well this will be do in less than an hour if you be train with a normal cpu it might take a lot long as it train the accuracy will increase after the first pass I get 75 4 % accuracy after just 10 pass it be already up to 91 7 % after 50 or so pass it cap out around 95 5 % accuracy and additional training didn t help so I stop it there congrat our program can now recognize bird in image now that we have a train neural network we can use it here s a simple script that take in a single image file and predict if it be a bird or not but to really see how effective our network be we need to test it with lot of image the datum set I create hold back 15 000 image for validation when I run those 15 000 image through the network it predict the correct answer 95 % of the time that seem pretty good right well it depend our network claim to be 95 % accurate but the devil be in the detail that could mean all sort of different thing for example what if 5 % of our training image be bird and the other 95 % be not bird a program that guess not a bird every single time would be 95 % accurate but it would also be 100 % useless we need to look more closely at the number than just the overall accuracy to judge how good a classification system really be we need to look closely at how it fail not just the percentage of the time that it fail instead of think about our prediction as right and wrong let s break they down into four separate category — use our validation set of 15 000 image here s how many time our prediction fall into each category why do we break our result down like this because not all mistake be create equal imagine if we be write a program to detect cancer from an mri image if we be detect cancer we d rather have false positive than false negative false negative would be the bad possible case — that s when the program tell someone they definitely didn t have cancer but they actually do instead of just look at overall accuracy we calculate precision and recall metric precision and recall metric give we a clear picture of how well we do this tell we that 97 % of the time we guess bird we be right but it also tell we that we only find 90 % of the actual bird in the datum set in other word we might not find every bird but we be pretty sure about it when we do find one now that you know the basic of deep convolutional network you can try out some of the example that come with tflearn to get your hand dirty with different neural network architecture it even come with build in datum set so you don t even have to find your own image you also know enough now to start branch and learn about other area of machine learn why not learn how to use algorithm to train computer how to play atari game next if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 4 part 5 and part 6 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Adam Geitgey,15.2K,13,https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------3----------------,machine learning be fun part 4 modern face recognition with deep learning,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 普通话 русский 한국어 português tiếng việt or italiano have you notice that facebook have develop an uncanny ability to recognize your friend in your photograph in the old day facebook use to make you to tag your friend in photo by click on they and type in their name now as soon as you upload a photo facebook tag everyone for you like magic this technology be call face recognition facebook s algorithm be able to recognize your friend face after they have be tag only a few time it s pretty amazing technology — facebook can recognize face with 98 % accuracy which be pretty much as good as human can do let s learn how modern face recognition work but just recognize your friend would be too easy we can push this tech to the limit to solve a more challenging problem — tell will ferrell famous actor apart from chad smith famous rock musician so far in part 1 2 and 3 we ve use machine learning to solve isolated problem that have only one step — estimate the price of a house generate new datum base on exist datum and tell if an image contain a certain object all of those problem can be solve by choose one machine learning algorithm feed in datum and get the result but face recognition be really a series of several relate problem as a human your brain be wire to do all of this automatically and instantly in fact human be too good at recognize face and end up see face in everyday object computer be not capable of this kind of high level generalization at least not yet so we have to teach they how to do each step in this process separately we need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step in other word we will chain together several machine learning algorithm let s tackle this problem one step at a time for each step we ll learn about a different machine learning algorithm I m not go to explain every single algorithm completely to keep this from turn into a book but you ll learn the main idea behind each one and you ll learn how you can build your own facial recognition system in python use openface and dlib the first step in our pipeline be face detection obviously we need to locate the face in a photograph before we can try to tell they apart if you ve use any camera in the last 10 year you ve probably see face detection in action face detection be a great feature for camera when the camera can automatically pick out face it can make sure that all the face be in focus before it take the picture but we ll use it for a different purpose — find the area of the image we want to pass on to the next step in our pipeline face detection go mainstream in the early 2000 s when paul viola and michael jones invent a way to detect face that be fast enough to run on cheap camera however much more reliable solution exist now we re go to use a method invent in 2005 call histogram of orient gradient — or just hog for short to find face in an image we ll start by make our image black and white because we don t need color datum to find face then we ll look at every single pixel in our image one at a time for every single pixel we want to look at the pixel that directly surround it our goal be to figure out how dark the current pixel be compare to the pixel directly surround it then we want to draw an arrow showing in which direction the image be get dark if you repeat that process for every single pixel in the image you end up with every pixel be replace by an arrow these arrow be call gradient and they show the flow from light to dark across the entire image this might seem like a random thing to do but there s a really good reason for replace the pixel with gradient if we analyze pixel directly really dark image and really light image of the same person will have totally different pixel value but by only consider the direction that brightness change both really dark image and really bright image will end up with the same exact representation that make the problem a lot easy to solve but save the gradient for every single pixel give we way too much detail we end up miss the forest for the tree it would be well if we could just see the basic flow of lightness darkness at a high level so we could see the basic pattern of the image to do this we ll break up the image into small square of 16x16 pixel each in each square we ll count up how many gradient point in each major direction how many point up point up right point right etc then we ll replace that square in the image with the arrow direction that be the strong the end result be we turn the original image into a very simple representation that capture the basic structure of a face in a simple way to find face in this hog image all we have to do be find the part of our image that look the most similar to a know hog pattern that be extract from a bunch of other training face use this technique we can now easily find face in any image if you want to try this step out yourself use python and dlib here s code show how to generate and view hog representation of image whew we isolate the face in our image but now we have to deal with the problem that face turn different direction look totally different to a computer to account for this we will try to warp each picture so that the eye and lip be always in the sample place in the image this will make it a lot easy for we to compare face in the next step to do this we be go to use an algorithm call face landmark estimation there be lot of way to do this but we be go to use the approach invent in 2014 by vahid kazemi and josephine sullivan the basic idea be we will come up with 68 specific point call landmark that exist on every face — the top of the chin the outside edge of each eye the inner edge of each eyebrow etc then we will train a machine learn algorithm to be able to find these 68 specific point on any face here s the result of locate the 68 face landmark on our test image now that we know be the eye and mouth be we ll simply rotate scale and shear the image so that the eye and mouth be center as well as possible we win t do any fancy 3d warps because that would introduce distortion into the image we be only go to use basic image transformation like rotation and scale that preserve parallel line call affine transformation now no matter how the face be turn we be able to center the eye and mouth be in roughly the same position in the image this will make our next step a lot more accurate if you want to try this step out yourself use python and dlib here s the code for find face landmark and here s the code for transform the image use those landmark now we be to the meat of the problem — actually tell face apart this be where thing get really interesting the simple approach to face recognition be to directly compare the unknown face we find in step 2 with all the picture we have of people that have already be tag when we find a previously tag face that look very similar to our unknown face it must be the same person seem like a pretty good idea right there s actually a huge problem with that approach a site like facebook with billion of user and a trillion photo can t possibly loop through every previous tag face to compare it to every newly uploaded picture that would take way too long they need to be able to recognize face in millisecond not hour what we need be a way to extract a few basic measurement from each face then we could measure our unknown face the same way and find the know face with the close measurement for example we might measure the size of each ear the spacing between the eye the length of the nose etc if you ve ever watch a bad crime show like csi you know what I be talk about ok so which measurement should we collect from each face to build our know face database ear size nose length eye color something else it turn out that the measurement that seem obvious to we human like eye color don t really make sense to a computer look at individual pixel in an image researcher have discover that the most accurate approach be to let the computer figure out the measurement to collect itself deep learning do a well job than human at figure out which part of a face be important to measure the solution be to train a deep convolutional neural network just like we do in part 3 but instead of train the network to recognize picture object like we do last time we be go to train it to generate 128 measurement for each face the training process work by look at 3 face image at a time then the algorithm look at the measurement it be currently generate for each of those three image it then tweak the neural network slightly so that it make sure the measurement it generate for # 1 and # 2 be slightly close while make sure the measurement for # 2 and # 3 be slightly far apart after repeat this step million of time for million of image of thousand of different people the neural network learn to reliably generate 128 measurement for each person any ten different picture of the same person should give roughly the same measurement machine learn people call the 128 measurement of each face an embed the idea of reduce complicate raw datum like a picture into a list of computer generate number come up a lot in machine learning especially in language translation the exact approach for face we be use be invent in 2015 by researcher at google but many similar approach exist this process of train a convolutional neural network to output face embedding require a lot of datum and computer power even with an expensive nvidia telsa video card it take about 24 hour of continuous training to get good accuracy but once the network have be train it can generate measurement for any face even one it have never see before so this step only need to be do once lucky for we the fine folk at openface already do this and they publish several train network which we can directly use thank brandon amos and team so all we need to do ourselves be run our face image through their pre train network to get the 128 measurement for each face here s the measurement for our test image so what part of the face be these 128 number measure exactly it turn out that we have no idea it doesn t really matter to we all that we care be that the network generate nearly the same number when look at two different picture of the same person if you want to try this step yourself openface provide a lua script that will generate embedding all image in a folder and write they to a csv file you run it like this this last step be actually the easy step in the whole process all we have to do be find the person in our database of know people who have the close measurement to our test image you can do that by use any basic machine learning classification algorithm no fancy deep learning trick be need we ll use a simple linear svm classifier but lot of classification algorithm could work all we need to do be train a classifier that can take in the measurement from a new test image and tell which known person be the close match run this classifier take millisecond the result of the classifier be the name of the person so let s try out our system first I train a classifier with the embedding of about 20 picture each of will ferrell chad smith and jimmy falon then I run the classifier on every frame of the famous youtube video of will ferrell and chad smith pretend to be each other on the jimmy fallon show it work and look how well it work for face in different pose — even sideways face let s review the step we follow now that you know how this all work here s instruction from start to finish of how run this entire face recognition pipeline on your own computer update 4 9 2017 you can still follow the step below to use openface however I ve release a new python base face recognition library call face_recognition that be much easy to install and use so I d recommend try out face_recognition first instead of continue below I even put together a pre configure virtual machine with face_recognition opencv tensorflow and lot of other deep learning tool pre instal you can download and run it on your computer very easily give the virtual machine a shot if you don t want to install all these librarie yourself original openface instruction if you like this article please consider sign up for my machine learning be fun newsletter you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 5 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Xiaohan Zeng,48K,13,https://medium.com/@XiaohanZeng/i-interviewed-at-five-top-companies-in-silicon-valley-in-five-days-and-luckily-got-five-job-offers-25178cf74e0f?source=tag_archive---------4----------------,I interview at five top company in silicon valley in five day and luckily get five job offer,in the five day from july 24th to 28th 2017 I interview at linkedin salesforce einstein google airbnb and facebook and get all five job offer it be a great experience and I feel fortunate that my effort pay off so I decide to write something about it I will discuss how I prepare review the interview process and share my impression about the five company I have be at groupon for almost three year it s my first job and I have be work with an amazing team and on awesome project we ve be build cool stuff make impact within the company publish paper and all that but I feel my learning rate be be anneal read slow down yet my mind be crave more also as a software engineer in chicago there be so many great company that all attract I in the bay area life be short and professional life short still after talk with my wife and gain her full support I decide to take action and make my first ever career change although I m interested in machine learning position the position at the five company be slightly different in the title and the interview process three be machine learning engineer linkedin google facebook one be data engineer salesforce and one be software engineer in general airbnb therefore I need to prepare for three different area code machine learning and system design since I also have a full time job it take I 2 3 month in total to prepare here be how I prepare for the three area while I agree that code interview might not be the good way to assess all your skill as a developer there be arguably no well way to tell if you be a good engineer in a short period of time imo it be the necessary evil to get you that job I mainly use leetcode and geeksforgeek for practice but hackerrank and lintcode be also good place I spend several week go over common datum structure and algorithm then focus on area I wasn t too familiar with and finally do some frequently see problem due to my time constraint I usually do two problem per day here be some thought this area be more closely related to the actual working experience many question can be ask during system design interview include but not limit to system architecture object orient design database schema design distribute system design scalability etc there be many resource online that can help you with the preparation for the most part I read article on system design interview architecture of large scale system and case study here be some resource that I find really helpful although system design interview can cover a lot of topic there be some general guideline for how to approach the problem with all that say the good way to practice for system design interview be to actually sit down and design a system I e your day to day work instead of do the minimal work go deeply into the tool framework and library you use for example if you use hbase rather than simply use the client to run some ddl and do some fetch try to understand its overall architecture such as the read write flow how hbase ensure strong consistency what minor major compaction do and where lru cache and bloom filter be use in the system you can even compare hbase with cassandra and see the similarity and difference in their design then when you be ask to design a distribute key value store you win t feel ambush many blog be also a great source of knowledge such as hacker noon and engineering blog of some company as well as the official documentation of open source project the most important thing be to keep your curiosity and modesty be a sponge that absorb everything it be submerge into machine learning interview can be divide into two aspect theory and product design unless you be have experience in machine learning research or do really well in your ml course it help to read some textbook classical one such as the element of statistical learning and pattern recognition and machine learning be great choice and if you be interested in specific area you can read more on those make sure you understand basic concept such as bias variance trade off overfitte gradient descent l1 l2 regularization baye theorem bag boost collaborative filtering dimension reduction etc familiarize yourself with common formula such as bayes theorem and the derivation of popular model such as logistic regression and svm try to implement simple model such as decision tree and k mean cluster if you put some model on your resume make sure you understand it thoroughly and can comment on its pro and con for ml product design understand the general process of build a ml product here s what I try to do here I want to emphasize again on the importance of remain curious and learning continuously try not to merely use the api for spark mllib or xgboost and call it do but try to understand why stochastic gradient descent be appropriate for distribute training or understand how xgboost differ from traditional gbdt e g what be special about its loss function why it need to compute the second order derivative etc I start by reply to hr s message on linkedin and ask for referral after a fail attempt at a rock star startup which I will touch upon later I prepare hard for several month and with help from my recruiter I schedule a full week of onsite in the bay area I fly in on sunday have five full day of interview with around 30 interviewer at some good tech company in the world and very luckily get job offer from all five of they all phone screening be standard the only difference be in the duration for some company like linkedin it s one hour while for facebook and airbnb it s 45 minute proficiency be the key here since you be under the time gun and usually you only get one chance you would have to very quickly recognize the type of problem and give a high level solution be sure to talk to the interviewer about your thinking and intention it might slow you down a little at the beginning but communication be more important than anything and it only help with the interview do not recite the solution as the interviewer would almost certainly see through it for machine learning position some company would ask ml question if you be interview for those make sure you brush up your ml skill as well to make well use of my time I schedule three phone screening in the same afternoon one hour apart from each the upside be that you might benefit from the hot hand and the downside be that the later one might be affect if the first one do not go well so I don t recommend it for everyone one good thing about interview with multiple company at the same time be that it give you certain advantage I be able to skip the second round phone screening with airbnb and salesforce because I get the onsite at linkedin and facebook after only one phone screening more surprisingly google even let I skip their phone screening entirely and schedule my onsite to fill the vacancy after learn I have four onsite come in the next week I know it be go to make it extremely tiring but hey nobody can refuse a google onsite invitation linkedin this be my first onsite and I interview at the sunnyvale location the office be very neat and people look very professional as always the session be one hour each code question be standard but the ml question can get a bit tough that say I get an email from my hr contain the preparation material which be very helpful and in the end I do not see anything that be too surprising I hear the rumor that linkedin have the good meal in the silicon valley and from what I see if it s not true it s not too far from the truth acquisition by microsoft seem to have lift the financial burden from linkedin and free they up to do really cool thing new feature such as video and professional advertisement be exciting as a company focus on professional development linkedin prioritize the growth of its own employee a lot of team such as ad relevance and feed ranking be expand so act quickly if you want to join salesforce einstein rock star project by rock star team the team be pretty new and feel very much like a startup the product be build on the scala stack so type safety be a real thing there great talk on the optimus prime library by matthew tovbin at scala days chicago 2017 and leah mcguire at spark summit west 2017 I interview at their palo alto office the team have a cohesive culture and work life balance be great there everybody be passionate about what they be do and really enjoy it with four session it be short compare to the other onsite interview but I wish I could have stay long after the interview matthew even take I for a walk to the hp garage google absolutely the industry leader and nothing to say about it that people don t already know but it s huge like really really huge it take I 20 minute to ride a bicycle to meet my friend there also line for food can be too long forever a great place for developer I interview at one of the many building on the mountain view campus and I don t know which one it be because it s huge my interviewer all look very smart and once they start talk they be even smart it would be very enjoyable to work with these people one thing that I feel special about google s interview be that the analysis of algorithm complexity be really important make sure you really understand what big o notation mean airbnb fast expand unicorn with a unique culture and arguably the most beautiful office in the silicon valley new product such as experience and restaurant reservation high end niche market and expansion into china all contribute to a positive prospect perfect choice if you be risk tolerant and want a fast grow pre ipo experience airbnb s code interview be a bit unique because you ll be code in an ide instead of whiteboarde so your code need to compile and give the right answer some problem can get really hard and they ve get the one of a kind cross functional interview this be how airbnb take culture seriously and be technically excellent doesn t guarantee a job offer for I the two cross functional be really enjoyable I have casual conversation with the interviewer and we all feel happy at the end of the session overall I think airbnb s onsite be the hard due to the difficulty of the problem long duration and unique cross functional interview if you be interested be sure to understand their culture and core value facebook another giant that be still grow fast and small and fast pace compare to google with its product line dominate the social network market and big investment in ai and vr I can only see more growth potential for facebook in the future with star like yann lecun and yangqe jia it s the perfect place if you be interested in machine learning I interview at build 20 the one with the rooftop garden and ocean view and also where zuckerberg s office be locate I m not sure if the interviewer get instruction but I didn t get clear sign whether my solution be correct although I believe they be by noon the prior four day start to take its toll and I be have a headache I persist through the afternoon session but feel I didn t do well at all I be a bit surprised to learn that I be get an offer from they as well generally I feel people there believe the company s vision and be proud of what they be build be a company with half a trillion market cap and grow facebook be a perfect place to grow your career at this be a big topic that I win t cover in this post but I find this article to be very helpful some thing that I do think be important all success start with failure include interview before I start interview for these company I fail my interview at databrick in may back in april xiangrui contact I via linkedin ask I if I be interested in a position on the spark mllib team I be extremely thrilled because 1 I use spark and love scala 2 databrick engineer be top notch and 3 spark be revolutionize the whole big data world it be an opportunity I couldn t miss so I start interview after a few day the bar be very high and the process be quite long include one pre screening questionnaire one phone screen one code assignment and one full onsite I manage to get the onsite invitation and visit their office in downtown san francisco where treasure island can be see my interviewer be incredibly intelligent yet equally modest during the interview I often feel be push to the limit it be fine until one disastrous session where I totally mess up due to insufficient skill and preparation and it end up a fiasco xiangrui be very kind and walk I to where I want to go after the interview be over and I really enjoy talk to he I get the rejection several day later it be expect but I feel frustrated for a few day nonetheless although I miss the opportunity to work there I wholeheartedly wish they will continue to make great impact and achievement from the first interview in may to finally accept the job offer in late september my first career change be long and not easy it be difficult for I to prepare because I need to keep do well at my current job for several week I be on a regular schedule of prepare for the interview till 1 am get up at 8 30am the next day and fully devote myself to another day at work interview at five company in five day be also highly stressful and risky and I don t recommend do it unless you have a very tight schedule but it do give you a good advantage during negotiation should you secure multiple offer I d like to thank all my recruiter who patiently walk I through the process the people who spend their precious time talk to I and all the company that give I the opportunity to interview and extend I offer lastly but most importantly I want to thank my family for their love and support — my parent for watch I take the first and every step my dear wife for everything she have do for I and my daughter for her warm smile thank for read through this long post you can find I on linkedin or twitter xiaohan zeng 10 22 17 ps since the publication of this post it have unexpectedly receive some attention I would like to thank everybody for the congratulation and share and apologize for not be able to respond to each of they this post have be translate into some other language it have be reposte in tech in asia break into startup invite I to a live video streaming together with sophia ciocca covershr do a short qna with I from a quick cheer to a stand ovation clap to show how much you enjoy this story critical mind & romantic heart
Gil Fewster,3.3K,5,https://medium.freecodecamp.org/the-mind-blowing-ai-announcement-from-google-that-you-probably-missed-2ffd31334805?source=tag_archive---------5----------------,the mind blow ai announcement from google that you probably miss,disclaimer I m not an expert in neural network or machine learning since originally write this article many people with far more expertise in these field than myself have indicate that while impressive what google have achieve be evolutionary not revolutionary in the very least it s fair to say that I m guilty of anthropomorphise in part of the text I ve leave the article s content unchanged because I think it s interesting to compare the gut reaction I have with the subsequent comment of expert in the field I strongly encourage reader to browse the comment after read the article for some perspective more sober and inform than my own in the closing week of 2016 google publish an article that quietly sail under most people s radar which be a shame because it may just be the most astonishing article about machine learning that I read last year don t feel bad if you miss it not only be the article compete with the pre christmas rush that most of we be navigate — it be also tuck away on google s research blog beneath the geektastic headline zero shot translation with google s multilingual neural machine translation system this doesn t exactly scream must read do it especially when you ve get project to wind up gift to buy and family feud to be resolve — all while the advent calendar relentlessly count down the day until christmas like some kind of chocolate fill yuletide doomsday clock luckily I m here to bring you up to speed here s the deal up until september of last year google translate use phrase base translation it basically do the same thing you and I do when we look up key word and phrase in our lonely planet language guide it s effective enough and blisteringly fast compare to awkwardly thumb your way through a bunch of page look for the french equivalent of please bring I all of your cheese and don t stop until I fall over but it lack nuance phrase base translation be a blunt instrument it do the job well enough to get by but map roughly equivalent word and phrase without an understanding of linguistic structure can only produce crude result this approach be also limit by the extent of an available vocabulary phrase base translation have no capacity to make educated guess at word it doesn t recognize and can t learn from new input all that change in september when google give their translation tool a new engine the google neural machine translation system gnmt this new engine come fully load with all the hot 2016 buzzword like neural network and machine learn the short version be that google translate get smart it develop the ability to learn from the people who use it it learn how to make educated guess about the content tone and meaning of phrase base on the context of other word and phrase around they and — here s the bit that should make your brain explode — it get creative google translate invent its own language to help it translate more effectively what s more nobody tell it to it didn t develop a language or interlingua as google call it because it be code to it develop a new language because the software determine over time that this be the most efficient way to solve the problem of translation stop and think about that for a moment let it sink in a neural computing system design to translate content from one human language into another develop its own internal language to make the task more efficient without be tell to do so in a matter of week I ve add a correction retraction of this paragraph in the note to understand what s go on we need to understand what zero shot translation capability be here s google s mike schuster nikhil thorat and melvin johnson from the original blog post here you can see an advantage of google s new neural machine over the old phrase base approach the gmnt be able to learn how to translate between two language without be explicitly teach this wouldn t be possible in a phrase base model where translation be dependent upon an explicit dictionary to map word and phrase between each pair of language be translate and this lead the google engineer onto that truly astonishing discovery of creation so there you have it in the last week of 2016 as journos around the world start pen their be this the bad year in living memory thinkpiece google engineer be quietly document a genuinely astonishing breakthrough in software engineering and linguistic I just think maybe you d want to know ok to really understand what s go on we probably need multiple computer science and linguistic degree I m just barely scrape the surface here if you ve get time to get a few degree or if you ve already get they please drop I a line and explain it all I to slowly update 1 in my excitement it s fair to say that I ve exaggerate the idea of this as an intelligent system — at least so far as we would think about human intelligence and decision making make sure you read chris mcdonald s comment after the article for a more sober perspective update 2 nafrondel s excellent detailed reply be also a must read for an expert explanation of how neural network function from a quick cheer to a stand ovation clap to show how much you enjoy this story a tinkerer our community publish story worth read on development design and datum science
Adam Geitgey,10.4K,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=tag_archive---------6----------------,machine learning be fun part 2 adam geitgey medium,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in italiano español français türkçe русский 한국어 português فارسی tiếng việt or 普通话 in part 1 we say that machine learning be use generic algorithm to tell you something interesting about your datum without write any code specific to the problem you be solve if you haven t already read part 1 read it now this time we be go to see one of these generic algorithm do something really cool — create video game level that look like they be make by human we ll build a neural network feed it exist super mario level and watch new one pop out just like part 1 this guide be for anyone who be curious about machine learning but have no idea where to start the goal be be accessible to anyone — which mean that there s a lot of generalization and we skip lot of detail but who care if this get anyone more interested in ml then mission accomplish back in part 1 we create a simple algorithm that estimate the value of a house base on its attribute give datum about a house like this we end up with this simple estimation function in other word we estimate the value of the house by multiply each of its attribute by a weight then we just add those number up to get the house s value instead of use code let s represent that same function as a simple diagram however this algorithm only work for simple problem where the result have a linear relationship with the input what if the truth behind house price isn t so simple for example maybe the neighborhood matter a lot for big house and small house but doesn t matter at all for medium sized house how could we capture that kind of complicated detail in our model to be more clever we could run this algorithm multiple time with different of weight that each capture different edge case now we have four different price estimate let s combine those four price estimate into one final estimate we ll run they through the same algorithm again but use another set of weight our new super answer combine the estimate from our four different attempt to solve the problem because of this it can model more case than we could capture in one simple model let s combine our four attempt to guess into one big diagram this be a neural network each node know how to take in a set of input apply weight to they and calculate an output value by chain together lot of these node we can model complex function there s a lot that I m skip over to keep this brief include feature scaling and the activation function but the most important part be that these basic idea click it s just like lego we can t model much with one single lego block but we can model anything if we have enough basic lego block to stick together the neural network we ve see always return the same answer when you give it the same input it have no memory in programming term it s a stateless algorithm in many case like estimate the price of house that s exactly what you want but the one thing this kind of model can t do be respond to pattern in datum over time imagine I hand you a keyboard and ask you to write a story but before you start my job be to guess the very first letter that you will type what letter should I guess I can use my knowledge of english to increase my odd of guess the right letter for example you will probably type a letter that be common at the beginning of word if I look at story you write in the past I could narrow it down far base on the word you usually use at the beginning of your story once I have all that datum I could use it to build a neural network to model how likely it be that you would start with any give letter our model might look like this but let s make the problem hard let s say I need to guess the next letter you be go to type at any point in your story this be a much more interesting problem let s use the first few word of ernest hemingway s the sun also rise as an example what letter be go to come next you probably guess n — the word be probably go to be box we know this base on the letter we ve already see in the sentence and our knowledge of common word in english also the word middleweight give we an extra clue that we be talk about box in other word it s easy to guess the next letter if we take into account the sequence of letter that come right before it and combine that with our knowledge of the rule of english to solve this problem with a neural network we need to add state to our model each time we ask our neural network for an answer we also save a set of our intermediate calculation and re use they the next time as part of our input that way our model will adjust its prediction base on the input that it have see recently keep track of state in our model make it possible to not just predict the most likely first letter in the story but to predict the most likely next letter give all previous letter this be the basic idea of a recurrent neural network we be update the network each time we use it this allow it to update its prediction base on what it see most recently it can even model pattern over time as long as we give it enough of a memory predict the next letter in a story might seem pretty useless what s the point one cool use might be auto predict for a mobile phone keyboard but what if we take this idea to the extreme what if we ask the model to predict the next most likely character over and over — forever we d be ask it to write a complete story for we we see how we could guess the next letter in hemingway s sentence let s try generate a whole story in the style of hemingway to do this we be go to use the recurrent neural network implementation that andrej karpathy write andrej be a deep learning researcher at stanford and he write an excellent introduction to generate text with rnn you can view all the code for the model on github we ll create our model from the complete text of the sun also rise — 362 239 character use 84 unique letter include punctuation uppercase lowercase etc this datum set be actually really small compare to typical real world application to generate a really good model of hemingway s style it would be much well to have at several time as much sample text but this be good enough to play around with as an example as we just start to train the rnn it s not very good at predict letter here s what it generate after a 100 loop of training you can see that it have figure out that sometimes word have space between they but that s about it after about 1000 iteration thing be look more promising the model have start to identify the pattern in basic sentence structure it s add period at the end of sentence and even quote dialog a few word be recognizable but there s also still a lot of nonsense but after several thousand more training iteration it look pretty good at this point the algorithm have capture the basic pattern of hemingway s short direct dialog a few sentence even sort of make sense compare that with some real text from the book even by only look for pattern one character at a time our algorithm have reproduce plausible looking prose with proper formatting that be kind of amazing we don t have to generate text completely from scratch either we can seed the algorithm by supply the first few letter and just let it find the next few letter for fun let s make a fake book cover for our imaginary book by generate a new author name and a new title use the seed text of er he and the s not bad but the really mind blow part be that this algorithm can figure out pattern in any sequence of datum it can easily generate real looking recipe or fake obama speech but why limit ourselves human language we can apply this same idea to any kind of sequential datum that have a pattern in 2015 nintendo release super mario makertm for the wii u gaming system this game let you draw out your own super mario brother level on the gamepad and then upload they to the internet so you friend can play through they you can include all the classic power up and enemy from the original mario game in your level it s like a virtual lego set for people who grow up play super mario brother can we use the same model that generate fake hemingway text to generate fake super mario brother level first we need a datum set for train our model let s take all the outdoor level from the original super mario brothers game release in 1985 this game have 32 level and about 70 % of they have the same outdoor style so we ll stick to those to get the design for each level I take an original copy of the game and write a program to pull the level design out of the game s memory super mario bros be a 30 year old game and there be lot of resource online that help you figure out how the level be store in the game s memory extract level datum from an old video game be a fun programming exercise that you should try sometime here s the first level from the game which you probably remember if you ever play it if we look closely we can see the level be make of a simple grid of object we could just as easily represent this grid as a sequence of character with one character represent each object we ve replace each object in the level with a letter and so on use a different letter for each different kind of object in the level I end up with text file that look like this look at the text file you can see that mario level don t really have much of a pattern if you read they line by line the pattern in a level really emerge when you think of the level as a series of column so in order for the algorithm to find the pattern in our datum we need to feed the datum in column by column figure out the most effective representation of your input datum call feature selection be one of the key of use machine learning algorithm well to train the model I need to rotate my text file by 90 degree this make sure the character be feed into the model in an order where a pattern would more easily show up just like we see when create the model of hemingway s prose a model improve as we train it after a little training our model be generate junk it sort of have an idea that s and = s should show up a lot but that s about it it hasn t figure out the pattern yet after several thousand iteration it s start to look like something the model have almost figure out that each line should be the same length it have even start to figure out some of the logic of mario the pipe in mario be always two block wide and at least two block high so the p s in the datum should appear in 2x2 cluster that s pretty cool with a lot more training the model get to the point where it generate perfectly valid datum let s sample an entire level s worth of datum from our model and rotate it back horizontal this data look great there be several awesome thing to notice finally let s take this level and recreate it in super mario maker play it yourself if you have super mario maker you can play this level by bookmarke it online or by look it up use level code 4ac9 0000 0157 f3c3 the recurrent neural network algorithm we use to train our model be the same kind of algorithm use by real world company to solve hard problem like speech detection and language translation what make our model a toy instead of cut edge be that our model be generate from very little datum there just aren t enough level in the original super mario brothers game to provide enough datum for a really good model if we could get access to the hundred of thousand of user create super mario maker level that nintendo have we could make an amazing model but we can t — because nintendo win t let we have they big company don t give away their datum for free as machine learning become more important in more industry the difference between a good program and a bad program will be how much datum you have to train your model that s why company like google and facebook need your datum so badly for example google recently open source tensorflow its software toolkit for build large scale machine learning application it be a pretty big deal that google give away such important capable technology for free this be the same stuff that power google translate but without google s massive trove of datum in every language you can t create a competitor to google translate data be what give google its edge think about that the next time you open up your google map location history or facebook location history and notice that it store every place you ve ever be in machine learn there s never a single way to solve a problem you have limitless option when decide how to pre process your datum and which algorithm to use often combine multiple approach will give you well result than any single approach reader have send I link to other interesting approach to generate super mario level if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 3 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
David Venturi,10.6K,20,https://medium.freecodecamp.org/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0?source=tag_archive---------7----------------,every single machine learning course on the internet rank by your review,a year and a half ago I drop out of one of the good computer science program in canada I start create my own data science master s program use online resource I realize that I could learn everything I need through edx coursera and udacity instead and I could learn it fast more efficiently and for a fraction of the cost I m almost finish now I ve take many data science relate course and audit portion of many more I know the option out there and what skill be need for learner prepare for a data analyst or data scientist role so I start create a review drive guide that recommend the good course for each subject within data science for the first guide in the series I recommend a few code class for the beginner datum scientist then it be statistic and probability class then introduction to data science also data visualization for this guide I spend a dozen hour try to identify every online machine learning course offer as of may 2017 extract key bit of information from their syllabus and review and compile their rating my end goal be to identify the three good course available and present they to you below for this task I turn to none other than the open source class central community and its database of thousand of course rating and review since 2011 class central founder dhawal shah have keep a close eye on online course than arguably anyone else in the world dhawal personally help I assemble this list of resource each course must fit three criterion we believe we cover every notable course that fit the above criterion since there be seemingly hundred of course on udemy we choose to consider the most reviewed and high rate one only there s always a chance that we miss something though so please let we know in the comment section if we leave a good course out we compile average rating and number of review from class central and other review site to calculate a weighted average rating for each course we read text review and use this feedback to supplement the numerical rating we make subjective syllabus judgment call base on three factor a popular definition originate from arthur samuel in 1959 machine learning be a subfield of computer science that give computer the ability to learn without be explicitly program in practice this mean develop computer program that can make prediction base on datum just as human can learn from experience so can computer where datum = experience a machine learn workflow be the process require for carry out a machine learning project though individual project can differ most workflow share several common task problem evaluation datum exploration datum preprocesse model training testing deployment etc below you ll find helpful visualization of these core step the ideal course introduce the entire process and provide interactive example assignment and or quiz where student can perform each task themselves first off let s define deep learning here be a succinct description as would be expect portion of some of the machine learn course contain deep learning content I choose not to include deep learning only course however if you be interested in deep learning specifically we ve get you cover with the follow article my top three recommendation from that list would be several course list below ask student to have prior programming calculus linear algebra and statistic experience these prerequisite be understandable give that machine learning be an advanced discipline miss a few subject good news some of this experience can be acquire through our recommendation in the first two article program statistic of this data science career guide several top rank course below also provide gentle calculus and linear algebra refresher and highlight the aspect most relevant to machine learning for those less familiar stanford university s machine learning on coursera be the clear current winner in term of rating review and syllabus fit teach by the famous andrew ng google brain founder and former chief scientist at baidu this be the class that spark the founding of coursera it have a 4 7 star weight average rating over 422 review release in 2011 it cover all aspect of the machine learn workflow though it have a small scope than the original stanford class upon which it be base it still manage to cover a large number of technique and algorithm the estimate timeline be eleven week with two week dedicate to neural network and deep learning free and pay option be available ng be a dynamic yet gentle instructor with a palpable experience he inspire confidence especially when share practical implementation tip and warning about common pitfall a linear algebra refresher be provide and ng highlight the aspect of calculus most relevant to machine learning evaluation be automatic and be do via multiple choice quiz that follow each lesson and programming assignment the assignment there be eight of they can be complete in matlab or octave which be an open source version of matlab ng explain his language choice though python and r be likely more compelling choice in 2017 with the increase popularity of those language reviewer note that that shouldn t stop you from take the course a few prominent reviewer note the follow columbia university s machine learning be a relatively new offering that be part of their artificial intelligence micromaster on edx though it be new and doesn t have a large number of review the one that it do have be exceptionally strong professor john paisley be note as brilliant clear and clever it have a 4 8 star weight average rating over 10 review the course also cover all aspect of the machine learn workflow and more algorithm than the above stanford offer columbia s be a more advanced introduction with reviewer note that student should be comfortable with the recommend prerequisite calculus linear algebra statistic probability and code quiz 11 programming assignment 4 and a final exam be the mode of evaluation student can use either python octave or matlab to complete the assignment the course s total estimate timeline be eight to ten hour per week over twelve week it be free with a verify certificate available for purchase below be a few of the aforementioned sparkling review machine learn a ztm on udemy be an impressively detailed offering that provide instruction in both python and r which be rare and can t be say for any of the other top course it have a 4 5 star weight average rating over 8 119 review which make it the most review course of the one consider it cover the entire machine learn workflow and an almost ridiculous in a good way number of algorithm through 40 5 hour of on demand video the course take a more apply approach and be light math wise than the above two course each section start with an intuition video from eremenko that summarize the underlie theory of the concept be teach de ponteve then walk through implementation with separate video for both python and r as a bonus the course include python and r code template for student to download and use on their own project there be quiz and homework challenge though these aren t the strong point of the course eremenko and the superdatascience team be revere for their ability to make the complex simple also the prerequisite list be just some high school mathematic so this course might be a well option for those daunt by the stanford and columbia offering a few prominent reviewer note the follow our # 1 pick have a weight average rating of 4 7 out of 5 star over 422 review let s look at the other alternative sort by descend rating a reminder that deep learning only course be not include in this guide — you can find those here the analytic edge massachusetts institute of technology edx more focused on analytic in general though it do cover several machine learning topic use r strong narrative that leverage familiar real world example challenge ten to fifteen hour per week over twelve week free with a verify certificate available for purchase it have a 4 9 star weight average rating over 214 review python for datum science and machine learning bootcamp jose portilla udemy have large chunk of machine learning content but cover the whole data science process more of a very detailed intro to python amazing course though not ideal for the scope of this guide 21 5 hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 6 star weight average rating over 3316 review data science and machine learning bootcamp with r jose portilla udemy the comment for portilla s above course apply here as well except for r 17 5 hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 6 star weight average rating over 1317 review machine learning series lazy programmer inc udemy teach by a data scientist big data engineer full stack software engineer with an impressive resume lazy programmer currently have a series of 16 machine learning focus course on udemy in total the course have 5000 + rating and almost all of they have 4 6 star a useful course ordering be provide in each individual course s description use python cost varie depend on udemy discount which be frequent machine learn georgia tech udacity a compilation of what be three separate course supervise unsupervised and reinforcement learning part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms bite sized video as be udacity s style friendly professor estimate timeline of four month free it have a 4 56 star weight average rating over 9 review implement predictive analytic with spark in azure hdinsight microsoft edx introduce the core concept of machine learning and a variety of algorithms leverage several big data friendly tool include apache spark scala and hadoop use both python and r four hour per week over six week free with a verify certificate available for purchase it have a 4 5 star weight average rating over 6 review data science and machine learning with python — hand on frank kane udemy use python kane have nine year of experience at amazon and imdb nine hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 5 star weight average rating over 4139 review scala and spark for big datum and machine learning jose portilla udemy big datum focus specifically on implementation in scala and spark ten hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 5 star weight average rating over 607 review machine learning engineer nanodegree udacity udacity s flagship machine learning program which feature a good in class project review system and career support the program be a compilation of several individual udacity course which be free co create by kaggle estimate timeline of six month currently cost $ 199 usd per month with a 50 % tuition refund available for those who graduate within 12 month it have a 4 5 star weight average rating over 2 review learn from datum introductory machine learning california institute of technology edx enrollment be currently close on edx but be also available via caltech s independent platform see below it have a 4 49 star weight average rating over 42 review learn from datum introductory machine learn yaser abu mostafa california institute of technology a real caltech course not a water down version review note it be excellent for understand machine learning theory the professor yaser abu mostafa be popular among student and also write the textbook upon which this course be base video be tape lecture with lecture slide picture in picture upload to youtube homework assignment be pdf file the course experience for online student isn t as polished as the top three recommendation it have a 4 43 star weight average rating over 7 review mining massive dataset stanford university machine learn with a focus on big datum introduce modern distribute file system and mapreduce ten hour per week over seven week free it have a 4 4 star weight average rating over 30 review aws machine learn a complete guide with python chandra lingam udemy a unique focus on cloud base machine learning and specifically amazon web service use python nine hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 4 star weight average rating over 62 review introduction to machine learning & face detection in python holczer balazs udemy use python eight hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 4 star weight average rating over 162 review statlearne statistical learning stanford university base on the excellent textbook an introduction to statistical learning with application in r and teach by the professor who write it reviewer note that the mooc isn t as good as the book cite thin exercise and mediocre video five hour per week over nine week free it have a 4 35 star weight average rating over 84 review machine learning specialization university of washington coursera great course but last two class include the capstone project be cancel reviewer note that this series be more digestable read easy for those without strong technical background than other top machine learn course e g stanford s or caltech s be aware that the series be incomplete with recommend system deep learning and a summary miss free and pay option available it have a 4 31 star weight average rating over 80 review from 0 to 1 machine learn nlp & python cut to the chase loony corn udemy a down to earth shy but confident take on machine learning technique teach by four person team with decade of industry experience together use python cost varie depend on udemy discount which be frequent it have a 4 2 star weight average rating over 494 review principle of machine learn microsoft edx use r python and microsoft azure machine learn part of the microsoft professional program certificate in data science three to four hour per week over six week free with a verify certificate available for purchase it have a 4 09 star weight average rating over 11 review big data statistical inference and machine learning queensland university of technology futurelearn a nice brief exploratory machine learning course with a focus on big datum cover a few tool like r h2o flow and weka only three week in duration at a recommend two hour per week but one reviewer note that six hour per week would be more appropriate free and pay option available it have a 4 star weight average rating over 4 review genomic datum science and cluster bioinformatics v university of california san diego coursera for those interested in the intersection of computer science and biology and how it represent an important frontier in modern science focus on clustering and dimensionality reduction part of ucsd s bioinformatics specialization free and pay option available it have a 4 star weight average rating over 3 review intro to machine learn udacity prioritize topic breadth and practical tool in python over depth and theory the instructor sebastian thrun and katie malone make this class so fun consist of bite sized video and quiz follow by a mini project for each lesson currently part of udacity s data analyst nanodegree estimate timeline of ten week free it have a 3 95 star weight average rating over 19 review machine learning for data analysis wesleyan university coursera a brief intro machine learning and a few select algorithms cover decision tree random forest lasso regression and k mean cluster part of wesleyan s datum analysis and interpretation specialization estimate timeline of four week free and pay option available it have a 3 6 star weight average rating over 5 review program with python for data science microsoft edx produce by microsoft in partnership with code dojo use python eight hour per week over six week free and pay option available it have a 3 46 star weight average rating over 37 review machine learning for trade georgia tech udacity focus on apply probabilistic machine learning approach to trading decision use python part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree om estimate timeline of four month free it have a 3 29 star weight average rating over 14 review practical machine learning johns hopkins university coursera a brief practical introduction to a number of machine learning algorithm several one two star review express a variety of concern part of jhu s data science specialization four to nine hour per week over four week free and pay option available it have a 3 11 star weight average rating over 37 review machine learning for datum science and analytics columbia university edx introduce a wide range of machine learn topic some passionate negative review with concern include content choice a lack of programming assignment and uninspire presentation seven to ten hour per week over five week free with a verify certificate available for purchase it have a 2 74 star weight average rating over 36 review recommender system specialization university of minnesota coursera strong focus one specific type of machine learning — recommender system a four course specialization plus a capstone project which be a case study teach use lenskit an open source toolkit for recommender system free and pay option available it have a 2 star weight average rating over 2 review machine learning with big datum university of california san diego coursera terrible review that highlight poor instruction and evaluation some note it take they mere hour to complete the whole course part of ucsd s big datum specialization free and pay option available it have a 1 86 star weight average rating over 14 review practical predictive analytic model and method university of washington coursera a brief intro to core machine learning concept one reviewer note that there be a lack of quiz and that the assignment be not challenge part of uw s data science at scale specialization six to eight hour per week over four week free and pay option available it have a 1 75 star weight average rating over 4 review the follow course have one or no review as of may 2017 machine learn for musician and artist goldsmith university of london kadenze unique student learn algorithms software tool and machine learn good practice to make sense of human gesture musical audio and other real time datum seven session in length audit free and premium $ 10 usd per month option available it have one 5 star review apply machine learning in python university of michigan coursera teach use python and the scikit learn toolkit part of the apply data science with python specialization schedule to start may 29th free and pay option available apply machine learn microsoft edx teach use various tool include python r and microsoft azure machine learning note microsoft produce the course include hand on lab to reinforce the lecture content three to four hour per week over six week free with a verify certificate available for purchase machine learn with python big datum university teach use python target towards beginner estimate completion time of four hour big datum university be affiliate with ibm free machine learning with apache systemml big data university teach use apache systemml which be a declarative style language design for large scale machine learning estimate completion time of eight hour big datum university be affiliate with ibm free machine learning for data science university of california san diego edx doesn t launch until january 2018 programming example and assignment be in python use jupyter notebook eight hour per week over ten week free with a verify certificate available for purchase introduction to analytic model georgia tech edx the course advertise r as its primary programming tool five to ten hour per week over ten week free with a verify certificate available for purchase predictive analytic gain insight from big datum queensland university of technology futurelearn brief overview of a few algorithm use hewlett packard enterprise s vertica analytic platform as an apply tool start date to be announce two hour per week over four week free with a certificate of achievement available for purchase introducción al machine learning universita telefónica miríada x teach in spanish an introduction to machine learning that cover supervised and unsupervised learn a total of twenty estimate hour over four week machine learning path step dataquest teach in python use dataquest s interactive in browser platform multiple guide project and a plus project where you build your own machine learning system use your own datum subscription require the follow six course be offer by datacamp datacamp s hybrid teaching style leverage video and text base instruction with lot of example through an in browser code editor a subscription be require for full access to each course introduction to machine learn datacamp cover classification regression and clustering algorithm use r fifteen video and 81 exercise with an estimate timeline of six hour supervised learning with scikit learn datacamp use python and scikit learn cover classification and regression algorithms seventeen video and 54 exercise with an estimate timeline of four hour unsupervised learning in r datacamp provide a basic introduction to clustering and dimensionality reduction in r sixteen video and 49 exercise with an estimate timeline of four hour machine learn toolbox datacamp teach the big idea in machine learning use r 24 video and 88 exercise with an estimate timeline of four hour machine learn with the expert school budget datacamp a case study from a machine learning competition on drivendata involve build a model to automatically classify item in a school s budget datacamp s supervised learning with scikit learn be a prerequisite fifteen video and 51 exercise with an estimate timeline of four hour unsupervised learning in python datacamp cover a variety of unsupervised learning algorithm use python scikit learn and scipy the course end with student build a recommend system to recommend popular musical artist thirteen video and 52 exercise with an estimate timeline of four hour machine learning tom mitchell carnegie mellon university carnegie mellon s graduate introductory machine learning course a prerequisite to their second graduate level course statistical machine learning tape university lecture with practice problem homework assignment and a midterm all with solution post online a 2011 version of the course also exist cmu be one of the good graduate school for study machine learning and have a whole department dedicate to ml free statistical machine learn larry wasserman carnegie mellon university likely the most advanced course in this guide a follow up to carnegie mellon s machine learning course tape university lecture with practice problem homework assignment and a midterm all with solution post online free undergraduate machine learn nando de freitas university of british columbia an undergraduate machine learning course lecture be film and put on youtube with the slide post on the course website the course assignment be post as well no solution though de freita be now a full time professor at the university of oxford and receive praise for his teaching ability in various forum graduate version available see below machine learn nando de freitas university of british columbia a graduate machine learning course the comment in de freitas undergraduate course above apply here as well this be the fifth of a six piece series that cover the good online course for launch yourself into the data science field we cover programming in the first article statistic and probability in the second article intro to data science in the third article and datum visualization in the fourth the final piece will be a summary of those article plus the good online course for other key topic such as datum wrangle database and even software engineering if you re look for a complete list of data science online course you can find they on class central s data science and big data subject page if you enjoy read this check out some of class central s other piece if you have suggestion for course I miss let I know in the response if you find this helpful click the 💚 so more people will see it here on medium this be a condensed version of my original article publish on class central where I ve include detailed course syllabus from a quick cheer to a stand ovation clap to show how much you enjoy this story curriculum lead project @ datacamp I create my own data science master s program our community publish story worth read on development design and datum science
Michael Jordan,34K,16,https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7?source=tag_archive---------8----------------,artificial intelligence — the revolution hasn t happen yet,artificial intelligence ai be the mantra of the current era the phrase be intone by technologist academician journalist and venture capitalist alike as with many phrase that cross over from technical academic field into general circulation there be significant misunderstanding accompany the use of the phrase but this be not the classical case of the public not understand the scientist — here the scientist be often as befuddle as the public the idea that our era be somehow see the emergence of an intelligence in silicon that rival our own entertain all of we — enthral we and frightening we in equal measure and unfortunately it distract we there be a different narrative that one can tell about the current era consider the follow story which involve human computer datum and life or death decision but where the focus be something other than intelligence in silicon fantasy when my spouse be pregnant 14 year ago we have an ultrasound there be a geneticist in the room and she point out some white spot around the heart of the fetus those be marker for down syndrome she note and your risk have now go up to 1 in 20 she far let we know that we could learn whether the fetus in fact have the genetic modification underlie down syndrome via an amniocentesis but amniocentesis be risky — the risk of kill the fetus during the procedure be roughly 1 in 300 be a statistician I determine to find out where these number be come from to cut a long story short I discover that a statistical analysis have be do a decade previously in the uk where these white spot which reflect calcium buildup be indeed establish as a predictor of down syndrome but I also notice that the imaging machine use in our test have a few hundred more pixel per square inch than the machine use in the uk study I go back to tell the geneticist that I believe that the white spot be likely false positive — that they be literally white noise she say ah that explain why we start see an uptick in down syndrome diagnose a few year ago ; it s when the new machine arrive we didn t do the amniocentesis and a healthy girl be bear a few month later but the episode trouble I particularly after a back of the envelope calculation convince I that many thousand of people have get that diagnosis that same day worldwide that many of they have opt for amniocentesis and that a number of baby have die needlessly and this happen day after day until it somehow get fix the problem that this episode reveal wasn t about my individual medical care ; it be about a medical system that measure variable and outcome in various place and time conduct statistical analysis and make use of the result in other place and time the problem have to do not just with datum analysis per se but with what database researcher call provenance — broadly where do datum arise what inference be draw from the datum and how relevant be those inference to the present situation while a train human might be able to work all of this out on a case by case basis the issue be that of design a planetary scale medical system that could do this without the need for such detailed human oversight I m also a computer scientist and it occur to I that the principle need to build planetary scale inference and decision make system of this kind blend computer science with statistic and take into account human utility be nowhere to be find in my education and it occur to I that the development of such principle — which will be need not only in the medical domain but also in domain such as commerce transportation and education — be at least as important as those of build ai system that can dazzle we with their game playing or sensorimotor skill whether or not we come to understand intelligence any time soon we do have a major challenge on our hand in bring together computer and human in way that enhance human life while this challenge be view by some as subservient to the creation of artificial intelligence it can also be view more prosaically — but with no less reverence — as the creation of a new branch of engineering much like civil engineering and chemical engineering in decade past this new discipline aim to corral the power of a few key idea bring new resource and capability to people and do so safely whereas civil engineering and chemical engineering be build on physics and chemistry this new engineering discipline will be build on idea that the precede century give substance to — idea such as information algorithm data uncertainty computing inference and optimization moreover since much of the focus of the new discipline will be on datum from and about human its development will require perspective from the social science and humanity while the building block have begin to emerge the principle for put these block together have not yet emerge and so the block be currently be put together in ad hoc way thus just as human build building and bridge before there be civil engineering human be proceed with the building of societal scale inference and decision make system that involve machine human and the environment just as early building and bridge sometimes fall to the ground — in unforeseen way and with tragic consequence — many of our early societal scale inference and decision make system be already expose serious conceptual flaw and unfortunately we be not very good at anticipate what the next emerge serious flaw will be what we re miss be an engineering discipline with its principle of analysis and design the current public dialog about these issue too often use ai as an intellectual wildcard one that make it difficult to reason about the scope and consequence of emerge technology let we begin by consider more carefully what ai have be use to refer to both recently and historically most of what be be call ai today particularly in the public sphere be what have be call machine learning ml for the past several decade ml be an algorithmic field that blend idea from statistic computer science and many other discipline see below to design algorithm that process datum make prediction and help make decision in term of impact on the real world ml be the real thing and not just recently indeed that ml would grow into massive industrial relevance be already clear in the early 1990 and by the turn of the century forward look company such as amazon be already use ml throughout their business solving mission critical back end problem in fraud detection and supply chain prediction and build innovative consumer facing service such as recommendation system as dataset and computing resource grow rapidly over the ensue two decade it become clear that ml would soon power not only amazon but essentially any company in which decision could be tie to large scale datum new business model would emerge the phrase datum science begin to be use to refer to this phenomenon reflect the need of ml algorithms expert to partner with database and distribute system expert to build scalable robust ml system and reflect the large social and environmental scope of the result system this confluence of idea and technology trend have be rebrande as ai over the past few year this rebranding be worthy of some scrutiny historically the phrase ai be coin in the late 1950 s to refer to the heady aspiration of realize in software and hardware an entity possess human level intelligence we will use the phrase human imitative ai to refer to this aspiration emphasize the notion that the artificially intelligent entity should seem to be one of we if not physically at least mentally whatever that might mean this be largely an academic enterprise while relate academic field such as operation research statistic pattern recognition information theory and control theory already exist and be often inspire by human intelligence and animal intelligence these field be arguably focus on low level signal and decision the ability of say a squirrel to perceive the three dimensional structure of the forest it live in and to leap among its branch be inspirational to these field ai be mean to focus on something different — the high level or cognitive capability of human to reason and to think sixty year later however high level reasoning and think remain elusive the development which be now be call ai arise mostly in the engineering field associate with low level pattern recognition and movement control and in the field of statistic — the discipline focus on find pattern in datum and on make well found prediction test of hypothesis and decision indeed the famous backpropagation algorithm that be rediscover by david rumelhart in the early 1980 and which be now view as be at the core of the so call ai revolution first arise in the field of control theory in the 1950 and 1960 one of its early application be to optimize the thrust of the apollo spaceship as they head towards the moon since the 1960s much progress have be make but it have arguably not come about from the pursuit of human imitative ai rather as in the case of the apollo spaceship these idea have often be hide behind the scene and have be the handiwork of researcher focus on specific engineering challenge although not visible to the general public research and system building in area such as document retrieval text classification fraud detection recommendation system personalize search social network analysis planning diagnostic and a b testing have be a major success — these be the advance that have power company such as google netflix facebook and amazon one could simply agree to refer to all of this as ai and indeed that be what appear to have happen such labeling may come as a surprise to optimization or statistic researcher who wake up to find themselves suddenly refer to as ai researcher but labeling of researcher aside the big problem be that the use of this single ill define acronym prevent a clear understanding of the range of intellectual and commercial issue at play the past two decade have see major progress — in industry and academia — in a complementary aspiration to human imitative ai that be often refer to as intelligence augmentation ia here computation and datum be use to create service that augment human intelligence and creativity a search engine can be view as an example of ia it augment human memory and factual knowledge as can natural language translation it augment the ability of a human to communicate computing base generation of sound and image serve as a palette and creativity enhancer for artist while service of this kind could conceivably involve high level reasoning and think currently they don t — they mostly perform various kind of string matching and numerical operation that capture pattern that human can make use of hope that the reader will tolerate one last acronym let we conceive broadly of a discipline of intelligent infrastructure ii whereby a web of computation datum and physical entity exist that make human environment more supportive interesting and safe such infrastructure be begin to make its appearance in domain such as transportation medicine commerce and finance with vast implication for individual human and society this emergence sometimes arise in conversation about an internet of thing but that effort generally refer to the mere problem of get thing onto the internet — not to the far grander set of challenge associate with these thing capable of analyze those datum stream to discover fact about the world and interact with human and other thing at a far high level of abstraction than mere bit for example return to my personal anecdote we might imagine live our life in a societal scale medical system that set up datum flow and datum analysis flow between doctor and device position in and around human body thereby able to aid human intelligence in make diagnosis and provide care the system would incorporate information from cell in the body dna blood test environment population genetic and the vast scientific literature on drug and treatment it would not just focus on a single patient and a doctor but on relationship among all human — just as current medical testing allow experiment do on one set of human or animal to be bring to bear in the care of other human it would help maintain notion of relevance provenance and reliability in the way that the current banking system focus on such challenge in the domain of finance and payment and while one can foresee many problem arise in such a system — involve privacy issue liability issue security issue etc — these problem should properly be view as challenge not show stopper we now come to a critical issue be work on classical human imitative ai the good or only way to focus on these large challenge some of the most herald recent success story of ml have in fact be in area associate with human imitative ai — area such as computer vision speech recognition game playing and robotic so perhaps we should simply await further progress in domain such as these there be two point to make here first although one would not know it from read the newspaper success in human imitative ai have in fact be limit — we be very far from realize human imitative ai aspiration unfortunately the thrill and fear of make even limited progress on human imitative ai give rise to level of over exuberance and medium attention that be not present in other area of engineering second and more importantly success in these domain be neither sufficient nor necessary to solve important ia and ii problem on the sufficiency side consider self drive car for such technology to be realize a range of engineering problem will need to be solve that may have little relationship to human competency or human lack of competency the overall transportation system an ii system will likely more closely resemble the current air traffic control system than the current collection of loosely couple forward face inattentive human driver it will be vastly more complex than the current air traffic control system specifically in its use of massive amount of datum and adaptive statistical modeling to inform fine grain decision it be those challenge that need to be in the forefront and in such an effort a focus on human imitative ai may be a distraction as for the necessity argument it be sometimes argue that the human imitative ai aspiration subsume ia and ii aspiration because a human imitative ai system would not only be able to solve the classical problem of ai as embody e g in the ture test but it would also be our good bet for solve ia and ii problem such an argument have little historical precedent do civil engineering develop by envisage the creation of an artificial carpenter or bricklayer should chemical engineering have be frame in term of create an artificial chemist even more polemically if our goal be to build chemical factory should we have first create an artificial chemist who would have then work out how to build a chemical factory a related argument be that human intelligence be the only kind of intelligence that we know and that we should aim to mimic it as a first step but human be in fact not very good at some kind of reasoning — we have our lapse bias and limitation moreover critically we do not evolve to perform the kind of large scale decision make that modern ii system must face nor to cope with the kind of uncertainty that arise in ii contexts one could argue that an ai system would not only imitate human intelligence but also correct it and would also scale to arbitrarily large problem but we be now in the realm of science fiction — such speculative argument while entertain in the setting of fiction should not be our principal strategy go forward in the face of the critical ia and ii problem that be begin to emerge we need to solve ia and ii problem on their own merit not as a mere corollary to a human imitative ai agenda it be not hard to pinpoint algorithmic and infrastructure challenge in ii system that be not central theme in human imitative ai research ii system require the ability to manage distribute repository of knowledge that be rapidly change and be likely to be globally incoherent such system must cope with cloud edge interaction in make timely distribute decision and they must deal with long tail phenomenon whereby there be lot of datum on some individual and little datum on most individual they must address the difficulty of share datum across administrative and competitive boundary finally and of particular importance ii system must bring economic idea such as incentive and pricing into the realm of the statistical and computational infrastructure that link human to each other and to value good such ii system can be view as not merely provide a service but as create market there be domain such as music literature and journalism that be cry out for the emergence of such market where datum analysis link producer and consumer and this must all be do within the context of evolve societal ethical and legal norm of course classical human imitative ai problem remain of great interest as well however the current focus on do ai research via the gathering of datum the deployment of deep learning infrastructure and the demonstration of system that mimic certain narrowly define human skill — with little in the way of emerge explanatory principle — tend to deflect attention from major open problem in classical ai these problem include the need to bring meaning and reasoning into system that perform natural language process the need to infer and represent causality the need to develop computationally tractable representation of uncertainty and the need to develop system that formulate and pursue long term goal these be classical goal in human imitative ai but in the current hubbub over the ai revolution it be easy to forget that they be not yet solve ia will also remain quite essential because for the foreseeable future computer will not be able to match human in their ability to reason abstractly about real world situation we will need well think out interaction of human and computer to solve our most pressing problem and we will want computer to trigger new level of human creativity not replace human creativity whatever that might mean it be john mccarthy while a professor at dartmouth and soon to take a position at mit who coin the term ai apparently to distinguish his bud research agenda from that of norbert wiener then an old professor at mit wiener have coin cybernetic to refer to his own vision of intelligent system — a vision that be closely tie to operation research statistic pattern recognition information theory and control theory mccarthy on the other hand emphasize the tie to logic in an interesting reversal it be wiener s intellectual agenda that have come to dominate in the current era under the banner of mccarthy s terminology this state of affair be surely however only temporary ; the pendulum swing more in ai than in most field but we need to move beyond the particular historical perspective of mccarthy and wiener we need to realize that the current public dialog on ai — which focus on a narrow subset of industry and a narrow subset of academia — risk blind we to the challenge and opportunity that be present by the full scope of ai ia and ii this scope be less about the realization of science fiction dream or nightmare of super human machine and more about the need for human to understand and shape technology as it become ever more present and influential in their daily life moreover in this understanding and shape there be a need for a diverse set of voice from all walk of life not merely a dialog among the technologically attune focus narrowly on human imitative ai prevent an appropriately wide range of voice from be hear while industry will continue to drive many development academia will also continue to play an essential role not only in provide some of the most innovative technical idea but also in bring researcher from the computational and statistical discipline together with researcher from other discipline whose contribution and perspective be sorely need — notably the social science the cognitive science and the humanity on the other hand while the humanity and the science be essential as we go forward we should also not pretend that we be talk about something other than an engineering effort of unprecedented scale and scope — society be aim to build new kind of artifact these artifact should be build to work as claim we do not want to build system that help we with medical treatment transportation option and commercial opportunity to find out after the fact that these system don t really work — that they make error that take their toll in term of human life and happiness in this regard as I have emphasize there be an engineering discipline yet to emerge for the datum focus and learn focus field as exciting as these latter field appear to be they can not yet be view as constitute an engineering discipline moreover we should embrace the fact that what we be witness be the creation of a new branch of engineer the term engineering be often invoke in a narrow sense — in academia and beyond — with overtone of cold affectless machinery and negative connotation of loss of control by human but an engineering discipline can be what we want it to be in the current era we have a real opportunity to conceive of something historically new — a human centric engineering discipline I will resist give this emerge discipline a name but if the acronym ai continue to be use as placeholder nomenclature go forward let s be aware of the very real limitation of this placeholder let s broaden our scope tone down the hype and recognize the serious challenge ahead michael i jordan from a quick cheer to a stand ovation clap to show how much you enjoy this story michael i jordan be a professor in the department of electrical engineering and computer science and the department of statistic at uc berkeley
Eran Kampf,57,3,https://developerzen.com/data-mining-handling-missing-values-the-database-bd2241882e72?source=tag_archive---------0----------------,datum mining — handle miss value the database developerzen,I ve recently answer predict miss data value in a database on stackoverflow and think it deserve a mention on developerzen one of the important stage of data mining be preprocesse where we prepare the datum for mine real world datum tend to be incomplete noisy and inconsistent and an important task when preprocesse the data be to fill in miss value smooth out noise and correct inconsistency if we specifically look at deal with miss datum there be several technique that can be use choose the right technique be a choice that depend on the problem domain — the datum s domain sale datum crm datum and our goal for the data mining process so how can you handle miss value in your database this be usually do when the class label be miss assume your data mining goal be classification or many attribute be miss from the row not just one however you ll obviously get poor performance if the percentage of such row be high for example let s say we have a database of student enrolment datum age sit score state of residence etc and a column classify their success in college to low medium and high let s say our goal be to build a model predict a student s success in college datum row who be miss the success column be not useful in predict success so they could very well be ignore and remove before run the algorithm decide on a new global constant value like unknown n a or minus infinity that will be use to fill all the miss value this technique be use because sometimes it just doesn t make sense to try and predict the miss value for example let s look at the student enrollment database again assume the state of residence attribute datum be miss for some student fill it up with some state doesn t really make sense as oppose to use something like n a replace miss value of an attribute with the mean or median if its discrete value for that attribute in the database for example in a database of us family income if the average income of a us family be x you can use that value to replace miss income value instead of use the mean or median of a certain attribute calculate by look at all the row in a database we can limit the calculation to the relevant class to make the value more relevant to the row we re look at let s say you have a car pricing database that among other thing classify car to luxury and low budget and you re deal with miss value in the cost field replace miss cost of a luxury car with the average cost of all luxury car be probably more accurate than the value you d get if you factor in the low budget car the value can be determine use regression inference base tool use bayesian formalism decision tree cluster algorithm k mean median etc for example we could use cluster algorithm to create cluster of row which will then be use for calculate an attribute mean or median as specify in technique # 3 another example could be use a decision tree to try and predict the probable value in the miss attribute accord to other attribute in the datum I d suggest look into regression and decision tree first id3 tree generation as they re relatively easy and there be plenty of example on the net additional note originally publish at www developerzen com on august 14 2009 from a quick cheer to a stand ovation clap to show how much you enjoy this story maker of thing big datum geek food lover the essence of software development
Oliver Lindberg,1,7,https://medium.com/the-lindberg-interviews/interview-with-googles-alfred-spector-on-voice-search-hybrid-intelligence-and-more-2f6216aa480c?source=tag_archive---------0----------------,interview with google s alfred spector on voice search hybrid intelligence and more,google s a pretty good search engine right well you ain t see nothing yet vp of research alfred spector talk to oliver lindberg about the technology emerge from google labs — from voice search to hybrid intelligence and beyond this article originally appear in issue 198 of net magazine in 2010 and be republish at www techradar com google have always be tight lip about product that haven t launch yet it s no secret however that thank to the company s bottom up culture its engineer be work on ton of new project at the same time follow the mantra of release early release often the speed at which the search engine giant be churn out tool be staggering at the heart of it all be alfred spector google s vice president of research and special initiative one of the area google be make significant advance in be voice search spector be astounded by how rapidly it s come along the google mobile app feature search by voice capability that be available for the iphone blackberry window mobile and android all version understand english include we uk australian and indian english accent but the late addition for nokia s60 phone even introduce mandarin speech recognition which — because of its many different accent and tonal characteristic — pose a huge engineering challenge it s the most spoken language in the world but as it isn t exactly keyboard friendly voice search could become immensely popular in china voice be one of these grand technology challenge in computer science spector explain can a computer understand the human voice it s be work on for many decade and what we ve realise over the last couple of year be that search particularly on handheld device be amenable to voice as an import mechanism it s very valuable to be able to use voice all of we know that no matter how good the keyboard it s tricky to type exactly the right thing into a searchbar while hold your backpack and everything else to get a computer to take account of your voice be no mean feat of course one idea be to take all of the voice that the system hear over time into one huge pan human voice model so on the one hand we have a voice that s high and with an english accent and on the other hand my voice which be deep and with an american accent they both go into one model or it just become personalise to the individual ; voice scientist be a little unclear as to which be the good approach the research department be also make progress in machine translation google translate already feature 51 language include swahili and yiddish the late version introduce instant real time translation phonetic input and text to speech support in english we re able to go from any language to any of the other and there be 51 time 50 so 2 550 possibility spector explain we re focus on increase the number of language because we d like to handle even those language where there s not an enormous volume of usage it will make the web far more valuable to more people if they can access the english or chinese language web for example but we also continue to focus on quality because almost always the translation be valuable but imperfect sometimes it come from train our translation system over more raw datum so we have say eu document in english and french and can compare they and learn rule for translation the other approach be to bring more knowledge into translation for example we re use more syntactic knowledge today and do automate parsing with language it s be a grand challenge of the field since the late 1950 now it s finally achieve mass usage the team lead by scientist franz josef och have be collect datum for more than 100 language and the google translator toolkit which make use of the wisdom of the crowd now even support 345 language many of which be minority language the editor enable user to translate text correct the automatic translation and publish it spector think that this approach be the future as computer become even fast handle more and more datum — a lot of it in the cloud — machine learn from user and thus become smart he call this concept hybrid intelligence it s very difficult to solve these technological problem without human input he say it s hard to create a robot that s as clever smart and knowledgeable of the world as we human be but it s not as tough to build a computational system like google which extend what we do greatly and gradually learn something about the world from we but that require our interpretation to make it really successful we need to get computer and people communicate in both direction so the computer learn from the human and make the human more effective example of hybrid intelligence be google suggest which instantly offer popular search as you type a search query and the do you mean feature in google search which correct you when you misspell a query in the search bar the more you use it the well the system get training computer to become seemingly more intelligent pose major hurdle for google s engineer computer don t train as efficiently as people do spector explain let s take the chess example if a kasparov be the educator we could count on almost anything he say as be accurate but if you try to learn from a million chess player you learn from my child as well who play chess but they re 10 and eight they ll be right sometimes and not right other time there s noise in that and some of the noise be spam one also have to have careful regard for privacy issue by collect enormous amount of datum google hope to create a powerful database that eventually will understand the relationship between word for example a dog be an animal and a dog have four leg the challenge be to try to establish these relationship automatically use ton of information instead of have expert teach the system this database would then improve search result and language translation because it would have a well understanding of the meaning of the word there s also a lot of research around conceptual search let s take a video of a couple in front of the empire state building we watch the video and it s clear they re on their honeymoon but what be the video about be it about love or honeymoon or be it about rent office space it s a fundamentally challenging problem one example of conceptual search be google image swirl which be add to lab in november enter a keyword and you get a list of 12 image ; click on each one bring up a cluster of related picture click on any of they to expand the wonder wheel further google note that they re not just the most relevant image ; the algorithm determine the most relevant group of image with similar appearance and meaning to improve the world s datum google continue to focus on the importance of the open internet another lab project google fusion table facilitate data management in the cloud it enable user to create table filter and aggregate datum merge it with other datum source and visualise it with google map or the google visualisation api the data set can then be publish share or keep private and comment on by people around the world it s an example of open collaboration spector say if it s public we can crawl it to make it searchable and easily visible to people we hire one of the good database researcher in the world alon halevy to lead it google be aim to make more information available more easily across multiple device whether it s image video speech or map no matter which language we re use spector call the impact totally transparent processing — it revolutionise the role of computation in day today life the computer can break down all these barrier to communication and knowledge no matter what device we re use we have access to thing we can do translation there be book or government document and some day we hope to have medical record whatever you want no matter where you be you can find it spector retire in early 2015 and now serve as the cto of two sigma investment this article originally appear in issue 198 of net magazine in 2010 and be republish at www techradar com photography by andy short from a quick cheer to a stand ovation clap to show how much you enjoy this story independent editor and content consultant founder and captain of @pixelpioneers co founder and curator of www generateconf com former editor of @netmag interview with lead tech entrepreneur and web designer conduct by @oliverlindberg at @netmag
Xu Wenhao,1,4,https://xuwenhao.com/%E5%BB%BA%E8%AE%AE%E7%9A%84%E7%A8%8B%E5%BA%8F%E5%91%98%E5%AD%A6%E4%B9%A0lda%E7%AE%97%E6%B3%95%E7%9A%84%E6%AD%A5%E9%AA%A4-54168e081bc1?source=tag_archive---------0----------------,建议的程序员学习lda算法的步骤 蒸汽与魔法,这一阵为了工作上的关系 花了点时间学习了一下lda算法 说实话 对于我这个学cs而非学数学的人来说 除了集体智慧编程这本书之外基本没怎么看过机器学习的人来说 一开始还真是摸不太到门道 前前后后快要四个月了 算是基本了解了这个算法的实现 记录一下 也供后来人快速入门做个参考 。 一开始直接就下了blei的原始的那篇论文来看 但是看了个开头就被dirichlet分布和几个数学公式打倒 然后因为专心在写项目中的具体的代码 也就先放下了。但是因为发现完全忘记了本科学的概率和统计的内容 只好回头去看大学时候概率论的教材 发现早不知道借给谁了 于是上网买了本 花了几天时间大致回顾了一遍概率论的知识 什么贝叶斯全概率公式 正态分布 二项分布之类的。后来晚上没事儿的时候 去水木的ai版转了转 了解到了machine learning的圣经prml 考虑到反正也是要长期学习了 搞了电子版 同时上淘宝买了个打印胶装的版本。春节里每天晚上看一点儿 扫了一下前两章 再次回顾了一下基本数学知识 然后了解了下贝叶斯学派那种采用共轭先验来建模的方式。于是再次尝试回头去看blei的那篇论文 发现还是看不太懂 于是又放下了。然后某天tony让我准备准备给复旦的同学们share一下我们项目中lda的使用 为了不露怯 又去翻论文 正好看到science上这篇topic model vs unstructured data的科普性质的文章 翻了一遍之后 再去prml里看了一遍graphic models那一张 觉得对于lda想解决的问题和方法了解了更清楚了。之后从search engine里搜到这篇文章 然后根据推荐读了一部分的gibbs sample for the uninitiated。之后忘了怎么又搜到了mark steyvers和tom griffiths合著的probabilistic topic model 在某个周末往返北京的飞机上读完了 觉得基本上模型训练过程也明白了。再之后就是读了一下这个最简版的lda gibbs sampling的实现 再回过头读了一下plda的源码 基本上算是对lda有了个相对清楚的了解 。 这样前前后后 也过去了三个月 其实不少时间都是浪费掉的 比如blei的论文在没有任何相关知识的情况下一开始读了好几次 都没读完而且得到到信息也很有限 如果重新总结一下 我觉得对于我们这些门外汉程序员来说 想了解lda大概需要这些知识 基本上这样一圈下来 基本概念和算法实现都应该搞定了 当然 数学证明其实没那么容易就搞定 但是对于工程师来说 先把这些搞定就能干活了 这个步骤并不适合各位读博士发论文的同学们 但是这样先看看也比较容易对于这些数学问题的兴趣 不然 成天对这符号和数学公式 没有整块业余时间的我是觉得还是容易退缩放弃的 。 发现作为工程师来说 还是看代码比较有感觉 看实际应用的实例比较有感觉 看来不能把大部分时间花在prml上 还是要多对照着代码看 。 from a quick cheer to a stand ovation clap to show how much you enjoy this story facebook messenger & chatbot machine learning & big datum 生命如此短暂 掌握技艺却要如此长久
Netflix Technology Blog,439,9,https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429?source=tag_archive---------0----------------,netflix recommendation beyond the 5 star part 1,by xavier amatriain and justin basilico personalization science and engineering in this two part blog post we will open the door of one of the most value netflix asset our recommendation system in part 1 we will relate the netflix prize to the broad recommendation challenge outline the external component of our personalized service and highlight how our task have evolve with the business in part 2 we will describe some of the datum and model that we use and discuss our approach to algorithmic innovation that combine offline machine learning experimentation with online ab testing enjoy and remember that we be always look for more star talent to add to our great team so please take a look at our job page in 2006 we announce the netflix prize a machine learning and data mining competition for movie rating prediction we offer $ 1 million to whoever improve the accuracy of our exist system call cinematch by 10 % we conduct this competition to find new way to improve the recommendation we provide to our member which be a key part of our business however we have to come up with a proxy question that be easy to evaluate and quantify the root mean square error rmse of the predict rating the race be on to beat our rmse of 0 9525 with the finish line of reduce it to 0 8572 or less a year into the competition the korbell team win the first progress prize with an 8 43 % improvement they report more than 2000 hour of work in order to come up with the final combination of 107 algorithm that give they this prize and they give we the source code we look at the two underlie algorithm with the good performance in the ensemble matrix factorization which the community generally call svd singular value decomposition and restrict boltzmann machines rbm svd by itself provide a 0 8914 rmse while rbm alone provide a competitive but slightly bad 0 8990 rmse a linear blend of these two reduce the error to 0 88 to put these algorithm to use we have to work to overcome some limitation for instance that they be build to handle 100 million rating instead of the more than 5 billion that we have and that they be not build to adapt as member add more rating but once we overcome those challenge we put the two algorithm into production where they be still use as part of our recommendation engine if you follow the prize competition you might be wonder what happen with the final grand prize ensemble that win the $ 1 m two year later this be a truly impressive compilation and culmination of year of work blend hundred of predictive model to finally cross the finish line we evaluate some of the new method offline but the additional accuracy gain that we measure do not seem to justify the engineering effort need to bring they into a production environment also our focus on improve netflix personalization have shift to the next level by then in the remainder of this post we will explain how and why it have shift one of the reason our focus in the recommendation algorithms have change be because netflix as a whole have change dramatically in the last few year netflix launch an instant streaming service in 2007 one year after the netflix prize begin streaming have not only change the way our member interact with the service but also the type of datum available to use in our algorithm for dvds our goal be to help people fill their queue with title to receive in the mail over the come day and week ; selection be distant in time from view people select carefully because exchange a dvd for another take more than a day and we get no feedback during view for streaming member be look for something great to watch right now ; they can sample a few video before settle on one they can consume several in one session and we can observe view statistic such as whether a video be watch fully or only partially another big change be the move from a single website into hundred of device the integration with the roku player and the xbox be announce in 2008 two year into the netflix competition just a year later netflix streaming make it into the iphone now it be available on a multitude of device that go from a myriad of android device to the late appletv two year ago we go international with the launch in canada in 2011 we add 43 latin american country and territory to the list and just recently we launch in uk and ireland today netflix have more than 23 million subscriber in 47 country those subscriber stream 2 billion hour from hundred of different device in the last quarter of 2011 every day they add 2 million movie and tv show to the queue and generate 4 million rating we have adapt our personalization algorithm to this new scenario in such a way that now 75 % of what people watch be from some sort of recommendation we reach this point by continuously optimize the member experience and have measure significant gain in member satisfaction whenever we improve the personalization for our member let we now walk you through some of the technique and approach that we use to produce these recommendation we have discover through the year that there be tremendous value to our subscriber in incorporate recommendation to personalize as much of netflix as possible personalization start on our homepage which consist of group of video arrange in horizontal row each row have a title that convey the intend meaningful connection between the video in that group most of our personalization be base on the way we select row how we determine what item to include in they and in what order to place those item take as a first example the top 10 row this be our good guess at the ten title you be most likely to enjoy of course when we say you we really mean everyone in your household it be important to keep in mind that netflix personalization be intend to handle a household that be likely to have different people with different taste that be why when you see your top10 you be likely to discover item for dad mom the kid or the whole family even for a single person household we want to appeal to your range of interest and mood to achieve this in many part of our system we be not only optimize for accuracy but also for diversity another important element in netflix personalization be awareness we want member to be aware of how we be adapt to their taste this not only promote trust in the system but encourage member to give feedback that will result in well recommendation a different way of promote trust with the personalization component be to provide explanation as to why we decide to recommend a give movie or show we be not recommend it because it suit our business need but because it match the information we have from you your explicit taste preference and rating your view history or even your friend recommendation on the topic of friend we recently release our facebook connect feature in 46 out of the 47 country we operate — all but the us because of concern with the vppa law know about your friend not only give we another signal to use in our personalization algorithm but it also allow for different row that rely mostly on your social circle to generate recommendation some of the most recognizable personalization in our service be the collection of genre row these range from familiar high level category like comedy and drama to highly tailor slice such as imaginative time travel movie from the 1980 each row represent 3 layer of personalization the choice of genre itself the subset of title select within that genre and the ranking of those title member connect with these row so well that we measure an increase in member retention by place the most tailor row high on the page instead of low as with other personalization element freshness and diversity be take into account when decide what genre to show from the thousand possible we present an explanation for the choice of row use a member s implicit genre preference — recent play rating and other interaction — or explicit feedback provide through our taste preference survey we will also invite member to focus a row with additional explicit preference feedback when this be lack similarity be also an important source of personalization in our service we think of similarity in a very broad sense ; it can be between movie or between member and can be in multiple dimension such as metadata rating or view datum furthermore these similarity can be blend and use as feature in other model similarity be use in multiple context for example in response to a member s action such as search or add a title to the queue it be also use to generate row of adhoc genre base on similarity to title that a member have interact with recently if you be interested in a more in depth description of the architecture of the similarity system you can read about it in this past post on the blog in most of the previous contexts — be it in the top10 row the genre or the similar — rank the choice of what order to place the item in a row be critical in provide an effective personalized experience the goal of our rank system be to find the good possible ordering of a set of item for a member within a specific context in real time we decompose rank into scoring sort and filtering set of movie for presentation to a member our business objective be to maximize member satisfaction and month to month subscription retention which correlate well with maximize consumption of video content we therefore optimize our algorithm to give the high score to title that a member be most likely to play and enjoy now it be clear that the netflix prize objective accurate prediction of a movie s rating be just one of the many component of an effective recommendation system that optimize our member enjoyment we also need to take into account factor such as context title popularity interest evidence novelty diversity and freshness support all the different context in which we want to make recommendation require a range of algorithm that be tune to the need of those context in the next part of this post we will talk in more detail about the rank problem we will also dive into the datum and model that make all the above possible and discuss our approach to innovate in this space on to part 2 originally publish at techblog netflix com on april 6 2012 from a quick cheer to a stand ovation clap to show how much you enjoy this story learn more about how netflix design build and operate our system and engineering organization learn about netflix s world class engineering effort company culture product development and more
Netflix Technology Blog,365,10,https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-2-d9b96aa399f5?source=tag_archive---------1----------------,netflix recommendation beyond the 5 star part 2,by xavier amatriain and justin basilico personalization science and engineering in part one of this blog post we detail the different component of netflix personalization we also explain how netflix personalization and the service as a whole have change from the time we announce the netflix prize the $ 1 m prize deliver a great return on investment for we not only in algorithmic innovation but also in brand awareness and attract star no pun intend to join our team predict movie rating accurately be just one aspect of our world class recommender system in this second part of the blog post we will give more insight into our broad personalization technology we will discuss some of our current model datum and the approach we follow to lead innovation and research in this space the goal of recommend system be to present a number of attractive item for a person to choose from this be usually accomplish by select some item and sort they in the order of expect enjoyment or utility since the most common way of present recommend item be in some form of list such as the various row on netflix we need an appropriate ranking model that can use a wide variety of information to come up with an optimal ranking of the item for each of our member if you be look for a ranking function that optimize consumption an obvious baseline be item popularity the reason be clear on average a member be most likely to watch what most other be watch however popularity be the opposite of personalization it will produce the same ordering of item for every member thus the goal become to find a personalized ranking function that be well than item popularity so we can well satisfy member with vary taste recall that our goal be to recommend the title that each member be most likely to play and enjoy one obvious way to approach this be to use the member s predict rating of each item as an adjunct to item popularity use predict rating on their own as a rank function can lead to item that be too niche or unfamiliar be recommend and can exclude item that the member would want to watch even though they may not rate they highly to compensate for this rather than use either popularity or predict rating on their own we would like to produce ranking that balance both of these aspect at this point we be ready to build a ranking prediction model use these two feature there be many way one could construct a ranking function range from simple scoring method to pairwise preference to optimization over the entire ranking for the purpose of illustration let we start with a very simple scoring approach by choose our ranking function to be a linear combination of popularity and predict rating this give an equation of the form frank u v = w1 p v + w2 r u v + b where u = user v = video item p = popularity and r = predict rate this equation define a two dimensional space like the one depict below once we have such a function we can pass a set of video through our function and sort they in descend order accord to the score you might be wonder how we can set the weight w1 and w2 in our model the bias b be constant and thus end up not affect the final ordering in other word in our simple two dimensional model how do we determine whether popularity be more or less important than predict rating there be at least two possible approach to this you could sample the space of possible weight and let the member decide what make sense after many a b test this procedure might be time consume and not very cost effective another possible answer involve formulate this as a machine learning problem select positive and negative example from your historical datum and let a machine learn algorithm learn the weight that optimize your goal this family of machine learning problem be know as learn to rank and be central to application scenario such as search engine or ad target note though that a crucial difference in the case of rank recommendation be the importance of personalization we do not expect a global notion of relevance but rather look for way of optimize a personalized model as you might guess apart from popularity and rating prediction we have try many other feature at netflix some have show no positive effect while other have improve our rank accuracy tremendously the graph below show the rank improvement we have obtain by add different feature and optimize the machine learn algorithm many supervised classification method can be use for rank typical choice include logistic regression support vector machine neural network or decision tree base method such as gradient boost decision tree gbdt on the other hand a great number of algorithm specifically design for learn to rank have appear in recent year such as ranksvm or rankboost there be no easy answer to choose which model will perform well in a give rank problem the simple your feature space be the simple your model can be but it be easy to get trap in a situation where a new feature do not show value because the model can not learn it or the other way around to conclude that a more powerful model be not useful simply because you don t have the feature space that exploit its benefit the previous discussion on the rank algorithms highlight the importance of both datum and model in create an optimal personalized experience for our member at netflix we be fortunate to have many relevant data source and smart people who can select optimal algorithm to turn datum into product feature here be some of the data source we can use to optimize our recommendation so what about the model one thing we have find at netflix be that with the great availability of datum both in quantity and type a thoughtful approach be require to model selection training and testing we use all sort of machine learning approach from unsupervised method such as cluster algorithm to a number of supervised classifier that have show optimal result in various contexts this be an incomplete list of method you should probably know about if you be work in machine learning for personalization consumer datum science the abundance of source datum measurement and associate experiment allow we to operate a data drive organization netflix have embed this approach into its culture since the company be found and we have come to call it consumer datum science broadly speak the main goal of our consumer science approach be to innovate for member effectively the only real failure be the failure to innovate ; or as thomas watson sr founder of ibm put it if you want to increase your success rate double your failure rate we strive for an innovation culture that allow we to evaluate idea rapidly inexpensively and objectively and once we test something we want to understand why it fail or succeed this let we focus on the central goal of improve our service for our member so how do this work in practice it be a slight variation over the traditional scientific process call a b testing or bucket testing when we execute a b test we track many different metric but we ultimately trust member engagement e g hour of play and retention test usually have thousand of member and anywhere from 2 to 20 cell explore variation of a base idea we typically have score of a b test run in parallel a b test let we try radical idea or test many approach at the same time but the key advantage be that they allow our decision to be datum drive you can read more about our approach to a b testing in this previous tech blog post or in some of the quora answer by our chief product officer neil hunt an interesting follow up question that we have face be how to integrate our machine learning approach into this datum drive a b test culture at netflix we have do this with an offline online testing process that try to combine the good of both world the offline testing cycle be a step where we test and optimize our algorithm prior to perform online a b testing to measure model performance offline we track multiple metric use in the machine learn community from rank measure such as normalize discount cumulative gain mean reciprocal rank or fraction of concordant pair to classification metric such as accuracy precision recall or f score we also use the famous rmse from the netflix prize or other more exotic metric to track different aspect like diversity we keep track of how well those metric correlate to measurable online gain in our a b test however since the mapping be not perfect offline performance be use only as an indication to make informed decision on follow up test once offline testing have validate a hypothesis we be ready to design and launch the a b test that will prove the new feature valid from a member perspective if it do we will be ready to roll out in our continuous pursuit of the well product for our member the diagram below illustrate the detail of this process an extreme example of this innovation cycle be what we call the top10 marathon this be a focused 10 week effort to quickly test dozen of algorithmic idea relate to improve our top10 row think of it as a 2 month hackathon with metric different team and individual be invite to contribute idea and code in this effort we roll out 6 different idea as a b test each week and keep track of the offline and online metric the win result be already part of our production system the netflix prize abstract the recommendation problem to a proxy question of predict rating but member rating be only one of the many datum source we have and rating prediction be only part of our solution over time we have reformulate the recommendation problem to the question of optimize the probability a member choose to watch a title and enjoy it enough to come back to the service more data availability enable well result but in order to get those result we need to have optimize approach appropriate metric and rapid experimentation to excel at innovate personalization it be insufficient to be methodical in our research ; the space to explore be virtually infinite at netflix we love choose and watch movie and tv show we focus our research by translate this passion into strong intuition about fruitful direction to pursue ; under utilize data source well feature representation more appropriate model and metric and miss opportunity to personalize we use data mining and other experimental approach to incrementally inform our intuition and so prioritize investment of effort as with any scientific pursuit there s always a contribution from lady luck but as the adage go luck favor the prepared mind finally above all we look to our member as the final judge of the quality of our recommendation approach because this be all ultimately about increase our member enjoyment in their own netflix experience we be always look for more people to join our team of prepared mind make sure you take a look at our job page originally publish at techblog netflix com on june 20 2012 from a quick cheer to a stand ovation clap to show how much you enjoy this story learn more about how netflix design build and operate our system and engineering organization learn about netflix s world class engineering effort company culture product development and more
Wolf Garbe,6,6,https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f?source=tag_archive---------2----------------,1000x fast spelling correction algorithm 2012 wolf garbe medium,update1 an improve symspell implementation be now 1 000 000x fast update2 symspellcompound with compound aware spelling correction update3 benchmark of symspell bk tree und norvig s spell correct recently I answer a question on quora about spelling correction for search engine when I describe our symspell algorithm I be point to peter norvig s page where he outline his approach both algorithm be base on edit distance damerau levenshtein distance both try to find the dictionary entry with small edit distance from the query term if the edit distance be 0 the term be spell correctly if the edit distance be < = 2 the dictionary term be use as spelling suggestion but symspell use a different way to search the dictionary result in a significant performance gain and language independence three way to search for minimum edit distance in a dictionary 1 naive approachthe obvious way of do this be to compute the edit distance from the query term to each dictionary term before select the string s of minimum edit distance as spelling suggestion this exhaustive search be inordinately expensive source christopher d man prabhakar raghavan & hinrich schütze introduction to information retrieval the performance can be significantly improve by terminate the edit distance calculation as soon as a threshold of 2 or 3 have be reach 2 peter norviggenerate all possible term with an edit distance delete + transpose + replace + insert from the query term and search they in the dictionary for a word of length n an alphabet size a an edit distance d=1 there will be n deletion n 1 transposition a*n alteration and a * n+1 insertion for a total of 2n+2an+a 1 term at search time source peter norvig how to write a spelling corrector this be much well than the naive approach but still expensive at search time 114 324 term for n=9 a=36 d=2 and language dependent because the alphabet be use to generate the term which be different in many language and huge in chinese a=70 000 unicode han character 3 symmetric delete spelling correction symspell generate term with an edit distance delete only from each dictionary term and add they together with the original term to the dictionary this have to be do only once during a pre calculation step generate term with an edit distance delete only from the input term and search they in the dictionary for a word of length n an alphabet size of a an edit distance of 1 there will be just n deletion for a total of n term at search time this be three order of magnitude less expensive 36 term for n=9 and d=2 and language independent the alphabet be not require to generate delete the cost of this approach be the pre calculation time and storage space of x delete for every original dictionary entry which be acceptable in most case the number x of delete for a single dictionary entry depend on the maximum edit distance x = n for edit distance=1 x = n * n 1 2 for edit distance=2 x = n d n d for edit distance = d combinatoric k out of n combination without repetition and k = n d e g for a maximum edit distance of 2 and an average word length of 5 and 100 000 dictionary entry we need to additionally store 1 500 000 delete remark 1 during the precalculation different word in the dictionary might lead to same delete term delete sun 1 = = delete sin 1 = = sn while we generate only one new dictionary entry sn inside we need to store both original term as spelling correction suggestion sun sin remark 2 there be four different comparison pair type the last comparison type be require for replace and transpose only but we need to check whether the suggest dictionary term be really a replace or an adjacent transpose of the input term to prevent false positive of high edit distance bank==bnak and bank==bink but bank = kanb and bank = xban and bank = baxn remark 3 instead of a dedicated spelling dictionary we be use the search engine index itself this have several benefit remark 4 we have implement query suggestion completion in a similar fashion this be a good way to prevent spelling error in the first place every newly index word whose frequency be over a certain threshold be store as a suggestion to all of its prefix they be create in the index if they do not yet exist as we anyway provide an instant search feature the lookup for suggestion come also at almost no extra cost multiple term be sort by the number of result store in the index reasoningthe symspell algorithm exploit the fact that the edit distance between two term be symmetrical we be use variant 3 because the delete only transformation be language independent and three order of magnitude less expensive where do the speed come from computational complexity the symspell algorithm be constant time o 1 time I e independent of the dictionary size but depend on the average term length and maximum edit distance because our index be base on a hash table which have an average search time complexity of o 1 comparison to other approach bk tree have a search time of o log dictionary_size whereas the symspell algorithm be constant time o 1 time I e independent of the dictionary size try have a comparable search performance to our approach but a trie be a prefix tree which require a common prefix this make it suitable for autocomplete or search suggestion but not applicable for spell checking if your type error be e g in the first letter than you have no common prefix hence the trie will not work for spelling correction application possible application field of the symspell algorithm be those of fast approximate dictionary string matching spell checker for word processor and search engine correction system for optical character recognition natural language translation base on translation memory record linkage de duplication match dna sequence fuzzy string search and fraud detection source codethe c # implementation of the symmetric delete spelling correction algorithm be release on github as open source under the mit license https github com wolfgarbe symspell portsthere be port in c++ crystal go java javascript python ruby rust scala swift available originally publish at blog faroo com on june 7 2012 from a quick cheer to a stand ovation clap to show how much you enjoy this story founder seekstorm search as a service faroo p2p search http www seekstorm com https github com wolfgarbe https www quora com profile wolf garbe
Paul Christiano,43,31,https://ai-alignment.com/a-formalization-of-indirect-normativity-7e44db640160?source=tag_archive---------3----------------,formalize indirect normativity ai alignment,this post outline a formalization of what nick bostrom call indirect normativity I don t think it s an adequate solution to the ai control problem ; but to my knowledge it be the first precise specification of a goal that meet the not terrible bar I e which do not lead to terrible consequence if pursue without any caveat or restriction the proposal outline here be sketch in early 2012 while I be visit fhi and be my first serious foray into ai control when face with the challenge of write down precise moral principle adhere to the standard demand in mathematics moral philosopher encounter two serious difficulty in light of these difficulty a moral philosopher might simply declare it be not my place to aspire to mathematical standard of precision ethic as a project inherently require shared language understanding and experience ; it become impossible or meaningless without they this may be a defensible philosophical position but unfortunately the issue be not entirely philosophical in the interest of build institution or machine which reliably pursue what we value we may one day be force to describe precisely what we value in a way that do not depend on charitable or common sense interpretation in the same way that we today must describe what we want do precisely to computer often with considerable effort if some aspect of our value can not be describe formally then it may be more difficult to use institution or machine to reliably satisfy they this be not to say that describe our value formally be necessary to satisfy they merely that it might make it easy since we be focus on find any precise and satisfactory moral theory rather than resolve dispute in moral philosophy we will adopt a consequentialist approach without justification and focus on axiology moreover we will begin from the standpoint of expect utility maximization and leave aside question about how or over what space the maximization be perform we aim to mathematically define a utility function u such that we would be willing to build a hypothetical machine which exceptionlessly maximize u possibly at the catastrophic expense of any other value we will assume that the machine have an ability to reason which at least rival that of human and be willing to tolerate arbitrarily complex definition of u within its ability to reason about they we adopt an indirect approach rather than specify what exactly we want we specify a process for determine what we want this process be extremely complex so that any computationally limited agent will always be uncertain about the process output however by reasoning about the process it be possible to make judgment about which action have the high expect utility in light of this uncertainty for example I might adopt the principle a state of affair be valuable to the extent that I would judge it valuable after a century of reflection in general I will be uncertain about what I would say after a century but I can act on the basis of my good guess after a century I will probably prefer world with more happiness and so today I should prefer world with more happiness after a century I have only a small probability of value tree feeling and so today I should go out of my way to avoid hurt they if it be either instrumentally useful or extremely easy as I spend more time think my belief about what I would say after a century may change and I will start to pursue different state of affair even though the formal definition of my value be static similarly I might desire to think about the value of tree feeling if I expect that my opinion be unstable if I spend a month think about tree my current view will then be a much well predictor of my view after a hundred year and if I know well whether or not tree feeling be valuable I can make well decision this example be quite informal but it communicate the main idea of the approach we stress that the value of our contribution if any be in the possibility of a precise formulation our proposal itself will be relatively informal ; instead it be a description of how you would arrive at a precise formulation the use of indirection seem to be necessary to achieve the desire level of precision our proposal contain only two explicit step each of these step require substantial elaboration but we must also specify what we expect the human to do with these tool this proposal be well understand in the context of other fantastic seeming proposal such as my utility be whatever I would write down if I reflect for a thousand year without interruption or biological decay the counterfactual event which take place within the definition be far beyond the realm our intuition recognize as realistic and have no place except in thought experiment but to the extent that we can reason about these counterfactual and change our behavior on the basis of that reasoning if so motivated we can already see how such fantastic situation could affect our more prosaic reality the remainder of this document consist of brief elaboration of some of these step and a few argument about why this be a desirable process the first step of our proposal be a high fidelity mathematical model of human cognition we will set aside philosophical trouble and assume that the human brain be a purely physical system which may be characterize mathematically even grant this it be not clear how we can realistically obtain such a characterization the most obvious approach to characterize a brain be to combine measurement of its behavior or architecture with an understanding of biology chemistry and physic this project represent a massive engineering effort which be currently just begin most pessimistically our proposal could be postpone until this project s completion this could still be long before the mathematical characterization of the brain become useful for run experiment or automate human activity because we be interested only in a definition we do not care about have the computational resource necessary to simulate the brain an impractical mathematical definition however may be much easy to obtain we can define a model of a brain in term of exhaustive search which could never be practically carry out for example give some observation of a neuron we can formally define a brute force search for a model of that neuron similarly give model of individual neuron we may be able to specify a brute force search over all way of connect those neuron which account for our observation of the brain say some datum acquire through functional neuroimaging it may be possible to carry out this definition without exploit any structural knowledge about the brain beyond what be necessary to measure it effectively by collect image datum for a human expose to a wide variety of stimulus we can recover a large corpus of datum which must be explain by any model of a human brain moreover by use our explicit knowledge of human cognition we can algorithmically generate an extensive range of test which identify a successful simulation by probe response to question or performance on game or puzzle in fact this project may be possible use exist resource the complexity of the human brain be not as unapproachable as it may at first appear though it may contain 1014synapse each describe by many parameter it can be specify much more compactly a newborn s brain can be specify by about 109bits of genetic information together with a recipe for a physical simulation of development the human brain appear to form new long term memory at a rate of 1 2 bit per second suggest that it may be possible to specify an adult brain use 109additional bit of experiential information this suggest that it may require only about 1010bit of information to specify a human brain which be at the limit of what can be reasonably collect by exist technology for functional neuroimage this discussion have gloss over at least one question what do we mean by brain emulation human cognition do not reside in a physical system with sharp boundary and it be not clear how you would define or use a simulation of the input output behavior of such an object we will focus on some system which do have precisely define input output behavior and which capture the important aspect of human cognition consider a system contain a human a keyboard a monitor and some auxiliary instrument well insulate from the environment except for some wire carry input to the monitor and output from the keyboard and auxiliary instrument and wire carry power the input to this system be simply screen to be display on the monitor say deliver as a sequence to be display one after another at 30 frame per second while the output be the information convey from the keyboard and the other measuring apparatus also deliver as a sequence of datum dump each recording activity from the last 30th of a second this human in a box system can be easily formally define if a precise description of a human brain and coarse description of the human body and the environment be available alternatively the input output behavior of the human in a box can be directly observe and a computational model construct for the entire system let h be a mathematical definition of the result randomized function from input sequence in 1 in 2 in k to the next output out k h be by design a good approximation to what the human would output if present with any particular input sequence use h we can mathematically define what would happen if the human interact with a wide variety of system for example if we deliver out k as the input to an abstract computer run some arbitrary software and then define in k+1 as what the screen would next display we can mathematically define the distribution over transcript which would have arise if the human have interact with the abstract computer this computer could be run an interactive shell a video game or a message client note that h reflect the behavior of a particular human in a particular mental state this state be determine by the process use to design h or the datum use to learn it in general we can control h by choose an appropriate human and provide appropriate instruction train more emulation could be produce by similar measure if necessary use only a single human may seem problematic but we will not rely on this lone individual to make all relevant ethical judgment instead we will try to select a human with the motivational stability to carry out the subsequent step faithfully which will define u use the judgment of a community consist of many human this discussion have be brief and have necessarily gloss over several important difficulty one difficulty be the danger of use computationally unbounded brute force search give the possibility of short program which exhibit goal orient behavior another difficulty be that unless the emulation project be extremely conservative the model it produce be not likely to be fully functional human their thought may be blur in various way they may be miss many memory or skill and they may lack important functionality such as long term memory formation or emotional expression the scope of these issue depend on the availability of datum from which to learn the relevant aspect of human cognition realistic proposal along these line will need to accommodate these shortcoming rely on distorted emulation as a tool to construct increasingly accurate model for any idealized software with a distinguished instruction return we can use h to mathematically define the distribution over return value which would result if the human be to interact with that software we will informally define a particular program t which provide a rich environment in which the remainder of our proposal can be implement from a technical perspective this will be the last step of our proposal the remain step will be reflect only in the intention and behavior of the human being simulate in h fix a convenient and adequately expressive language say a dialect of python design to run on an abstract machine t implement a standard interface for an interactive shell in this language the user can look through all of the past instruction that have be execute and their return value render as string or execute a new instruction we also provide symbol represent h and t themselves as function from sequence of k inputs to a value for the kth output we also provide some useful information such as a snapshot of the internet and some information about the process use to create h and t which we encode as a bit string and store in a single environment variable datum we assume that our language of choice have a return instruction and we have t return whenever the user execute this instruction some care need to be take to define the behavior if t enter an infinite loop we want to minimize the probability that the human accidentally hang the terminal with catastrophic consequence but we can not provide a complete safety net without run into unresolvable issue with self reference we define u to be the value return by h interact with t if h represent an unfortunate mental state then this interaction could be short and unproductive the simulated human could just decide to type return 0 and be do with it however by choose an appropriate human to simulate and inculcate an appropriate mental state we can direct the process far we intend for h to use the resource in t to initiate a large deliberative process for example the first step of this process may be to instantiate many copy of h interact with variant of message client which be in contact with each other the return value from the original process could then be define as the value return by a designate leader from this community or as a majority vote amongst the copy of h or so on another step might be to create appropriate realistic virtual environment for simulated brain rather than confine they to box for motivational stability it may be helpful to design various coordination mechanism involve framework for interaction cache mental state which be frequently re instantiate or sanity check whereby one copy of h monitor the behavior of another the result community of simulated brain then engage in a protract planning process ensure that subsequent step can be carry out safely or develop alternative approach the main priority of this community be to reduce the probability of error as far as possible exactly what constitute an error will be discuss at more length later at the end of this process we obtain a formal definition of a new protocol h+ which submit its input for consideration to a large community and then produce its output use some deliberation mechanism democratic vote one leader use the rest of the community as advisor etc the next step require our community of simulate brain to construct a detailed simulation of earth which they can observe and manipulate once they have such a simulation they have access to all of the datum which would have be available on earth in particular they can now explore many possible future and construct simulation for each live human in order to locate earth we will again leverage an exhaustive search first h+ decide on informal desiderata for an earth simulation these be likely to be as follow once h+ have decide on the desiderata it use a brute force search to find a simulation satisfy they for each possible program it instantiate a new copy of h+ task with evaluate whether that program be an acceptable simulation we then define e to be a uniform distribution over program which pass this evaluation we might have doubt about whether this process produce the real earth perhaps even once we have verify that it be identical accord to a laundry list of measure it may still be different in other important way there be two reason why we might care about such difference first if the simulated earth have a substantially different set of people than the real earth then a different set of people will be involve in the subsequent decision make if we care particularly about the opinion of the people who actually exist which the reader might well be amongst such people then this may be unsatisfactory second if event transpire significantly differently on the simulated earth than the real earth value judgment design to guide behavior appropriately in the simulated earth may lead to less appropriate behavior in the real earth this will not be a problem if our ultimate definition of u consist of universalizable ethical principle but we will see that u might take other form these concern be address by a few broad argument first check a detailed but arbitrary laundry list actually provide a very strong guarantee for example if this laundry list include verify a snapshot of the internet then every event or person document on the internet must exist unchanged and every keystroke of every person compose a document on the internet must not be disturb if the world be well interconnected then it may be very difficult to modify part of the world without have substantial effect elsewhere and so if a long enough arbitrary list of property be fix we expect nearly all of the world to be the same as well second if the essential character of the world be fix but detailed be varied we should expect the sort of moral judgment reach by consensus to be relatively constant finally if the system whose behavior depend on these moral judgment be identical between the real and simulated world then output a u which cause that system to behave a certain way in the simulated world will also cause that system to behave that way in the real world once h+ have define a simulation of the world which permit inspection and intervention by careful trial and error h+ can inspect a variety of possible future in particular they can find intervention which cause the simulated human society to conduct a real brain emulation project and produce high fidelity brain scan for all live human once these scan have be obtain h+ can use they to define u as the output of a new community h++ which draw on the expertise of all live human operate under ideal condition there be two important degree of flexibility how to arrange the community for efficient communication and deliberation and how to delegate the authority to define u in term of organization the distinction between different approach be probably not very important for example it would probably be perfectly satisfactory to start from a community of human interact with each other over something like the exist internet but on abstract secure infrastructure more important be the safety measure which would be in place and the mechanism for resolve difference of value between different simulate human the basic approach to resolve dispute be to allow each human to independently create a utility function u each bound in the interval 0 1 and then to return their average this average can either be unweighte or can be weight by a measure of each individual s influence in the real world in accordance with a game theoretic notion like the shapley value apply to abstract game or simulation of the original world more sophisticated mechanism be also possible and may be desirable of course these question can and should be address in part by h+ during its deliberation in the previous step after all h+ have access to an unlimited length of time to deliberate and have infinitely powerful computational aid the role of our reasoning at this stage be simply to suggest that we can reasonably expect h+ to discover effective solution as when discuss discover a brain simulation by brute force we have skip over some critical issue in this section in general brute force search particularly over program which we would like to run be quite dangerous because such search will discover many program with destructive goal orient behavior to deal with these issue in both case we must rely on patience and powerful safety measure once we have a formal description of a community of interact human give as much time as necessary to deliberate and equip with infinitely powerful computational aid it become increasingly difficult to make coherent prediction about their behavior critically though we can also become increasingly confident that the outcome of their behavior will reflect their intention we sketch some possibility to illustrate the degree of flexibility available perhaps the most natural possibility be for this community to solve some outstanding philosophical problem and to produce a utility function which directly capture their preference however even if they quickly discover a formulation which appear to be attractive they would still be wise to spend a great length of time and to leverage some of these other technique to ensure that their propose solution be really satisfactory another natural possibility be to eschew a comprehensive theory of ethic and define value in term of the community s judgment we can define a utility function in term of the hypothetical judgment of astronomical number of simulated human collaboratively evaluate the goodness of a state of affair by examine its history at the atomic level understand the relevant high order structure and apply human intuition it seem quite likely that the community will gradually engage in self modification enlarge their cognitive capacity along various dimension as they come to understand the relevant aspect of cognition and judge such modification to preserve their essential character either independently or as an outgrowth of this process they may gradually or abruptly pass control to machine intelligence which they be suitably confident express their value this process could be use to acquire the power necessary to define a utility function in one of the above framework or understanding value preserve self modification or machine intelligence may itself prove an important ingredient in formalizing what it be we value any of these operation would be perform only after considerable analysis when the original simulated human be extremely confident in the desirability of the result whatever path they take and whatever coordination mechanism they use eventually they will output a utility function u we then define u = 0 if u < 0 u = 1 if u > 1 and u = u otherwise at this point we have offer a proposal for formally define a function u we have make some general observation about what this definition entail but now we may wonder to what extent u reflect our value or more relevantly to what extent our value be serve by the creation of u maximizer concern may be divide into a few natural category we respond to each of these objection in turn if the process work as intend we will reach a stage in which a large community of human reflect on their value undergo a process of discovery and potentially self modification and then output its result we may be concern that this dynamic do not adequately capture what we value for example we may believe that some other extrapolation dynamic capture our value or that it be morally desirable to act on the basis of our current belief without further reflection or that the presence of realistic disruption such as the threat of catastrophe have an important role in shape our moral deliberation the important observation in the defense of our proposal be that whatever objection we could think of today we could think of within the simulation if upon reflection we decide that too much reflection be undesirable we can simply change our plan appropriately if we decide that realistic interference be important for moral deliberation we can construct a simulation in which such interference occur or determine our moral principle by observe moral judgment in our own world s possible future there be some chance that this proposal be inadequate for some reason which win t be apparent upon reflection but then by definition this be a fact which we can not possibly hope to learn by deliberate now it therefore seem quite difficult to maintain objection to the proposal along these line one aspect of the proposal do get lock in however after be consider by only one human rather than by a large civilization the distribution of authority amongst different human and the nature of mechanism for resolve differ value judgment here we have two possible defense one be that the mechanism for resolve such disagreement can be reflect on at length by the individual simulate in h this individual can spend generation of subjective time and greatly expand her own cognitive capacity while attempt to determine the appropriate way to resolve such disagreement however this defense be not completely satisfactory we may be able to rely on this individual to produce a very technically sound and generally efficient proposal but the proposal itself be quite value laden and rely on one individual to make such a judgment be in some sense beg the question a second more compelling defense be that the structure of our world have already provide a mechanism for resolve value disagreement by assign decision make weight in a way that depend on current influence for example as determine by the simulated ability of various coalition to achieve various goal we can generate a class of proposal which be at a minimum no bad than the status quo of course these consideration will also be shape by the condition surround the creation or maintenance of system which will be guide by u for example if a nation be to create a u maximizer they might first adopt an internal policy for assign influence on u by perform this decision making in an idealized environment we can also reduce the likelihood of destructive conflict and increase the opportunity for mutually beneficial bargaining we may have moral objection to codify this sort of might make right policy favor a more democratic proposal or something else entirely but as a matter of empirical fact a more cosmopolitan proposal will be adopt only if it be support by those with the appropriate form of influence a situation which be unchanged by precisely codify exist power structure finally the value of the simulation in this process may diverge from the value of the original human model for one reaosn or another for example the simulated human may predictably disagree with the original model about ethical question by virtue of probably have no physical instantiation that be the output of this process be define in term of what a particular human would do in a situation which that human know will never come to pass if I ask what would I do if I be to wake up in a featureless room and tell that the future of humanity depend on my action the answer might begin with become distressed that I be clearly inhabit a hypothetical situation and adjust my ethical view to take into account the fact that people in hypothetical situation apparently have relevant first person experience set aside the question of whether such adjustment be justify they at least raise the possibility that our value may diverge from those of the simulation in this process these change might be minimize by understand their nature in advance and treat they on a case by case basis if we can become convinced that our understanding be exhaustive for example we could try and use human who robustly employ updateless decision theory which never undergo such predictable change or we could attempt to engineer a situation in which all of the human be emulate do have physical instantiation and naive self interest for those emulation align roughly with the desire behavior for example by allow the early emulation to write themselves into our world we can imagine many way in which this process can fail to work as intend the original brain emulation may accurately model human behavior the original subject may deviate from the intend plan or simulate human can make an error when interact with their virtual environment which cause the process to get hijack by some unintended dynamic we can argue that the proposal be likely to succeed and can bolster the argument in various way by reduce the number of assumption necessary for succee build in fault tolerance justify each assumption more rigorously and so on however we be unlikely to eliminate the possibility of error therefore we need to argue that if the process fail with some small probability the result value will only be slightly disturb this be the reason for require u to lie in the interval 0 1 we will see that this restriction bound the damage which may be do by an unlikely failure if the process fail with some small probability ε then we can represent the result utility function as u = 1 — ε u1 + ε u2 where u1 be the intended utility function and u2 be a utility function produce by some arbitrary error process now consider two possible state of affair a and b such that u1 a > u1 b + ε 1 — ε ≈ u1 b + ε then since 0 ≤ u2 ≤ 1 we have u a = 1 — ε u1 a + ε u2 a > 1 — ε u1 b + ε ≥ 1 — ε u1 b + ε u2 b = u b thus if a be substantially well than b accord to u1 then a be well than b accord to u this show that a small probability of error whether come from the stochasticity of our process or an agent s uncertainty about the process output have only a small effect on the result value moreover the process contain a human who have access to a simulation of our world this imply in particular that they have access to a simulation of whatever u maximize agent exist in the world and they have knowledge of those agent belief about u this allow they to choose u with perfect knowledge of the effect of error in these agent judgment in some case this will allow they to completely negate the effect of error term for example if the randomness in our process cause a perfectly cooperate community of simulated human to control u with probability 2⁄3 and cause an arbitrary adversary to control it with probability 1⁄3 then the simulated human can spend half of their mass outputting a utility function which exactly counter the effect of the adversary in general the situation be not quite so simple the fraction of mass control by any particular coalition will vary as the system s uncertainty about u varie and so it will be impossible to counteract the effect of an error term in a way which be time independent instead we will argue later that an appropriate choice of a bounded and noisy u can be use to achieve a very wide variety of effective behavior of u maximizer overcome the limitation both of bound utility maximization and of noisy specification of utility function many possible problem with this scheme be describe or implicitly address above but that discussion be not exhaustive and there be some class of error that fall through the crack one interesting class of failure concern change in the value of the hypothetical human h this human be in a very strange situation and it seem quite possible that the physical universe we know contain extremely few instance of that situation especially as the process unfold and become more exotic so h s first person experience of this situation may lead to significant change in h s view for example our intuition that our own universe be valuable seem to be derive substantially from our judgment that our own first person experience be valuable if hypothetically we find ourselves in a very alien universe it seem quite plausible that we would judge the experience within that universe to be morally valuable as well depend perhaps on our initial philosophical inclination another example concern our self interest much of individual human value seem to depend on their own anticipation about what will happen to they especially when face with the prospect of very negative outcome if hypothetically we wake up in a completely non physical situation it be not exactly clear what we would anticipate and this may distort our behavior would we anticipate the plan thought experiment occur as plan would we focus our attention on those location in the universe where a simulation of the thought experiment might be occur this possibility be particularly troubling in light of the incentive our scheme create — anyone who can manipulate h s behavior can have a significant effect on the future of our world and so many may be motivate to create simulation of h a realistic u maximizer will not be able to carry out the process describe in the definition of u in fact this process probably require immensely more computing resource than be available in the universe it may even involve the reaction of a simulated human to watch a simulation of the universe to what extent can we make robust guarantee about the behavior of such an agent we have already touch on this difficulty when discuss the maxim a state of affair be valuable to the extent I would judge it valuable after a century of reflection we can not generally predict our own judgment in a hundred year time but we can have well found belief about those judgment and act on the basis of those belief we can also have belief about the value of further deliberation and can strike a balance between such deliberation and act on our current good guess a u maximizer face a similar set of problem it can not understand the exact form of u but it can still have well found belief about u and about what sort of action be good accord to u for example if we suppose that the u maximizer can carry out any reasoning that we can carry out then the u maximizer know to avoid anything which we suspect would be bad accord to u for example torture human even if the u maximizer can not carry out this reasoning as long as it can recognize that human have powerful predictive model for other human it can simply appropriate those model either by carry out reasoning inspire by human model or by simply ask moreover the community of human be simulate in our process have access to a simulation of whatever u maximizer be operate under this uncertainty and have a detailed understanding of that uncertainty this allow the community to shape their action in a way with predictable to the u maximizer consequence it be easily conceivable that our value can not be capture by a bound utility function easiest to imagine be the possibility that some state of the world be much well than other in a way that require unbounded utility function but it be also conceivable that the framework of utility maximization be fundamentally not an appropriate one for guide such an agent s action or that the notion of utility maximization hide subtlety which we do not yet appreciate we will argue that it be possible to transform bound utility maximization into an arbitrary alternative system of decision making by design a utility function which reward world in which the u maximizer replace itself with an alternative decision maker it be straightforward to design a utility function which be maximize in world where any particular u maximizer convert itself into a non u maximizer even if no simple characterization can be find for the desire act we can simply instantiate many community of human to look over a world history and decide whether or not they judge the u maximizer to have act appropriately the more complicated question be whether a realistic u maximizer can be make to convert itself into a non u maximizer give that it be logically uncertain about the nature of u it be at least conceivable that it couldn t if the desirability of some other behavior be only reveal by philosophical consideration which be too complex to ever be discover by physically limit agent then we should not expect any physically limited u maximizer to respond to those consideration of course in this case we could also not expect normal human deliberation to correctly capture our value the relevant question be whether a u maximizer could switch to a different normative framework if an ordinary investment of effort by human society reveal that a different normative framework be more appropriate if a u maximizer do not spend any time investigate this possibility than it may not be expect to act on it but to the extent that we assign a significant probability to the simulated human decide that a different normative framework be more appropriate and to the extent that the u maximizer be able to either emulate or accept our reasoning it will also assign a significant probability to this possibility unless it be able to rule it out by more sophisticated reasoning if we and the u maximizer expect the simulation to output a u which reward a switch to a different normative framework and this possibility be consider seriously then u maximization entail explore this possibility if these exploration suggest that the simulate human probably do recommend some particular alternative framework and will output a u which assign high value to world in which this framework be adopt and low value to world in which it isn t then a u maximizer will change framework such a change of framework may involve sweeping action in the world for example the u maximizer may have create many other agent which be pursue activity instrumentally useful to maximize u these agent may then need to be destroy or alter ; anticipate this possibility the u maximizer be likely to take action to ensure that its current good guess about u do not get lock in this argument suggest that a u maximizer could adopt an arbitrary alternative framework if it be feasible to conclude that human would endorse that framework upon reflection our proposal appear to be something of a cop out in that it decline to directly take a stance on any ethical issue indeed not only do we fail to specify a utility function ourselves but we expect the simulation to which we have delegate the problem to in turn delegate it at least a few more time clearly at some point this process must bottom out with actual value judgment and we may be concern that this sort of pass the buck be just obscure deep problem which will arise when the process do bottom out as observe above whatever such concern we might have can also be discover by the simulation we create if there be some fundamental difficulty which always arise when try to assign value then we certainly have not exacerbate this problem by delegation nevertheless there be at least two coherent objection one might raise both of these objection can be meet with a single response in the current world we face a broad range of difficult and often urgent problem by pass the buck the first time we delegate resolution of ethical challenge to a civilization which do not have to deal with some of these difficulty in particular it face no urgent existential threat this allow we to divert as much energy as possible to deal with practical problem today while still capture most of the benefit of nearly arbitrarily extensive ethical deliberation this process be define in term of the behavior of unthinkably many hypothetical brain emulation it be conceivable that the moral status of these emulation may be significant we must make a distinction between two possible source of moral value it could be the case that a u maximizer carry out simulation on physical hardware in order to well understand u and these simulation have moral value or it could be the case that the hypothetical emulation themselves have moral value in the first case we can remark that the moral value of such simulation be itself incorporate into the definition of u therefore a u maximizer will be sensitive to the possible suffering of simulation it run while try to learn about u as long as it believe that we may might be concern about the simulation welfare upon reflection it can rely as much as possible on approach which do not involve run simulation which deprive simulation of the first person experience of discomfort or which estimate outcome by run simulation in more pleasant circumstance if the u maximizer be able to foresee that we will consider certain sacrifice in simulation welfare worthwhile then it will make those sacrifice in general in the same way that we can argue that estimate of u reflect our value over state of affair we can argue that estimate of u reflect our value over process for learn about u in the second case a u maximizer in our world may have little ability to influence the welfare of hypothetical simulation invoke in the definition of u however the possible disvalue of these simulation experience be probably seriously diminish in general the moral value of such hypothetical simulation experience be somewhat dubious if we simply write down the definition of u these simulation seem to have no more reality than story book character whose activity we describe the good argument for their moral relevance come from the great causal significance of their decision if the action of a powerful u maximizer depend on its belief about what a particular simulation would do in a particular situation include for example that simulation s awareness of discomfort or fear or confusion at the absurdity of the hypothetical situation in which they find themselves then it may be the case that those emotional response be grant moral significance however although we may define astronomical number of hypothetical simulation the detailed emotional response of very view of these simulation will play an important role in the definition of u moreover for the most part the existence of the hypothetical simulation we define be extremely well control by those simulation themselves and may be expect to be count as unusually happy by the light of the simulation themselves the early simulation who have less such control be create from an individual who have provide consent and be select to find such situation particularly non distress finally we observe that u can exert control over the experience of even hypothetical simulation if the early simulation would experience morally relevant suffering because of their causal significance but the later simulation they generate robustly disvalue this suffer the later simulation can simulate each other and ensure that they all take the same action eliminate the causal significance of the early simulation originally publish at ordinaryideas wordpress com on april 21 2012 from a quick cheer to a stand ovation clap to show how much you enjoy this story openai align ai system with human interest
Robbie Tilton,3,15,https://medium.com/@robbietilton/emotional-computing-with-ai-3513884055fa?source=tag_archive---------4----------------,emotional computing robbie tilton medium,investigate the human to computer relationship through reverse engineer the ture test human be get close to create a computer with the ability to feel and think although the process of the human brain be at large unknown computer scientist have be work to simulate the human capacity to feel and understand emotion this paper explore what it mean to live in an age where computer can have emotional depth and what this mean for the future of human to computer interaction in an experiment between a human and a human disguise as a computer the ture test be reverse engineer in order to understand the role computer will play as they become more adept to the process of the human mind implication for this study be discuss and the direction for future research suggest the computer be a gateway technology that have open up new way of creation communication and expression computer in first world country be a standard household item approximately 70 % of americans own one as of 2009 us census bereau and be utilize as a tool to achieve a diverse range of goal as this product continue to become more globalize transistor be become small processor be become fast hard drive be hold information in new networked pattern and human be adapt to the method of interaction expect of machine at the same time with more powerful computer and quick mean of communication — many researcher be explore how a computer can serve as a tool to simulate the brain cognition if a computer be able to achieve the same intellectual and emotional property as the human brain — we could potentially understand how we ourselves think and feel coin by mit the term affective computing relate to computation of emotion or the affective phenomenon and be a study that break down complex process of the brain relate they to machine like activity marvin minsky rosalind picard clifford nass and scott brave — along with many other — have contribute to this field and what it would mean to have a computer that could fully understand its user in their research it be very clear that human have the capacity to associate human emotion and personality trait with a machine nass and brave 2005 but can a human ever truly treat machine as a person in this paper we will uncover what it mean for human to interact with machine of great intelligence and attempt to predict the future of human to computer interaction the human to computer relationship be continuously evolve and be dependent on the software interface user interact with with regard to current wide scale interface — osx window linux ios and android — the tool and ability that a computer provide remain to be the central focus of computational advancement for commercial purpose this relationship to software be drive by utilitarian need and human do not expect emotional comprehension or intellectually equivalent thought in their household device as face track eye tracking speech recognition and kinetic recognition be advance in their experimental laboratory it be anticipate that these technology will eventually make their way to the mainstream market to provide a new relationship to what a computer can understand about its user and how a user can interact with a computer this paper be not about if a computer will have the ability to feel and love its user but ask the question — to what capacity will human be able to reciprocate feeling to a machine how do intelligence quotient iq differ from emotional quotient eq an iq be a representational relationship of intelligence that measure cognitive ability like learn understanding and deal with new situation an eq be a method of measure emotional intelligence and the ability to both use emotion and cognitive skill cherry advance in computer iq have be astonishing and have prove that machine be capable of answer difficult question accurately be able to hold a conversation with human like understanding and allow for emotional connection between a human and machine the turing test in particular have show the machine ability to think and even fool a person into believe that it be a human ture test explain in detail in section 4 machine like deep blue watson eliza svetlana cleverbot and many more — have all expand the perception of what a computer be and can be if an increase computational iq can allow a human to computer relationship to feel more like a human to human interaction what would the advancement of computational eq bring we peter robinson a professor at the university of cambridge state that if a computer understand its user feeling that it can then respond with an interaction that be more intuitive for its user robinson in essence eq advocate feel that it can facilitate a more natural interaction process where collaboration can occur with a computer in alan ture s computing machinery and intelligence ture 1950 a variant on the classic british parlor imitation game be propose the original game revolve around three player a man a a woman b and an interrogator © the interrogator stay in a room apart from a and b and only can communicate to the participant through text base communication a typewriter or instant messenger style interface when the game begin one contestant a or b be ask to pretend to be the opposite gender and to try and convince the interrogator © of this at the same time the oppose participant be give full knowledge that the other contestant be try to fool the interrogator with alan ture s computational background he take this imitation game one step far by replace one of the participant a or b with a machine — thus make the investigator try and depict if he she be speak to a human or machine in 1950 turing propose that by 2000 the average interrogator would not have more than a 70 percent chance of make the right identification after five minute of question the ture test be first pass in 1966 with eliza by joseph weizenbaum a chat robot program to act like a rogerian psychotherapist weizenbaum 1966 in 1972 kenneth colby create a similar bot call parry that incorporate more personality than eliza and be program to act like a paranoid schizophrenic bowden 2006 since these initial victory for the test the 21st century have prove to continue to provide machine with more human like quality and trait that have make people fall in love with they convince they of be human and have human like reasoning brian christian the author of the most human human argue that the problem with design artificial intelligence with great ability be that even though these machine be capable of learn and speak that they have no self they be mere accumulation of identity and thought that be foreign to the machine and have no central identity of their own he also argue that people be begin to idealize the machine and admire machine capabilitie more than their fellow human — in essence — he argue human be evolve to become more like machine with less of a notion of self christian 2011 ture state we like to believe that man be in some subtle way superior to the rest of creation and it be likely to be quite strong in intellectual people since they value the power of think more highly than other and be more inclined to base their belief in the superiority of man on this power if this be true will human idealize the future of the machine for its intelligence or will they remain an inferior being as an object of our creation reverse the ture test allow we to understand how human will treat machine when machine provide an equivalent emotional and intellectual capacity this also hit directly on jefferson lister s quote not until a machine can write a sonnet or compose a concerto because of thought and emotion feel and not by the chance fall of symbol could we agree that machine equal brain that be not only write it but know that it have write it participant be give a chat room simulation between two participant a a human interrogator and b a human disguise as a computer in this simulation a and b be both place in different room to avoid influence and communicate through a text base interface a be inform that b be an advanced computer chat bot with the capacity to feel understand learn and speak like a human b be inform to be his or herself text base communication be choose to follow ture s argument that a computer voice should not help an interrogator determine if it s a human or computer pairing of participant be choose to participate in the interaction one at a time to avoid influence from other participant each experiment be five minute in length to replicate ture s time restraint twenty eight graduate student be recruit from the nyu interactive telecommunication program to participate in the study — 50 % male and 50 % female the experiment be evenly distribute across man and woman after be recruit in person participant be direct to a website that give instruction and run the experiment upon enter the website a participant be tell that we be in the process of evaluate an advanced cloud base computing system that have the capacity to feel emotion understand learn and converse like a human b participant be instruct that they would be communicate with another person through text and to be themselves they be also tell that participant a think they be a computer but that they shouldn t act like a computer or pretend to be one in any way this allow a to explicitly understand that they be talk to a computer while b know a perspective and explicitly be not go to play the role of a computer participant be then direct to communicate with the bot or human freely without restriction after five minute of conversation the participant be ask to stop and then fill out a questionnaire participant be ask to rate iq and eq of the person they be converse with a participant perceive the following of b iq 0 % — not good 0 % — barely acceptable 21 4 % — okay 50 % — great 28 6 % excellent iq average rating 81 4 % eq 0 % — not good 7 1 % — barely acceptable 50 % — okay 14 3 % — great 28 6 % — excellent eq average rating 72 8 % ability to hold a conversation 0 % — not good 0 % — barely acceptable 28 6 % — okay 35 7 % — great 35 7 % — excellent ability to hold a conversation average 81 4 % b participant perceive the following of a iq 0 % — not good 21 4 % — barely acceptable 35 7 % — okay 28 6 % — great 14 3 % excellent iq average rating 67 % eq 7 1 % — not good 14 3 % — barely acceptable 28 6 % — okay 35 7 % — great 14 3 % — excellent eq average rating 67 % ability to hold a conversation 7 1 % — not good 28 6 % — barely acceptable 35 7 % — okay 0 % — great 28 6 % — excellent ability to hold a conversation average 62 8 % overall a participant give the perceive chabot high rating than b participant give a in particular the high rating be in regard to the chat bot s iq this data state that people view the chat bot to be more intellectually competent it also imply that people talk with bot decrease their iq eq and conversation ability when communicate with computer a participant be allow to decide their username within the chat system to well reflect how they want to portray themselves to the machine b participant be designate the gender neutral name bot in an attempt to ganger gender perception for the machine the male to female ratio be divide evenly with all participant 50 % be male and 50 % be female a participant 50 % of the time think b be a male 7 1 % a female and 42 9 % gender neutral on the other hand b participant 28 6 % of the time think a be a male 57 1 % a female and 14 3 % gender neutral the username a chose be as follow hihi inessah somade3 willzing jihyun g ann divagrrl93 thisdoug jono minion10 p 123 itslynnburke from these result it be clear that people associate the male gender and gender neutrality with machine it also demonstrate that people modify their identity when speak with machine b participant be ask if they would like to pursue a friendship with the person they chat with 50 % of participant respond affirmatively that they would indeed like to pursue a friendship while 50 % say maybe or no one response state I would like to continue the conversation but I don t think I would be entice to pursue a friendship another respond maybe I like people who be intellectually curious but I worry that the person might be a bit of a smart ass overall the participant disguise as a machine may or may not pursue a friendship after five minute of text base conversation b participant be also ask if they feel a care about their feeling 21 4 % state that a indeed do care about their feeling 21 4 % state that they weren t sure if a care about their feeling and 57 2 % state that a do not care about their feeling these result indicate a user s lack of attention to b s emotional state a participant be ask what they feel could be improve about the b participant the follow improvement be note should be funny give it a well sense of humor it can be well if he know about my friend or preference the response be inconsistent and too slow it should share more about itself your algorithm be prime prude just like that letdown siri well I guess I like it well but it should be more engaged and human consistency not after the first cold prompt it push I on too many question I feel that it give up on answering and the response time be a bit slow outsource the chatbot to fluent english speaker elsewhere and pretend they be bot — if the response be this slow to this many inquiry then it should be about the same experience I be very impressed with its parse ability so far not as much with its reasoning I think some parameter for the conversation would help like ask a question maybe make the response fast I be confuse at first because I ask a question wait a bit then ask another question wait and then get a response from the bot the response from this indicate that even if a computer be a human that its user may not necessarily be fully satisfied with its performance the response imply that each user would like the machine to accommodate his or her need in order to cause less personality and cognitive friction with several participant comment incorporate response time it also indicate people expect machine to have consistent response time human clearly vary in speed when listen thinking and respond but it be expect of machine to act in a rhythmic fashion it also suggest that there be an expectation that a machine will answer all question ask and will not ask its user more question than perceive necessary a participant be ask if they feel b s artificial intelligence could improve their relationship to computer if integrate in their daily product 57 1 % of participant respond affirmatively that they feel this could improve their relationship well I think I prefer talk to a person well but yes for ipod smart phone etc would be very handy for everyday use product yes especially iphone be always with I so it can track my daily behavior that make the algorithm smarter possibly I should have query it for information that would have be more relevant to I absolutely yes the 42 9 % which respond negatively have doubt that it would be necessary or desirable not sure it might creep I out if it be I like siri as much as the next gal but honestly we re approach the uncanny valley now its not clear to I why this type of relationship need to improve I think human relationship still need a lot of work nope I still prefer flesh sack no the finding of the paper be relevant to the future of affective computation whether a super computer with a human like iq and eq can improve the human to computer interaction the uncertainty of computational equivalency that turing bring forth be indeed an interesting starting point to understand what we want out of the future of computer the response from the experiment affirm gender perception of machine and show how we display ourselves to machine it seem that we limit our intelligence limit our emotion and obscure our identity when communicate to a machine this lead we to question if we would want to give our true self to a computer if it doesn t have a self of its own it also could indicate that people censor themselves for machine because they lack a similarity that bond human to human or that there s a stigma associate with place information in a digital device the inverse relationship be also show through the datum that people perceive a bot iq eq and discussion ability to be high even though the chat bot be indeed a human this data can imply human perceive bot to not have restriction and to be competent at certain procedure the result also imply that human aren t really sure what they want out of artificial intelligence in the future and that we be not certain that an affective computer would even enjoy a user company and or conversation the result also state that we currently think of computer as a very personal device that should be passive not active but reactive when interact with it suggest a consistent reliability we expect upon machine and that we expect to take more information from a machine than it take from we a major limitation of this experiment be the sample size and sample diversity the sample size of twenty eight student be too small to fully understand and gather a stable result set it be also only conduct with nyu interactive telecommunication student who all have extensive experience with computer and technology to get a more accurate assessment of emotion a more diverse sample range need to be take five minute be a short amount of time to create an emotional connection or friendship to stay true to the ture test limitation this be enforce but further relational understanding could be understand if more time be grant beside the visual interface of the chat window it would be important to show the emotion of participant b through a virtual avatar not have this visual feedback could have limit emotional resonance with participant a time be also a limitation people aren t use to speak to inquisitive machine yet and even through a familiar interface a chat room many participant haven t hold conversation with machine previously perhaps if chat bot become more active conversational participant in commercial application user will feel less censor to give themselves to the conversation in addition to the refinement note in the limitation describe above there be several other experiment for possible future study for example investigate a long term human to bot relationship this would provide a well understanding toward the emotion a human can share with a machine and how a machine can reciprocate these emotion it would also well allow computer scientist to understand what really create a significant relationship when physical limitation be present future study should attempt to push these result far by understand how a large sample react to a computer algorithm with high intellectual and emotional understanding it should also attempt to understand the boundary of emotional computing and what be ideal for the user and what be ideal for the machine without compromise either party capacity this paper demonstrate the diverse range of emotion that people can feel for affective computation and indicate that we be not in a time where computational equivalency be fully desire or accept positive reaction indicate that there be optimism for more adept artificial intelligence and that there be interest in the field for commercial use it also provide insight that human limit themselves when communicate with machine and that inversely machine don t limit themselves when communicate with human books & articlesbowden m 2006 mind as machine a history of cognitive science oxford university press christian b 2011 the most human human marvin m 2006 the emotion machine commonsense think artificial intelligence and the future of the human mind simon & schuster paperback nass c brave s 2005 wire for speech how voice activate and advance the human computer relationship mit press nass c brave s 2005 hutchinson k computer that care investigate the effect of orientation of emotion exhibit by an embody computer agent human computer study 161 178 elsevi picard r 1997 affective computing mit press searle j 1980 mind brain and program cambridge university press 417 457 ture a 1950 computing machinery and intelligence mind stor 59 433 460 wilson r keil f 2001 the mit encyclopedia of the cognitive science mit press weizenbaum j 1966 eliza — a computer program for the study of natural language communication between man and machine communication of the acm 36 45 website cherry k what be emotional intelligence http psychology about com od personalitydevelopment a emotionalintell htm epstein r 2006 clever bot radio lab http www radiolab org 2011 may 31 clever bot ibm 1977 deep blue ibm http www research ibm com deepblue ibm 2011 watson ibm http www 03 ibm com innovation us watson index html leavitt d 2011 I take the ture test new york times http www nytimes com 2011 03 20 book review book review the most human human by brian christian html personal robotic group 2008 nexi mit http robotic medium mit edu robinson p the emotional computer camrbidge idea http www cam ac uk research news the emotional computer us census bereau 2009 household with a computer and internet use 1984 to 2009 http www census gov hhe computer 1960 s eliza mit http www manifestation com neurotoy eliza php3 from a quick cheer to a stand ovation clap to show how much you enjoy this story
Netflix Technology Blog,330,11,https://medium.com/netflix-techblog/system-architectures-for-personalization-and-recommendation-e081aa94b5d8?source=tag_archive---------0----------------,system architecture for personalization and recommendation,by xavier amatriain and justin basilico in our previous post about netflix personalization we highlight the importance of use both datum and algorithm to create the good possible experience for netflix member we also talk about the importance of enrich the interaction and engage the user with the recommendation system today we re explore another important piece of the puzzle how to create a software architecture that can deliver this experience and support rapid innovation come up with a software architecture that handle large volume of exist datum be responsive to user interaction and make it easy to experiment with new recommendation approach be not a trivial task in this post we will describe how we address some of these challenge at netflix to start with we present an overall system diagram for recommendation system in the follow figure the main component of the architecture contain one or more machine learning algorithm the simple thing we can do with datum be to store it for later offline processing which lead to part of the architecture for manage offline job however computation can be do offline nearline or online online computation can respond well to recent event and user interaction but have to respond to request in real time this can limit the computational complexity of the algorithm employ as well as the amount of datum that can be process offline computation have less limitation on the amount of datum and the computational complexity of the algorithm since it run in a batch manner with relaxed timing requirement however it can easily grow stale between update because the most recent datum be not incorporate one of the key issue in a personalization architecture be how to combine and manage online and offline computation in a seamless manner nearline computation be an intermediate compromise between these two mode in which we can perform online like computation but do not require they to be serve in real time model training be another form of computation that use exist datum to generate a model that will later be use during the actual computation of result another part of the architecture describe how the different kind of event and datum need to be handle by the event and data distribution system a related issue be how to combine the different signal and model that be need across the offline nearline and online regime finally we also need to figure out how to combine intermediate recommendation result in a way that make sense for the user the rest of this post will detail these component of this architecture as well as their interaction in order to do so we will break the general diagram into different sub system and we will go into the detail of each of they as you read on it be worth keep in mind that our whole infrastructure run across the public amazon web service cloud as mention above our algorithmic result can be compute either online in real time offline in batch or nearline in between each approach have its advantage and disadvantage which need to be take into account for each use case online computation can respond quickly to event and use the most recent datum an example be to assemble a gallery of action movie sort for the member use the current context online component be subject to an availability and response time service level agreement sla that specify the maximum latency of the process in respond to request from client application while our member be wait for recommendation to appear this can make it hard to fit complex and computationally costly algorithm in this approach also a purely online computation may fail to meet its sla in some circumstance so it be always important to think of a fast fallback mechanism such as revert to a precomputed result compute online also mean that the various datum source involve also need to be available online which can require additional infrastructure on the other end of the spectrum offline computation allow for more choice in algorithmic approach such as complex algorithm and less limitation on the amount of datum that be use a trivial example might be to periodically aggregate statistic from million of movie play event to compile baseline popularity metric for recommendation offline system also have simple engineering requirement for example relaxed response time sla impose by client can be easily meet new algorithm can be deploy in production without the need to put too much effort into performance tune this flexibility support agile innovation at netflix we take advantage of this to support rapid experimentation if a new experimental algorithm be slow to execute we can choose to simply deploy more amazon ec2 instance to achieve the throughput require to run the experiment instead of spend valuable engineering time optimize performance for an algorithm that may prove to be of little business value however because offline processing do not have strong latency requirement it will not react quickly to change in context or new datum ultimately this can lead to staleness that may degrade the member experience offline computation also require have infrastructure for store computing and access large set of precompute result nearline computation can be see as a compromise between the two previous mode in this case computation be perform exactly like in the online case however we remove the requirement to serve result as soon as they be compute and can instead store they allow it to be asynchronous the nearline computation be do in response to user event so that the system can be more responsive between request this open the door for potentially more complex processing to be do per event an example be to update recommendation to reflect that a movie have be watch immediately after a member begin to watch it result can be store in an intermediate cache or storage back end nearline computation be also a natural setting for apply incremental learning algorithm in any case the choice of online nearline offline processing be not an either or question all approach can and should be combine there be many way to combine they we already mention the idea of use offline computation as a fallback another option be to precompute part of a result with an offline process and leave the less costly or more context sensitive part of the algorithm for online computation even the modeling part can be do in a hybrid offline online manner this be not a natural fit for traditional supervised classification application where the classifier have to be train in batch from label datum and will only be apply online to classify new input however approach such as matrix factorization be a more natural fit for hybrid online offline modeling some factor can be precompute offline while other can be update in real time to create a more fresh result other unsupervised approach such as clustering also allow for offline computation of the cluster center and online assignment of cluster these example point to the possibility of separate our model training into a large scale and potentially complex global model training on the one hand and a light user specific model training or update phase that can be perform online much of the computation we need to do when run personalization machine learning algorithm can be do offline this mean that the job can be schedule to be execute periodically and their execution do not need to be synchronous with the request or presentation of the result there be two main kind of task that fall in this category model training and batch computation of intermediate or final result in the model training job we collect relevant exist datum and apply a machine learn algorithm produce a set of model parameter which we will henceforth refer to as the model this model will usually be encode and store in a file for later consumption although most of the model be train offline in batch mode we also have some online learning technique where incremental training be indeed perform online batch computation of result be the offline computation process define above in which we use exist model and correspond input datum to compute result that will be use at a later time either for subsequent online processing or direct presentation to the user both of these task need refined datum to process which usually be generate by run a database query since these query run over large amount of datum it can be beneficial to run they in a distribute fashion which make they very good candidate for run on hadoop via either hive or pig job once the query have complete we need a mechanism for publish the result datum we have several requirement for that mechanism first it should notify subscriber when the result of a query be ready second it should support different repository not only hdfs but also s3 or cassandra for instance finally it should transparently handle error allow for monitoring and alert at netflix we use an internal tool name herme that provide all of these capability and integrate they into a coherent publish subscribe framework it allow datum to be deliver to subscriber in near real time in some sense it cover some of the same use case as apache kafka but it be not a message event queue system regardless of whether we be do an online or offline computation we need to think about how an algorithm will handle three kind of inputs model datum and signal model be usually small file of parameter that have be previously train offline datum be previously process information that have be store in some sort of database such as movie metadata or popularity we use the term signal to refer to fresh information we input to algorithm this data be obtain from live service and can be make of user relate information such as what the member have watch recently or context datum such as session device date or time our goal be to turn member interaction datum into insight that can be use to improve the member s experience for that reason we would like the various netflix user interface application smart tvs tablet game console etc to not only deliver a delightful user experience but also collect as many user event as possible these action can be relate to click browse view or even the content of the viewport at any time event can then be aggregate to provide base datum for our algorithm here we try to make a distinction between datum and event although the boundary be certainly blurry we think of event as small unit of time sensitive information that need to be process with the least amount of latency possible these event be route to trigger a subsequent action or process such as update a nearline result set on the other hand we think of datum as more dense information unit that might need to be process and store for later use here the latency be not as important as the information quality and quantity of course there be user event that can be treat as both event and datum and therefore send to both flow at netflix our near real time event flow be manage through an internal framework call manhattan manhattan be a distribute computation system that be central to our algorithmic architecture for recommendation it be somewhat similar to twitter s storm but it address different concern and respond to a different set of internal requirement the datum flow be manage mostly through log through chukwa to hadoop for the initial step of the process later we use herme as our publish subscribe mechanism the goal of our machine learning approach be to come up with personalized recommendation these recommendation result can be service directly from list that we have previously compute or they can be generate on the fly by online algorithm of course we can think of use a combination of both where the bulk of the recommendation be compute offline and we add some freshness by post process the list with online algorithm that use real time signal at netflix we store offline and intermediate result in various repository to be later consume at request time the primary data store we use be cassandra evcache and mysql each solution have advantage and disadvantage over the other mysql allow for storage of structured relational datum that might be require for some future process through general purpose query however the generality come at the cost of scalability issue in distribute environment cassandra and evcache both offer the advantage of key value store cassandra be a well known and standard solution when in need of a distribute and scalable no sql store cassandra work well in some situation however in case where we need intensive and constant write operation we find evcache to be a well fit the key issue however be not so much where to store they as to how to handle the requirement in a way that conflict goal such as query complexity read write latency and transactional consistency meet at an optimal point for each use case in previous post we have highlight the importance of data model and user interface for create a world class recommendation system when build such a system it be critical to also think of the software architecture in which it will be deploy we want the ability to use sophisticated machine learning algorithm that can grow to arbitrary complexity and can deal with huge amount of datum we also want an architecture that allow for flexible and agile innovation where new approach can be develop and plug in easily plus we want our recommendation result to be fresh and respond quickly to new datum and user action find the sweet spot between these desire be not trivial it require a thoughtful analysis of requirement careful selection of technology and a strategic decomposition of recommendation algorithm to achieve the good outcome for our member we be always look for great engineer to join our team if you think you can help we be sure to look at our job page originally publish at techblog netflix com on march 27 2013 from a quick cheer to a stand ovation clap to show how much you enjoy this story learn more about how netflix design build and operate our system and engineering organization learn about netflix s world class engineering effort company culture product development and more
James Faghmous ,187,6,https://medium.com/@nomadic_mind/new-to-machine-learning-avoid-these-three-mistakes-73258b3848a4?source=tag_archive---------1----------------,new to machine learning avoid these three mistake,machine learning ml be one of the hot field in data science as soon as ml enter the mainstream through amazon netflix and facebook people have be giddy about what they can learn from their datum however modern machine learn I e not the theoretical statistical learning that emerge in the 70 be very much an evolve field and despite its many success we be still learn what exactly can ml do for datum practitioner I give a talk on this topic early this fall at northwestern university and I want to share these cautionary tale with a wide audience machine learning be a field of computer science where algorithm improve their performance at a certain task as more datum be observe to do so algorithm select a hypothesis that good explain the datum at hand with the hope that the hypothesis would generalize to future unseen datum take the left panel in the figure in the header the crosse denote the observe datum project in a two dimensional space — in this case house price and their corresponding size in square meter the blue line be the algorithm s good hypothesis to explain the observed datum it state there be a linear relationship between the price and size of a house as the house s size increase so do its price in linear increment now use this hypothesis I can predict the price of an unseen datapoint base on its size as the dimension of the datum increase the hypothesis that explain the datum become more complex however give that we be use a finite sample of observation to learn our hypothesis find an adequate hypothesis that generalize to unseen datum be nontrivial there be three major pitfall one can fall into that will prevent you from have a generalizable model and hence the conclusion of your hypothesis will be in doubt occam s razor be a principle attribute to william of occam a 14th century philosopher occam s razor advocate for choose the simple hypothesis that explain your datum yet no simple while this notion be simple and elegant it be often misunderstood to mean that we must select the simple hypothesis possible regardless of performance in their 2008 paper in nature johan nyberg and colleague use a 4 level artificial neural network to predict seasonal hurricane count use two or three environmental variable the author report stellar accuracy in predict seasonal north atlantic hurricane count however their model violate occam s razor and most certainly doesn t generalize to unseen datum the razor be violate when the hypothesis or model select to describe the relationship between environmental datum and seasonal hurricane count be generate use a four layer neural network a four layer neural network can model virtually any function no matter how complex and could fit a small dataset very well but fail to generalize to unseen datum the rightmost panel in the top figure show such incident the hypothesis select by the algorithm the blue curve to explain the data be so complex that it fit through every single data point that be for any give house size in the training datum I can give you with pinpoint accuracy the price it would sell for it doesn t take much to observe that even a human couldn t be that accurate we could give you a very close estimate of the price but to predict the selling price of a house within a few dollar every single time be impossible the pitfall of select too complex a hypothesis be know as overfitte think of overfitte as memorizing as oppose to learn if you be a child and you be memorize how to add number you may memorize the sum of any pair of integer between 0 and 10 however when ask to calculate 11 + 12 you will be unable to because you have never see 11 or 12 and therefore couldn t memorize their sum that s what happen to an overfitte model it get too lazy to learn the general principle that explain the datum and instead memorize the datum data leakage occur when the datum you be use to learn a hypothesis happen to have the information you be try to predict the most basic form of data leakage would be to use the same datum that we want to predict as input to our model e g use the price of a house to predict the price of the same house however most often data leakage occur subtly and inadvertently for example one may wish to learn for anomaly as oppose to raw datum that be a deviation from a long term mean however many fail to remove the test datum before compute the anomaly and hence the anomaly carry some information about the datum you want to predict since they influence the mean and standard deviation before be remove the be several way to avoid data leakage as outline by claudia perlich in her great paper on the subject however there be no silver bullet — sometimes you may inherit a corrupt dataset without even realize it one way to spot data leakage be if you be do very poorly on unseen independent datum for example say you get a dataset from someone that span 2000 2010 but you start collect you own datum from 2011 onward if your model s performance be poor on the newly collect datum it may be a sign of data leakage you must resist the urge to retrain the model with both the potentially corrupt and new datum instate either try to identify the cause of poor performance on the new datum or well yet independently reconstruct the entire dataset as a rule of thumb your good defense be to always be mindful of the possibility of data leakage in any dataset sampling bias be the case when you shortchange your model by train it on a biased or non random dataset which result in a poorly generalizable hypothesis in the case of housing price sample bias occur if for some reason all the house price size you collect be of huge mansion however when it be time to test your model and the first price you need to predict be that of a 2 bedroom apartment you couldn t predict it sample bias happen very frequently mainly because as human we be notorious for be bias nonrandom sampler one of the most common example of this bias happen in startup and invest if you attend any business school course they will use all these case study of how to build a successful company such case study actually depict the anomaly and not the norm as most company fail — for every apple that become a success there be 1000 other startup that die try so to build an automate data drive investment strategy you would need sample from both successful and unsuccessful company the figure above figure 13 be a concrete example of sample bias say you want to predict whether a tornado be go to originate at certain location base on two environmental condition wind shear and convective available potential energy cape we don t have to worry about what these variable actually mean but figure 13 show the wind shear and cape associate with 242 tornado case we can fit a model to these datum but it will certainly not generalize because we fail to include shear and cape value when tornado do not occur in order for our model to separate between positive tornado and negative no tornado event we must train it use both population there you have it be mindful of these limitation do not guarantee that your ml algorithm will solve all your problem but it certainly reduce the risk of be disappoint when your model doesn t generalize to unseen datum now go on young jedi train your model you must from a quick cheer to a stand ovation clap to show how much you enjoy this story @nomadic_mind sometimes the difference between success and failure be the same as between = and = = living be in the detail
Datafiniti,3,5,https://blog.datafiniti.co/classifying-websites-with-neural-networks-39123a464055?source=tag_archive---------2----------------,classify website with neural network knowledge from datum the datafiniti blog,at datafiniti we have a strong need for convert unstructured web content into structured datum for example we d like to find a page like and do the following both of these be hard thing for a computer to do in an automate manner while it s easy for you or I to realize that the above web page be sell some jean a computer would have a hard time make the distinction from the above page from either of the follow web page or both of these page share many similarity to the actual product page but also have many key difference the real challenge though be that if we look at the entire set of possible web page those similarity and difference become somewhat blurred which mean hard and fast rule for classification will fail often in fact we can t even rely on just look at the underlie html since there be huge variation in how product page be lay out in html while we could try and develop a complicated set of rule to account for all the condition that perfectly identify a product page do so would be extremely time consume and frankly incredibly boring work instead we can try use a classical technique out of the artificial intelligence handbook neural network here s a quick primer on neural network let s say we want to know whether any particular mushroom be poisonous or not we re not entirely sure what determine this but we do have a record of mushroom with their diameter and height along with which of these mushroom be poisonous to eat for sure in order to see if we could use diameter and height to determine poisonous ness we could set up the follow equation a * diameter + b * height = 0 or 1 for not poisonous poisonous we would then try various combination of a and b for all possible diameter and height until we find a combination that correctly determine poisonous ness for as many mushroom as possible neural network provide a structure for use the output of one set of input datum to adjust a and b to the most likely good value for the next set of input datum by constantly adjust a and b this way we can quickly get to the good possible value for they in order to introduce more complex relationship in our datum we can introduce hidden layer in this model which would end up look something like for a more detailed explanation of neural network you can check out the follow link in our product page classifier algorithm we setup a neural network with 1 input layer with 27 node 1 hide layer with 25 node and 1 output layer with 3 output node our input layer model several feature include our output layer have the follow our algorithm for the neural network take the follow step the ultimate output be two set of input layer t1 and t2 that we can use in a matrix equation to predict page type for any give web page this work like so so how do we do in order to determine how successful we be in our prediction we need to determine how to measure success in general we want to measure how many true positive tp result as compare to false positive fp and false negative fn conventional measurement for these be our implementation have the follow result these score be just over our training set of course the actual score on real life datum may be a bit low but not by much this be pretty good we should have an algorithm on our hand that can accurately classify product page about 90 % of the time of course identify product page isn t enough we also want to pull out the actual structured datum in particular we re interested in product name price and any unique identifier e g upc ean & isbn this information would help we fill out our product search we don t actually use neural network for do this neural network be well suit toward classification problem and extract datum from a web page be a different type of problem instead we use a variety of heuristic specific to each attribute we re try to extract for example for product name we look at the < h1 > and < h2 > tag and use a few metric to determine the good choice we ve be able to achieve around a 80 % accuracy here we may go into the actual metric and methodology for develop they in a separate post we feel pretty good about our ability to classify and extract product datum the extraction part could be well but it s steadily be improve in the meantime we re also work on classify other type of page such as business datum company team page event datum and more as we roll out these classifier and datum extractor we re include each one in our crawl of the entire internet this mean that we can scan the entire internet and pull out any available datum that exist out there exciting stuff you can connect with we and learn more about our business people product and property api and dataset by select one of the option below from a quick cheer to a stand ovation clap to show how much you enjoy this story instant access to web datum build the world s large database of web datum — follow our journey
Arjan Haring 🔮🔨,3,8,https://medium.com/i-love-experiments/reinventing-social-sciences-in-the-era-of-big-data-d255f3e391f3?source=tag_archive---------3----------------,reinvent social science in the era of big datum I love experiment medium,sune lehmann be an associate professor at dtu informatics technical university of denmark in the past he have work as a postdoctoral fellow at institute for quantitative social science at harvard university and the college of computer and information science at northeasthern university ; before that he be at laszlo barabási s center for complex network research at northeastern university and the center for cancer system biology at the dana farber cancer institute I wouldn t call he stupid he be okay well he be actually pretty great forget that he be freak fantastic we should get he over for one of our event and so we do sune speak at the 2nd # projectwaalhalla this time let s begin at the beginning before we dive in deep your main research project have to do with measure real social network with high resolution I know for a fact you don t mean 3d print social network but what be you aim for and how be you go to get there my humble research goal be to reinvent social science in the age of big datum my background be in mathematical analysis of large network but over the past 10 year I ve slowly grow more and more interested in understand social system as a scientist I be blow away by the promise of all of the digital trace of human behavior collect as a consequence of cheap hard drive and database everywhere but in spite of the promise of big datum the result so far have be less exciting than I have hope for all the hype deep new scientific insight from big datum be far and few between a central hypothesis in my work be that in order to advance our quantitative understanding of social interaction we can not get by with noisy incomplete big datum we need good datum let I explain why and use my own field as an example let s say you have a massive cell phone datum set from a telco that provide service to 30 % or the population of a large country of 66 million people that s something like 20 million people and easily terabyte of monthly datum so a massive dataset but when you start think about the network you run into problem the standard approach be to simply look at the network between the individual in your sample assume that people be randomly sample and link be randomly distribute you realize that 30 % of the population correspond to only 9 % of the link be 9 % of cell phone call enough to understand how the network work with only one in ten link remain in the dataset the social structure almost completely erase and it get bad telecommunication be only one small & biased aspect of human communication human interaction may also unfold face to face via text message email facebook skype etc and these stream be collect in silos where we can not generally identify individual entity across dataset so if we think about all these way we can communicate access to only one in ten of my cell phone contact be very likely insufficient for make valid inference and the bad part be that we can t know without access to the full datum set we can t even tell what we can and can t tell from a sample so when I start out as an assistant professor I decide to change the course of my career and move from sit comfortably in front of my computer as a computational theoretical scientist to become an experimenter to try and attack this problem head on now a few of year later we have put together a dataset of human social interaction that be unparalleled in term of quality and size we record social interaction within more than 1000 student at my university use top of the line cell phone as censor we can capture detailed interaction pattern such as face to face via bluetooth social network data e g facebook and twitter via app telecommunication datum from call log and geolocation via gps & wifi we like to call this type of datum deep datum a densely connect group of participant all the link observation across many communication channel high frequency observation minute by minute scale but with long observation window year of collection and with behavioral datum supplement by classic questionnaire as well as the possibility of run intervention experiment but my expertise and ultimate interest be not in build a deep data collection platform although that have be a lot of fun I want to get back to the question that motivate the enthusiasm for computational social science in the first place reinvent social science be what it s all about what can we learn from just one channel now that we know about all the communication channel we can begin to understand what kind of thing one may learn from a single channel let s get quantitative about the usefulness of e g large cell phone datum set or facebook when that s the only datum available my heart be still with the network science in some way this whole project be design to build a system that will really take we place in term of model human social network lot of network science be still about unweighted undirected static network ; we be already use this dataset to create well model for dynamic multiplex network understand spread process influence behavior disease etc be a central goal if we look a bit forward in time we have an system where n be big enough to perform intervention experiment with randomized control etc we re still far from implement this goal but we re work on find the right question — and work closely with social scientist to get our protocol for these question just right what a coincidence we be all about model behavior and learn across channel and with contagionapi prominently on our product roadmap we want to start dabble with spread process as well in the near future what would you say be major challenge the last year in model behavior and what do see as big challenge & opportunity for the future there be many challenge although we ve make amazing progress in network science for example it s still a fact that our fundamental understanding of dynamic multi channel network be still in its infancy there aren t a lot of easily interpretable model that really explain the underlie network so that s an area with lot of challenge and correspond opportunity and when we want to figure out question about thing take place on network we run into all kind of problem about how to do statistic right brilliant statistician have show that homophily and contagion be generically confound in observational social network study on that front guy like sinan aral be do really exciting work use intervention to get at some of the issue but there be still lot to do in that area finally privacy be a big issue we re work closely with collaborator at the mit medialab to develop new responsible solution — and we ve already get far on that topic but in term of datum sharing that respect the privacy of study participant there be still a long way to go but since study of digital trace of human behavior will not be go away anytime soon we have to make progress in this area and oh yeah why do this all matter and should we be concern by these thing I think there be many reason to be concern and excite the more we learn about how system work the more we be able to influence they to control they that be also true for system of human if we think about spread of disease it d be great to know how to slow down or stop the spread of sar or similar contagious virus or as a society we may be able to increase spread of thing we support such as tolerance good exercise habit etc and similarly we can use an understanding influence in social system to inhibit negative behavior such as intolerance smoking etc and all this tie into another good reason to be concern company like google facebook apple or governmental agency like nsa be commit serious resource to research in this area it s not a coincidence that both google and facebook be develop their own cell phone but none of these wall off player be share their result they re simply apply they to the public in my opinion that s one of the key problem of the current state of affair the imbalance of information we hand over our personal datum to powerful corporation but have nearly zero insight into a what they know about we and b what they re do with all the stuff they know about we by do research that be open collaborative explicit about privacy and public I hope we can act as a counter point and work to diminish the information gap okay great but should company be interested in the stuff you be do and if so why I think so one of the exciting thing about this area be that basic research be very close to apply research insight into the mechanism that drive human nature be indeed valuable for company I presume that s why science rockstar exist for example note from the editor not stupid at all we already know that human behavior can be influence significantly with nudge that certain kind of collective behavior influence our opinion and purchase behavior the more we uncover about the detail of these mechanism the more precise and effective we can be about influence other let s discuss the ethic of this another time but it s not just marketing if use for good this be the science of what make people happy so inside organization work like this could be use to re think organizational structure incentive etc ; to make employee happy & more fulfil or if we think about organization as organism have access to realtime information about employee can be think of as a nervous system for the company allow for fast reaction time when crisis arise identification of pain point etc finally for the medical field we know that gene only explain part of what make we sick be able to quantify and analyze behavior mean know more about the environment the nurture part of nurture vs nature in that sense detailed datum on how we behave could also help we understand how to be healthy originally publish at www sciencerockstar com on november 2 2013 from a quick cheer to a stand ovation clap to show how much you enjoy this story let s fix the future scientific advisor @jadatascience a blog series about the discipline of business experimentation how to run and learn from experiment in different contexts be a complex matter but lay at the heart of innovation
Eventbrite,10,8,https://medium.com/@eventbrite/multi-index-locality-sensitive-hashing-for-fun-and-profit-ee04292a6e37?source=tag_archive---------4----------------,multi index locality sensitive hash for fun and profit,one way that we deal with this volume of datum be to cluster up all the similar message together to find pattern in behavior of sender for example if someone be contact thousand of different organizer with similar message that behavior be suspect and will be examine the big question be how can we compare every single message we see with every other message efficiently and accurately in this article we ll be explore a technique know as multi index locality sensitive hashing to perform the the comparison efficiently we pre process the datum with a series of step let s first define what similar message be here we have and example of two similar message a and b to our human eye of course they re similar but we want determine this similarity quantitatively the solution be to break up the message into token and then treat each message as a bag of token the simple naive way to do tokenization be to split up a message on space punctuation and convert each character to lowercase so our result from our tokenization of the above message would be I ll leave as an exercise to the reader to come up with more interesting way to do tokenization for handle contraction plural foreign language etc to calculate the similarity between these two bag of token we ll use an estimation know as the jaccard similarity coefficient this be define as the ratio of size of the intersection and union of a and b therefore in our example we ll then set a threshold above which we will consider two message to be similar so then when give a set of m message we simply compute the similarity of a message to every other message this work in theory but in practice there be case where this metric be unreliable eg if one message be significantly long than the other ; not to mention horribly inefficient o n2 m2 where n be the number of token per message we need do thing smart one problem with do a simple jaccard similarity be that the scale of the value change with the size number of token of the message to address this we can transform our token with a method know as minhash here s a psuedo code snippet the interesting property of the minhash transformation be that it leave we with a constant n number of hash and that choose hash will be in the same position in the vector after the minhash transformation the jaccard similarity can be approximate by an element wise comparison of two hash vector implement as pseudo code above so we can stop here but we re have so much fun and we can do so much well notice when we do comparison we have to to o n integer comparison and if we have m message then compare every message to each other be o n m2 integer comparison this be still not acceptable to reduce the time complexity of compare minhashe to each other we can do well with a technique know as bit sample the main idea be that we don t need to know the exact value of each hash but only that the hash be equal at their respective position in each hash vector with this insight let s only look at the least significant bit lsb of each hash value more pseudo code when compare two message if the hash be equal in the same position in the minhash vector then the bit in the equivalent position after bit sample should be also equal so we can emulate the jaccard similarity of two minhashe by count the equal bit in the two bit vector aka the hamming distance and divide by the number of bit of course two different hash will have the same lsb 50 % of the time ; to increase our efficacy we would pick a large n initially here be some naive and inefficient pseudo code in practice more efficient implementation of the bitsimilarity function can calculate in near o 1 time for reasonable size of n bit twiddle hack this mean that when compare m message to each other we ve reduce the time complexity to o m2 but wait there s more remember how I say we have a lot of datum o m2 be still unreasonable when m be a very large number of message so we need to try to reduce the number of comparison to make use a divide and conquer strategy let start with an example where we set n=32 and we want to have a bitsimilarity of 9 in the bad case to do this we need 28 of the 32 bit to be equal or 4 bit unequal we will refer to the number of unequal bit as the radius of the bit vector ; ie if two bit vector be within a certain radius of bit then they be similar the unequal bit can be find by take the bit wise xor of the two bit vector for example if we split up xor_mask into 4 chunk of 8 bit then at least one chunk will have exactly zero or exactly one of the bit difference pigeonhole principal more generally if we split xor_mask of size n into k chunk with an expect radius r then at least one chunk be guarantee to have floor r k or less bit unequal for the purpose of explanation we will assume that we have choose all the parameter such that floor r k = 1 now you re wonder how this piece of logic help we we can now design a data structure lshtable to index the bit vector to reduce the number of bitsimilarity comparison drastically but increase memory consumption in o m fast search in hamming space with multi index hash we will define lshtable with some pseudo code basically in lshtable initialization we create k hash table for each k chunk during add of a bit vector we split the bit vector into k chunk for each of these chunk we add the original bit vector into the associated hash table under the index chunk upon the lookup of a bit vector we once again split it into chunk and for each chunk look up the associated hash table for a chunk that s close zero or one bit off the returned list be a set of candidate bit vector to check bitsimilarity because of the property explain in the previous section at least one hash table will contain a set of candidate that contain a similar bit vector to compare every m message to every other message we first insert its bit vector into an lshtable an o k operation k be constant then to find similar message we simply do a lookup from the lshtable another o k operation and then check bitsimilarity for each of the candidate return the number of candidate to check be usually on the order of m 2^ n k if at all therefore the time complexity to compare all m message to each other be o m * m 2^ n k in practice n and k be empirically choose such that 2^ n k > > m so the final time complexity be o m — remember we start with o n m2 phew what a ride so we ve detail how to find similar message in a very large set of message efficiently by use multi index locality sensitivity hash we can reduce the time complexity of from quadratic with a very high constant to near linear with a more manageable constant I should also mention that many of the ancillary pseudo code excerpt use here describe the most naive implementation of each method and be for instructive purpose only from a quick cheer to a stand ovation clap to show how much you enjoy this story we help bring the world together through live experience
Akash Shende,1,3,https://medium.com/@akash0x53/color-based-object-segmentation-baf8044ec6a3?source=tag_archive---------7----------------,color base object segmentation akash shende medium,in this picture pranav mistry be use color marker on his finger to track the gesture and his wearable computer perform action base on gesture that sound easy but no it s not computer need to understand those color marker first for that it need to separate marker from any surrounding segmentation can be helpful to achieve this various method be available for segmentation however this article talk about robust color base object segmentation create binary mask that separate blue t shirt from rest to find blue t shirt in give image I use opencv s inrange method which take color or greyscale image low & high range value as its parameter and return binary image where pixel value set to 0 when input pixel doesn t fall in specified range otherwise pixel value set to 1 with the help of this function and after determine range value I end up with this mask but you can see there be problem it s not able to create mask for complete t shirt also it mask eye which aren t blue this be happen because light from one side of body whiten the right side at the same time create shadow in left region thus it create different shade of blue and result into partial segmentation normalization of color plane reduce variation in light by average pixel value thus it remove highlight and shadow region and make image flatten follow image be free from highlight & shadow and it be divide into one large green background blue t shirt and skin now the inrange method able to mask only t shirt follow function convert a pixel at x y location into its corresponding normalize rgb pixel let r g b be pixel value then normalize pixel g x y be calculate as divide the individual color component by sum of all color component and multiply by 255 division result into float point number in range of 0 0 to 1 0 and as this be 8 bit image result be scale up by 255 this function accept 8 bit rgb image matrix of size 800x600 and return normalize rgb image originally publish at akash0x53 github io on april 29 2013 from a quick cheer to a stand ovation clap to show how much you enjoy this story python आणि बरच काही
Hrishikesh Huilgolkar,1,4,https://medium.com/@hrishikeshio/traveling-santa-problem-an-incompetent-algorists-attempt-49ad9d26b26?source=tag_archive---------8----------------,travel santa problem — an incompetent algorist s attempt,kaggle announce the travel santa problem in the christmas season I join in excitedly but soon realize this be not an easy problem solve this problem would require expertise on datum structure and some good familiarity with tsp problem and its many heuristic algorithms I have neither I have to find a way to deal with this problem I compensete my lack of algorithmic expertise with common sense logic and intuition I finish 65th out of 356 total competitor I do some research on package tsp solver and top tsp algorithm I find concorde but I could not get it to work on my ubuntu machine so I settle with lkh which use lin kernighan heuristic for solve tsp and related problem I write script for file conversion and for run lkh lkh easily solve my tsp problem in around 30 hour but it be just one path I still have to figure out how to make it find the second path a simple idea to get 2 disjoint path be to generate first path and then make weight of those edge infinite and run lkh on the problem again but this require the problem to be in distance matrix format then I find a major problem problem ram too lowcreate distance matrix for 150 000 point be unimaginable it would requirememory for one digit * 150 000 * 150 000assuming memory for one digit = 8 byte memory require = 8 * 150 0002which be 167 gb correct I if I be wrong solution a simple solution be to divide the map in manageable chunk I use scipy s distance matrix creation function scipy spatial distance pdist it create distance matrix from coordinate the matrix create by pdist be in compress form a flatten matrix of upper diagonal element scipy spatial distance squareform can create a square distance matrix from compress matrix but that would waste a lot of ram so I create a custom function which divide compress matrix by row so lkh can read it input coordinate 1 12 34 1 output of pdist compress upper column 1 2 4 output of squreform uncompressed square matrix 0 1 21 0 42 4 0 output of my function which process compress matrix upper diagonal element 1 2 4 lot of ram save I try use manhattan distance instead of euclidean distance but after divide the problem in grid time take by distance calculation be manageable so I stick with euclidean distance through trial and error I find that on my laptop with 4 gb ram a 6 by 6 grid in the above format be manageable for both create distance matrix and for lkh I run lkh on result distance matrix and join the individual solution I join the result solution in different combination for both path so as to avoid common path I get 7 415 334 with this method I try time limit on lkh algorithm from 40 000 second I reduce it to 300 20 5 1 second but it make the result slightly worse mingle the solution above be good but it could have be well the problem be that the first path be so good that the second path struggle to find good path the difference between the two path be big path1 ~= 6 2mpath2 ~= 7 4mfor a long time I think this would require either solve both path simultaneously or use genetic algorithm or similar algorithm to combine both path both be pretty difficult to implement then I get a simple idea my map be divide in 36 square if I combine 18 square of first path and 18 square of second path I will have a path whose distance will be approximately average of the two path I try this trick and use different combination of the two path square and get the good score of 6 807 498 for new path1 select blue squre from old path1 and grey square from old path2 use remain square for new path2 remove cross line my square be join in a zigzag manner I remove the zig zag line for a further improvement I score 6 744 291 which be my good score another idea be to make end point of one square and the beginning point of next square as near as possible but I couldn t implement the idea before deadline my score be around 200 000 point away from the first place which be 6 526 972 not bad public repo https bitbucket org hrishikeshio travel santa more documentation for source code come soon originally publish at www blogicious com on january 19 2013 from a quick cheer to a stand ovation clap to show how much you enjoy this story blockchain cryptocurrencie and the decentralised future
Adam Geitgey,35K,15,https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471?source=tag_archive---------0----------------,machine learning be fun adam geitgey medium,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 日本語 português português alternate türkçe français 한국어 العَرَبِيَّة‎‎ español méxico español españa polski italiano 普通话 русский 한국어 tiếng việt or فارسی big update the content of this article be now available as a full length video course that walk you through every step of the code you can take the course for free and access everything else on lynda com free for 30 day if you sign up with this link have you hear people talk about machine learning but only have a fuzzy idea of what that mean be you tired of nod your way through conversation with co worker let s change that this guide be for anyone who be curious about machine learning but have no idea where to start I imagine there be a lot of people who try read the wikipedia article get frustrate and give up wish someone would just give they a high level explanation that s what this be the goal be be accessible to anyone — which mean that there s a lot of generalization but who care if this get anyone more interested in ml then mission accomplished machine learning be the idea that there be generic algorithm that can tell you something interesting about a set of datum without you have to write any custom code specific to the problem instead of write code you feed datum to the generic algorithm and it build its own logic base on the datum for example one kind of algorithm be a classification algorithm it can put datum into different group the same classification algorithm use to recognize handwritten number could also be use to classify email into spam and not spam without change a line of code it s the same algorithm but it s feed different training datum so it come up with different classification logic machine learning be an umbrella term cover lot of these kind of generic algorithm you can think of machine learning algorithm as fall into one of two main category — supervise learning and unsupervise learn the difference be simple but really important let s say you be a real estate agent your business be grow so you hire a bunch of new trainee agent to help you out but there s a problem — you can glance at a house and have a pretty good idea of what a house be worth but your trainee don t have your experience so they don t know how to price their house to help your trainee and maybe free yourself up for a vacation you decide to write a little app that can estimate the value of a house in your area base on it s size neighborhood etc and what similar house have sell for so you write down every time someone sell a house in your city for 3 month for each house you write down a bunch of detail — number of bedroom size in square foot neighborhood etc but most importantly you write down the final sale price use that training datum we want to create a program that can estimate how much any other house in your area be worth this be call supervised learning you know how much each house sell for so in other word you know the answer to the problem and could work backwards from there to figure out the logic to build your app you feed your training datum about each house into your machine learn algorithm the algorithm be try to figure out what kind of math need to be do to make the number work out this kind of like have the answer key to a math test with all the arithmetic symbol erase from this can you figure out what kind of math problem be on the test you know you be suppose to do something with the number on the left to get each answer on the right in supervised learning you be let the computer work out that relationship for you and once you know what math be require to solve this specific set of problem you could answer to any other problem of the same type let s go back to our original example with the real estate agent what if you didn t know the sale price for each house even if all you know be the size location etc of each house it turn out you can still do some really cool stuff this be call unsupervised learn this be kind of like someone give you a list of number on a sheet of paper and say I don t really know what these number mean but maybe you can figure out if there be a pattern or group or something — good luck so what could do with this datum for starter you could have an algorithm that automatically identify different market segment in your datum maybe you d find out that home buyer in the neighborhood near the local college really like small house with lot of bedroom but home buyer in the suburb prefer 3 bedroom house with lot of square footage know about these different kind of customer could help direct your marketing effort another cool thing you could do be automatically identify any outlier house that be way different than everything else maybe those outlier house be giant mansion and you can focus your good sale people on those area because they have big commission supervise learning be what we ll focus on for the rest of this post but that s not because unsupervised learning be any less useful or interesting in fact unsupervised learning be become increasingly important as the algorithm get well because it can be use without have to label the datum with the correct answer side note there be lot of other type of machine learning algorithm but this be a pretty good place to start as a human your brain can approach most any situation and learn how to deal with that situation without any explicit instruction if you sell house for a long time you will instinctively have a feel for the right price for a house the good way to market that house the kind of client who would be interested etc the goal of strong ai research be to be able to replicate this ability with computer but current machine learning algorithm aren t that good yet — they only work when focus a very specific limited problem maybe a well definition for learn in this case be figure out an equation to solve a specific problem base on some example datum unfortunately machine figure out an equation to solve a specific problem base on some example datum isn t really a great name so we end up with machine learning instead of course if you be read this 50 year in the future and we ve figure out the algorithm for strong ai then this whole post will all seem a little quaint maybe stop read and go tell your robot servant to go make you a sandwich future human so how would you write the program to estimate the value of a house like in our example above think about it for a second before you read far if you didn t know anything about machine learn you d probably try to write out some basic rule for estimate the price of a house like this if you fiddle with this for hour and hour you might end up with something that sort of work but your program will never be perfect and it will be hard to maintain as price change wouldn t it be well if the computer could just figure out how to implement this function for you who care what exactly the function do as long be it return the correct number one way to think about this problem be that the price be a delicious stew and the ingredient be the number of bedroom the square footage and the neighborhood if you could just figure out how much each ingredient impact the final price maybe there s an exact ratio of ingredient to stir in to make the final price that would reduce your original function with all those crazy if s and else s down to something really simple like this notice the magic number in bold — 841231951398213 1231 1231231 2 3242341421 and 201 23432095 these be our weight if we could just figure out the perfect weight to use that work for every house our function could predict house price a dumb way to figure out the good weight would be something like this start with each weight set to 1 0 run every house you know about through your function and see how far off the function be at guess the correct price for each house for example if the first house really sell for $ 250 000 but your function guess it sell for $ 178 000 you be off by $ 72 000 for that single house now add up the square amount you be off for each house you have in your data set let s say that you have 500 home sale in your datum set and the square of how much your function be off for each house be a grand total of $ 86 123 373 that s how wrong your function currently be now take that sum total and divide it by 500 to get an average of how far off you be for each house call this average error amount the cost of your function if you could get this cost to be zero by play with the weight your function would be perfect it would mean that in every case your function perfectly guess the price of the house base on the input datum so that s our goal — get this cost to be as low as possible by try different weight repeat step 2 over and over with every single possible combination of weight whichever combination of weight make the cost close to zero be what you use when you find the weight that work you ve solve the problem that s pretty simple right well think about what you just do you take some datum you feed it through three generic really simple step and you end up with a function that can guess the price of any house in your area watch out zillow but here s a few more fact that will blow your mind pretty crazy right ok of course you can t just try every combination of all possible weight to find the combo that work the good that would literally take forever since you d never run out of number to try to avoid that mathematician have figure out lot of clever way to quickly find good value for those weight without have to try very many here s one way first write a simple equation that represent step # 2 above now let s re write exactly the same equation but use a bunch of machine learn math jargon that you can ignore for now this equation represent how wrong our price estimating function be for the weight we currently have set if we graph this cost equation for all possible value of our weight for number_of_bedroom and sqft we d get a graph that might look something like this in this graph the low point in blue be where our cost be the low — thus our function be the least wrong the high point be where we be most wrong so if we can find the weight that get we to the low point on this graph we ll have our answer so we just need to adjust our weight so we be walk down hill on this graph towards the low point if we keep make small adjustment to our weight that be always move towards the low point we ll eventually get there without have to try too many different weight if you remember anything from calculus you might remember that if you take the derivative of a function it tell you the slope of the function s tangent at any point in other word it tell we which way be downhill for any give point on our graph we can use that knowledge to walk downhill so if we calculate a partial derivative of our cost function with respect to each of our weight then we can subtract that value from each weight that will walk we one step close to the bottom of the hill keep do that and eventually we ll reach the bottom of the hill and have the good possible value for our weight if that didn t make sense don t worry and keep read that s a high level summary of one way to find the good weight for your function call batch gradient descent don t be afraid to dig deeply if you be interested on learn the detail when you use a machine learning library to solve a real problem all of this will be do for you but it s still useful to have a good idea of what be happen the three step algorithm I describe be call multivariate linear regression you be estimate the equation for a line that fit through all of your house data point then you be use that equation to guess the sale price of house you ve never see before base where that house would appear on your line it s a really powerful idea and you can solve real problem with it but while the approach I show you might work in simple case it win t work in all case one reason be because house price aren t always simple enough to follow a continuous line but luckily there be lot of way to handle that there be plenty of other machine learning algorithm that can handle non linear datum like neural network or svms with kernel there be also way to use linear regression more cleverly that allow for more complicated line to be fit in all case the same basic idea of need to find the good weight still apply also I ignore the idea of overfitte it s easy to come up with a set of weight that always work perfectly for predict the price of the house in your original datum set but never actually work for any new house that weren t in your original datum set but there be way to deal with this like regularization and use a cross validation datum set learn how to deal with this issue be a key part of learn how to apply machine learning successfully in other word while the basic concept be pretty simple it take some skill and experience to apply machine learning and get useful result but it s a skill that any developer can learn once you start see how easily machine learning technique can be apply to problem that seem really hard like handwriting recognition you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough datum just feed in the datum and watch the computer magically figure out the equation that fit the datum but it s important to remember that machine learn only work if the problem be actually solvable with the datum that you have for example if you build a model that predict home price base on the type of pot plant in each house it s never go to work there just isn t any kind of relationship between the pot plant in each house and the home s sale price so no matter how hard it try the computer can never deduce a relationship between the two so remember if a human expert couldn t use the datum to solve the problem manually a computer probably win t be able to either instead focus on problem where a human could solve the problem but where it would be great if a computer could solve it much more quickly in my mind the big problem with machine learning right now be that it mostly live in the world of academia and commercial research group there isn t a lot of easy to understand material out there for people who would like to get a broad understanding without actually become expert but it s get a little well every day if you want to try out what you ve learn in this article I make a course that walk you through every step of this article include write all the code give it a try if you want to go deep andrew ng s free machine learning class on coursera be pretty amazing as a next step I highly recommend it it should be accessible to anyone who have a comp sci degree and who remember a very minimal amount of math also you can play around with ton of machine learning algorithm by download and instal scikit learn it s a python framework that have black box version of all the standard algorithm if you like this article please consider sign up for my machine learning be fun newsletter also please check out the full length course version of this article it cover everything in this article in more detail include write the actual code in python you can get a free 30 day trial to watch the course if you sign up with this link you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 2 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Shivon Zilis,1.2K,10,https://medium.com/@shivon/the-current-state-of-machine-intelligence-f76c20db2fe1?source=tag_archive---------1----------------,the current state of machine intelligence shivon zilis medium,the 2016 machine intelligence landscape and post can be find here I spend the last three month learn about every artificial intelligence machine learning or datum relate startup I could find — my current list have 2 529 of they to be exact yes I should find well thing to do with my evening and weekend but until then why do this a few year ago investor and startup be chase big datum I help put together a landscape on that industry now we re see a similar explosion of company call themselves artificial intelligence machine learning or somesuch — collectively I call these machine intelligence I ll get into the definition in a second our fund bloomberg beta which be focus on the future of work have be invest in these approach I create this landscape to start to put startup into context I m a thesis orient investor and it s much easy to identify crowded area and see white space once the landscape have some sort of taxonomy what be machine intelligence anyway I mean machine intelligence as a unifying term for what other call machine learning and artificial intelligence some other have use the term before without quite describe it or understand how laden this field have be with debate over description I would have prefer to avoid a different label but when I try either artificial intelligence or machine learning both prove to too narrow when I call it artificial intelligence too many people be distract by whether certain company be true ai and when I call it machine learn many think I wasn t do justice to the more ai esque like the various flavor of deep learning people have immediately grasp machine intelligence so here we be ☺ computer be learn to think read and write they re also pick up human sensory function with the ability to see and hear arguably to touch taste and smell though those have be of a less focus machine intelligence technology cut across a vast array of problem type from classification and clustering to natural language processing and computer vision and method from support vector machine to deep belief network all of these technology be reflect on this landscape what this landscape doesn t include however important be big datum technology some have use this term interchangeably with machine learning and artificial intelligence but I want to focus on the intelligence method rather than datum storage and computation piece of the puzzle for this landscape though of course data technology enable machine intelligence which company be on the landscape I consider thousand of company so while the chart be crowd it s still a small subset of the overall ecosystem admission rate to the chart be fairly in line with those of yale or harvard and perhaps equally arbitrary ☺ I try to pick company that use machine intelligence method as a define part of their technology many of these company clearly belong in multiple area but for the sake of simplicity I try to keep company in their primary area and categorize they by the language they use to describe themselves instead of quibble over whether a company use nlp accurately in its self description if you want to get a sense for innovation at the heart of machine intelligence focus on the core technology layer some of these company have api that power other application some sell their platform directly into enterprise some be at the stage of cryptic demos and some be so stealthy that all we have be a few sentence to describe they the most exciting part for I be see how much be happen in the application space these company separate nicely into those that reinvent the enterprise industry and ourselves if I be look to build a company right now I d use this landscape to help figure out what core and support technology I could package into a novel industry application everyone like solve the sexy problem but there be an incredible amount of unsexy industry use case that have massive market opportunity and powerful enable technology that be beg to be use for creative application e g watson developer cloud alchemyapi reflection on the landscape we ve see a few great article recently outline why machine intelligence be experience a resurgence document the enable factor of this resurgence kevin kelly for example chalk it up to cheap parallel compute large dataset and well algorithm I focus on understand the ecosystem on a company by company level and draw implication from that yes it s true machine intelligence be transform the enterprise industry and human alike on a high level it s easy to understand why machine intelligence be important but it wasn t until I lay out what many of these company be actually do that I start to grok how much it be already transform everything around we as kevin kelly more provocatively put it the business plan of the next 10 000 startup be easy to forecast take x and add ai in many case you don t even need the x — machine intelligence will certainly transform exist industry but will also likely create entirely new one machine intelligence be enable application we already expect like automate assistant siri adorable robot jibo and identify people in image like the highly effective but unfortunately name deepface however it s also do the unexpected protect child from sex trafficking reduce the chemical content in the lettuce we eat help we buy shoe online that fit our foot precisely and destroy 80 s classic video game many company will be acquire I be surprised to find that over 10 % of the eligible non public company on the slide have be acquire it be in stark contrast to big datum landscape we create which have very few acquisition at the time no jaw will drop when I reveal that google be the number one acquirer though there be more than 15 different acquirer just for the company on this chart my guess be that by the end of 2015 almost another 10 % will be acquire for thought on which specific one will get snap up in the next year you ll have to twist my arm big company have a disproportionate advantage especially those that build consumer product the giant in search google baidu social network facebook linkedin pinter content netflix yahoo mobile apple and e commerce amazon be in an incredible position they have massive dataset and constant consumer interaction that enable tight feedback loop for their algorithm and these factor combine to create powerful network effect — and they have the most to gain from the low hang fruit that machine intelligence bear well in class personalization and recommendation algorithm have enable these company success it s both impressive and disconcert that facebook recommend you add the person you have a crush on in college and netflix tee up that perfect guilty pleasure sitcom now they be all compete in a new battlefield the move to mobile win mobile will require lot of machine intelligence state of the art natural language interface like apple s siri visual search like amazon s firefly and dynamic question answer technology that tell you the answer instead of provide a menu of link all of the search company be wrestle with this large enterprise company ibm and microsoft have also make incredible stride in the field though they don t have the same human facing requirement so be focus their attention more on knowledge representation task on large industry dataset like ibm watson s application to assist doctor with diagnosis the talent s in the new ai vy league in the last 20 year most of the good mind in machine intelligence especially the hardcore ai type work in academia they develop new machine intelligence method but there be few real world application that could drive business value now that real world application of more complex machine intelligence method like deep belief net and hierarchical neural network be start to solve real world problem we re see academic talent move to corporate setting facebook recruit nyu professor yann lecun and rob fergus to their ai lab google hire university of toronto s geoffrey hinton baidu woo andrew ng it s important to note that they all still give back significantly to the academic community one of lecun s lab mandate be to work on core research to give back to the community hinton spend half of his time teach ng have make machine intelligence more accessible through coursera but it be clear that a lot of the intellectual horsepower be move away from academia for aspire mind in the space these corporate lab not only offer lucrative salary and access to the godfather of the industry but the most important ingredient datum these lab offer talent access to dataset they could never get otherwise the imagenet dataset be fantastic but can t compare to what facebook google and baidu have in house as a result we ll likely see corporation become the home of many of the most important innovation in machine intelligence and recruit many of the graduate student and postdoc that would have otherwise stay in academia there will be a peace dividend big company have an inherent advantage and it s likely that the one who will win the machine intelligence race will be even more powerful than they be today however the good news for the rest of the world be that the core technology they develop will rapidly spill into other area both via depart talent and publish research similar to the big datum revolution which be spark by the release of google s bigtable and bigquery paper we will see corporation release equally groundbreake new technology into the community those innovation will be adapt to new industry and use case that the google of the world don t have the dna or desire to tackle opportunity for entrepreneur my company do deep learning for x few word will make you more popular in 2015 that be if you can credibly say they deep learning be a particularly popular method in the machine intelligence field that have be get a lot of attention google facebook and baidu have achieve excellent result with the method for vision and language base task and startup like enlitic have show promise result as well yes it will be an overused buzzword with excitement ahead of result and business model but unlike the hundred of company that say they do big datum it s much easy to cut to the chase in term of verify credibility here if you re pay attention the most exciting part about the deep learning method be that when apply with the appropriate level of care and feed it can replace some of the intuition that come from domain expertise with automatically learn feature the hope be that in many case it will allow we to fundamentally rethink what a good in class solution be as an investor who be curious about the quirki application of datum and machine intelligence I can t wait to see what creative problem deep learning practitioner try to solve I completely agree with jeff hawkins when he say a lot of the killer application of these type of technology will sneak up on we I fully intend to keep an open mind acquihire as a business model people say that datum scientist be unicorn in short supply the talent crunch in machine intelligence will make it look like we have a glut of data scientist in the data field many people have industry experience over the past decade most hardcore machine intelligence work have only be in academia we win t be able to grow this talent overnight this shortage of talent be a boon for founder who actually understand machine intelligence a lot of company in the space will get seed funding because there be early sign that the acquihire price for a machine intelligence expert be north of 5x that of a normal technical acquihire take for example deep mind where price per technical head be somewhere between $ 5 10 m if we choose to consider it in the acquihire category I ve have multiple friend ask I only semi jokingly shivon should I just round up all of my smart friend in the ai world and call it a company to be honest I m not sure what to tell they at bloomberg beta we d rather back company build for the long term but that doesn t mean this win t be a lucrative strategy for many enterprise founder a good demo be disproportionately valuable in machine intelligence I remember watch watson play jeopardy when it struggle at the beginning I feel really sad for it when it start trounce its competitor I remember cheer it on as if it be the toronto maple leafs in the stanley cup final disclaimer 1 I be an ibmer at the time so be bias towards my team 2 the maple leafs have not make the final during my lifetime — yet — so that be purely a hypothetical why do these awe inspire demos matter the last wave of technology company to ipo didn t have demos that most of we would watch so why should machine intelligence company the last wave of company be very computer like database company enterprise application and the like sure I d like to see a 10x more performant database but most people wouldn t care machine intelligence win and lose on demos because 1 the technology be very human enough to inspire shock and awe 2 business model tend to take a while to form so they need more funding for long period of time to get they there 3 they be fantastic acquisition bait watson beat the world s good human at trivium even if it think toronto be a us city deepmind blow people away by beat video game vicarious take on captcha there be a few company still in stealth that promise to impress beyond that and I can t wait to see if they get there demo or not I d love to talk to anyone use machine intelligence to change the world there s no industry too unsexy no problem too geeky I d love to be there to help so don t be shy I hope this landscape chart spark a conversation the goal to be make this a live document and I want to know if there be company or category miss I welcome feedback and would like to put together a dynamic visualization where I can add more company and dimension to the data method use datum type end user investment to date location etc so that folk can interact with it to well explore the space question and comment please email I thank you to andrew paprocki aria haghighi beau cronin ben lorica doug fulop david andrzejewski eric berlow eric jonas gary kazantsev gideon mann greg smithies heidi skinner jack clark jon lehr kurt keutzer lauren barless pete skomoroch pete warden roger magoulas sean gourley stephen purpura wes mckinney zach bogue the quid team and the bloomberg beta team for your ever helpful perspective disclaimer bloomberg beta be an investor in adatao alation aviso brightfunnel context relevant mavrx newsle orbital insight pop up archive and two other on the chart that be still undisclosed we re also investor in a few other machine intelligence company that aren t focus on area that be a fit for this landscape so we leave they off for the full resolution version of the landscape please click here from a quick cheer to a stand ovation clap to show how much you enjoy this story partner at bloomberg beta all about machine intelligence for good equal part nerd and athlete straight up canadian stereotype and proud of it
AirbnbEng,369,11,https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60?source=tag_archive---------2----------------,architecte a machine learning system for risk airbnb engineering & data science medium,by naseem hakim & aaron keys at airbnb we want to build the world s most trust community guest trust airbnb to connect they with world class host for unique and memorable travel experience airbnb host trust that guest will treat their home with the same care and respect that they would their own the airbnb review system help user find community member who earn this trust through positive interaction with other and the ecosystem as a whole prosper the overwhelming majority of web user act in good faith but unfortunately there exist a small number of bad actor who attempt to profit by defraud website and their community the trust and safety team at airbnb work across many discipline to help protect our user from these bad actor ideally before they have the opportunity to impart negativity on the community there be many different kind of risk that online business may have to protect against with vary exposure depend on the particular business for example email provider devote significant resource to protect user from spam whereas payment company deal more with credit card chargeback we can mitigate the potential for bad actor to carry out different type of attack in different way many risk can be mitigate through user face change to the product that require additional verification from the user for example require email confirmation or implement 2fa to combat account takeover as many bank have do script attack be often associate with a noticeable increase in some measurable metric over a short period of time for example a sudden 1000 % increase in reservation in a particular city could be a result of excellent marketing or fraud fraudulent actor often exhibit repetitive pattern as we recognize these pattern we can apply heuristic to predict when they be about to occur again and help stop they for complex evolve fraud vector heuristic eventually become too complicated and therefore unwieldy in such case we turn to machine learning which will be the focus of this blog post for a more detailed look at other aspect of online risk management check out ohad samet s great ebook different risk vector can require different architecture for example some risk vector be not time critical but require computationally intensive technique to detect an offline architecture be well suited for this kind of detection for the purpose of this post we be focus on risk require realtime or near realtime action from a broad perspective a machine learn pipeline for these kind of risk must balance two important goal these may seem like compete goal since optimize for realtime calculation during a web transaction create a focus on speed and reliability whereas optimize for model building and iteration create more of a focus on flexibility at airbnb engineering and datum team have work closely together to develop a framework that accommodate both goal a fast robust scoring framework with an agile model building pipeline in keep with our service orient architecture we build a separate fraud prediction service to handle derive all the feature for a particular model when a critical event occur in our system e g a reservation be create we query the fraud prediction service for this event this service can then calculate all the feature for the reservation creation model and send these feature to our openscoring service which be describe in more detail below the openscore service return a score and a decision base on a threshold we ve set and the fraud prediction service can then use this information to take action I e put the reservation on hold the fraud prediction service have to be fast to ensure that we be take action on suspicious event in near realtime like many of our backend service for which performance be critical it be build in java and we parallelize the database query necessary for feature generation however we also want the freedom to occasionally do some heavy computation in derive feature so we run it asynchronously so that we be never block for reservation etc this asynchronous model work for many situation where a few second of delay in fraud detection have no negative effect it s worth note however that there be case where you may want to react in realtime to block transaction in which case a synchronous query and precomputed feature may be necessary this service be build in a very modular way and expose an internal restful api making add new event and model easy openscoring be a java service that provide a json rest interface to the java predictive model markup language pmml evaluator jpmml both jpmml and openscoring be open source project release under the apache 2 0 license and author by villu ruusmann edit — the most recent version be license the under agpl 3 0 the jpmml backend of openscore consume pmml an xml markup language that encode several common type of machine learning model include tree model logit model svms and neural network we have streamline openscore for a production environment by add several feature include kafka logging and statsd monitoring andy kramolisch have modify openscore to permit use several model simultaneously as describe below there be several consideration that we weigh carefully before move forward with openscore after consider all of these factor we decide that openscore well satisfy our two pronged goal of have a fast and robust yet flexible machine learning framework a schematic of our model building pipeline use pmml be illustrate above the first step involve derive feature from the datum store on the site since the combination of feature that give the optimal signal be constantly change we store the feature in a json format which allow we to generalize the process of loading and transform feature base on their name and type we then transform the raw feature through bucketing or bin value and replace miss value with reasonable estimate to improve signal we also remove feature that be show to be statistically unimportant from our dataset while we omit most of the detail regard how we perform these transformation for brevity here it be important to recognize that these step take a significant amount of time and care we then use our transform feature to train and cross validate the model use our favorite pmml compatible machine learning library and upload the pmml model to openscore the final model be test and then use for decision making if it become the good performer the model training step can be perform in any language with a library that output pmml one commonly use and well support library be the r pmml package as illustrate below generate a pmml with r require very little code this r script have the advantage of simplicity and a script similar to this be a great way to start build pmmls and to get a first model into production in the long run however a setup like this have some disadvantage first our script require that we perform feature transformation as a pre processing step and therefore we have add these transformation instruction to the pmml by edit it afterwards the r pmml package support many pmml transformation and datum manipulation but it be far from universal we deploy the model as a separate step — post model training — and so we have to manually test it for validity which can be a time consuming process yet another disadvantage of r be that the implementation of the pmml exporter be somewhat slow for a random forest model with many feature and many tree however we ve find that simply re write the export function in c++ decrease run time by a factor of 10 000 from a few day to a few second we can get around the drawback of r while maintain its advantage by build a pipeline base on python and scikit learn scikit learn be a python package that support many standard machine learning model and include helpful utility for validate model and perform feature transformation we find that python be a more natural language than r for ad hoc data manipulation and feature extraction we automate the process of feature extraction base on a set of rule encode in the name and type of variable in the feature json ; thus new feature can be incorporate into the model pipeline with no change to the exist code deployment and testing can also be perform automatically in python by use its standard network library to interface with openscore standard model performance test precision recall roc curve etc be carry out use sklearn s build in capability sklearn do not support pmml export out of the box so have write an in house exporter for particular sklearn classifier when the pmml file be upload to openscore it be automatically test for correspondence with the scikit learn model it represent because feature transformation model building model validation deployment and testing be all carry out in a single script a data scientist or engineer be able to quickly iterate on a model base on new feature or more recent datum and then rapidly deploy the new model into production although this blog post have focus mostly on our architecture and model building pipeline the truth be that much of our time have be spend elsewhere our process be very successful for some model but for other we encounter poor precision recall initially we consider whether we be experience a bias or a variance problem and try use more datum and more feature however after find no improvement we start dig deep into the datum and find that the problem be that our ground truth be not accurate consider chargeback as an example a chargeback can be not as describe nad or fraud this be a simplification and group both type of chargeback together for a single model would be a bad idea because legitimate user can file nad chargeback this be an easy problem to resolve and not one we actually have agent categorize chargeback as part of our workflow ; however there be other type of attack where distinguish legitimate activity from illegitimate be more subtle and necessitate the creation of new datum store and log pipeline most people who ve work in machine learning will find this obvious but it s worth re stress towards this end sometimes you don t know what datum you re go to need until you ve see a new attack especially if you haven t work in the risk space before or have work in the risk space but only in a different sector so the good advice we can offer in this case be to log everything throw it all in hdfs whether you need it now or not in the future you can always use this datum to backfill new data store if you find it useful this can be invaluable in respond to a new attack vector although our current ml pipeline use scikit learn and openscore our system be constantly evolve our current setup be a function of the stage of the company and the amount of resource both in term of personnel and datum that be currently available small company may only have a few ml model in production and a small number of analyst and can take time to manually curate datum and train the model in many non standardized step large company might have many many model and require a high degree of automation and get a sizable boost from online training a unique challenge of work at a hyper growth company be that landscape fundamentally change year over year and pipeline need to adjust to account for this as our datum and log pipeline improve invest in improve learning algorithm will become more worthwhile and we will likely shift to test new algorithm incorporate online learning and expand on our model building framework to support large datum set additionally some of the most important opportunity to improve our model be base on insight into our unique datum feature selection and other aspect our risk system that we be not able to share publicly we would like to acknowledge the other engineer and analyst who have contribute to these critical aspect of this project we work in a dynamic highly collaborative environment and this project be an example of how engineer and datum scientist at airbnb work together to arrive at a solution that meet a diverse set of need if you re interested in learn more contact we about our data science and engineering team originally publish at nerd airbnb com on june 16 2014 from a quick cheer to a stand ovation clap to show how much you enjoy this story creative engineer and datum scientist build a world where you can belong anywhere http airbnb io creative engineer and datum scientist build a world where you can belong anywhere http airbnb io
Yingjie Miao ,43,6,https://medium.com/kifi-engineering/from-word2vec-to-doc2vec-an-approach-driven-by-chinese-restaurant-process-93d3602eaa31?source=tag_archive---------3----------------,from word2vec to doc2vec an approach drive by chinese restaurant process,google s word2vec project have create lot of interest in the text mining community it s a neural network language model that be both supervise and unsupervise unsupervised in the sense that you only have to provide a big corpus say english wiki supervise in the sense that the model cleverly generate supervised learning task from the corpus how two approach know as continuous bag of word cbow and skip gram see figure 1 in this paper cbow force the neural net to predict current word by surround word and skip gram force the neural net to predict surround word of the current word training be essentially a classic back propagation method with a few optimization and approximation trick e g hierarchical softmax word vector generate by the neural net have nice semantic and syntactic behavior semantically io be close to android syntactically boy minus boy be close to girl minus girl one can checkout more example here although this provide high quality word vector there be still no clear way to combine they into a high quality document vector in this article we discuss one possible heuristic inspire by a stochastic process call chinese restaurant process crp basic idea be to use crp to drive a clustering process and sum word vector in the right cluster imagine we have an document about chicken recipe it contain word like chicken pepper salt cheese it also contain word like use buy definitely my the the word2vec model give we a vector for each word one could naively sum up every word vector as the doc vector this clearly introduce lot of noise a well heuristic be to use a weighted sum base on other information like idf or part of speech pos tag the question be could we be more selective when add term if this be a chicken recipe document I shouldn t even consider word like definitely use my in the summation one can argue that idf base weight can significantly reduce noise of boring word like the and be however for word like definitely overwhelm the idfs be not necessarily small as you would hope it s natural to think that if we can first group word into cluster word like chicken pepper may stay in one cluster along with other cluster of junk word if we can identify the relevant cluster and only sum up word vector from relevant cluster we should have a good doc vector this boil down to cluster the word in the document one can of course use off the shelf algorithm like k mean but most these algorithm require a distance metric word2vec behave nicely by cosine similarity this doesn t necessarily mean it behave as well under eucledian distance even after projection to unit sphere it s perhaps good to use geodesic distance it would be nice if we can directly work with cosine similarity we have do a quick experiment on cluster word drive by crp like stochastic process it work surprisingly well — so far now let s explain crp imagine you go to a chinese restaurant there be already n table with different number of people there be also an empty table crp have a hyperparamt r > 0 which can be regard as the imagine number of people on the empty table you go to one of the n+1 table with probability proportional to exist number of people on the table for the empty table the number be r if you go to one of the n exist table you be do if you decide to sit down at the empty table the chinese restaurant will automatically create a new empty table in that case the next customer come in will choose from n+2 table include the new empty table inspire by crp we try the follow variation of crp to include the similarity factor common setup be the following we be give m vector to be cluster we maintain two thing cluster sum not centroid and vector in cluster we iterate through vector for current vector v suppose we have n cluster already now we find the cluster c whose cluster sum be most similar to current vector call this score sim v c variant 1 v create a new cluster with probability 1 1 + n otherwise v go to cluster c variant 2 if sim v c > 1 1 + n go to cluster c otherwise with probability 1 1+n it create a new cluster and with probability n 1+n it go to c in any of the two variant if v go to a cluster we update cluster sum and cluster membership there be one distinct difference to traditional crp if we don t go to empty table we deterministically go to the most similar table in practice we find these variant create similar result one difference be that variant 1 tend to have more cluster and small cluster variant 2 tend to have few but large cluster the example below be from variant 2 for example for a chicken recipe document the cluster look like this apparently the first cluster be most relevant now let s take the cluster sum vector which be the sum of all vector from this cluster and test if it really preserve semantic below be a snippet of python console we train word vector use the c implementation on a fraction of english wiki and read the model file use python library gensim model word2vec c 0 below denote the cluster 0 look like the semantic be preserve well it s convincing that we can use this as the doc vector the recipe document seem easy now let s try something more challenging like a news article news article tend to tell story and thus have less concentrated topic word we try the clustering on this article title signal on radar puzzle official in hunt for malaysian jet we get 4 cluster again look decent note that this be a simple 1 pass clustering process and we don t have to specify number of cluster could be very helpful for latency sensitive service there be still a miss step how to find out the relevant cluster s we haven t yet do extensive experiment on this part a few heuristic to consider there be other problem to think about 1 how do we merge cluster base on similarity among cluster sum vector or average similarity between cluster member 2 what be the minimal set of word that can reconstruct cluster sum vector in the sense of cosine similarity this could be use as a semantic keyword extraction method conclusion google s word2vec provide powerful word vector we be interested in use these vector to generate high quality document vector in an efficient way we try a strategy base on a variant of chinese restaurant process and obtain interesting result there be some open problem to explore and we would like to hear what you think appendix python style pseudo code for similarity drive crp we write this post while work on kifi — connect people with knowledge learn more originally publish at eng kifi com on march 17 2014 from a quick cheer to a stand ovation clap to show how much you enjoy this story the kifi engineering blog
Pinterest Engineering,113,6,https://medium.com/@Pinterest_Engineering/building-a-smarter-home-feed-ad1918fdfbe3?source=tag_archive---------4----------------,build a smart home feed pinter engineering medium,chris pinchak | pinter engineer discovery the home feed should be a reflection of what each user care about content be source from input such as people and board the user follow interest and recommendation to ensure we maintain fast reliable and personalized home feed we build the smart feed with the follow design value in mind 1 different source of pin should be mix together at different rate 2 some pin should be selectively drop or defer until a later time some source may produce pin of poor quality for a user so instead of show everything available immediately we can be selective about what to show and what to hold back for a future session 3 pin should be arrange in the order of good first rather than new first for some source new pin be intuitively well while for other newness be less important we shift away from our previously time order home feed system and onto a more flexible one the core feature of the smart feed architecture be its separation of available but unseen content and content that s already be present to the user we leverage knowledge of what the user hasn t yet see to our advantage when decide how the feed evolve over time smart feed be a composition of three independent service each of which have a specific role in the construction of a home feed the smart feed worker be the first to process pin and have two primary responsibility — to accept incoming pin and assign some score proportional to their quality or value to the receive user and to remember these score pin in some storage for later consumption essentially the worker manage pin as they become newly available such as those from the repin of the people the user follow pin have vary value to the receive user so the worker be task with decide the magnitude of their subjective quality incoming pin be currently obtain from three separate source repin make by follow user related pin and pin from followed interest each be score by the worker and then insert into a pool for that particular type of pin each pool be a priority queue sort on score and belong to a single user newly add pin mix with those add before allow the high quality pin to be accessible over time at the front of the queue pool can be implement in a variety of way so long as the priority queue requirement be meet we choose to do this by exploit the key base sorting of hbase each key be a combination of user score and pin such that for any user we may scan a list of available pin accord to their score newly add triple will be insert at their appropriate location to maintain the score order this combination of user score and pin into a key value can be use to create a priority queue in other storage system aside from hbase a property we may use in the future depend on evolve storage requirement distinct from the smart feed worker the smart feed content generator be concern primarily with define what new mean in the context of a home feed when a user access the home feed we ask the content generator for new pin since their last visit the generator decide the quantity composition and arrangement of new pin to return in response to this request the content generator assemble available pin into chunk for consumption by the user as part of their home feed the generator be free to choose any arrangement base on a variety of input signal and may elect to use some or all of the pin available in the pool pin that be select for inclusion in a chunk be thereafter remove from from the pool so they can not be return as part of subsequent chunk the content generator be generally free to perform any rearrangement it like but be bind to the priority queue nature of the pool when the generator ask for n pin from a pool it ll get the n high scoring I e good pin available therefore the generator doesn t need to concern itself with find the well available content but instead with how the well available content should be present in addition to provide high availability of the home feed the smart feed service be responsible for combine new pin return by the content generator with those that previously appear in the home feed we can separate these into the chunk return by the content generator and the materialize feed manage by the smart feed service the materialize feed represent a frozen view of the feed as it be the last time the user view it to the materialize pin we add the pin from the content generator in the chunk the service make no decision about order instead it add the pin in exactly the order give by the chunk because it have a fairly low rate of reading and write the materialize feed be likely to suffer from few availability event in addition feed can be trim to restrict they to a maximum size the need for less storage mean we can easily increase the availability and reliability of the materialize feed through replication and the use of fast storage hardware the smart feed service rely on the content generator to provide new pin if the generator experience a degradation in performance the service can gracefully handle the loss of its availability in the event the content generator encounter an exception while generate a chunk or if it simply take too long to produce one the smart feed service will return the content contain in the materialize feed in this instance the feed will appear to the end user as unchanged from last time future feed view will produce chunk as large as or large than the last so that eventually the user will see new pin by move to smart feed we achieve the goal of a highly flexible architecture and well control over the composition of home feed the home feed be now power by three separate service each with a well define role in its production and distribution the individual service can be alter or replace with component that serve the same general purpose the use of pool to buffer pin accord to their quality allow we a great amount of control over the composition of home feed continue with this project we intend to well model user preference with respect to pin in their home feed our accuracy of recommendation quality vary considerably over our user base and we would benefit from use preference information gather from recent interaction with the home feed knowledge of personal preference will also help we order home feed so the pin of most value can be discover with the least amount of effort if you re interested in tackle challenge and make improvement like this join our team chris pinchak be a software engineer at pinter acknowledgement this technology be build in collaboration with dan feng dmitry chechik raghavendra prabhu jeremy carroll xun liu varun sharma joe lau yuchen liu tian ying chang and yun park this team as well as people from across the company help make this project a reality with their technical insight and invaluable feedback from a quick cheer to a stand ovation clap to show how much you enjoy this story inventive engineer build the first visual discovery engine 100 billion idea and count https career pinterest com career engineering
Nikhil Dandekar,116,3,https://towardsdatascience.com/what-makes-a-good-data-scientist-engineer-a8b4d7948a86?source=tag_archive---------5----------------,what make a good data scientist engineer towards data science,the term data scientist have be use lately to describe a wide variety of skill & role in this post I will focus on a particular flavor of data scientist I will talk about the quality need to be a good data scientist engineer who ship relevance product to user some example of relevance product be these folk need to be strong at data science and engineering to be successful some place call these folk as machine learning engineer since most of the work they do involve machine learn more generally I feel relevance engineer be a good term to describe they relevance engineer have a common set of skill that they draw upon to get their job do the list below doesn t include some of the know obvious skill you obviously need to be smart you obviously need to have or be able to learn quickly the require book knowledge but beyond that there be a bunch of not so obvious skill that you can t learn from a book here be some of those in no particular order this list be by no mean exhaustive but do capture some of the quality of the smart folk I have work with happy to hear what you think thank to peter bailey and andrew hogue for feedback on the initial revision * in this post feature mean a software feature not a machine learning feature from a quick cheer to a stand ovation clap to show how much you enjoy this story engineering manager do machine learning @ google previously work on ml and search at quora foursquare and bing sharing concept idea and code
Jeff Smith,20,7,https://medium.com/data-engineering/modeling-madly-8b2c72eb52be?source=tag_archive---------6----------------,model madly data engineering medium,I recently wrap up my second hackathon at intent medium you can see my summary of one of our previous hackathon here these past two hackathon I ve take on some slightly different challenge than people usually go after in a hackathon develop new machine learning model while I ve be work on data science and machine learning system for a while I ve find that try to do so under extreme constraint can be a distinctly different experience a very good datum hacker can easily find themselves with a great idea at a hackathon but with little to nothing to demo at the end accept that my personal experience be just my own let I offer three tip for build new model at a hackathon when you re do a more traditional web app hack at a hackathon you can almost run out of time and still come up with something pretty good as long as you get that last bug fix before the demo this be a great characteristic to build into the plan of a hack but one that simply do not apply to a machine learn hack think about what happen when you do find that last bug in a machine learning project you still need to potentially do all of the below that s no just hit refresh workflow even with a well oiled workflow some of those task can take all of the time your average one day hackathon be schedule for take # 3 for example train a production grade model use say hadoop can take a lot of time even if you have the cash to spin up a fair sized cluster of ec2 instance what that mean for your hack can vary but you re just ask for trouble if you don t start with that fact take into account in the scope and goal of your project a solid project design be absolutely crucial if you re go to hope to take all of the little step involve in get your model ready to demo which lead I to my next point one of the good thing about work in data science be all of the really smart people but of course the corollary be that one of the bad thing about work in data science be all of the really smart people sharp engineer and datum scientist can take the nugget of an idea and envision a useful powerful suite of product that would take year to build which be not so useful when you have a day or two mature dataist know just how much ambition be too much and plan accordingly I happen to be lucky enough to work with some very smart and very mature data scientist and engineer so this have not be a problem for either of my last few hack but I m just lucky that way you might not be so lucky unrealistic ambition be a constant danger in a machine learning hack run along the edge of all activity like a precipice beckon you to dive off and see where you land if you take one thing away from this post let it be this don t dive off the cliff just don t do it you win t like where you land you ll wind up with more question than answer and you ll have nothing to show come demo time moreover your fellow devs who work on app and not model will simply not understand what you spend your time on what do a precipice look like it could be a novel distance metric it could be a fundamental improvement to a widely use technique like svrs or it could just be something really benign sound like a long training set I would say that even choose to pose the problem as a regression one instead of a classification one could qualify the danger originate in the intrinsic tension between the rigorous and exploratory mode of academic data science machine learning education and the pedal to the metal pace mandate by a hackathon they be very different mode of work and you re just go to have suspend some of your good habit for a day or so if you want to have something to demo this last point can be the trickiest to put in practice but I think it can totally be the difference between a project that feel like a hack and one that feel like just get warm up on a weeklong story if you ve figure out how to scope your project appropriately and design something that can really be build in a day or two you can still actually fail to do so I think it can the difference can easily come down to technology choice for example I currently make my living writing cascalog clojure and java on top of hadoop to process file store in s3 I know these tool well enough to pay my rent but I would absolutely hesitate to use any of they in a tight pace context I have spend week try to understand a single cascalog bug seriously if you know the language python offer an unbeatable value proposition for this use case scikit learn have nearly everything you could imagine need panda numpy and scipy be all sit there to be bring in when appropriate and don t forget how awesome it can be to prototype in a purpose build exploratory development environment like ipython but this be machine learning and sometimes our data be just big maybe even web scale some people hate these phrase but they serve a purpose we don t all use hadoop out of love for horrendously complex java application big datum be not just statistic on a mac pro although it can often look like that scale can be a real necessity even in a hackathon when it be there be no easy answer if you re lucky maybe you can actually work with multiple hour model learn time if you re really lucky you might be use spark and not hadoop in which case it might not take hour to learn your model my point be that insofar as you have a choice choose the lean mean tool the one that will let you do more with less input require from you don t use that c++ library that promise awesome runtime but with python binding that you ve never try you ll never figure out its quirk in time write as little datum cleanup code as you can manage command like dropna can save you precious minute to hour and if you can get your datum from database or an api instead of file then for the love of cthulhu do it hell even if you have to load your datum from file to a database first it might be worth your time sql be one of the high productivity rapid prototyping tool I know and though I love to bash on the clunkiness of hadoop there be even way of take some serious pain out of use it under pressure depend on what you re do elastic map reduce or predictionio can get you to the point of be productive much fast I love hackathon and their variation they remind I of the fun old day in grad school furiously hack away to come up with something interesting to say about definitionally uncertain stuff the furious pace and the pragmatic compromise be part of the fun compare to thing like pitch event hackathon have way less problem even if they have their issue as well at their good they re about the love of unconstrained creation I ve try to do machine learn hack because it s just so damn cool to go from zero to have a program that make decision it amaze I every time it work and doubly so when I can manage to get something work on a deadline take on a challenge like build a new model in a hackathon be also a great learning experience especially if you get to work as part of a strong team machine learn in the real world be an even large topic than its academic cousin and there s always interesting thing to learn hackathon can be great place to rapidly iterate through approach and learn from your teammate how to build thing well and fast that s pretty likely to come in handy sometime the main part of the post be over but I want to make sure to leave a note for anyone who be interested in what we hack at intent medium or what we build for our customer we re hire all sort of smart people to build system for machine learning and more please reach out if you want to hear more about how and why we do what we do from a quick cheer to a stand ovation clap to show how much you enjoy this story author of reactive machine learning system @manningbooks building ais for fun and profit friend of animal lay the foundation of tomorrow s big datum
Chris Jagers,45,5,https://medium.com/@chrisjagers/the-wolfram-language-b853337f8427?source=tag_archive---------7----------------,explain the wolfram language chris jager medium,many people be already familiar with apple s voice search call siri or the search engine behind it call wolfram alpha this search engine can use natural language to search vast set of datum and even compute math however this be just a tiny fraction of what the language can do and I don t even think it s a good introduction to what s possible to understand the raw power of the underlie technology you really have to understand what it be and a little about how it work the wolfram website have wonderful documentation and explanation but for the uninitiated it can seem bewildering they have repackage the language so many different way that it can be hard for the beginner to understand exactly what it be that s why I want to venture my own introduction let s start with it s origin mathematica be design as a desktop tool for computational research and exploration it continue evolve and the breakthrough be realize those symbol could be anything image sound algorithms geometry datum set anything so it become more than just a language stephen wolfram call this a knowledge base language because it have smart object build in that can be compute the language doesn t simply find result it compute result into actual model analysis and other symbolic object the real power be that the result remain symbolic object that can be far manipulate symbolically I e embed in another symbolic object operate on it in short anything can be compute pretty abstract I know don t worry we ll get to example soon the actual syntax be a combination of object and operator which be group and order by square bracket the stuff at the center of the formula get read first and then it expand out like a russian doll out of many potential example I have carefully select one from their site to illustrate it s simplicity and power let s say we want our system to determine the difference between poetry and prose this would be difficult to program directly because there be so many variable and the difference be so subtle with wolfram language that hard stuff become easy you can train it recognize the difference very quickly here s how it work let s use shakespeare for an example first scan all of hamlet and call that type of stuff prose then scan all of shakespeare s sonnet and call that stuff poetry easy next train the system with machine learning classify and predict be the two big function we want to classify which be poetry and which be prose wolfram look at our situation and instantly determine that the markov method be the good for differentiate among all the subtle difference between prose and poetry that s it any system use this bit of training will automatically be able to detect the difference between poetry and prose with a high degree of accuracy the key to this accuracy be the size of the datum set you really need at million of data point to train it reliably but with wolfram many of those datum set be already build in easy this be just one tiny example to illustrate what the language look like and how it go beyond symbol to work with computable object we could continue translate poem into interactive map and interaction into music and so on how do wolfram compare with other product like apache hadoop and other well it s a totally different thing in those product everything be manual the various axis and all the variable be manually define instead wolfram intelligently apply formula and make choice to optimize result base on specific condition it make the hard stuff automatic plus it s capable of much more than machine learning ; that s just one example of hundred sound 3d geometry language image etc — and a mixture of they all mathematica be still the most powerful and polished way to access the wolfram language their new programming cloud and other cloud offering signal serious intent to move to the web but it be still early day the language be very mature for desktop exploration and some company have even make mathematica application for small scale internal use which can be quite useful even though the wolfram website have signal intent to make it more broadly deployable within commercial service I don t think this be the proper way to use the language within my own company we find wolfram extremely handy for research but not deployment within a web base product in short it isn t performant commercial product require more than a powerful language they be make within an ecosystem of service and vendor that all have to work together without machine learning build into the native cloud where datum be store it can t be deploy in a saas product in a way that live up to expectation while stephen wolfram would love for his language to be use within commercial product I think he resent have to play nice with low level language his alternative of make api request across the web isn t a good way to embed intelligence within product and I don t think we will ever see entire saas product build entirely with a functional language programming be the art of automation the wolfram community be full of very smart people use the language for research and exploration they represent the cut edge of computation personally I m look forward to when we can see intelligence weave into commercial and consumer product that solve real problem for people on a daily basis from a quick cheer to a stand ovation clap to show how much you enjoy this story ceo of learn machine www learningmachine com
John Wittenauer,2,9,https://medium.com/@jdwittenauer/machine-learning-exercises-in-python-part-1-60db0df846a4?source=tag_archive---------8----------------,machine learning exercise in python part 1 john wittenauer medium,this content originally appear on curious insight this post be part of a series cover the exercise from andrew ng s machine learning class on coursera the original code exercise text and data file for this post be available here part 1 — simple linear regressionpart 2 — multivariate linear regressionpart 3 — logistic regressionpart 4 — multivariate logistic regressionpart 5 — neural networkspart 6 — support vector machinespart 7 — k mean clustering & pcapart 8 — anomaly detection & recommendation one of the pivotal moment in my professional development this year come when I discover coursera i d hear of the mooc phenomenon but have not have the time to dive in and take a class early this year I finally pull the trigger and sign up for andrew ng s machine learning class I complete the whole thing from start to finish include all of the programming exercise the experience open my eye to the power of this type of education platform and I ve be hook ever since this blog post will be the first in a series cover the programming exercise from andrew s class one aspect of the course that I didn t particularly care for be the use of octave for assignment although octave matlab be a fine platform most real world datum science be do in either r or python certainly there be other language and tool be use but these two be unquestionably at the top of the list since I m try to develop my python skill I decide to start work through the exercise from scratch in python the full source code be available at my ipython repo on github you ll also find the datum use in these exercise and the original exercise pdfs in sub folder off the root directory if you re interested while I can explain some of the concept involve in this exercise along the way it s impossible for I to convey all the information you might need to fully comprehend it if you re really interested in machine learning but haven t be expose to it yet I encourage you to check out the class it s completely free and there s no commitment whatsoever with that let s get start in the first part of exercise 1 we re task with implement simple linear regression to predict profit for a food truck suppose you be the ceo of a restaurant franchise and be consider different city for open a new outlet the chain already have truck in various city and you have datum for profit and population from the city you d like to figure out what the expect profit of a new food truck might be give only the population of the city that it would be place in let s start by examine the datum which be in a file call ex1data1 txt in the data directory of my repository above first we need to import a few library now let s get thing roll we can use panda to load the datum into a data frame and display the first few row use the head function note medium can t render table — the full example be here another useful function that panda provide out of the box be the describe function which calculate some basic statistic on a datum set this be helpful to get a feel for the datum during the exploratory analysis stage of a project note medium can t render table — the full example be here examine stat about your datum can be helpful but sometimes you need to find way to visualize it too fortunately this datum set only have one dependent variable so we can toss it in a scatter plot to get a well idea of what it look like we can use the plot function provide by panda for this which be really just a wrapper for matplotlib it really help to actually look at what s go on doesn t it we can clearly see that there s a cluster of value around city with small population and a somewhat linear trend of increase profit as the size of the city increase now let s get to the fun part — implement a linear regression algorithm in python from scratch if you re not familiar with linear regression it s an approach to model the relationship between a dependent variable and one or more independent variable if there s one independent variable then it s call simple linear regression and if there s more than one independent variable then it s call multiple linear regression there be lot of different type and variance of linear regression that be outside the scope of this discussion so I win t go into that here but to put it simply — we re try to create a * linear model * of the data x use some number of parameter theta that describe the variance of the datum such that give a new data point that s not in x we could accurately predict what the outcome y would be without actually know what y be in this implementation we re go to use an optimization technique call gradient descent to find the parameter theta if you re familiar with linear algebra you may be aware that there s another way to find the optimal parameter for a linear model call the normal equation which basically solve the problem at once use a series of matrix calculation however the issue with this approach be that it doesn t scale very well for large datum set in contrast we can use variant of gradient descent and other optimization method to scale to data set of unlimited size so for machine learning problem this approach be more practical okay that s enough theory let s write some code the first thing we need be a cost function the cost function evaluate the quality of our model by calculate the error between our model s prediction for a data point use the model parameter and the actual data point for example if the population for a give city be 4 and we predict that it be 7 our error be 7 4 ^2 = 3 ^ 2 = 9 assume an l2 or least square loss function we do this for each datum point in x and sum the result to get the cost here s the function notice that there be no loop we re take advantage of numpy s linear algrebra capability to compute the result as a series of matrix operation this be far more computationally efficient than an unoptimizted for loop in order to make this cost function work seamlessly with the panda datum frame we create above we need to do some manipulating first we need to insert a column of 1s at the beginning of the data frame in order to make the matrix operation work correctly I win t go into detail on why this be need but it s in the exercise text if you re interested — basically it account for the intercept term in the linear equation second we need to separate our datum into independent variable x and our dependent variable y finally we re go to convert our datum frame to numpy matrix and instantiate a parameter matirx one useful trick to remember when debug matrix operation be to look at the shape of the matrix you re deal with it s also helpful to remember when walk through the step in your head that matrix multiplication look like i x j * j x k = i x k where i j and k be the shape of the relative dimension of the matrix 97l 2l 1l 2l 97l 1l okay so now we can try out our cost function remember the parameter be initialize to 0 so the solution isn t optimal yet but we can see if it work 32 072733877455676 so far so good now we need to define a function to perform gradient descent on the parameter * theta * use the update rule define in the exercise text here s the function for gradient descent the idea with gradient descent be that for each iteration we compute the gradient of the error term in order to figure out the appropriate direction to move our parameter vector in other word we re calculate the change to make to our parameter in order to reduce the error thus bring our solution close to the optimal solution I e well fit this be a fairly complex topic and I could easily devote a whole blog post just to discuss gradient descent if you re interested in learn more I would recommend start with this article and branch out from there once again we re rely on numpy and linear algebra for our solution you may notice that my implementation be not 100 % optimal in particular there s a way to get rid of that inner loop and update all of the parameter at once I ll leave it up to the reader to figure it out for now I ll cover it in a later post now that we ve get a way to evaluate solution and a way to find a good solution it s time to apply this to our datum set matrix 3 24140214 1 1272942 note that we ve initialize a few new variable here if you look closely at the gradient descent function it have parameter call alpha and iter alpha be the learning rate — it s a factor in the update rule for the parameter that help determine how quickly the algorithm will converge to the optimal solution iter be just the number of iteration there be no hard and fast rule for how to initialize these parameter and typically some trial and error be involve we now have a parameter vector descibe what we believe be the optimal linear model for our datum set one quick way to evaluate just how good our regression model be might be to look at the total error of our new solution on the datum set 4 5159555030789118 that s certainly a lot well than 32 but it s not a very intuitive way to look at it fortunately we have some other technique at our disposal we re now go to use matplotlib to visualize our solution remember the scatter plot from before let s overlay a line represent our model on top of a scatter plot of the datum to see how well it fit we can use numpy s linspace function to create an evenly space series of point within the range of our datum and then evaluate those point use our model to see what the expected profit would be we can then turn it into a line graph and plot it not bad our solution look like and optimal linear model of the datum set since the gradient decent function also output a vector with the cost at each training iteration we can plot that as well notice that the cost always decrease — this be an example of what s call a convex optimization problem if you be to plot the entire solution space for the problem I e plot the cost as a function of the model parameter for every possible value of the parameter you would see that it look like a bowl shape with a basin represent the optimal solution that s all for now in part 2 we ll finish off the first exercise by extend this example to more than 1 variable I ll also show how the above solution can be reach by use a popular machine learning library call scikit learn to comment on this article check out the original post at curious insight follow I on twitter to get new post update from a quick cheer to a stand ovation clap to show how much you enjoy this story data scientist engineer author investor entrepreneur
Pinterest Engineering,25,7,https://medium.com/@Pinterest_Engineering/building-the-interests-platform-73a3a3755c21?source=tag_archive---------9----------------,build the interest platform pinter engineering medium,ningne hu | pinter engineer discovery the core value of pinter be to help people find the thing they care about by connect they to pin and people that relate to their interest we re build a service that s power by people and supercharge with technology the interest graph — the connection that make up the pinter index — create bridge between pin board and pinner it s our job to build a system that help people to collect the thing they love and connect they to community of engaged people who share similar interest and can help they discover more from category like travel fitness and humor to more niche area like vintage motorcycle craft beer or japanese architecture we re build a visual discovery tool for all interest the interest platform be build to support this vision specifically it s responsible for produce high quality datum on interest interest relationship and their association with pin board and pinner figure 1 feedback loop between machine intelligence and human curation in contrast with conventional method of generate such datum which rely primarily on machine learning and data mining technique our system rely heavily on human curation the ultimate goal be to build a system that s both machine and human power create a feedback mechanism by which human curate datum help drive improvement in our machine algorithm and vice versa figure 2 system component raw input to the system include existing datum about pin board pinner and search query as well as explicit human curation signal about interest with this datum we re able to construct a continuously evolve interest dictionary which provide the foundation to support other key component such as interest feed interest recommendation and related interest from a technology standpoint interest be text string that represent entity for which a group of pinner might have a share passion we generate an initial collection of interest by extract frequently occur n gram from pin and board description as well as board title and filter these n gram use custom build grammar while this approach provide a high coverage set of interest we find many term to be malforme phrase for instance we would extract phrase such as lamborghini yellow instead of yellow lamborghini this prove problematic because we want interest term to represent how pinner would describe they and so we employ a variety of method to eliminate malformed interest term we first compare term with repeat search query perform by a group of pinner over a few month intuitively this criterion match well with the notion that an interest should be an entity for which a group of pinner be passionate later we filter the candidate set through public domain ontology like wikipedia title these ontology be primarily use to validate proper noun as oppose to common phrase as all available ontology represent only a subset of possible interest this be especially true for pinter where pinner themselves curate special interest like mid century modern style finally we also maintain an internal blacklist to filter abusive word and x rate term as well as pinter specific stop word like love this filtering be especially important to interest term which might be recommend to million of user we arrive at a fair quality collection of interest follow the above algorithmic approach in order to understand the quality of our effort we give a 50 000 term subset of our collection to a third party vendor which use crowdsource to rate our datum to be rigorous we compose a set of four criterion by which user would evaluate candidate interest term be it english be it a valid phrase in grammar be it a standalone concept be it a proper name the crowdsource rating be both interesting if not somewhat expect there be a low rate of agreement amongst rater with especially high discrepancy in determine whether an interest s term represent a standalone concept despite the ambiguity we be able to confirm that 80 % of the collection generate use the above algorithm satisfy our interest criterion this type of effort however be not easy to scale the real solution be to allow pinner to provide both implicit and explicit signal to help we determine the validity of an interest implicit signal behavior like click and view while explicit signal include ask pinner to specifically provide information which can be action like a thumb up thumb down star or skip recommendation to capture all the signal use for define the collection of term we build a dictionary that store all the datum associate with each interest include invalid interest and the reason why it s invalid this service play a key role in human curation by aggregate signal from different people on top of this dictionary service we can build different level of review system with the interest dictionary we can associate pin board and pinner with representative interest one of the initial way we experiment with this be launch a preview of a page where pinner can explore their interest figure 3 explore interest in order to match interest to pinner we need to aggregate all the information relate with a person s interest at its core our system recommend interest base upon pin with which a pinner interact every pin on pinterest have be collect and give context by someone who think it s important and in do so be help other people discover great content each individual pin be an incredibly rich source of datum as discuss in a previous blog post on discovery data model one pin often have multiple copy — different people may pin it from different source and the same pin can be repinne multiple time during this process each pin accumulate numerous unique textual description which allow we to connect pin with interest term with high precision however this conceptually simple process require non trivial engineering effort to scale to the amount of pin and pinner that the service have today the data process pipeline manage by pinball compose over 35 hadoop job and run periodically to update the user interest mapping to capture user late interest information the initial feedback on the explore interest page have be positive prove the capability of our system we ll continue test different way of expose a person s interest and related content base on implicit signal as well as explicit signal such as the ability to create custom category of interest relate interest be an important way of enable the ability to browse interest and discover new one to compute relate interest we simply combine the co occurrence relationship for interest compute at pin and board level figure 4 computing relate interest the quality of the related interest be surprisingly high give the simplicity of the algorithm we attribute this effect to the cleanness of pinter datum text datum on pin tend to be very concise and contain less noise than other type of datum like web page also relate interest calculation already make use of board which be heavily curate by people vs machine in regard to organize related content we find that utilize the co occurrence of interest term at the level of both pin and board provide the good tradeoff between achieve high precision as well as recall when compute the related interest one of the initial way we begin show people relate content be through related pin when you pin an object you ll see a recommendation for a related board with that same pin so you can explore similar object additionally if you scroll beneath a pin you ll see pin from other people who ve also pin that original object at this point 90 % of all pin have relate pin and we ve see 20 % growth in engagement with relate pin in the last six month interest feed provide pinner with a continuous feed of pin that be highly related our feed be populate use a variety of source include search and through our annotation pipeline a key property of the feed be flow only feed with decent flow can attract pinner to come back repeatedly thereby maintain high engagement in order to optimize for our feed we ve utilize a number of real time indexing and retrieval system include real time search real time annotate and also human curation for some of the interest to ensure quality we need to guarantee quality from all source for that purpose we measure the engagement of pin from each source and address quality issue accordingly figure 5 how interest feed be generate accurately capture pinner interest and interest relationship and make this datum understandable and actionable for ten of million of people collect ten of billion of pin be not only an engineering challenge but also a product design one we re just at the beginning as we continue to improve the datum and design way to empower people to provide feedback that allow we to build a hybrid system combine machine and human curation to power discovery result of these effort will be reflect in future product release if you re interested in build new way of help people discover the thing they care about join our team acknowledgement the core team member for the interest backend platform be ningne hu leon lin ryan shih and yuan wei many other folk from other part of the company especially the discovery team and the infrastructure team have provide very useful feedback and help along the way to make the ongoing project successful ningning hu be an engineer at pinter from a quick cheer to a stand ovation clap to show how much you enjoy this story inventive engineer build the first visual discovery engine 100 billion idea and count https career pinterest com career engineering
Christopher Nguyen,991,8,https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4?source=tag_archive---------0----------------,algorithm of the mind deep learn 101 medium,what machine learning teach we about ourselves originally publish at blog arimo com follow I on twitter to keep inform of interesting development on these topic science often follow technology because invention give we new way to think about the world and new phenomenon in need of explanation or so aram harrow an mit physics professor counter intuitively argue in why now be the right time to study quantum computing he suggest that the scientific idea of entropy could not really be conceive until steam engine technology necessitate understanding of thermodynamic quantum computing similarly arise from attempt to simulate quantum mechanic on ordinary computer so what do all this have to do with machine learn much like steam engine machine learning be a technology intend to solve specific class of problem yet result from the field be indicate intrigue — possibly profound — scientific clue about how our own brain might operate perceive and learn the technology of machine learning be give we new way to think about the science of human thought and imagination five year ago deep learning pioneer geoff hinton who currently split his time between the university of toronto and google publish the follow demo hinton have train a five layer neural network to recognize handwritten digit when give their bitmappe image it be a form of computer vision one that make handwriting machine readable but unlike previous work on the same topic where the main objective be simply to recognize digit hinton s network could also run in reverse that be give the concept of a digit it can regenerate image corresponding to that very concept we be see quite literally a machine imagine an image of the concept of 8 the magic be encode in the layer between input and output these layer act as a kind of associative memory mapping back and forth from image and concept from concept to image all in one neural network but beyond the simplistic brain inspire machine vision technology here the broad scientific question be whether this be how human imagination — visualization — work if so there s a huge a ha moment here after all isn t this something our brain do quite naturally when we see the digit 4 we think of the concept 4 conversely when someone say 8 we can conjure up in our mind eye an image of the digit 8 be it all a kind of run backwards by the brain from concept to image or sound smell feel etc through the information encode in the layer aren t we watch this network create new picture — and perhaps in a more advanced version even new internal connection — as it do so if visual recognition and imagination be indeed just back and forth mapping between image and concept what s happen between those layer do deep neural network have some insight or analogy to offer we here let s first go back 234 year to immanuel kant s critique of pure reason in which he argue that intuition be nothing but the representation of phenomena kant rail against the idea that human knowledge could be explain purely as empirical and rational thought it be necessary he argue to consider intuition in his definition intuition be representation leave in a person s mind by sensory perception where as concept be description of empirical object or sensory datum together these make up human knowledge fast forward two century later berkeley cs professor alyosha efros who specialize in visual understanding point out that there be many more thing in our visual world than we have word to describe they with use word label to train model efros argue expose our technique to a language bottleneck there be many more un namable intuition than we have word for in train deep network such as the seminal cat recognition work lead by quoc le at google stanford we re discover that the activation in successive layer appear to go from low to high conceptual level an image recognition network encode bitmap at the low layer then apparent corner and edge at the next layer common shape at the next and so on these intermediate layer don t necessarily have any activation corresponding to explicit high level concept like cat or dog yet they do encode a distribute representation of the sensory input only the final output layer have such a mapping to human define label because they be constrain to match those label therefore the above encoding and label seem to correspond to exactly what kant refer to as intuition and concept in yet another example of machine learning technology reveal insight about human think the network diagram above make you wonder whether this be how the architecture of intuition — albeit vastly simplify — be be express if — as efros have point out — there be a lot more conceptual pattern than word can describe then do word constrain our thought this question be at the heart of the sapir whorf or linguistic relativity hypothesis and the debate about whether language completely determine the boundary of our cognition or whether we be unconstrained to conceptualize anything — regardless of the language we speak in its strong form the hypothesis posit that the structure and lexicon of language constrain how one perceive and conceptualize the world one of the most striking effect of this be demonstrate in the color test show here when ask to pick out the one square with a shade of green that s distinct from all the other the himba people of northern namibia — who have distinct word for the two shade of green — can find it almost instantly the rest of we however have a much hard time do so the theory be that — once we have word to distinguish one shade from another our brain will train itself to discriminate between the shade so the difference would become more and more obvious over time in see with our brain not with our eye language drive perception with machine learning we also observe something similar in supervised learning we train our model to good match image or text audio etc against provide label or category by definition these model be train to discriminate much more effectively between category that have provide label than between other possible category for which we have not provide label when view from the perspective of supervised machine learn this outcome be not at all surprising so perhaps we shouldn t be too surprised by the result of the color experiment above either language do indeed influence our perception of the world in the same way that label in supervised machine learning influence the model s ability to discriminate among category and yet we also know that label be not strictly require to discriminate between cue in google s cat recognize brain the network eventually discover the concept of cat dog etc all by itself — even without train the algorithm against explicit label after this unsupervised training whenever the network be feed an image belong to a certain category like cat the same corresponding set of cat neuron always get fire up simply by look at the vast set of training image this network have discover the essential pattern of each category as well as the difference of one category vs another in the same way an infant who be repeatedly show a paper cup would soon recognize the visual pattern of such a thing even before it ever learn the word paper cup to attach that pattern to a name in this sense the strong form of the sapir whorf hypothesis can not be entirely correct — we can and do discover concept even without the word to describe they supervised and unsupervised machine learning turn out to represent the two side of the controversy s coin and if we recognize they as such perhaps sapir whorf would not be such a controversy and more of a reflection of supervised and unsupervised human learning I find these correspondence deeply fascinating — and we ve only scratch the surface philosopher psychologist linguist and neuroscientist have study these topic for a long time the connection to machine learning and computer science be more recent especially with the advance in big datum and deep learning when feed with huge amount of text image or audio datum the late deep learning architecture be demonstrate near or even well than human performance in language translation image classification and speech recognition every new discovery in machine learning demystifie a bit more of what may be go on in our brain we re increasingly able to borrow from the vocabulary of machine learning to talk about our mind thank to sonal chokshi and vu pham for extensive review & edit also chrisjager chickamade from a quick cheer to a stand ovation clap to show how much you enjoy this story @arimoinc ceo & co founder leader entrepreneur hacker xoogler executive professor # dataviz # parallelcompute # deeplearning & former # googleapps fundamental and late development in # deeplearne
Per Harald Borgen,2.1K,6,https://medium.com/learning-new-stuff/machine-learning-in-a-week-a0da25d59850?source=tag_archive---------1----------------,machine learning in a week learn new stuff medium,get into machine learning ml can seem like an unachievable task from the outside however after dedicate one week to learn the basic of the subject I find it to be much more accessible than I anticipate this article be intend to give other who re interested in get into ml a roadmap of how to get start draw from the experience I make in my intro week before my machine learning week I have be read about the subject for a while and have go through half of andrew ng s course on coursera and a few other theoretical course so I have a tiny bit of conceptual understanding of ml though I be completely unable to transfer any of my knowledge into code this be what I want to change I want to be able to solve problem with ml by the end of the week even through this mean skip a lot of fundamental and go for a top down approach instead of bottom up after ask for advice on hacker news I come to the conclusion that python s scikit learn module be the good starting point this module give you a wealth of algorithm to choose from reduce the actual machine learning to a few line of code I start off the week by look for video tutorial which involve scikit learn I finally land on sentdex s tutorial on how to use ml for invest in stock which give I the necessary knowledge to move on to the next step the good thing about the sentdex tutorial be that the instructor take you through all the step of gather the datum as you go along you realize that fetch and clean up the datum can be much more time consume than do the actually machine learning so the ability to write script to scrape datum from file or crawl the web be essential skill for aspire machine learning geek I have re watch several of the video later on to help I when I ve be stick with problem so I d recommend you to do the same however if you already know how to scrape datum from website this tutorial might not be the perfect fit as a lot of the video evolve around datum fetch in that case the udacity s intro to machine learning might be a well place to start tuesday I want to see if I could use what I have learn to solve an actual problem as another developer in my code cooperative be work on bank of england s data visualization competition I team up with he to check out the dataset the bank have release the most interesting datum be their household survey this be an annual survey the bank perform on a few thousand household regard money relate subject the problem we decide to solve be the following I play around with the dataset spend a few hour clean up the datum and use the scikit learn map to find a suitable algorithm for the problem we end up with a success ratio at around 63 % which isn t impressive at all but the machine do at least manage to guess a little well than flip a coin which would have give a success rate at 50 % see result be like fuel to your motivation so I d recommend you do this for yourself once you have a basic grasp of how to use scikit learn after play around with various scikit learn module I decide to try and write a linear regression algorithm from the ground up I want to do this because I feel and still feel that I really don t understand what s happen on under the hood luckily the coursera course go into detail on how a few of the algorithm work which come to great use at this point more specifically it describe the underlying concept of use linear regression with gradient descent this have definitely be the most effective of learn technique as it force you to understand the step that be go on under the hood I strongly recommend you to do this at some point I plan to rewrite my own implementation of more complex algorithm as I go along but I prefer do this after I ve play around with the respective algorithm in scikit learn on thursday I start do kaggle s introductory tutorial kaggle be a platform for machine learning competition where you can submit solution to problem release by company or organization I recommend you try out kaggle after have a little bit of a theoretical and practical understanding of machine learn you ll need this in order to start use kaggle otherwise it will be more frustrating than reward the bag of word tutorial guide you through every step you need to take in order to enter a submission to a competition plus give you a brief and exciting introduction into natural language processing nlp I end the tutorial with much high interest in nlp than I have when enter it friday I continue work on the kaggle tutorial and also start udacity s intro to machine learning I m currently half way through and find it quite enjoyable it s a lot easy the coursera course as it doesn t go in depth in the algorithms but it s also more practical as it teach you scikit learn which be a whole lot easy to apply to the real world than write algorithm from the ground up in octave as you do in the coursera course do it for a week hasn t just be great fun it have also help my awareness of its usefulness of machine learning in society the more I learn about it the more I see which area it can be use to solve problem choose a top down approach if you re not ready for the heavy stuff and get into problem solve as quickly as possible good luck thank for read my name be per I m a co founder of scrimba — a well way to teach and learn code if you ve read this far I d recommend you to check out this demo from a quick cheer to a stand ovation clap to show how much you enjoy this story co founder of scrimba the next generation platform for teaching and learn code https scrimba com a publication about improve your technical skill
Ahmed El Deeb,593,7,https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89?source=tag_archive---------2----------------,what to do with small datum rant on machine learning medium,by ahmed el deeb many technology company now have team of smart datum scientist verse in big datum infrastructure tool and machine learning algorithm but every now and then a datum set with very few datum point turn up and none of these algorithm seem to be work properly anymore what the hell be happen what can you do about it most data science relevance and machine learning activity in technology company have be focus around big datum and scenario with huge datum set set where the row represent document user file query song image etc thing that be in the thousand hundred of thousand million or even billion the infrastructure tool and algorithm to deal with these kind of data set have be evolve very quickly and improve continuously during the last decade or so and most data scientist and machine learning practitioner have gain experience be such situation have grow accustomed to the appropriate algorithm and gain good intuition about the usual trade off bias variance flexibility stability hand craft feature vs feature learn etc but small datum set still arise in the wild every now and then and often they be tricky to handle require a different set of algorithm and a different set of skill small datum set arise be several situation problem of small datum be numerous but mainly revolve around high variance 1 hire a statistician I m not kid statistician be the original datum scientist the field of statistic be develop when datum be much hard to come by and as such be very aware of small sample problem statistical test parametric model bootstrappe and other useful mathematical tool be the domain of classical statistic not modern machine learning lack a good general purpose statistician get a marine biologist a zoologist a psychologist or anyone who be train in a domain that deal with small sample experiment the close to your domain the well if you don t want to hire a statistician full time on your team make it a temporary consultation but hire a classically train statistician could be a very good investment 2 stick to simple model more precisely stick to a limited set of hypothesis one way to look at predictive modeling be as a search problem from an initial set of possible model which be the most appropriate model to fit our datum in a way each data point we use for fit down vote all model that make it unlikely or up vote model that agree with it when you have heap of datum you can afford to explore huge set of model hypothesis effectively and end up with one that be suitable when you don t have so many datum point to begin with you need to start from a fairly small set of possible hypothesis e g the set of all linear model with 3 non zero weight the set of decision tree with depth < = 4 the set of histogram with 10 equally space bin this mean that you rule out complex hypothesis like those that deal with non linearity or feature interaction this also mean that you can t afford to fit model with too many degree of freedom too many weight or parameter whenever appropriate use strong assumption e g no negative weight no interaction between feature specific distribution etc to restrict the space of possible hypothesis 3 pool datum when possible be you build a personalize spam filter try build it on top of a universal model train for all user be you model gdp for a specific country try fit your model on gdp for all country for which you can get datum maybe use importance sample to emphasize the country you re interested in be you try to predict the eruption of a specific volcano you get the idea 4 limit experimentation don t over use your validation set if you try too many different technique and use a hold out set to compare between they be aware of the statistical power of the result you be get and be aware that the performance you be get on this set be not a good estimator for out of sample performance 5 do clean up your datum with small datum set noise and outlier be especially troublesome cleaning up your datum could be crucial here to get sensible model alternatively you can restrict your modeling to technique especially design to be robust to outlier e g quantile regression 6 do perform feature selection I be not a big fan of explicit feature selection I typically go for regularization and model average next two point to avoid over fitting but if the data be truly limit sometimes explicit feature selection be essential wherever possible use domain expertise to do feature selection or elimination as brute force approach e g all subset or greedy forward selection be as likely to cause over fit as include all feature 7 do use regularization regularization be an almost magical solution that constraint model fitting and reduce the effective degree of freedom without reduce the actual number of parameter in the model l1 regularization produce model with few non zero parameter effectively perform implicit feature selection which could be desirable for explainability of performance in production while l2 regularization produce model with more conservative close to zero parameter and be effectively similar to have strong zero center prior for the parameter in the bayesian world l2 be usually well for prediction accuracy than l1 8 do use model average model averaging have similar effect to regularization be that it reduce variance and enhance generalization but it be a generic technique that can be use with any type of model or even with heterogeneous set of model the downside here be that you end up with huge collection of model which could be slow to evaluate or awkward to deploy to a production system two very reasonable form of model averaging be bag and bayesian model average 9 try bayesian modeling and model average again not a favorite technique of mine but bayesian inference may be well suited for deal with small datum set especially if you can use domain expertise to construct sensible prior 10 prefer confidence interval to point estimate it be usually a good idea to get an estimate of confidence in your prediction in addition to produce the prediction itself for regression analysis this usually take the form of predict a range of value that be calibrate to cover the true value 95 % of the time or in the case of classification it could be just a matter of produce class probability this become more crucial with small datum set as it become more likely that certain region in your feature space be less represented than other model average as refer to in the previous two point allow we to do that pretty easily in a generic way for regression classification and density estimation it be also useful to do that when evaluate your model produce confidence interval on the metric you be use to compare model performance be likely to save you from jump to many wrong conclusion this could be a somewhat long list of thing to do or try but they all revolve around three main theme constrain modeling smoothing and quantification of uncertainty most figure use in this post be take from the book pattern recognition and machine learning by christopher bishop from a quick cheer to a stand ovation clap to show how much you enjoy this story relevance engineer machine learning practitioner and hobbyist former entrepreneur rant about machine learning and its future
Matt Fogel,938,4,https://medium.com/swlh/the-7-best-data-science-and-machine-learning-podcasts-e8f0d5a4a419?source=tag_archive---------3----------------,the 7 good datum science and machine learning podcast,datum science and machine learning have long be interest of mine but now that I m work on fuzzy ai and try to make ai and machine learning accessible to all developer I need to keep on top of all the news in both field my preferred way to do this be through listen to podcast I ve listen to a bunch of machine learning and data science podcast in the last few month so I think I d share my favorite a great starting point on some of the basic of data science and machine learning every other week they release a 10 15 minute episode where host kyle and linda polich give a short primer on topic like k mean cluster natural language processing and decision tree learning often use analogy relate to their pet parrot yoshi this be the only place where you ll learn about k mean cluster via placement of parrot dropping website | itune host by katie malone and ben jaffe of online education startup udacity this weekly podcast cover diverse topic in data science and machine learning teach specific concept like hide markov model and how they apply to real world problem and dataset they make complex topic extremely accessible website | itune each week host chris albon and jonathon morgan both experienced technologist and datum scientist talk about the late news in data science over drink listen to partially derivative be a great way to keep up on the late datum news website | itune this podcast feature ben lorica o reilly medium s chief data scientist speak with other expert about timely big datum and data science topic it can often get quite technical but the topic of discussion be always really interesting website | itune datum story be a little more focused on datum visualization than data science but there be often some interesting overlap between the topic every other week enrico bertini and moritz stefaner cover diverse topic in datum with their guest recent episode about smart city and nicholas felton s annual report be particularly interesting website | itune bill itself as a gentle introduction to artificial intelligence and machine learn this podcast can still get quite technical and complex cover topic like how to reason about uncertain event use fuzzy set theory and fuzzy measure theory and how to represent knowledge use logical rule website | itune the new podcast on this list with 8 episode release as of this writing every other week host katherine gorman and ryan adam speak with a guest about their work and news story relate to machine learning website | itune feel I ve unfairly leave a podcast off this list leave I a note to let I know publish in startup wanderlust and life hack from a quick cheer to a stand ovation clap to show how much you enjoy this story cofounder of @fuzzyai help developer make their software smart fast medium s large publication for maker subscribe to receive our top story here → https goo gl zhclji
Illia Polosukhin,255,3,https://medium.com/@ilblackdragon/tensorflow-tutorial-part-1-c559c63c0cb1?source=tag_archive---------4----------------,tensorflow tutorial — part 1 illia polosukhin medium,upd april 20 2016 scikit flow have be merge into tensorflow since version 0 8 and now call tensorflow learn or tf learn google release a machine learning framework call tensorflow and it s take the world by storm 10k+ star on github a lot of publicity and general excitement in between ai researcher now but how you to use it for something regular problem data scientist may have and if you be ai researcher — we will build up to interesting problem over time a reasonable question why as a data scientist who already have a number of tool in your toolbox r scikit learn etc you care about yet another framework the answer be two part let s start with simple example — take titanic dataset from kaggle first make sure you have instal tensorflow and scikit learn with few helpful lib include scikit flow that be simplify a lot of work with tensorflow you can get dataset and the code from http github com ilblackdragon tf_examples quick look at the datum use ipython or ipython notebook for ease of interactive exploration let s test how we can predict survived class base on float variable in scikit learn we separate dataset into feature and target fill in n a in the datum with zero and build a logistic regression predict on the training data give we some measure of accuracy of cause it doesn t properly evaluate the model quality and test dataset should be use but for simplicity we will look at train only for now now use tf learn previously scikit flow congratulation you just build your first tensorflow model tf learn be a library that wrap a lot of new api by tensorflow with nice and familiar scikit learn api tensorflow be all about a building and execute graph this be a very powerful concept but it be also cumbersome to start with look under the hood of tf learn we just use three part even as you get more familiar with tensorflow piece of scikit flow will be useful like graph_action and layer and host of other op and tool see future post for example of handle categorical variable text and image part 2 — deep neural network custom tensorflow model with scikit flow and digit recognition with convolutional network from a quick cheer to a stand ovation clap to show how much you enjoy this story co founder @ near ai — teach machine to code I m tweet as @ilblackdragon
Ahmed El Deeb,283,3,https://medium.com/rants-on-machine-learning/the-unreasonable-effectiveness-of-random-forests-f33c3ce28883?source=tag_archive---------5----------------,the unreasonable effectiveness of random forest rant on machine learning medium,it s very common for machine learning practitioner to have favorite algorithm it s a bit irrational since no algorithm strictly dominate in all application the performance of ml algorithms vary wildly depend on the application and the dimensionality of the dataset and even for a give problem and a give dataset any single model will likely be beat by an ensemble of diverse model train by diverse algorithm anyway but people have favorite nevertheless some like svms for the elegance of their formulation or the quality of the available implementation some like decision rule for their simplicity and interpretability and some be crazy about neural network for their flexibility my favorite out of the box algorithm be as you might have guess the random forest and it s the second modeling technique I typically try on any give datum set after a linear model this beautiful visualization from scikit learn illustrate the modelling capacity of a decision forest here s a paper by leo breiman the inventor of the algorithm describe random forest here s another amazing paper by rich caruana et al evaluate several supervised learning algorithm on many different dataset from a quick cheer to a stand ovation clap to show how much you enjoy this story relevance engineer machine learning practitioner and hobbyist former entrepreneur rant about machine learning and its future
Christophe Bourguignat,157,4,https://medium.com/@chris_bour/6-tricks-i-learned-from-the-otto-kaggle-challenge-a9299378cd61?source=tag_archive---------6----------------,6 trick I learn from the otto kaggle challenge christophe bourguignat medium,here be a few thing I learn from the otto group kaggle competition I have the chance to team up with great kaggle master xavier conort and the french community as a whole have be very active teaming with xavier have be the opportunity to practice some ensemble technic we heavily use stack we add to an initial set of 93 feature new feature be the prediction of n different classifier random forest gbm neural network and then retrain p classifier over the 93 + n feature and finally make a weighted average of the p output we test two trick this be one of the great functionality of the last scikit learn version 0 16 it allow to rescale the classifier prediction by take observation predict within a segment e g 0 3 04 and compare to the actual truth ratio of these observation e g 0 23 with mean that a rescaling be need here be a mini notebook explain how to use calibration and demonstrate how well it work on the otto challenge datum at the beginning of the competition it appear quickly that — once again — gradient boost tree be one of the good perform algorithm provide that you find the right hyper parameter on the scikit learn implementation most important hyper parameter be learning_rate the shrinkage parameter n_estimator the number of boost stage and max_depth limit the number of node in the tree the good value depend on the interaction of the input variable min_samples_split and min_samples_leaf can also be a way to control depth of the tree for optimal performance I also discover that two other parameter be crucial for this competition I must admit I never pay attention on it before this challenge namely subsample the fraction of sample to be use for fit the individual base learner and max_feature the number of feature to consider when look for the good split the problem be to find a way to quickly find the good hyperparameter combination I first discover gridsearchcv that make an exhaustive search over specify parameter range as always with scikit learn it have a convenient programming interface handle for example smoothly cross validation and parallel distributing of search however the number of parameter to tune and their range be too large to discover the good one in the acceptable time frame I have in mind typically while sleep I e 7 to 10 hour I have to fall back to an other option I then use randomizedsearchcv that appear in 0 14 version with this method search be do randomly on a subspace of parameter it give generally very good result as describe in this paper and I be able to find a suitable parameter set within a few hour note that some competitor like french kaggler amine use hyperopt for hyperparameter optimization xgboost be a gradient boost implementation heavily use by kaggler and I now understand why I never use it before but it be a hot topic discuss in the forum I decide to have a look at it even if its main interface be in r but there be a python api that I didn t use yet xgboost be much fast than scikit learn and give well prediction it will remain for sure part of my toolblox someone post on the forum he be right it have be for I the opportunity to play with neural network for the first time several implementation have be use by the competitor h2o keras cxxnet I personally use lasagne main challenge be to fine tune the number of layer number of neuron dropout and learning rate here be a notebook on what I learn one of the secret of the competition be to run several time the same algorithm with random selection of observation and feature and take the average of the output to do that easily I discover the scikit learn baggingclassifier meta estimator it hide the tedious complexity of loop over model fit random subset selection and averaging — and expose easy fit predict_proba entry point from a quick cheer to a stand ovation clap to show how much you enjoy this story data enthusiast # bigdata # datascience # machinelearne # frenchdata # kaggle
I'Boss Potiwarakorn,128,3,https://medium.com/o-v-e-r-f-i-t-t-e-d/machine-learning-%E0%B9%80%E0%B8%A3%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3-%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B9%84%E0%B8%9B%E0%B8%97%E0%B8%B3%E0%B9%84%E0%B8%A1-4a1dcae85b96?source=tag_archive---------7----------------,machine learn เรียนอะไร รู้ไปทําไม o v e r f I t t e d medium,machine learning จริงๆแล้วมันคืออะไรกันแน่ ครั้งแรกที่ผมได้ยินคําคํานี้ ผมก็พูดกับตัวเองในใจ เครื่องจักรที่เรียนรู้ได้ด้วยตัวเองงั้นเหรอ ใครมาถามว่ารู้จัก machine learning หรือเปล่า มันคืออะไรอะ ผมก็ได้แต่บอกเขาไปว่า เคยได้ยินแต่ชื่อว่ะ ผมหวังเป็นอย่างยิ่งว่า คุณที่หลงเข้ามาอ่านบทความนี้ ที่อาจจะเป็นเหมือนผมที่เคยได้ยินมาแต่ชื่อ เมื่ออ่านจบ หากมีใครมาถามว่า รู้จัก machine learning หรือเปล่า มันคืออะไรอะ จะมีความมั่นใจมากพอที่จะตอบเขาไปว่า รู้ดิ นั่งๆ เดี๋ยวเล่าให้ฟัง ก่อนที่จะอธิบายว่า machine learn คืออะไร ขอโม้สักเล็กน้อย ให้เห็นความสําคัญของมันเสียก่อน แล้วจะพบว่า machine learn นี่แทบจะเป็นส่วนหนึ่งของชีวิตประจําวันไปแล้ว machine learning เป็นเรื่องที่ใกล้ตัวเรามากๆ ยิ่งสําหรับคนที่ใช้ internet เป็นประจํานั้น แทบทุกวันจะได้ใช้ประโยชน์จาก machine learn ไม่ว่าจะรู้ตัวหรือไม่ก็ตาม ยกตัวอย่างเช่น เมื่อเราต้องการค้นหาอะไรบางอย่างด้วย google search แต่ไม่ค่อยแน่ใจ คับคล้ายคับคลาว่ามันน่าจะสะกดแบบนี้ machn lean ตัวอย่างโง่นิดนึง ๕๕๕๕๕ ปรากฏว่า โอ้ พระสงฆ์ มันเดาใจเราได้ เป็นหมอดูหรืออย่างไร แน่นอนว่าโค้ดก็คงจะไม่ใช่แบบด้านล่างนี้แน่ๆ แล้วทําไม google ถึงได้รู้ใจเรากันนะ ยังมีตัวอย่างอีกมากมายที่นํา machine learning ไปใช้ เช่น spam filter face recognition handwriting recognition แม้แต่การทํา marketing ในปัจจุบันก็เริ่มใช้ประโยชน์ machine learning เช่นกัน อาทิ การแบ่งกลุ่มลูกค้า customer segmentation การทํานายการสูญเสียลูกค้า customer churn prediction เป็นต้น และที่จะไม่พูดถึงไม่ได้เลยคือ facebook s friend suggestion ที่ผมเองก็ไม่รู้ว่าทําไมมันถึงแนะนําสาวสวยให้ผมอย่างสม่ําเสมอ แต่สิ่งที่ผมมั่นใจคือ facebook ไม่ได้ใช้คนมานั่งเลือกให้แน่ๆ แล้วมันทําได้ยังไงกัน แน่นอนว่า machine learn คือคําตอบ สมองของมนุษย์นั้นมีความสามารถที่น่าทึ่งมากมาย เช่น การตระหนักรู้ อารมณ์ความรู้สึก ความทรงจํา ความสามารถในการควบคุมร่างกาย รวมถึงประสาทสัมผัสทั้งห้าที่ทําให้เรามีความสามารถในการรับรู้ แต่ก็มีปัญหาบางอย่างที่ซับซ้อน และไม่เหมาะที่จะแก้ปัญหาโดยการใช้สมองของมนุษย์เพียงอย่างเดียว เมื่อต้องเขียนโปรแกรมที่จัดการกับข้อมูลจํานวนมาก และมีรูปแบบที่แตกต่างกันออกไป เป็นเรื่องยากที่เราจะทําความเข้าใจข้อมูลและเขียนโปรแกรมที่จะตอบสนองต่อมัน เมื่อมีข้อมูลเข้ามาเพิ่มและมีลักษณะที่ต่างไปอีกก็เหมือนกับ requirement เปลี่ยนตลอดเวลา เราก็ต้องวิเคราะห์ข้อมูลใหม่และแก้โปรแกรมของเราเรื่อยๆซึ่งลําบากมาก arthur samuel หนึ่งในผู้บุกเบิก computer game artificial intelligence และ machine learn ชาวอเมริกัน ได้นิยาม machine learn เอาไว้ว่า เป็น การศึกษาเกี่ยวกับการทําให้คอมพิวเตอร์มีความสามารถที่จะเรียนรู้โดยที่ไม่ต้องเขียนโปรแกรมลงไปตรงๆ กล่าวคือ machine learning นั้น ไม่ได้กําหนดลงไปในโปรแกรมว่า สําหรับลักษณะ a b ใดๆ หากข้อมูลมีลักษณะแบบ a ต้องทําอย่างไร แบบ b ต้องทําอย่างไร แต่เป็นโปรแกรมที่ทําความเข้าใจความสัมพันธ์ของข้อมูล แล้วสร้างวิธีการตอบสนองต่อข้อมูลขึ้นมาเอง ในเมื่อโปรแกรมสามารถเปลี่ยนแปลงวิธีการตอบสนองต่อข้อมูลได้ด้วยตัวเอง เราจึงไม่จําเป็นต้องคอยวิเคราะห์ข้อมูลและแก้โปรแกรมทุกครั้งที่มีข้อมูลใหม่เข้ามาอีกต่อไป ตัวผมเองเคยสงสัยว่า machine learn artificial intelligence และdata mining มันต่างกันยังไง รู้สึกว่ามันก็เป็นเรื่องที่คล้ายกันมากๆ แต่แล้วผมก็พบความจริงว่า จริงๆแล้ว ทั้ง ai artificial intelligence และ data mining นั้นนํา machine learning ไปใช้ สิ่งที่ต่างกันก็คือเป้าหมาย ai นั้นโฟกัสที่การสร้าง intelligent agent หรือตัวตนที่มีความคิดขึ้นมา ซึ่งไม่จําเป็นที่จะต้องใช้ machine learning ก็ได้ ถึงแม้จะใช้เพียงแค่การ search หากสามารถตอบสนองได้อย่างชาญฉลาด ก็สามารถเรียกว่าเป็น ai ได้ แต่ไม่จําเป็น ไม่ได้แปลว่าไม่ได้นําไปใช้ ในทางกลับกัน machine learn ถูกนําไปใช้ประโยชน์ใน ai เยอะมากๆ โดยถูกใช้เพื่อที่จะสร้างความรู้ใหม่ๆ และนําไปสู่การตอบสนองต่อเหตุการณ์ที่ต่างออกไปจากที่กําหนดไว้แต่แรก ส่วน data mining นั้น เป็นขั้นตอนการวิเคราะห์ใน knowledge discovery หรือการค้นหาความรู้ โดยจะเปลี่ยนจากข้อมูลดิบ data ให้เป็นข้อมูลที่ทําความเข้าใจได้ information เพื่อที่จะนําไปใช้ต่อในอนาคต datum mining ใช้วิธีการของทั้ง ai machine learn สถิติ และ database system ในการได้มาซึ่งข้อมูลเชิงลึก หรือ insight โดยสรุปแล้ว ทั้ง 3 ศาสตร์นั้นมีความเกี่ยวข้องกันอย่างมาก และต่างก็นําวิธีการของกันและกันไปใช้ จึงไม่แปลกที่จะรู้สึกว่ามันดูคล้ายๆกัน เพียงแต่เป้าหมายของมันต่างกัน ทําให้วิธีการนั้นไม่เหมือนกันซะทีเดียว machine learn algorithm นั้น โดยพื้นฐานแล้วจะแบ่งออกได้เป็น 2 ประเภทคือ supervise learning กับ unsupervised learning supervise learning คือ การเรียนรู้ที่ได้รับคําแนะนํา สมมติว่าเราเกิดเสียความทรงจํา ไม่สามารถแยกแยะ แอปเปิ้ล มะม่วง และส้มออกจากกันได้ คุณหมอผู้น่ารักจึงเอา แอปเปิ้ล มะม่วง และส้ม มาให้ดู ผลไม้ทั้งหมดที่คุณหมอเอามาให้ดูนี้เรียกว่า training datum คือข้อมูลที่นํามาใช้ในการฝึกสอน คุณหมอเริ่มจากการนําแอปเปิ้ลหลากหลายแบบมาให้ดู และบอกว่า นี่คือแอปเปิ้ล นี่เป็นการให้ label หรือป้ายที่แปะบอกว่าข้อมูลที่ได้มานี้คืออะไร เมื่อเราได้เห็นแอปเปิ้ลก็จะพบว่า แอปเปิ้ลนั้นมีสีแดง หรือสีเขียว รูปทรงของแอปเปิ้ลนั้นหากผ่าครึ่งจะมีลักษณะคล้ายผีเสื้อ สิ่งเหล่านี้เรียกว่า feature หรือคุณสมบัติของข้อมูล หลังจากนั้นคุณหมอก็นํามะม่วงและส้มมาให้ดู และพบว่า มะม่วงมีสีเขียวหรือเหลือง รูปทรงค่อนข้างยาว ส่วนส้มมีสีส้ม และมีรูปทรงเป็นทรงรี เมื่อได้ข้อมูลมากเพียงพอ ก็จะเริ่มแยกแยะ แอปเปิ้ล มะม่วง และส้ม ออกจากกันได้ ต่อมา หากเจอส้มเปลือกสีเขียวก็อาจจะเดาได้ว่าเป็นส้ม เพราะรูปทรงของมัน นี่เป็นตัวอย่างหนึ่งของ classification หรือการจัดหมวดหมู่ เป็น supervise learning แบบหนึ่งที่ใช้กับข้อมูลที่ไม่ต่อเนื่อง discrete regression เป็นการวิเคราะห์สําหรับข้อมูลที่ต่อเนื่อง continuous ภาพด้านบนแสดงถึง linear regression หรือ line of good fit ซึ่งเป็น regression แบบหนึ่ง จะเห็นได้ว่าเรามี training datum อยู่ ซึ่งก็ไม่ได้เรียงตัวกันเป็นเส้นตรง แต่ก็พอจะเห็นรูปแบบและแนวโน้มของข้อมูล หากเราลากเส้นโดยพยายามให้ผลรวมของระยะห่างจาก training datum ซึ่งเป็นความคลาดเคลื่อนน้อยที่สุด เราก็จะได้ model ที่พอจะทํานายค่า y ต่อๆไปได้ george e p box นักสถิติศาสตร์กล่าวไว้ว่า อย่างที่เห็นในภาพ เส้นสีน้ําเงินนั่นคือ model หรือแบบจําลองเพื่อทํานายค่า y ที่มี x สูงกว่านี้ แต่จะเห็นได้ว่าแทบไม่มีจุดไหนที่ตรงเป๊ะๆเลย เราใช้ได้แค่พอทํานายได้คร่าวๆเท่านั้น หาก supervise learning คือการเรียนรู้ที่มีคําแนะนํา unsupervised learning ก็คือการไปตายเอาดาบหน้า ไม่มีใครมาแนะนําเรา แต่คงต้องขอไปลุยซักตั้ง เมื่อ supervise learning มี classification ทางด้าน unsupervise learning ก็จะมี cluster ซึ่งหลายคนรวมถึงผมเอง เมื่อได้รู้จักครั้งแรก ก็สงสัยว่า เอ๊ะ มันต่างกันยังไง classification เป็นการจัดหมวดหมู่ ส่วน cluster เป็นการจัดกลุ่ม ฟังๆดูก็คล้ายๆกันอยู่ดี เช่นนั้นแล้ว ลองกลับไปสวมบทผู้ป่วยสูญเสียความทรงจํา กับคุณหมอน่ารักอีกสักครั้งนะครับ คราวนี้ คุณหมอ เอาผลไม้มาอีกสามชนิดที่หน้าตาไม่เหมือนทั้งแอปเปิ้ล มะม่วงและส้ม แต่คุณหมอไม่ยอมบอกอะไรเกี่ยวกับเจ้าพวกนี้เลยสักนิด หรือก็คือตอนนี้ ผลไม้เหล่านี้ไม่มี label แต่คุณหมอก็สั่งให้เราแยกมันออกมาเป็นสามกลุ่ม เมื่อเราสังเกต feature ของผลไม้พวกนี้ก็จะพอแยกแยะได้ว่าผลไม้ลูกไหนควรจะอยู่กลุ่มเดียวกัน เมื่อแยกได้สามกลุ่มแล้ว คุณหมอก็เดินจากไปเสียเฉยๆ ไม่บอกไม่กล่าวอะไร สุดท้ายเราก็รู้แค่ว่า ผลไม้แต่ละลูกอยู่กลุ่มเดียวกับใคร แต่บอกไม่ได้ว่ามันคืออะไรกันแน่ นี่คือ cluster ต่างจาก classification ที่บอกเราตั้งแต่แรกว่าผลไม้แต่ละชนิดมีชื่อว่าอะไรบ้าง หลายครั้งที่ข้อมูลนั้นมี feature หลายชนิด หรือก็คือเป็นข้อมูลที่มีมิติมาก เมื่อเป็นเช่นนั้นแล้วก็จะเป็นเรื่องยากที่จะแสดงภาพ หรือ visualize ข้อมูล เราจึงควรลดมิติของข้อมูลลง โดยพยายามคงความหมายเดิมอยู่ dimensionality reduction หรือ dimension reduction เป็นการลดมิติของข้อมูล ซึ่งนอกจากจะทําให้ง่ายที่จะ visualize แล้ว เมื่อมีมิติที่น้อยลง นั่นหมายถึงมี feature ที่น้อยลง ซึ่งทําให้ performance ดีขึ้น และลด space complexity อีกด้วย นอกจากประเภทของ machine learn algorithm แบบ basic ที่เขียนไว้ด้านบนแล้วยังมี semi supervised learn และ reinforcement learning ที่พี่ต้า @konpat เขียนเอาไว้ครับ สุดท้ายนี้ ผมเชื่อว่า machine learn เป็นศาสตร์หนึ่งที่สําคัญมากๆสําหรับวงการคอมพิวเตอร์ในปัจจุบัน และอนาคต และส่วนตัวผมคิดว่าศาสตร์นี้มันเท่มากๆเลยนะ ใครสนใจใน machine learning ก็เข้ามาคุยกันได้นะครับ d from a quick cheer to a stand ovation clap to show how much you enjoy this story software choreographer at thoughtwork ; functional programming devop and machine learn enthusiast a half score day a story blog by league of machine learning from chulalongkorn university
samim,301,4,https://medium.com/@samim/generating-stories-about-images-d163ba41e4ed?source=tag_archive---------8----------------,generate story about image samim medium,story be a fundamental human tool that we use to communicate think create a story about a image be a difficult task that many struggle with new machine learning experiment be enable we to generate story base on the content of image this experiment explore how to generate little romantic story about image incl guest star taylor swift neural storyteller be a recently publish experiment by ryan kiros university of toronto it combine recurrent neural network rnn skip thought vector and other technique to generate little story about image neural storyteller s output be creative and often comedic it be open source this experiment start by run 5000 randomly select web image through neural storyteller and experiment with hyper parameter neural storyteller come with 2 pre train model one train on 14 million passage of romance novel the other train on taylor swift lyric input and output be manually filter and recombine into two video use romantic novel model voice generate with a text to speech use taylor swift model combine with a well know swift instrumental neural storyteller give we a fascinating glimpse into the future of storytelle even though these technology be not fully mature yet the art of storytelling be bind to change in the near future author will be train custom model combine style across genre and generate text with image & sound explore this exit new medium be rewarding from a quick cheer to a stand ovation clap to show how much you enjoy this story designer & code magician work at the intersection of hci machine learning & creativity building tool for enlightenment narrative engineering
AirbnbEng,517,9,https://medium.com/airbnb-engineering/how-airbnb-uses-machine-learning-to-detect-host-preferences-18ce07150fa3?source=tag_archive---------9----------------,how airbnb use machine learning to detect host preference,by bar ifrach at airbnb we seek to match people who be look for accommodation — guest — with those look to rent out their place — host guest reach out to host whose listing they wish to stay in however a match succeed only if the host also want to accommodate the guest I first hear about airbnb in 2012 from a friend he offer his nice apartment on the site when he travel to see his family during our vacation from grad school his main goal be to fit as many book night as possible into the 1 2 week when he be away my friend would accept or reject request depend on whether or not the request would help he to maximize his occupancy about two year later I join airbnb as a data scientist I remember my friend s behavior and be curious to discover what affect host decision to accept accommodation request and how airbnb could increase acceptance and match on the platform what start as a small research project result in the development of a machine learning model that learn our host preference for accommodation request base on their past behavior for each search query that a guest enter on airbnb s search engine our model compute the likelihood that relevant host will want to accommodate the guest s request then we surface likely match more prominently in the search result in our a b testing the model show about a 3 75 % increase in book conversion result in many more match on airbnb in this blog post I outline the process that bring we to this model I kick off my research into host acceptance by check if other host maximize their occupancy like my friend every accommodation request fall in a sequence or in a window of available day in the calendar such as on april 5 10 in the calendar show below the gray day surround the window be either block by the host or already book if accept and book a request may leave the host with a sub window before the check in date check in gap — april 5 7 and or a sub window after the check out check out gap — april 10 a host look to have a high occupancy will try to avoid such gap indeed when I plot host tendency to accept over the sum of the check in gap and the check out gap 3 + 1= 4 in the example above as in the next plot I find the effect that I expect to see host be more likely to accept request that fit well in their calendar and minimize gap day but do all host try to maximize occupancy and prefer stay with short gap perhaps some host be not interested in maximize their occupancy and would rather host occasionally and maybe host in big market like my friend be different from host in small market indeed when I look at listing from big and small market separately I find that they behave quite differently host in big market care a lot about their occupancy — a request with no gap be almost 6 % likeli to be accept than one with 7 gap night for small market I find the opposite effect ; host prefer to have a small number of night between request so host in different market have different preference but it seem likely that even within a market host may prefer different stay a similar story reveal itself when I look at host tendency to accept base on other characteristic of the accommodation request for example on average airbnb host prefer accommodation request that be at least a week in advance over last minute request but perhaps some host prefer short notice the plot below look at the dispersion of host preference for last minute stay less than 7 day versus far in advance stay more than 7 day indeed the dispersion in preference reveal that some host like last minute stay well than far in advance stay — those in the bottom right — even though on average host prefer long notice I find similar dispersion in host tendency to accept other trip characteristic like the number of guest whether it be a weekend trip etc all these finding point to the same conclusion if we could promote in our search result host who would be more likely to accept an accommodation request result from that search query we would expect to see happy guest and host and more match that turn into fun vacation or productive business trip in other word we could personalize our search result but not in the way you might expect typically personalize search result promote result that would fit the unique preference of the searcher — the guest at a two sided marketplace like airbnb we also want to personalize search by the preference of the host whose listing would appear in the search result encourage by my finding I join force with another data scientist and a software engineer to create a personalized search signal we set out to associate host prior acceptance and decline decision by the follow characteristic of the trip check in date check out date and number of guest by add host preference to our exist rank model capturing guest preference we hope to enable more and well match at first glance this seem like a perfect case for collaborative filtering — we have user host and item trip and we want to understand the preference for those item by combine historical rating accept decline with statistical learning from similar host however the application do not fully fit in the collaborative filtering framework for two reason with these point in mind we decide to massage the problem into something resemble collaborative filtering we use the multiplicity of response for the same trip to reduce the noise come from the latent factor in the guest host interaction to do so we consider host average response to a certain trip characteristic in isolation instead of look at the combination of trip length size of guest party size of calendar gap and so on we look at each of these trip characteristic by itself with this coarser structure of preference we be able to resolve some of the noise in our datum as well as the potentially conflicting label for the same trip we use the mean acceptance rate for each trip characteristic as a proxy for preference still our datum set be relatively sparse on average for each trip characteristic we could not determine the preference for about 26 % of host because they never receive an accommodation request that meet those trip characteristic as a method of imputation we smooth the preference use a weight function that for each trip characteristic average the median preference of host in the region with the host s preference the weight on the median preference be 1 when the host have no data point and go to 0 monotonically the more datum point the host have use these newly define preference we create prediction for host acceptance use a l 2 regularize logistic regression essentially we combine the preference for different trip characteristic into a single prediction for the probability of acceptance the weight the preference of each trip characteristic have on the acceptance decision be the coefficient that come out of the logistic regression to improve the prediction we include a few more geographic and host specific feature in the logistic regression this flow chart summarize the modeling technique we run this model on segment of host on our cluster use a user generate function udf on hive the udf be write in python ; its input be accommodation request host response to they and a few other host feature depend on the flag pass to it the udf either build the preference for the different trip characteristic or train the logistic regression model use scikit learn our main off line evaluation metric for the model be mean square error mse which be more appropriate in a setting when we care about the predict probability more than about classification in our off line evaluation of the model we be able to get a 10 % decrease in mse over our previous model that capture host acceptance probability this be a promising result but we still have to test the performance of the model live on our site to test the online performance of the model we launch an experiment that use the predict probability of host acceptance as a significant weight in our rank algorithm that also include many other feature that capture guest preference every time a guest in the treatment group enter a search query our model predict the probability of acceptance for all relevant host and influence the order in which listing be present to the guest rank likeli match high we evaluate the experiment by look at multiple metric but the most important one be the likelihood that a guest request accommodation would get a booking booking conversion we find a 3 75 % lift in our booking conversion and a significant increase in the number of successful match between guest and host after conclude the initial experiment we make a few more optimization that improve conversion by approximately another 1 % and then launch the experiment to 100 % of user this be an exciting outcome for our first full fledge personalization search signal and a sizable contributor to our success first this project teach we that in a two sided marketplace personalization can be effective on the buyer as well as the seller side second the project teach we that sometimes you have to roll up your sleeve and build a machine learning model tailor for your own application in this case the application do not quite fit in the collaborative filtering and a multilevel model with host fix effect be too computationally demanding and not suit for a sparse datum set while build our own model take more time it be a fun learning experience finally this project would not have succeed without the fantastic work of spencer de mar and lukasz dziurzynski originally publish at nerd airbnb com on april 14 2015 from a quick cheer to a stand ovation clap to show how much you enjoy this story creative engineer and datum scientist build a world where you can belong anywhere http airbnb io creative engineer and datum scientist build a world where you can belong anywhere http airbnb io
Adam Geitgey,14.2K,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=tag_archive---------1----------------,machine learning be fun part 3 deep learning and convolutional neural network,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 普通话 русский 한국어 português tiếng việt or italiano be you tired of read endless news story about deep learning and not really know what that mean let s change that this time we be go to learn how to write program that recognize object in image use deep learning in other word we re go to explain the black magic that allow google photo to search your photo base on what be in the picture just like part 1 and part 2 this guide be for anyone who be curious about machine learning but have no idea where to start the goal be be accessible to anyone — which mean that there s a lot of generalization and we skip lot of detail but who care if this get anyone more interested in ml then mission accomplish if you haven t already read part 1 and part 2 read they now you might have see this famous xkcd comic before the goof be base on the idea that any 3 year old child can recognize a photo of a bird but figure out how to make a computer recognize object have puzzle the very good computer scientist for over 50 year in the last few year we ve finally find a good approach to object recognition use deep convolutional neural network that sound like a a bunch of make up word from a william gibson sci fi novel but the idea be totally understandable if you break they down one by one so let s do it — let s write a program that can recognize bird before we learn how to recognize picture of bird let s learn how to recognize something much simple — the handwritten number 8 in part 2 we learn about how neural network can solve complex problem by chain together lot of simple neuron we create a small neural network to estimate the price of a house base on how many bedroom it have how big it be and which neighborhood it be in we also know that the idea of machine learning be that the same generic algorithm can be reuse with different datum to solve different problem so let s modify this same neural network to recognize handwritten text but to make the job really simple we ll only try to recognize one letter — the numeral 8 machine learning only work when you have datum — preferably a lot of datum so we need lot and lot of handwritten 8 s to get start luckily researcher create the mnist datum set of handwritten number for this very purpose mnist provide 60 000 image of handwritten digit each as an 18x18 image here be some 8 s from the datum set the neural network we make in part 2 only take in a three number as the input 3 bedroom 2000 sq foot etc but now we want to process image with our neural network how in the world do we feed image into a neural network instead of just number the answer be incredible simple a neural network take number as input to a computer an image be really just a grid of number that represent how dark each pixel be to feed an image into our neural network we simply treat the 18x18 pixel image as an array of 324 number the handle 324 input we ll just enlarge our neural network to have 324 input node notice that our neural network also have two output now instead of just one the first output will predict the likelihood that the image be an 8 and thee second output will predict the likelihood it isn t an 8 by have a separate output for each type of object we want to recognize we can use a neural network to classify object into group our neural network be a lot big than last time 324 input instead of 3 but any modern computer can handle a neural network with a few hundred node without blink this would even work fine on your cell phone all that s leave be to train the neural network with image of 8 s and not 8 s so it learn to tell they apart when we feed in an 8 we ll tell it the probability the image be an 8 be 100 % and the probability it s not an 8 be 0 % vice versa for the counter example image here s some of our training datum we can train this kind of neural network in a few minute on a modern laptop when it s do we ll have a neural network that can recognize picture of 8 s with a pretty high accuracy welcome to the world of late 1980 s era image recognition it s really neat that simply feed pixel into a neural network actually work to build image recognition machine learning be magic right well of course it s not that simple first the good news be that our 8 recognizer really do work well on simple image where the letter be right in the middle of the image but now the really bad news our 8 recognizer totally fail to work when the letter isn t perfectly center in the image just the slight position change ruin everything this be because our network only learn the pattern of a perfectly center 8 it have absolutely no idea what an off center 8 be it know exactly one pattern and one pattern only that s not very useful in the real world real world problem be never that clean and simple so we need to figure out how to make our neural network work in case where the 8 isn t perfectly center we already create a really good program for find an 8 center in an image what if we just scan all around the image for possible 8 s in small section one section at a time until we find one this approach call a slide window it s the brute force solution it work well in some limited case but it s really inefficient you have to check the same image over and over look for object of different size we can do well than this when we train our network we only show it 8 s that be perfectly center what if we train it with more datum include 8 s in all different position and size all around the image we don t even need to collect new training datum we can just write a script to generate new image with the 8 s in all kind of different position in the image use this technique we can easily create an endless supply of training datum more datum make the problem hard for our neural network to solve but we can compensate for that by make our network big and thus able to learn more complicated pattern to make the network big we just stack up layer upon layer of node we call this a deep neural network because it have more layer than a traditional neural network this idea have be around since the late 1960 but until recently train this large of a neural network be just too slow to be useful but once we figure out how to use 3d graphic card which be design to do matrix multiplication really fast instead of normal computer processor work with large neural network suddenly become practical in fact the exact same nvidia geforce gtx 1080 video card that you use to play overwatch can be use to train neural network incredibly quickly but even though we can make our neural network really big and train it quickly with a 3d graphic card that still isn t go to get we all the way to a solution we need to be smart about how we process image into our neural network think about it it doesn t make sense to train a network to recognize an 8 at the top of a picture separately from train it to recognize an 8 at the bottom of a picture as if those be two totally different object there should be some way to make the neural network smart enough to know that an 8 anywhere in the picture be the same thing without all that extra training luckily there be as a human you intuitively know that picture have a hierarchy or conceptual structure consider this picture as a human you instantly recognize the hierarchy in this picture most importantly we recognize the idea of a child no matter what surface the child be on we don t have to re learn the idea of child for every possible surface it could appear on but right now our neural network can t do this it think that an 8 in a different part of the image be an entirely different thing it doesn t understand that move an object around in the picture doesn t make it something different this mean it have to re learn the identify of each object in every possible position that suck we need to give our neural network understanding of translation invariance — an 8 be an 8 no matter where in the picture it show up we ll do this use a process call convolution the idea of convolution be inspire partly by computer science and partly by biology I e mad scientist literally poke cat brain with weird probe to figure out how cat process image instead of feed entire image into our neural network as one grid of number we re go to do something a lot smart that take advantage of the idea that an object be the same no matter where it appear in a picture here s how it s go to work step by step — similar to our slide window search above let s pass a slide window over the entire original image and save each result as a separate tiny picture tile by do this we turn our original image into 77 equally sized tiny image tile early we feed a single image into a neural network to see if it be an 8 we ll do the exact same thing here but we ll do it for each individual image tile however there s one big twist we ll keep the same neural network weight for every single tile in the same original image in other word we be treat every image tile equally if something interesting appear in any give tile we ll mark that tile as interesting we don t want to lose track of the arrangement of the original tile so we save the result from process each tile into a grid in the same arrangement as the original image it look like this in other word we ve start with a large image and we end with a slightly small array that record which section of our original image be the most interesting the result of step 3 be an array that map out which part of the original image be the most interesting but that array be still pretty big to reduce the size of the array we downsample it use an algorithm call max pooling it sound fancy but it isn t at all we ll just look at each 2x2 square of the array and keep the big number the idea here be that if we find something interesting in any of the four input tile that make up each 2x2 grid square we ll just keep the most interesting bit this reduce the size of our array while keep the most important bit so far we ve reduce a giant image down into a fairly small array guess what that array be just a bunch of number so we can use that small array as input into another neural network this final neural network will decide if the image be or isn t a match to differentiate it from the convolution step we call it a fully connect network so from start to finish our whole five step pipeline look like this our image processing pipeline be a series of step convolution max pooling and finally a fully connect network when solve problem in the real world these step can be combine and stack as many time as you want you can have two three or even ten convolution layer you can throw in max pooling wherever you want to reduce the size of your datum the basic idea be to start with a large image and continually boil it down step by step until you finally have a single result the more convolution step you have the more complicated feature your network will be able to learn to recognize for example the first convolution step might learn to recognize sharp edge the second convolution step might recognize beak use it s knowledge of sharp edge the third step might recognize entire bird use it s knowledge of beak etc here s what a more realistic deep convolutional network like you would find in a research paper look like in this case they start a 224 x 224 pixel image apply convolution and max pooling twice apply convolution 3 more time apply max pooling and then have two fully connect layer the end result be that the image be classify into one of 1000 category so how do you know which step you need to combine to make your image classifier work honestly you have to answer this by do a lot of experimentation and testing you might have to train 100 network before you find the optimal structure and parameter for the problem you be solve machine learning involve a lot of trial and error now finally we know enough to write a program that can decide if a picture be a bird or not as always we need some datum to get start the free cifar10 datum set contain 6 000 picture of bird and 52 000 picture of thing that be not bird but to get even more datum we ll also add in the caltech ucsd bird 200 2011 datum set that have another 12 000 bird pic here s a few of the bird from our combine datum set and here s some of the 52 000 non bird image this datum set will work fine for our purpose but 72 000 low re image be still pretty small for real world application if you want google level performance you need million of large image in machine learning have more datum be almost always more important that have well algorithm now you know why google be so happy to offer you unlimited photo storage they want your sweet sweet datum to build our classifier we ll use tflearn tflearn be a wrapper around google s tensorflow deep learning library that expose a simplified api it make build convolutional neural network as easy as write a few line of code to define the layer of our network here s the code to define and train the network if you be train with a good video card with enough ram like an nvidia geforce gtx 980 ti or well this will be do in less than an hour if you be train with a normal cpu it might take a lot long as it train the accuracy will increase after the first pass I get 75 4 % accuracy after just 10 pass it be already up to 91 7 % after 50 or so pass it cap out around 95 5 % accuracy and additional training didn t help so I stop it there congrat our program can now recognize bird in image now that we have a train neural network we can use it here s a simple script that take in a single image file and predict if it be a bird or not but to really see how effective our network be we need to test it with lot of image the datum set I create hold back 15 000 image for validation when I run those 15 000 image through the network it predict the correct answer 95 % of the time that seem pretty good right well it depend our network claim to be 95 % accurate but the devil be in the detail that could mean all sort of different thing for example what if 5 % of our training image be bird and the other 95 % be not bird a program that guess not a bird every single time would be 95 % accurate but it would also be 100 % useless we need to look more closely at the number than just the overall accuracy to judge how good a classification system really be we need to look closely at how it fail not just the percentage of the time that it fail instead of think about our prediction as right and wrong let s break they down into four separate category — use our validation set of 15 000 image here s how many time our prediction fall into each category why do we break our result down like this because not all mistake be create equal imagine if we be write a program to detect cancer from an mri image if we be detect cancer we d rather have false positive than false negative false negative would be the bad possible case — that s when the program tell someone they definitely didn t have cancer but they actually do instead of just look at overall accuracy we calculate precision and recall metric precision and recall metric give we a clear picture of how well we do this tell we that 97 % of the time we guess bird we be right but it also tell we that we only find 90 % of the actual bird in the datum set in other word we might not find every bird but we be pretty sure about it when we do find one now that you know the basic of deep convolutional network you can try out some of the example that come with tflearn to get your hand dirty with different neural network architecture it even come with build in datum set so you don t even have to find your own image you also know enough now to start branch and learn about other area of machine learn why not learn how to use algorithm to train computer how to play atari game next if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 4 part 5 and part 6 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Adam Geitgey,15.2K,13,https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------2----------------,machine learning be fun part 4 modern face recognition with deep learning,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 普通话 русский 한국어 português tiếng việt or italiano have you notice that facebook have develop an uncanny ability to recognize your friend in your photograph in the old day facebook use to make you to tag your friend in photo by click on they and type in their name now as soon as you upload a photo facebook tag everyone for you like magic this technology be call face recognition facebook s algorithm be able to recognize your friend face after they have be tag only a few time it s pretty amazing technology — facebook can recognize face with 98 % accuracy which be pretty much as good as human can do let s learn how modern face recognition work but just recognize your friend would be too easy we can push this tech to the limit to solve a more challenging problem — tell will ferrell famous actor apart from chad smith famous rock musician so far in part 1 2 and 3 we ve use machine learning to solve isolated problem that have only one step — estimate the price of a house generate new datum base on exist datum and tell if an image contain a certain object all of those problem can be solve by choose one machine learning algorithm feed in datum and get the result but face recognition be really a series of several relate problem as a human your brain be wire to do all of this automatically and instantly in fact human be too good at recognize face and end up see face in everyday object computer be not capable of this kind of high level generalization at least not yet so we have to teach they how to do each step in this process separately we need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step in other word we will chain together several machine learning algorithm let s tackle this problem one step at a time for each step we ll learn about a different machine learning algorithm I m not go to explain every single algorithm completely to keep this from turn into a book but you ll learn the main idea behind each one and you ll learn how you can build your own facial recognition system in python use openface and dlib the first step in our pipeline be face detection obviously we need to locate the face in a photograph before we can try to tell they apart if you ve use any camera in the last 10 year you ve probably see face detection in action face detection be a great feature for camera when the camera can automatically pick out face it can make sure that all the face be in focus before it take the picture but we ll use it for a different purpose — find the area of the image we want to pass on to the next step in our pipeline face detection go mainstream in the early 2000 s when paul viola and michael jones invent a way to detect face that be fast enough to run on cheap camera however much more reliable solution exist now we re go to use a method invent in 2005 call histogram of orient gradient — or just hog for short to find face in an image we ll start by make our image black and white because we don t need color datum to find face then we ll look at every single pixel in our image one at a time for every single pixel we want to look at the pixel that directly surround it our goal be to figure out how dark the current pixel be compare to the pixel directly surround it then we want to draw an arrow showing in which direction the image be get dark if you repeat that process for every single pixel in the image you end up with every pixel be replace by an arrow these arrow be call gradient and they show the flow from light to dark across the entire image this might seem like a random thing to do but there s a really good reason for replace the pixel with gradient if we analyze pixel directly really dark image and really light image of the same person will have totally different pixel value but by only consider the direction that brightness change both really dark image and really bright image will end up with the same exact representation that make the problem a lot easy to solve but save the gradient for every single pixel give we way too much detail we end up miss the forest for the tree it would be well if we could just see the basic flow of lightness darkness at a high level so we could see the basic pattern of the image to do this we ll break up the image into small square of 16x16 pixel each in each square we ll count up how many gradient point in each major direction how many point up point up right point right etc then we ll replace that square in the image with the arrow direction that be the strong the end result be we turn the original image into a very simple representation that capture the basic structure of a face in a simple way to find face in this hog image all we have to do be find the part of our image that look the most similar to a know hog pattern that be extract from a bunch of other training face use this technique we can now easily find face in any image if you want to try this step out yourself use python and dlib here s code show how to generate and view hog representation of image whew we isolate the face in our image but now we have to deal with the problem that face turn different direction look totally different to a computer to account for this we will try to warp each picture so that the eye and lip be always in the sample place in the image this will make it a lot easy for we to compare face in the next step to do this we be go to use an algorithm call face landmark estimation there be lot of way to do this but we be go to use the approach invent in 2014 by vahid kazemi and josephine sullivan the basic idea be we will come up with 68 specific point call landmark that exist on every face — the top of the chin the outside edge of each eye the inner edge of each eyebrow etc then we will train a machine learn algorithm to be able to find these 68 specific point on any face here s the result of locate the 68 face landmark on our test image now that we know be the eye and mouth be we ll simply rotate scale and shear the image so that the eye and mouth be center as well as possible we win t do any fancy 3d warps because that would introduce distortion into the image we be only go to use basic image transformation like rotation and scale that preserve parallel line call affine transformation now no matter how the face be turn we be able to center the eye and mouth be in roughly the same position in the image this will make our next step a lot more accurate if you want to try this step out yourself use python and dlib here s the code for find face landmark and here s the code for transform the image use those landmark now we be to the meat of the problem — actually tell face apart this be where thing get really interesting the simple approach to face recognition be to directly compare the unknown face we find in step 2 with all the picture we have of people that have already be tag when we find a previously tag face that look very similar to our unknown face it must be the same person seem like a pretty good idea right there s actually a huge problem with that approach a site like facebook with billion of user and a trillion photo can t possibly loop through every previous tag face to compare it to every newly uploaded picture that would take way too long they need to be able to recognize face in millisecond not hour what we need be a way to extract a few basic measurement from each face then we could measure our unknown face the same way and find the know face with the close measurement for example we might measure the size of each ear the spacing between the eye the length of the nose etc if you ve ever watch a bad crime show like csi you know what I be talk about ok so which measurement should we collect from each face to build our know face database ear size nose length eye color something else it turn out that the measurement that seem obvious to we human like eye color don t really make sense to a computer look at individual pixel in an image researcher have discover that the most accurate approach be to let the computer figure out the measurement to collect itself deep learning do a well job than human at figure out which part of a face be important to measure the solution be to train a deep convolutional neural network just like we do in part 3 but instead of train the network to recognize picture object like we do last time we be go to train it to generate 128 measurement for each face the training process work by look at 3 face image at a time then the algorithm look at the measurement it be currently generate for each of those three image it then tweak the neural network slightly so that it make sure the measurement it generate for # 1 and # 2 be slightly close while make sure the measurement for # 2 and # 3 be slightly far apart after repeat this step million of time for million of image of thousand of different people the neural network learn to reliably generate 128 measurement for each person any ten different picture of the same person should give roughly the same measurement machine learn people call the 128 measurement of each face an embed the idea of reduce complicate raw datum like a picture into a list of computer generate number come up a lot in machine learning especially in language translation the exact approach for face we be use be invent in 2015 by researcher at google but many similar approach exist this process of train a convolutional neural network to output face embedding require a lot of datum and computer power even with an expensive nvidia telsa video card it take about 24 hour of continuous training to get good accuracy but once the network have be train it can generate measurement for any face even one it have never see before so this step only need to be do once lucky for we the fine folk at openface already do this and they publish several train network which we can directly use thank brandon amos and team so all we need to do ourselves be run our face image through their pre train network to get the 128 measurement for each face here s the measurement for our test image so what part of the face be these 128 number measure exactly it turn out that we have no idea it doesn t really matter to we all that we care be that the network generate nearly the same number when look at two different picture of the same person if you want to try this step yourself openface provide a lua script that will generate embedding all image in a folder and write they to a csv file you run it like this this last step be actually the easy step in the whole process all we have to do be find the person in our database of know people who have the close measurement to our test image you can do that by use any basic machine learning classification algorithm no fancy deep learning trick be need we ll use a simple linear svm classifier but lot of classification algorithm could work all we need to do be train a classifier that can take in the measurement from a new test image and tell which known person be the close match run this classifier take millisecond the result of the classifier be the name of the person so let s try out our system first I train a classifier with the embedding of about 20 picture each of will ferrell chad smith and jimmy falon then I run the classifier on every frame of the famous youtube video of will ferrell and chad smith pretend to be each other on the jimmy fallon show it work and look how well it work for face in different pose — even sideways face let s review the step we follow now that you know how this all work here s instruction from start to finish of how run this entire face recognition pipeline on your own computer update 4 9 2017 you can still follow the step below to use openface however I ve release a new python base face recognition library call face_recognition that be much easy to install and use so I d recommend try out face_recognition first instead of continue below I even put together a pre configure virtual machine with face_recognition opencv tensorflow and lot of other deep learning tool pre instal you can download and run it on your computer very easily give the virtual machine a shot if you don t want to install all these librarie yourself original openface instruction if you like this article please consider sign up for my machine learning be fun newsletter you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 5 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Adam Geitgey,10.4K,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=tag_archive---------3----------------,machine learning be fun part 2 adam geitgey medium,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in italiano español français türkçe русский 한국어 português فارسی tiếng việt or 普通话 in part 1 we say that machine learning be use generic algorithm to tell you something interesting about your datum without write any code specific to the problem you be solve if you haven t already read part 1 read it now this time we be go to see one of these generic algorithm do something really cool — create video game level that look like they be make by human we ll build a neural network feed it exist super mario level and watch new one pop out just like part 1 this guide be for anyone who be curious about machine learning but have no idea where to start the goal be be accessible to anyone — which mean that there s a lot of generalization and we skip lot of detail but who care if this get anyone more interested in ml then mission accomplish back in part 1 we create a simple algorithm that estimate the value of a house base on its attribute give datum about a house like this we end up with this simple estimation function in other word we estimate the value of the house by multiply each of its attribute by a weight then we just add those number up to get the house s value instead of use code let s represent that same function as a simple diagram however this algorithm only work for simple problem where the result have a linear relationship with the input what if the truth behind house price isn t so simple for example maybe the neighborhood matter a lot for big house and small house but doesn t matter at all for medium sized house how could we capture that kind of complicated detail in our model to be more clever we could run this algorithm multiple time with different of weight that each capture different edge case now we have four different price estimate let s combine those four price estimate into one final estimate we ll run they through the same algorithm again but use another set of weight our new super answer combine the estimate from our four different attempt to solve the problem because of this it can model more case than we could capture in one simple model let s combine our four attempt to guess into one big diagram this be a neural network each node know how to take in a set of input apply weight to they and calculate an output value by chain together lot of these node we can model complex function there s a lot that I m skip over to keep this brief include feature scaling and the activation function but the most important part be that these basic idea click it s just like lego we can t model much with one single lego block but we can model anything if we have enough basic lego block to stick together the neural network we ve see always return the same answer when you give it the same input it have no memory in programming term it s a stateless algorithm in many case like estimate the price of house that s exactly what you want but the one thing this kind of model can t do be respond to pattern in datum over time imagine I hand you a keyboard and ask you to write a story but before you start my job be to guess the very first letter that you will type what letter should I guess I can use my knowledge of english to increase my odd of guess the right letter for example you will probably type a letter that be common at the beginning of word if I look at story you write in the past I could narrow it down far base on the word you usually use at the beginning of your story once I have all that datum I could use it to build a neural network to model how likely it be that you would start with any give letter our model might look like this but let s make the problem hard let s say I need to guess the next letter you be go to type at any point in your story this be a much more interesting problem let s use the first few word of ernest hemingway s the sun also rise as an example what letter be go to come next you probably guess n — the word be probably go to be box we know this base on the letter we ve already see in the sentence and our knowledge of common word in english also the word middleweight give we an extra clue that we be talk about box in other word it s easy to guess the next letter if we take into account the sequence of letter that come right before it and combine that with our knowledge of the rule of english to solve this problem with a neural network we need to add state to our model each time we ask our neural network for an answer we also save a set of our intermediate calculation and re use they the next time as part of our input that way our model will adjust its prediction base on the input that it have see recently keep track of state in our model make it possible to not just predict the most likely first letter in the story but to predict the most likely next letter give all previous letter this be the basic idea of a recurrent neural network we be update the network each time we use it this allow it to update its prediction base on what it see most recently it can even model pattern over time as long as we give it enough of a memory predict the next letter in a story might seem pretty useless what s the point one cool use might be auto predict for a mobile phone keyboard but what if we take this idea to the extreme what if we ask the model to predict the next most likely character over and over — forever we d be ask it to write a complete story for we we see how we could guess the next letter in hemingway s sentence let s try generate a whole story in the style of hemingway to do this we be go to use the recurrent neural network implementation that andrej karpathy write andrej be a deep learning researcher at stanford and he write an excellent introduction to generate text with rnn you can view all the code for the model on github we ll create our model from the complete text of the sun also rise — 362 239 character use 84 unique letter include punctuation uppercase lowercase etc this datum set be actually really small compare to typical real world application to generate a really good model of hemingway s style it would be much well to have at several time as much sample text but this be good enough to play around with as an example as we just start to train the rnn it s not very good at predict letter here s what it generate after a 100 loop of training you can see that it have figure out that sometimes word have space between they but that s about it after about 1000 iteration thing be look more promising the model have start to identify the pattern in basic sentence structure it s add period at the end of sentence and even quote dialog a few word be recognizable but there s also still a lot of nonsense but after several thousand more training iteration it look pretty good at this point the algorithm have capture the basic pattern of hemingway s short direct dialog a few sentence even sort of make sense compare that with some real text from the book even by only look for pattern one character at a time our algorithm have reproduce plausible looking prose with proper formatting that be kind of amazing we don t have to generate text completely from scratch either we can seed the algorithm by supply the first few letter and just let it find the next few letter for fun let s make a fake book cover for our imaginary book by generate a new author name and a new title use the seed text of er he and the s not bad but the really mind blow part be that this algorithm can figure out pattern in any sequence of datum it can easily generate real looking recipe or fake obama speech but why limit ourselves human language we can apply this same idea to any kind of sequential datum that have a pattern in 2015 nintendo release super mario makertm for the wii u gaming system this game let you draw out your own super mario brother level on the gamepad and then upload they to the internet so you friend can play through they you can include all the classic power up and enemy from the original mario game in your level it s like a virtual lego set for people who grow up play super mario brother can we use the same model that generate fake hemingway text to generate fake super mario brother level first we need a datum set for train our model let s take all the outdoor level from the original super mario brothers game release in 1985 this game have 32 level and about 70 % of they have the same outdoor style so we ll stick to those to get the design for each level I take an original copy of the game and write a program to pull the level design out of the game s memory super mario bros be a 30 year old game and there be lot of resource online that help you figure out how the level be store in the game s memory extract level datum from an old video game be a fun programming exercise that you should try sometime here s the first level from the game which you probably remember if you ever play it if we look closely we can see the level be make of a simple grid of object we could just as easily represent this grid as a sequence of character with one character represent each object we ve replace each object in the level with a letter and so on use a different letter for each different kind of object in the level I end up with text file that look like this look at the text file you can see that mario level don t really have much of a pattern if you read they line by line the pattern in a level really emerge when you think of the level as a series of column so in order for the algorithm to find the pattern in our datum we need to feed the datum in column by column figure out the most effective representation of your input datum call feature selection be one of the key of use machine learning algorithm well to train the model I need to rotate my text file by 90 degree this make sure the character be feed into the model in an order where a pattern would more easily show up just like we see when create the model of hemingway s prose a model improve as we train it after a little training our model be generate junk it sort of have an idea that s and = s should show up a lot but that s about it it hasn t figure out the pattern yet after several thousand iteration it s start to look like something the model have almost figure out that each line should be the same length it have even start to figure out some of the logic of mario the pipe in mario be always two block wide and at least two block high so the p s in the datum should appear in 2x2 cluster that s pretty cool with a lot more training the model get to the point where it generate perfectly valid datum let s sample an entire level s worth of datum from our model and rotate it back horizontal this data look great there be several awesome thing to notice finally let s take this level and recreate it in super mario maker play it yourself if you have super mario maker you can play this level by bookmarke it online or by look it up use level code 4ac9 0000 0157 f3c3 the recurrent neural network algorithm we use to train our model be the same kind of algorithm use by real world company to solve hard problem like speech detection and language translation what make our model a toy instead of cut edge be that our model be generate from very little datum there just aren t enough level in the original super mario brothers game to provide enough datum for a really good model if we could get access to the hundred of thousand of user create super mario maker level that nintendo have we could make an amazing model but we can t — because nintendo win t let we have they big company don t give away their datum for free as machine learning become more important in more industry the difference between a good program and a bad program will be how much datum you have to train your model that s why company like google and facebook need your datum so badly for example google recently open source tensorflow its software toolkit for build large scale machine learning application it be a pretty big deal that google give away such important capable technology for free this be the same stuff that power google translate but without google s massive trove of datum in every language you can t create a competitor to google translate data be what give google its edge think about that the next time you open up your google map location history or facebook location history and notice that it store every place you ve ever be in machine learn there s never a single way to solve a problem you have limitless option when decide how to pre process your datum and which algorithm to use often combine multiple approach will give you well result than any single approach reader have send I link to other interesting approach to generate super mario level if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 3 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Arthur Juliani,9K,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------4----------------,simple reinforcement learning with tensorflow part 0 q learning with table and neural network,for this tutorial in my reinforcement learning series we be go to be explore a family of rl algorithms call q learning algorithms these be a little different than the policy base algorithm that will be look at in the the follow tutorial part 1 3 instead of start with a complex and unwieldy deep neural network we will begin by implement a simple lookup table version of the algorithm and then show how to implement a neural network equivalent use tensorflow give that we be go back to basic it may be good to think of this as part 0 of the series it will hopefully give an intuition into what be really happen in q learning that we can then build on go forward when we eventually combine the policy gradient and q learning approach to build state of the art rl agent if you be more interested in policy network or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient method which attempt to learn function which directly map an observation to an action q learning attempt to learn the value of be in a give state and take a specific action there while both approach ultimately allow we to take intelligent action give a situation the mean of get to that action differ significantly you may have hear about deepq network which can play atari game these be really just large and more complex implementation of the q learning algorithm we be go to discuss here for this tutorial we be go to be attempt to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provide an easy way for people to experiment with their learn agent in an array of provide toy game the frozenlake environment consist of a 4x4 grid of block each one either be the start block the goal block a safe frozen block or a dangerous hole the objective be to have an agent learn to navigate from the start to the goal without move onto a hole at any give time the agent can choose to move either up down left or right the catch be that there be a wind which occasionally blow the agent onto a space they didn t choose as such perfect performance every time be impossible but learn to avoid the hole and reach the goal be certainly still doable the reward at every step be 0 except for enter the goal which provide a reward of 1 thus we will need an algorithm that learn long term expect reward this be exactly what q learning be design to provide in it s simple implementation q learning be a table of value for every state row and action column possible in the environment within each cell of the table we learn a value for how good it be to take a give action within a give state in the case of the frozenlake environment we have 16 possible state one for each block and 4 possible action the four direction of movement give we a 16x4 table of q value we start by initialize the table to be uniform all zero and then as we observe the reward we obtain for various action we update the table accordingly we make update to our q table use something call the bellman equation which state that the expect long term reward for a give action be equal to the immediate reward from the current action combine with the expect reward from the good future action take at the follow state in this way we reuse our own q table when estimate how to update our table for future action in equation form the rule look like this this say that the q value for a give state s and action a should represent the current reward r plus the maximum discount γ future reward expect accord to our own table for the next state s we would end up in the discount variable allow we to decide how important the possible future reward be compare to the present reward by update in this way the table slowly begin to obtain accurate measure of the expect future reward for a give action in a give state below be a python walkthrough of the q table algorithm implement in the frozenlake environment thank to praneet d for find the optimal hyperparameter for this approach now you may be think table be great but they don t really scale do they while it be easy to have a 16x4 table for a simple grid world the number of possible state in any modern game or real world environment be nearly infinitely large for most interesting problem table simply don t work we instead need some way to take a description of our state and produce q value for action without a table that be where neural network come in by act as a function approximator we can take any number of possible state that can be represent as a vector and learn to map they to q value in the case of the frozenlake example we will be use a one layer network which take the state encode in a one hot vector 1x16 and produce a vector of 4 q value one for each action such a simple network act kind of like a glorify table with the network weight serve as the old cell the key difference be that we can easily expand the tensorflow network with add layer activation function and different input type whereas all that be impossible with a regular table the method of update be a little different as well instead of directly update our table with a network we will be use backpropagation and a loss function our loss function will be sum of square loss where the difference between the current predict q value and the target value be compute and the gradient pass through the network in this case our q target for the choose action be the equivalent to the q value compute in equation 1 above below be the tensorflow walkthrough of implement our simple q network while the network learn to solve the frozenlake problem it turn out it doesn t do so quite as efficiently as the q table while neural network allow for great flexibility they do so at the cost of stability when it come to q learning there be a number of possible extension to our simple q network which allow for great performance and more robust learn two trick in particular be refer to as experience replay and freeze target network those improvement and other tweak be the key to get atari play deep q network and we will be explore those addition in the future for more info on the theory behind q learning see this great post by tambet matiisen I hope this tutorial have be helpful for those curious about how to implement simple q learning algorithms if this post have be valuable to you please consider donate to help support future tutorial article and implementation any contribution be greatly appreciate if you d like to follow my work on deep learning ai and cognitive science follow I on medium @arthur juliani or on twitter @awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @unity3d & cognitive neuroscience phd student explore frontier technology through the lens of artificial intelligence datum science and the shape of thing to come
Adam Geitgey,6.8K,11,https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=tag_archive---------5----------------,machine learning be fun part 6 how to do speech recognition with deep learning,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 普通话 한국어 tiếng việt or русский speech recognition be invade our life it s build into our phone our game console and our smart watch it s even automate our home for just $ 50 you can get an amazon echo dot — a magic box that allow you to order pizza get a weather report or even buy trash bag — just by speak out loud the echo dot have be so popular this holiday season that amazon can t seem to keep they in stock but speech recognition have be around for decade so why be it just now hit the mainstream the reason be that deep learning finally make speech recognition accurate enough to be useful outside of carefully control environment andrew ng have long predict that as speech recognition go from 95 % accurate to 99 % accurate it will become a primary way that we interact with computer the idea be that this 4 % accuracy gap be the difference between annoyingly unreliable and incredibly useful thank to deep learning we re finally crest that peak let s learn how to do speech recognition with deep learning if you know how neural machine translation work you might guess that we could simply feed sound recording into a neural network and train it to produce text that s the holy grail of speech recognition with deep learning but we aren t quite there yet at least at the time that I write this — I bet that we will be in a couple of year the big problem be that speech vary in speed one person might say hello very quickly and another person might say heeeelllllllllllllooooo very slowly produce a much long sound file with much more datum both both sound file should be recognize as exactly the same text — hello automatically align audio file of various length to a fix length piece of text turn out to be pretty hard to work around this we have to use some special trick and extra precessing in addition to a deep neural network let s see how it work the first step in speech recognition be obvious — we need to feed sound wave into a computer in part 3 we learn how to take an image and treat it as an array of number so that we can feed directly into a neural network for image recognition but sound be transmit as wave how do we turn sound wave into number let s use this sound clip of I say hello sound wave be one dimensional at every moment in time they have a single value base on the height of the wave let s zoom in on one tiny part of the sound wave and take a look to turn this sound wave into number we just record of the height of the wave at equally space point this be call sample we be take a reading thousand of time a second and record a number represent the height of the sound wave at that point in time that s basically all an uncompressed wav audio file be cd quality audio be sample at 44 1khz 44 100 reading per second but for speech recognition a sampling rate of 16khz 16 000 sample per second be enough to cover the frequency range of human speech let sample our hello sound wave 16 000 time per second here s the first 100 sample you might be think that sampling be only create a rough approximation of the original sound wave because it s only take occasional reading there s gap in between our reading so we must be lose datum right but thank to the nyquist theorem we know that we can use math to perfectly reconstruct the original sound wave from the space out sample — as long as we sample at least twice as fast as the high frequency we want to record I mention this only because nearly everyone get this wrong and assume that use high sampling rate always lead to well audio quality it doesn t < end rant > we now have an array of number with each number represent the sound wave s amplitude at 1 16 000th of a second interval we could feed these number right into a neural network but try to recognize speech pattern by process these sample directly be difficult instead we can make the problem easy by do some pre processing on the audio datum let s start by group our sample audio into 20 millisecond long chunk here s our first 20 millisecond of audio I e our first 320 sample plot those number as a simple line graph give we a rough approximation of the original sound wave for that 20 millisecond period of time this recording be only 1 50th of a second long but even this short recording be a complex mish mash of different frequency of sound there s some low sound some mid range sound and even some high pitch sound sprinkle in but take all together these different frequency mix together to make up the complex sound of human speech to make this datum easy for a neural network to process we be go to break apart this complex sound wave into it s component part we ll break out the low pitch part the next low pitch part and so on then by add up how much energy be in each of those frequency band from low to high we create a fingerprint of sort for this audio snippet imagine you have a recording of someone play a c major chord on a piano that sound be the combination of three musical note — c e and g — all mixed together into one complex sound we want to break apart that complex sound into the individual note to discover that they be c e and g this be the exact same idea we do this use a mathematic operation call a fouri transform it break apart the complex sound wave into the simple sound wave that make it up once we have those individual sound wave we add up how much energy be contain in each one the end result be a score of how important each frequency range be from low pitch I e bass note to high pitch each number below represent how much energy be in each 50hz band of our 20 millisecond audio clip but this be a lot easy to see when you draw this as a chart if we repeat this process on every 20 millisecond chunk of audio we end up with a spectrogram each column from leave to right be one 20ms chunk a spectrogram be cool because you can actually see musical note and other pitch pattern in audio datum a neural network can find pattern in this kind of datum more easily than raw sound wave so this be the datum representation we ll actually feed into our neural network now that we have our audio in a format that s easy to process we will feed it into a deep neural network the input to the neural network will be 20 millisecond audio chunk for each little audio slice it will try to figure out the letter that correspond the sound currently be speak we ll use a recurrent neural network — that be a neural network that have a memory that influence future prediction that s because each letter it predict should affect the likelihood of the next letter it will predict too for example if we have say hel so far it s very likely we will say lo next to finish out the word hello it s much less likely that we will say something unpronounceable next like xyz so have that memory of previous prediction help the neural network make more accurate prediction go forward after we run our entire audio clip through the neural network one chunk at a time we ll end up with a mapping of each audio chunk to the letter most likely speak during that chunk here s what that mapping look like for I say hello our neural net be predict that one likely thing I say be hhhee_ll_lllooo but it also think that it be possible that I say hhhuu_ll_lllooo or even aaauu_ll_lllooo we have some step we follow to clean up this output first we ll replace any repeat character a single character then we ll remove any blank that leave we with three possible transcription — hello hullo and aullo if you say they out loud all of these sound similar to hello because it s predict one character at a time the neural network will come up with these very sound out transcription for example if you say he would not go it might give one possible transcription as he wud net go the trick be to combine these pronunciation base prediction with likelihood score base on large database of write text book news article etc you throw out transcription that seem the least likely to be real and keep the transcription that seem the most realistic of our possible transcription hello hullo and aullo obviously hello will appear more frequently in a database of text not to mention in our original audio base training datum and thus be probably correct so we ll pick hello as our final transcription instead of the other do you might be think but what if someone say hullo it s a valid word maybe hello be the wrong transcription of course it be possible that someone actually say hullo instead of hello but a speech recognition system like this train on american english will basically never produce hullo as the transcription it s just such an unlikely thing for a user to say compare to hello that it will always think you be say hello no matter how much you emphasize the u sound try it out if your phone be set to american english try to get your phone s digital assistant to recognize the world hullo you can t it refuse it will always understand it as hello not recognize hullo be a reasonable behavior but sometimes you ll find annoying case where your phone just refuse to understand something valid you be say that s why these speech recognition model be always be retrain with more datum to fix these edge case one of the cool thing about machine learning be how simple it sometimes seem you get a bunch of datum feed it into a machine learn algorithm and then magically you have a world class ai system run on your game laptop s video card right that sort of true in some case but not for speech recognize speech be a hard problem you have to overcome almost limitless challenge bad quality microphone background noise reverb and echo accent variation and on and on all of these issue need to be present in your training datum to make sure the neural network can deal with they here s another example do you know that when you speak in a loud room you unconsciously raise the pitch of your voice to be able to talk over the noise human have no problem understand you either way but neural network need to be train to handle this special case so you need training datum with people yell over noise to build a voice recognition system that perform on the level of siri google now or alexa you will need a lot of training datum — far more datum than you can likely get without hire hundred of people to record it for you and since user have low tolerance for poor quality voice recognition system you can t skimp on this no one want a voice recognition system that work 80 % of the time for a company like google or amazon hundred of thousand of hour of spoken audio record in real life situation be gold that s the single big thing that separate their world class speech recognition system from your hobby system the whole point of put google now and siri on every cell phone for free or sell $ 50 alexa unit that have no subscription fee be to get you to use they as much as possible every single thing you say into one of these system be record forever and use as training datum for future version of speech recognition algorithm that s the whole game don t believe I if you have an android phone with google now click here to listen to actual recording of yourself say every dumb thing you ve ever say into it so if you be look for a start up idea I wouldn t recommend try to build your own speech recognition system to compete with google instead figure out a way to get people to give you recording of themselves talk for hour the datum can be your product instead if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 7 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Adam Geitgey,5.8K,16,https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa?source=tag_archive---------6----------------,machine learning be fun part 5 language translation with deep learning and the magic of sequence,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 普通话 русский 한국어 tiếng việt or italiano we all know and love google translate the website that can instantly translate between 100 different human language as if by magic it be even available on our phone and smartwatche the technology behind google translate be call machine translation it have change the world by allow people to communicate when it wouldn t otherwise be possible but we all know that high school student have be use google translate to umm assist with their spanish homework for 15 year isn t this old news it turn out that over the past two year deep learning have totally rewrite our approach to machine translation deep learning researcher who know almost nothing about language translation be throw together relatively simple machine learning solution that be beat the good expert build language translation system in the world the technology behind this breakthrough be call sequence to sequence learn it s very powerful technique that be use to solve many kind problem after we see how it be use for translation we ll also learn how the exact same algorithm can be use to write ai chat bot and describe picture let s go so how do we program a computer to translate human language the simple approach be to replace every word in a sentence with the translate word in the target language here s a simple example of translate from spanish to english word by word this be easy to implement because all you need be a dictionary to look up each word s translation but the result be bad because it ignore grammar and context so the next thing you might do be start add language specific rule to improve the result for example you might translate common two word phrase as a single group and you might swap the order noun and adjective since they usually appear in reverse order in spanish from how they appear in english that work if we just keep add more rule until we can handle every part of grammar our program should be able to translate any sentence right this be how the early machine translation system work linguist come up with complicated rule and program they in one by one some of the smart linguist in the world labor for year during the cold war to create translation system as a way to interpret russian communication more easily unfortunately this only work for simple plainly structured document like weather report it didn t work reliably for real world document the problem be that human language doesn t follow a fix set of rule human language be full of special case regional variation and just flat out rule break the way we speak english more influence by who invade who hundred of year ago than it be by someone sit down and define grammar rule after the failure of rule base system new translation approach be develop use model base on probability and statistic instead of grammar rule build a statistic base translation system require lot of training datum where the exact same text be translate into at least two language this double translate text be call parallel corpora in the same way that the rosetta stone be use by scientist in the 1800 to figure out egyptian hieroglyph from greek computer can use parallel corpus to guess how to convert text from one language to another luckily there s lot of double translate text already sit around in strange place for example the european parliament translate their proceeding into 21 language so researcher often use that datum to help build translation system the fundamental difference with statistical translation system be that they don t try to generate one exact translation instead they generate thousand of possible translation and then they rank those translation by likely each be to be correct they estimate how correct something be by how similar it be to the training datum here s how it work first we break up our sentence into simple chunk that can each be easily translate next we will translate each of these chunk by find all the way human have translate those same chunk of word in our training datum it s important to note that we be not just look up these chunk in a simple translation dictionary instead we be see how actual people translate these same chunk of word in real world sentence this help we capture all of the different way they can be use in different context some of these possible translation be use more frequently than other base on how frequently each translation appear in our training datum we can give it a score for example it s much more common for someone to say quiero to mean I want than to mean I try so we can use how frequently quiero be translate to I want in our training datum to give that translation more weight than a less frequent translation next we will use every possible combination of these chunk to generate a bunch of possible sentence just from the chunk translation we list in step 2 we can already generate nearly 2 500 different variation of our sentence by combine the chunk in different way here be some example but in a real world system there will be even more possible chunk combination because we ll also try different ordering of word and different way of chunk the sentence now need to scan through all of these generate sentence to find the one that be that sound the most human to do this we compare each generate sentence to million of real sentence from book and news story write in english the more english text we can get our hand on the well take this possible translation it s likely that no one have ever write a sentence like this in english so it would not be very similar to any sentence in our datum set we ll give this possible translation a low probability score but look at this possible translation this sentence will be similar to something in our training set so it will get a high probability score after try all possible sentence we ll pick the sentence that have the most likely chunk translation while also be the most similar overall to real english sentence our final translation would be I want to go to the pretty beach not bad statistical machine translation system perform much well than rule base system if you give they enough training datum franz josef och improve on these idea and use they to build google translate in the early 2000 machine translation be finally available to the world in the early day it be surprising to everyone that the dumb approach to translate base on probability work well than rule base system design by linguist this lead to a somewhat mean say among researcher in the 80 statistical machine translation system work well but they be complicated to build and maintain every new pair of language you want to translate require expert to tweak and tune a new multi step translation pipeline because it be so much work to build these different pipeline trade off have to be make if you be ask google to translate georgian to telegu it have to internally translate it into english as an intermediate step because there s not enough georgain to telegu translation happen to justify invest heavily in that language pair and it might do that translation use a less advanced translation pipeline than if you have ask it for the more common choice of french to english wouldn t it be cool if we could have the computer do all that annoying development work for we the holy grail of machine translation be a black box system that learn how to translate by itself — just by look at training datum with statistical machine translation human be still need to build and tweak the multi step statistical model in 2014 kyunghyun cho s team make a breakthrough they find a way to apply deep learning to build this black box system their deep learning model take in a parallel corpus and and use it to learn how to translate between those two language without any human intervention two big idea make this possible — recurrent neural network and encoding by combine these two idea in a clever way we can build a self learn translation system we ve already talk about recurrent neural network in part 2 but let s quickly review a regular non recurrent neural network be a generic machine learning algorithm that take in a list of number and calculate a result base on previous training neural network can be use as a black box to solve lot of problem for example we can use a neural network to calculate the approximate value of a house base on attribute of that house but like most machine learning algorithm neural network be stateless you pass in a list of number and the neural network calculate a result if you pass in those same number again it will always calculate the same result it have no memory of past calculation in other word 2 + 2 always equal 4 a recurrent neural network or rnn for short be a slightly tweak version of a neural network where the previous state of the neural network be one of the input to the next calculation this mean that previous calculation change the result of future calculation why in the world would we want to do this shouldn t 2 + 2 always equal 4 no matter what we last calculate this trick allow neural network to learn pattern in a sequence of datum for example you can use it to predict the next most likely word in a sentence base on the first few word rnn be useful any time you want to learn pattern in datum because human language be just one big complicated pattern rnn be increasingly use in many area of natural language processing if you want to learn more about rnn you can read part 2 where we use one to generate a fake ernest hemingway book and then use another one to generate fake super mario brothers level the other idea we need to review be encoding we talk about encoding in part 4 as part of face recognition to explain encoding let s take a slight detour into how we can tell two different people apart with a computer when you be try to tell two face apart with a computer you collect different measurement from each face and use those measurement to compare face for example we might measure the size of each ear or the spacing between the eye and compare those measurement from two picture to see if they be the same person you re probably already familiar with this idea from watch any primetime detective show like csi the idea of turn a face into a list of measurement be an example of an encoding we be take raw datum a picture of a face and turn it into a list of measurement that represent it the encoding but like we see in part 4 we don t have to come up with a specific list of facial feature to measure ourselves instead we can use a neural network to generate measurement from a face the computer can do a well job than we in figure out which measurement be well able to differentiate two similar people this be our encoding it let we represent something very complicated a picture of a face with something simple 128 number now compare two different face be much easy because we only have to compare these 128 number for each face instead of compare full image guess what we can do the same thing with sentence we can come up with an encoding that represent every possible different sentence as a series of unique number to generate this encoding we ll feed the sentence into the rnn one word at time the final result after the last word be process will be the value that represent the entire sentence great so now we have a way to represent an entire sentence as a set of unique number we don t know what each number in the encoding mean but it doesn t really matter as long as each sentence be uniquely identify by it s own set of number we don t need to know exactly how those number be generate ok so we know how to use an rnn to encode a sentence into a set of unique number how do that help we here s where thing get really cool what if we take two rnn and hook they up end to end the first rnn could generate the encoding that represent a sentence then the second rnn could take that encoding and just do the same logic in reverse to decode the original sentence again of course be able to encode and then decode the original sentence again isn t very useful but what if and here s the big idea we could train the second rnn to decode the sentence into spanish instead of english we could use our parallel corpora training datum to train it to do that and just like that we have a generic way of convert a sequence of english word into an equivalent sequence of spanish word this be a powerful idea note that we gloss over some thing that be require to make this work with real world datum for example there s additional work you have to do to deal with different length of input and output sentence see bucketing and padding there s also issue with translate rare word correctly if you want to build your own language translation system there s a work demo include with tensorflow that will translate between english and french however this be not for the faint of heart or for those with limited budget this technology be still new and very resource intensive even if you have a fast computer with a high end video card it might take about a month of continuous processing time to train your own language translation system also sequence to sequence language translation technique be improve so rapidly that it s hard to keep up many recent improvement like add an attention mechanism or track context be significantly improve result but these development be so new that there aren t even wikipedia page for they yet if you want to do anything serious with sequence to sequence learn you ll need to keep with new development as they occur so what else can we do with sequence to sequence model about a year ago researcher at google show that you can use sequence to sequence model to build ai bot the idea be so simple that it s amazing it work at all first they capture chat log between google employee and google s tech support team then they train a sequence to sequence model where the employee s question be the input sentence and the tech support team s response be the translation of that sentence when a user interact with the bot they would translate each of the user s message with this system to get the bot s response the end result be a semi intelligent bot that could sometimes answer real tech support question here s part of a sample conversation between a user and the bot from their paper they also try build a chat bot base on million of movie subtitle the idea be to use conversation between movie character as a way to train a bot to talk like a human the input sentence be a line of dialog say by one character and the translation be what the next character say in response this produce really interesting result not only do the bot converse like a human but it display a small bit of intelligence this be only the beginning of the possibility we aren t limit to convert one sentence into another sentence it s also possible to make an image to sequence model that can turn an image into text a different team at google do this by replace the first rnn with a convolutional neural network like we learn about in part 3 this allow the input to be a picture instead of a sentence the rest work basically the same way and just like that we can turn picture into word as long as we have lot and lot of training datum andrej karpathy expand on these idea to build a system capable of describe image in great detail by process multiple region of an image separately this make it possible to build image search engine that be capable of find image that match oddly specific search query there s even researcher work on the reverse problem generate an entire picture base on just a text description just from these example you can start to imagine the possibility so far there have be sequence to sequence application in everything from speech recognition to computer vision I bet there will be a lot more over the next year if you want to learn more in depth about sequence to sequence model and translation here s some recommend resource if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 6 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Tal Perry,2.6K,17,https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02?source=tag_archive---------7----------------,deep learn the stock market tal perry medium,update 25 1 17 — take I a while but here be an ipython notebook with a rough implementation in the past few month I ve be fascinate with deep learning especially its application to language and text I ve spend the bulk of my career in financial technology mostly in algorithmic trading and alternative datum service you can see where this be go I write this to get my idea straight in my head while I ve become a deep learning enthusiast I don t have too many opportunity to brain dump an idea in most of its messy glory I think that a decent indication of a clear thought be the ability to articulate it to people not from the field I hope that I ve succeed in do that and that my articulation be also a pleasurable read why nlp be relevant to stock prediction in many nlp problem we end up take a sequence and encode it into a single fix size representation then decode that representation into another sequence for example we might tag entity in the text translate from english to french or convert audio frequency to text there be a torrent of work come out in these area and a lot of the result be achieve state of the art performance in my mind the big difference between the nlp and financial analysis be that language have some guarantee of structure it s just that the rule of the structure be vague market on the other hand don t come with a promise of a learnable structure that such a structure exist be the assumption that this project would prove or disprove rather it might prove or disprove if I can find that structure assume the structure be there the idea of summarize the current state of the market in the same way we encode the semantic of a paragraph seem plausible to I if that doesn t make sense yet keep read it will you shall know a word by the company it keep firth j r 1957 11 there be ton of literature on word embedding richard socher s lecture be a great place to start in short we can make a geometry of all the word in our language and that geometry capture the meaning of word and relationship between they you may have see the example of king man + woman = queen or something of the sort embedding be cool because they let we represent information in a condensed way the old way of represent word be hold a vector a big list of number that be as long as the number of word we know and set a 1 in a particular place if that be the current word we be look at that be not an efficient approach nor do it capture any meaning with embedding we can represent all of the word in a fix number of dimension 300 seem to be plenty 50 work great and then leverage their high dimensional geometry to understand they the picture below show an example an embedding be train on more or less the entire internet after a few day of intensive calculation each word be embed in some high dimensional space this space have a geometry concept like distance and so we can ask which word be close together the author inventor of that method make an example here be the word that be close to frog but we can embed more than just word we can do say stock market embedding market2vec the first word embed algorithm I hear about be word2vec I want to get the same effect for the market though I ll be use a different algorithm my input datum be a csv the first column be the date and there be 4 * 1000 column correspond to the high low open closing price of 1000 stock that be my input vector be 4000 dimensional which be too big so the first thing I m go to do be stuff it into a low dimensional space say 300 because I like the movie take something in 4000 dimension and stuff it into a 300 dimensional space my sound hard but its actually easy we just need to multiply matrix a matrix be a big excel spreadsheet that have number in every cell and no format problem imagine an excel table with 4000 column and 300 row and when we basically bang it against the vector a new vector come out that be only of size 300 I wish that s how they would have explain it in college the fanciness start here as we re go to set the number in our matrix at random and part of the deep learning be to update those number so that our excel spreadsheet change eventually this matrix spreadsheet I ll stick with matrix from now on will have number in it that bang our original 4000 dimensional vector into a concise 300 dimensional summary of itself we re go to get a little fancier here and apply what they call an activation function we re go to take a function and apply it to each number in the vector individually so that they all end up between 0 and 1 or 0 and infinity it depend why it make our vector more special and make our learning process able to understand more complicated thing how so what what I m expect to find be that that new embedding of the market price the vector into a small space capture all the essential information for the task at hand without waste time on the other stuff so I d expect they d capture correlation between other stock perhaps notice when a certain sector be decline or when the market be very hot I don t know what trait it will find but I assume they ll be useful now what let put aside our market vector for a moment and talk about language model andrej karpathy write the epic post the unreasonable effectiveness of recurrent neural network if I d summarize in the most liberal fashion the post boil down to and then as a punchline he generate a bunch of text that look like shakespeare and then he do it again with the linux source code and then again with a textbook on algebraic geometry so I ll get back to the mechanic of that magic box in a second but let I remind you that we want to predict the future market base on the past just like he predict the next word base on the previous one where karpathy use character we re go to use our market vector and feed they into the magic black box we haven t decide what we want it to predict yet but that be okay we win t be feed its output back into it either go deeply I want to point out that this be where we start to get into the deep part of deep learning so far we just have a single layer of learn that excel spreadsheet that condense the market now we re go to add a few more layer and stack they to make a deep something that s the deep in deep learning so karpathy show we some sample output from the linux source code this be stuff his black box write notice that it know how to open and close parenthesis and respect indentation convention ; the content of the function be properly indent and the multi line printk statement have an inner indentation that mean that this magic box understand long range dependency when it s indent within the print statement it know it s in a print statement and also remember that it s in a function or at least another indented scope that s nuts it s easy to gloss over that but an algorithm that have the ability to capture and remember long term dependency be super useful because we want to find long term dependency in the market inside the magical black box what s inside this magical black box it be a type of recurrent neural network rnn call an lstm an rnn be a deep learning algorithm that operate on sequence like sequence of character at every step it take a representation of the next character like the embedding we talk about before and operate on the representation with a matrix like we see before the thing be the rnn have some form of internal memory so it remember what it see previously it use that memory to decide how exactly it should operate on the next input use that memory the rnn can remember that it be inside of an intended scope and that be how we get properly nest output text a fancy version of an rnn be call a long short term memory lstm lstm have cleverly design memory that allow it to so an lstm can see a { and say to itself oh yeah that s important I should remember that and when it do it essentially remember an indication that it be in a nested scope once it see the corresponding } it can decide to forget the original opening brace and thus forget that it be in a nested scope we can have the lstm learn more abstract concept by stack a few of they on top of each other that would make we deep again now each output of the previous lstm become the input of the next lstm and each one go on to learn high abstraction of the datum come in in the example above and this be just illustrative speculation the first layer of lstms might learn that character separate by a space be word the next layer might learn word type like static void action_new_function the next layer might learn the concept of a function and its argument and so on it s hard to tell exactly what each layer be do though karpathy s blog have a really nice example of how he do visualize exactly that connect market2vec and lstms the studious reader will notice that karpathy use character as his input not embedding technically a one hot encoding of character but lar eidne actually use word embedding when he write auto generating clickbait with recurrent neural network the figure above show the network he use ignore the softmax part we ll get to it later for the moment check out how on the bottom he put in a sequence of word vector at the bottom and each one remember a word vector be a representation of a word in the form of a bunch of number like we see in the beginning of this post lar input a sequence of word vector and each one of they we re go to do the same thing with one difference instead of word vector we ll input marketvector those market vector we describe before to recap the marketvector should contain a summary of what s happen in the market at a give point in time by put a sequence of they through lstms I hope to capture the long term dynamic that have be happen in the market by stack together a few layer of lstms I hope to capture high level abstraction of the market s behavior what come out thus far we haven t talk at all about how the algorithm actually learn anything we just talk about all the clever transformation we ll do on the datum we ll defer that conversation to a few paragraph down but please keep this part in mind as it be the se up for the punch line that make everything else worthwhile in karpathy s example the output of the lstms be a vector that represent the next character in some abstract representation in eidne example the output of the lstms be a vector that represent what the next word will be in some abstract space the next step in both case be to change that abstract representation into a probability vector that be a list that say how likely each character or word respectively be likely to appear next that s the job of the softmax function once we have a list of likelihood we select the character or word that be the most likely to appear next in our case of predict the market we need to ask ourselves what exactly we want to market to predict some of the option that I think about be 1 and 2 be regression problem where we have to predict an actual number instead of the likelihood of a specific event like the letter n appear or the market go up those be fine but not what I want to do 3 and 4 be fairly similar they both ask to predict an event in technical jargon — a class label an event could be the letter n appear next or it could be move up 5 % while not go down more than 3 % in the last 10 minute the trade off between 3 and 4 be that 3 be much more common and thus easy to learn about while 4 be more valuable as not only be it an indicator of profit but also have some constraint on risk 5 be the one we ll continue with for this article because it s similar to 3 and 4 but have mechanic that be easy to follow the vix be sometimes call the fear index and it represent how volatile the stock in the s&p500 be it be derive by observe the imply volatility for specific option on each of the stock in the index sidenote — why predict the vix what make the vix an interesting target be that back to our lstm output and the softmax how do we use the formulation we see before to predict change in the vix a few minute in the future for each point in our dataset we ll look what happen to the vix 5 minute later if it go up by more than 1 % without go down more than 0 5 % during that time we ll output a 1 otherwise a 0 then we ll get a sequence that look like we want to take the vector that our lstms output and squish it so that it give we the probability of the next item in our sequence be a 1 the squishing happen in the softmax part of the diagram above technically since we only have 1 class now we use a sigmoid so before we get into how this thing learn let s recap what we ve do so far how do this thing learn now the fun part everything we do until now be call the forward pass we d do all of those step while we train the algorithm and also when we use it in production here we ll talk about the backward pass the part we do only while in training that make our algorithm learn so during train not only do we prepare year worth of historical datum we also prepare a sequence of prediction target that list of 0 and 1 that show if the vix move the way we want it to or not after each observation in our datum to learn we ll feed the market datum to our network and compare its output to what we calculate compare in our case will be simple subtraction that be we ll say that our model s error be or in english the square root of the square of the difference between what actually happen and what we predict here s the beauty that s a differential function that be we can tell by how much the error would have change if our prediction would have change a little our prediction be the outcome of a differentiable function the softmax the input to the softmax the lstms be all mathematical function that be differentiable now all of these function be full of parameter those big excel spreadsheet I talk about age ago so at this stage what we do be take the derivative of the error with respect to every one of the million of parameter in all of those excel spreadsheet we have in our model when we do that we can see how the error will change when we change each parameter so we ll change each parameter in a way that will reduce the error this procedure propagate all the way to the beginning of the model it tweak the way we embed the input into marketvector so that our marketvector represent the most significant information for our task it tweak when and what each lstm choose to remember so that their output be the most relevant to our task it tweak the abstraction our lstms learn so that they learn the most important abstraction for our task which in my opinion be amazing because we have all of this complexity and abstraction that we never have to specify anywhere it s all infer mathamagically from the specification of what we consider to be an error what s next now that I ve lay this out in writing and it still make sense to I I want so if you ve come this far please point out my error and share your input other thought here be some mostly more advanced thought about this project what other thing I might try and why it make sense to I that this may actually work liquidity and efficient use of capital generally the more liquid a particular market be the more efficient that be I think this be due to a chicken and egg cycle whereas a market become more liquid it be able to absorb more capital move in and out without that capital hurt itself as a market become more liquid and more capital can be use in it you ll find more sophisticated player move in this be because it be expensive to be sophisticated so you need to make return on a large chunk of capital in order to justify your operational cost a quick corollary be that in less liquid market the competition isn t quite as sophisticated and so the opportunity a system like this can bring may not have be trade away the point being be I to try and trade this I would try and trade it on less liquid segment of the market that be maybe the tase 100 instead of the s&p 500 this stuff be new the knowledge of these algorithm the framework to execute they and the computing power to train they be all new at least in the sense that they be available to the average joe such as myself I d assume that top player have figure this stuff out year ago and have have the capacity to execute for as long but as I mention in the above paragraph they be likely execute in liquid market that can support their size the next tier of market participant I assume have a slow velocity of technological assimilation and in that sense there be or soon will be a race to execute on this in as yet untapped market multiple time frame while I mention a single stream of input in the above I imagine that a more efficient way to train would be to train market vector at least on multiple time frame and feed they in at the inference stage that be my low time frame would be sample every 30 second and I d expect the network to learn dependency that stretch hour at most I don t know if they be relevant or not but I think there be pattern on multiple time frame and if the cost of computation can be bring low enough then it be worthwhile to incorporate they into the model I m still wrestle with how good to represent these on the computational graph and perhaps it be not mandatory to start with marketvector when use word vector in nlp we usually start with a pretraine model and continue adjust the embedding during training of our model in my case there be no pretraine market vector available nor be tehre a clear algorithm for train they my original consideration be to use an auto encoder like in this paper but end to end training be cool a more serious consideration be the success of sequence to sequence model in translation and speech recognition where a sequence be eventually encode as a single vector and then decode into a different representation like from speech to text or from english to french in that view the entire architecture I describe be essentially the encoder and I haven t really lay out a decoder but I want to achieve something specific with the first layer the one that take as input the 4000 dimensional vector and output a 300 dimensional one I want it to find correlation or relation between various stock and compose feature about they the alternative be to run each input through an lstm perhaps concatenate all of the output vector and consider that output of the encoder stage I think this will be inefficient as the interaction and correlation between instrument and their feature will be lose and thre will be 10x more computation require on the other hand such an architecture could naively be parallel across multiple gpu and host which be an advantage cnns recently there have be a spur of paper on character level machine translation this paper catch my eye as they manage to capture long range dependency with a convolutional layer rather than an rnn I haven t give it more than a brief read but I think that a modification where I d treat each stock as a channel and convolve over channel first like in rgb image would be another way to capture the market dynamic in the same way that they essentially encode semantic meaning from character from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of https lighttag io platform to annotate text for nlp google developer expert in ml I do deep learning on text for a living and for fun
Andrej Karpathy,9.2K,7,https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=tag_archive---------8----------------,yes you should understand backprop andrej karpathy medium,when we offer cs231n deep learning class at stanford we intentionally design the programming assignment to include explicit calculation involve in backpropagation on the low level the student have to implement the forward and the backward pass of each layer in raw numpy inevitably some student complain on the class message board this be seemingly a perfectly sensible appeal if you re never go to write backward pass once the class be over why practice write they be we just torture the student for our own amusement some easy answer could make argument along the line of it s worth know what s under the hood as an intellectual curiosity or perhaps you might want to improve on the core algorithm later but there be a much strong and practical argument which I want to devote a whole post to > the problem with backpropagation be that it be a leaky abstraction in other word it be easy to fall into the trap of abstract away the learning process — believe that you can simply stack arbitrary layer together and backprop will magically make they work on your datum so let look at a few explicit example where this be not the case in quite unintuitive way we re start off easy here at one point it be fashionable to use sigmoid or tanh non linearity in the fully connect layer the tricky part people might not realize until they think about the backward pass be that if you be sloppy with the weight initialization or datum preprocesse these non linearity can saturate and entirely stop learn — your training loss will be flat and refuse to go down for example a fully connect layer with sigmoid non linearity compute use raw numpy if your weight matrix w be initialize too large the output of the matrix multiply could have a very large range e g number between 400 and 400 which will make all output in the vector z almost binary either 1 or 0 but if that be the case z * 1 z which be local gradient of the sigmoid non linearity will in both case become zero vanish make the gradient for both x and w be zero the rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule another non obvious fun fact about sigmoid be that its local gradient z * 1 z achieve a maximum at 0 25 when z = 0 5 that mean that every time the gradient signal flow through a sigmoid gate its magnitude always diminish by one quarter or more if you re use basic sgd this would make the low layer of a network train much slow than the high one tldr if you re use sigmoid or tanh non linearity in your network and you understand backpropagation you should always be nervous about make sure that the initialization doesn t cause they to be fully saturate see a long explanation in this cs231n lecture video another fun non linearity be the relu which threshold neuron at zero from below the forward and backward pass for a fully connect layer that use relu would at the core include if you stare at this for a while you ll see that if a neuron get clamp to zero in the forward pass I e z=0 it doesn t fire then its weight will get zero gradient this can lead to what be call the dead relu problem where if a relu neuron be unfortunately initialize such that it never fire or if a neuron s weight ever get knock off with a large update during training into this regime then this neuron will remain permanently dead it s like permanent irrecoverable brain damage sometimes you can forward the entire training set through a train network and find that a large fraction e g 40 % of your neuron be zero the entire time tldr if you understand backpropagation and your network have relus you re always nervous about dead relus these be neuron that never turn on for any example in your entire training set and will remain permanently dead neuron can also die during training usually as a symptom of aggressive learning rate see a long explanation in cs231n lecture video vanilla rnn feature another good example of unintuitive effect of backpropagation I ll copy paste a slide from cs231n that have a simplified rnn that do not take any input x and only compute the recurrence on the hidden state equivalently the input x could always be zero this rnn be unrolled for t time step when you stare at what the backward pass be do you ll see that the gradient signal go backwards in time through all the hidden state be always be multiply by the same matrix the recurrence matrix whh intersperse with non linearity backprop what happen when you take one number a and start multiply it by some other number b I e a*b*b*b*b*b*b this sequence either go to zero if |b| < 1 or explode to infinity when |b|>1 the same thing happen in the backward pass of an rnn except b be a matrix and not just a number so we have to reason about its large eigenvalue instead tldr if you understand backpropagation and you re use rnn you be nervous about have to do gradient clipping or you prefer to use an lstm see a long explanation in this cs231n lecture video let look at one more — the one that actually inspire this post yesterday I be browse for a deep q learning implementation in tensorflow to see how other deal with compute the numpy equivalent of q a where a be an integer vector — turn out this trivial operation be not support in tf anyway I search dqn tensorflow click the first link and find the core code here be an excerpt if you re familiar with dqn you can see that there be the target_q_t which be just reward * gamma argmax_a q s a and then there be q_acte which be q s a of the action that be take the author here subtract the two into variable delta which they then want to minimize on line 295 with the l2 loss with tf reduce_mean tf square so far so good the problem be on line 291 the author be try to be robust to outlier so if the delta be too large they clip it with tf clip_by_value this be well intentione and look sensible from the perspective of the forward pass but it introduce a major bug if you think about the backward pass the clip_by_value function have a local gradient of zero outside of the range min_delta to max_delta so whenever the delta be above min max_delta the gradient become exactly zero during backprop the author be clip the raw q delta when they be likely try to clip the gradient for add robustness in that case the correct thing to do be to use the huber loss in place of tf square it s a bit gross in tensorflow because all we want to do be clip the gradient if it be above a threshold but since we can t meddle with the gradient directly we have to do it in this round about way of define the huber loss in torch this would be much more simple I submit an issue on the dqn repo and this be promptly fix backpropagation be a leaky abstraction ; it be a credit assignment scheme with non trivial consequence if you try to ignore how it work under the hood because tensorflow automagically make my network learn you will not be ready to wrestle with the danger it present and you will be much less effective at building and debug neural network the good news be that backpropagation be not that difficult to understand if present properly I have relatively strong feeling on this topic because it seem to I that 95 % of backpropagation material out there present it all wrong filling page with mechanical math instead I would recommend the cs231n lecture on backprop which emphasize intuition yay for shameless self advertising and if you can spare the time as a bonus work through the cs231n assignment which get you to write backprop manually and help you solidify your understanding that s it for now I hope you ll be much more suspicious of backpropagation go forward and think carefully through what the backward pass be do also I m aware that this post have unintentionally turn into several cs231n ad apology for that from a quick cheer to a stand ovation clap to show how much you enjoy this story director of ai at tesla previously research scientist at openai and phd student at stanford I like to train deep neural net on large dataset
Per Harald Borgen,4.8K,7,https://medium.com/learning-new-stuff/machine-learning-in-a-year-cdb0b0ebd29c?source=tag_archive---------9----------------,machine learning in a year learn new stuff medium,this be a follow up to an article I write last year machine learning in a week on how I kickstarte my way into machine learning ml by devote five day to the subject after this highly effective introduction I continue learn on my spare time and almost exactly one year later I do my first ml project at work which involve use various ml and natural language processing nlp technique to qualify sale lead at xeneta this feel like a blessing get pay to do something I normally do for fun it also rip I out of the delusion that only people with master degree or ph d s work with ml professionally in this post I want to share my journey as it might inspire other to do the same my interest in ml stem back to 2014 when I start read article about it on hacker news I simply find the idea of teach machine stuff by look at datum appeal at the time I wasn t even a professional developer but a hobby coder who d do a couple of small project so I begin watch the first few chapter of udacity s supervised learning course while also read all article I come across on the subject this give I a little bit of conceptual understanding though no practical skill I also didn t finish it as I rarely do with mooc s in january 2015 I join the founder and coder fac bootcamp in london in order to become a developer a few week in I want to learn how to actually code machine learning algorithm so I start a study group with a few of my peer every tuesday evening we d watch lecture from coursera s machine learning course it s a fantastic course and I learn a hell of a lot but it s tough for a beginner I have to watch the lecture over and over again before grasp the concept the octave code task be challenge as well especially if you don t know octave as a result of the difficulty one by one fall off the study group as the week pass eventually I fall off it myself as well in hindsight I should have start with a course that either use ml library for the code task — as oppose to build the algorithm from scratch — or at least use a programming language I know if I could go back in time I d choose udacity s intro to machine learning as it s easy and use python and scikit learn this way we would have get our hand dirty as soon as possible gain confidence and have more fun one of the last thing I do at fac be the ml week stunt my goal be to be able to apply machine learning to actual problem at the end of the week which I manage to do throughout the week I do the following it s by far the steep ml learn curve I ve ever experience go ahead and read the article if you want a more detailed overview after I finish fac in london and move back to norway I try to repeat the success from the ml week but for neural network instead this fail there be simply too many distraction to spend 10 hour of code and learning every day I have underestimate how important it be to be surround by peer at fac however I get start with neural net at least and slowly start to grasp the concept by july I manage to code my first net it s probably the crappi implementation ever create and I actually find it embarrassing to show off but it do the trick ; I prove to myself that I understand concept like backpropagation and gradient descent in the second half of the year my progression slow down as I start a new job the most important takeaway from this period be the leap from non vectorize to vectorize implementation of neural network which involve repeat linear algebra from university by the end of the year I write an article as a summary of how I learn this during the christmas vacation of 2015 I get a motivational boost again and decide try out kaggle so I spend quite some time experiment with various algorithm for their homesite quote conversion otto group product classification and bike sharing demand contest the main takeaway from this be the experience of iteratively improve the result by experiment with the algorithm and the datum I learn to trust my logic when do machine learning if tweak a parameter or engineer a new feature seem like a good idea logically it s quite likely that it actually will help back at work in january 2016 I want to continue in the flow I d get into during christmas so I ask my manager if I could spend some time learn stuff during my work hour as well which he happily approve have get a basic understanding of neural network at this point I want to move on to deep learn my first attempt be udacity s deep learning course which end up as a big disappointment the content of the video lecture be good but they be too short and shallow to I and the ipython notebook assignment end up be too frustrating as I spend most of my time debug code error which be the most effective way to kill motivation so after do that for a couple of session at work I simply give up to their defense I m a total noob when it come to ipython notebook so it might not be as bad for you as it be for I so it might be that I simply wasn t ready for the course luckily I then discover stanford s cs224d and decide to give it a shot it be a fantastic course and though it s difficult I never end up debug when do the problem set secondly they actually give you the solution code as well which I often look at when I m stuck so that I can work my way backwards to understand the step need to reach a solution though I ve haven t finish it yet it have significantly boost my knowledge in nlp and neural network so far however it s be tough really tough at one point I realize I need help from someone well than I so I come in touch with a ph d student who be willing to help I out for 40 usd per hour both with the problem set as well as the overall understanding this have be critical for I in order to move on as he have uncover a lot of black hole in my knowledge in addition to this xeneta also hire a data scientist recently he s get a masters degree in math so I often ask he for help when I m stick with various linear algebra an calculus task or ml in general so be sure to check out which resource you have internally in your company as well after do all this I finally feel ready to do a ml project at work it basically involve train an algorithm to qualify sale lead by read company description and have actually prove to be a big time saver for the sale guy use the tool check out out article I write about it below or head over to github to dive straight into the code get to this point have surely be a long journey but also a fast one ; when I start my machine learning in a week project I certainly didn t have any hope of actually use it professionally within a year but it s 100 percent possible and if I can do it so can anybody else thank for read my name be per I m a co founder of scrimba — a well way to teach and learn code if you ve read this far I d recommend you to check out this demo from a quick cheer to a stand ovation clap to show how much you enjoy this story co founder of scrimba the next generation platform for teaching and learn code https scrimba com a publication about improve your technical skill
Xiaohan Zeng,48K,13,https://medium.com/@XiaohanZeng/i-interviewed-at-five-top-companies-in-silicon-valley-in-five-days-and-luckily-got-five-job-offers-25178cf74e0f?source=tag_archive---------0----------------,I interview at five top company in silicon valley in five day and luckily get five job offer,in the five day from july 24th to 28th 2017 I interview at linkedin salesforce einstein google airbnb and facebook and get all five job offer it be a great experience and I feel fortunate that my effort pay off so I decide to write something about it I will discuss how I prepare review the interview process and share my impression about the five company I have be at groupon for almost three year it s my first job and I have be work with an amazing team and on awesome project we ve be build cool stuff make impact within the company publish paper and all that but I feel my learning rate be be anneal read slow down yet my mind be crave more also as a software engineer in chicago there be so many great company that all attract I in the bay area life be short and professional life short still after talk with my wife and gain her full support I decide to take action and make my first ever career change although I m interested in machine learning position the position at the five company be slightly different in the title and the interview process three be machine learning engineer linkedin google facebook one be data engineer salesforce and one be software engineer in general airbnb therefore I need to prepare for three different area code machine learning and system design since I also have a full time job it take I 2 3 month in total to prepare here be how I prepare for the three area while I agree that code interview might not be the good way to assess all your skill as a developer there be arguably no well way to tell if you be a good engineer in a short period of time imo it be the necessary evil to get you that job I mainly use leetcode and geeksforgeek for practice but hackerrank and lintcode be also good place I spend several week go over common datum structure and algorithm then focus on area I wasn t too familiar with and finally do some frequently see problem due to my time constraint I usually do two problem per day here be some thought this area be more closely related to the actual working experience many question can be ask during system design interview include but not limit to system architecture object orient design database schema design distribute system design scalability etc there be many resource online that can help you with the preparation for the most part I read article on system design interview architecture of large scale system and case study here be some resource that I find really helpful although system design interview can cover a lot of topic there be some general guideline for how to approach the problem with all that say the good way to practice for system design interview be to actually sit down and design a system I e your day to day work instead of do the minimal work go deeply into the tool framework and library you use for example if you use hbase rather than simply use the client to run some ddl and do some fetch try to understand its overall architecture such as the read write flow how hbase ensure strong consistency what minor major compaction do and where lru cache and bloom filter be use in the system you can even compare hbase with cassandra and see the similarity and difference in their design then when you be ask to design a distribute key value store you win t feel ambush many blog be also a great source of knowledge such as hacker noon and engineering blog of some company as well as the official documentation of open source project the most important thing be to keep your curiosity and modesty be a sponge that absorb everything it be submerge into machine learning interview can be divide into two aspect theory and product design unless you be have experience in machine learning research or do really well in your ml course it help to read some textbook classical one such as the element of statistical learning and pattern recognition and machine learning be great choice and if you be interested in specific area you can read more on those make sure you understand basic concept such as bias variance trade off overfitte gradient descent l1 l2 regularization baye theorem bag boost collaborative filtering dimension reduction etc familiarize yourself with common formula such as bayes theorem and the derivation of popular model such as logistic regression and svm try to implement simple model such as decision tree and k mean cluster if you put some model on your resume make sure you understand it thoroughly and can comment on its pro and con for ml product design understand the general process of build a ml product here s what I try to do here I want to emphasize again on the importance of remain curious and learning continuously try not to merely use the api for spark mllib or xgboost and call it do but try to understand why stochastic gradient descent be appropriate for distribute training or understand how xgboost differ from traditional gbdt e g what be special about its loss function why it need to compute the second order derivative etc I start by reply to hr s message on linkedin and ask for referral after a fail attempt at a rock star startup which I will touch upon later I prepare hard for several month and with help from my recruiter I schedule a full week of onsite in the bay area I fly in on sunday have five full day of interview with around 30 interviewer at some good tech company in the world and very luckily get job offer from all five of they all phone screening be standard the only difference be in the duration for some company like linkedin it s one hour while for facebook and airbnb it s 45 minute proficiency be the key here since you be under the time gun and usually you only get one chance you would have to very quickly recognize the type of problem and give a high level solution be sure to talk to the interviewer about your thinking and intention it might slow you down a little at the beginning but communication be more important than anything and it only help with the interview do not recite the solution as the interviewer would almost certainly see through it for machine learning position some company would ask ml question if you be interview for those make sure you brush up your ml skill as well to make well use of my time I schedule three phone screening in the same afternoon one hour apart from each the upside be that you might benefit from the hot hand and the downside be that the later one might be affect if the first one do not go well so I don t recommend it for everyone one good thing about interview with multiple company at the same time be that it give you certain advantage I be able to skip the second round phone screening with airbnb and salesforce because I get the onsite at linkedin and facebook after only one phone screening more surprisingly google even let I skip their phone screening entirely and schedule my onsite to fill the vacancy after learn I have four onsite come in the next week I know it be go to make it extremely tiring but hey nobody can refuse a google onsite invitation linkedin this be my first onsite and I interview at the sunnyvale location the office be very neat and people look very professional as always the session be one hour each code question be standard but the ml question can get a bit tough that say I get an email from my hr contain the preparation material which be very helpful and in the end I do not see anything that be too surprising I hear the rumor that linkedin have the good meal in the silicon valley and from what I see if it s not true it s not too far from the truth acquisition by microsoft seem to have lift the financial burden from linkedin and free they up to do really cool thing new feature such as video and professional advertisement be exciting as a company focus on professional development linkedin prioritize the growth of its own employee a lot of team such as ad relevance and feed ranking be expand so act quickly if you want to join salesforce einstein rock star project by rock star team the team be pretty new and feel very much like a startup the product be build on the scala stack so type safety be a real thing there great talk on the optimus prime library by matthew tovbin at scala days chicago 2017 and leah mcguire at spark summit west 2017 I interview at their palo alto office the team have a cohesive culture and work life balance be great there everybody be passionate about what they be do and really enjoy it with four session it be short compare to the other onsite interview but I wish I could have stay long after the interview matthew even take I for a walk to the hp garage google absolutely the industry leader and nothing to say about it that people don t already know but it s huge like really really huge it take I 20 minute to ride a bicycle to meet my friend there also line for food can be too long forever a great place for developer I interview at one of the many building on the mountain view campus and I don t know which one it be because it s huge my interviewer all look very smart and once they start talk they be even smart it would be very enjoyable to work with these people one thing that I feel special about google s interview be that the analysis of algorithm complexity be really important make sure you really understand what big o notation mean airbnb fast expand unicorn with a unique culture and arguably the most beautiful office in the silicon valley new product such as experience and restaurant reservation high end niche market and expansion into china all contribute to a positive prospect perfect choice if you be risk tolerant and want a fast grow pre ipo experience airbnb s code interview be a bit unique because you ll be code in an ide instead of whiteboarde so your code need to compile and give the right answer some problem can get really hard and they ve get the one of a kind cross functional interview this be how airbnb take culture seriously and be technically excellent doesn t guarantee a job offer for I the two cross functional be really enjoyable I have casual conversation with the interviewer and we all feel happy at the end of the session overall I think airbnb s onsite be the hard due to the difficulty of the problem long duration and unique cross functional interview if you be interested be sure to understand their culture and core value facebook another giant that be still grow fast and small and fast pace compare to google with its product line dominate the social network market and big investment in ai and vr I can only see more growth potential for facebook in the future with star like yann lecun and yangqe jia it s the perfect place if you be interested in machine learning I interview at build 20 the one with the rooftop garden and ocean view and also where zuckerberg s office be locate I m not sure if the interviewer get instruction but I didn t get clear sign whether my solution be correct although I believe they be by noon the prior four day start to take its toll and I be have a headache I persist through the afternoon session but feel I didn t do well at all I be a bit surprised to learn that I be get an offer from they as well generally I feel people there believe the company s vision and be proud of what they be build be a company with half a trillion market cap and grow facebook be a perfect place to grow your career at this be a big topic that I win t cover in this post but I find this article to be very helpful some thing that I do think be important all success start with failure include interview before I start interview for these company I fail my interview at databrick in may back in april xiangrui contact I via linkedin ask I if I be interested in a position on the spark mllib team I be extremely thrilled because 1 I use spark and love scala 2 databrick engineer be top notch and 3 spark be revolutionize the whole big data world it be an opportunity I couldn t miss so I start interview after a few day the bar be very high and the process be quite long include one pre screening questionnaire one phone screen one code assignment and one full onsite I manage to get the onsite invitation and visit their office in downtown san francisco where treasure island can be see my interviewer be incredibly intelligent yet equally modest during the interview I often feel be push to the limit it be fine until one disastrous session where I totally mess up due to insufficient skill and preparation and it end up a fiasco xiangrui be very kind and walk I to where I want to go after the interview be over and I really enjoy talk to he I get the rejection several day later it be expect but I feel frustrated for a few day nonetheless although I miss the opportunity to work there I wholeheartedly wish they will continue to make great impact and achievement from the first interview in may to finally accept the job offer in late september my first career change be long and not easy it be difficult for I to prepare because I need to keep do well at my current job for several week I be on a regular schedule of prepare for the interview till 1 am get up at 8 30am the next day and fully devote myself to another day at work interview at five company in five day be also highly stressful and risky and I don t recommend do it unless you have a very tight schedule but it do give you a good advantage during negotiation should you secure multiple offer I d like to thank all my recruiter who patiently walk I through the process the people who spend their precious time talk to I and all the company that give I the opportunity to interview and extend I offer lastly but most importantly I want to thank my family for their love and support — my parent for watch I take the first and every step my dear wife for everything she have do for I and my daughter for her warm smile thank for read through this long post you can find I on linkedin or twitter xiaohan zeng 10 22 17 ps since the publication of this post it have unexpectedly receive some attention I would like to thank everybody for the congratulation and share and apologize for not be able to respond to each of they this post have be translate into some other language it have be reposte in tech in asia break into startup invite I to a live video streaming together with sophia ciocca covershr do a short qna with I from a quick cheer to a stand ovation clap to show how much you enjoy this story critical mind & romantic heart
Gil Fewster,3.3K,5,https://medium.freecodecamp.org/the-mind-blowing-ai-announcement-from-google-that-you-probably-missed-2ffd31334805?source=tag_archive---------1----------------,the mind blow ai announcement from google that you probably miss,disclaimer I m not an expert in neural network or machine learning since originally write this article many people with far more expertise in these field than myself have indicate that while impressive what google have achieve be evolutionary not revolutionary in the very least it s fair to say that I m guilty of anthropomorphise in part of the text I ve leave the article s content unchanged because I think it s interesting to compare the gut reaction I have with the subsequent comment of expert in the field I strongly encourage reader to browse the comment after read the article for some perspective more sober and inform than my own in the closing week of 2016 google publish an article that quietly sail under most people s radar which be a shame because it may just be the most astonishing article about machine learning that I read last year don t feel bad if you miss it not only be the article compete with the pre christmas rush that most of we be navigate — it be also tuck away on google s research blog beneath the geektastic headline zero shot translation with google s multilingual neural machine translation system this doesn t exactly scream must read do it especially when you ve get project to wind up gift to buy and family feud to be resolve — all while the advent calendar relentlessly count down the day until christmas like some kind of chocolate fill yuletide doomsday clock luckily I m here to bring you up to speed here s the deal up until september of last year google translate use phrase base translation it basically do the same thing you and I do when we look up key word and phrase in our lonely planet language guide it s effective enough and blisteringly fast compare to awkwardly thumb your way through a bunch of page look for the french equivalent of please bring I all of your cheese and don t stop until I fall over but it lack nuance phrase base translation be a blunt instrument it do the job well enough to get by but map roughly equivalent word and phrase without an understanding of linguistic structure can only produce crude result this approach be also limit by the extent of an available vocabulary phrase base translation have no capacity to make educated guess at word it doesn t recognize and can t learn from new input all that change in september when google give their translation tool a new engine the google neural machine translation system gnmt this new engine come fully load with all the hot 2016 buzzword like neural network and machine learn the short version be that google translate get smart it develop the ability to learn from the people who use it it learn how to make educated guess about the content tone and meaning of phrase base on the context of other word and phrase around they and — here s the bit that should make your brain explode — it get creative google translate invent its own language to help it translate more effectively what s more nobody tell it to it didn t develop a language or interlingua as google call it because it be code to it develop a new language because the software determine over time that this be the most efficient way to solve the problem of translation stop and think about that for a moment let it sink in a neural computing system design to translate content from one human language into another develop its own internal language to make the task more efficient without be tell to do so in a matter of week I ve add a correction retraction of this paragraph in the note to understand what s go on we need to understand what zero shot translation capability be here s google s mike schuster nikhil thorat and melvin johnson from the original blog post here you can see an advantage of google s new neural machine over the old phrase base approach the gmnt be able to learn how to translate between two language without be explicitly teach this wouldn t be possible in a phrase base model where translation be dependent upon an explicit dictionary to map word and phrase between each pair of language be translate and this lead the google engineer onto that truly astonishing discovery of creation so there you have it in the last week of 2016 as journos around the world start pen their be this the bad year in living memory thinkpiece google engineer be quietly document a genuinely astonishing breakthrough in software engineering and linguistic I just think maybe you d want to know ok to really understand what s go on we probably need multiple computer science and linguistic degree I m just barely scrape the surface here if you ve get time to get a few degree or if you ve already get they please drop I a line and explain it all I to slowly update 1 in my excitement it s fair to say that I ve exaggerate the idea of this as an intelligent system — at least so far as we would think about human intelligence and decision making make sure you read chris mcdonald s comment after the article for a more sober perspective update 2 nafrondel s excellent detailed reply be also a must read for an expert explanation of how neural network function from a quick cheer to a stand ovation clap to show how much you enjoy this story a tinkerer our community publish story worth read on development design and datum science
David Venturi,10.6K,20,https://medium.freecodecamp.org/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0?source=tag_archive---------2----------------,every single machine learning course on the internet rank by your review,a year and a half ago I drop out of one of the good computer science program in canada I start create my own data science master s program use online resource I realize that I could learn everything I need through edx coursera and udacity instead and I could learn it fast more efficiently and for a fraction of the cost I m almost finish now I ve take many data science relate course and audit portion of many more I know the option out there and what skill be need for learner prepare for a data analyst or data scientist role so I start create a review drive guide that recommend the good course for each subject within data science for the first guide in the series I recommend a few code class for the beginner datum scientist then it be statistic and probability class then introduction to data science also data visualization for this guide I spend a dozen hour try to identify every online machine learning course offer as of may 2017 extract key bit of information from their syllabus and review and compile their rating my end goal be to identify the three good course available and present they to you below for this task I turn to none other than the open source class central community and its database of thousand of course rating and review since 2011 class central founder dhawal shah have keep a close eye on online course than arguably anyone else in the world dhawal personally help I assemble this list of resource each course must fit three criterion we believe we cover every notable course that fit the above criterion since there be seemingly hundred of course on udemy we choose to consider the most reviewed and high rate one only there s always a chance that we miss something though so please let we know in the comment section if we leave a good course out we compile average rating and number of review from class central and other review site to calculate a weighted average rating for each course we read text review and use this feedback to supplement the numerical rating we make subjective syllabus judgment call base on three factor a popular definition originate from arthur samuel in 1959 machine learning be a subfield of computer science that give computer the ability to learn without be explicitly program in practice this mean develop computer program that can make prediction base on datum just as human can learn from experience so can computer where datum = experience a machine learn workflow be the process require for carry out a machine learning project though individual project can differ most workflow share several common task problem evaluation datum exploration datum preprocesse model training testing deployment etc below you ll find helpful visualization of these core step the ideal course introduce the entire process and provide interactive example assignment and or quiz where student can perform each task themselves first off let s define deep learning here be a succinct description as would be expect portion of some of the machine learn course contain deep learning content I choose not to include deep learning only course however if you be interested in deep learning specifically we ve get you cover with the follow article my top three recommendation from that list would be several course list below ask student to have prior programming calculus linear algebra and statistic experience these prerequisite be understandable give that machine learning be an advanced discipline miss a few subject good news some of this experience can be acquire through our recommendation in the first two article program statistic of this data science career guide several top rank course below also provide gentle calculus and linear algebra refresher and highlight the aspect most relevant to machine learning for those less familiar stanford university s machine learning on coursera be the clear current winner in term of rating review and syllabus fit teach by the famous andrew ng google brain founder and former chief scientist at baidu this be the class that spark the founding of coursera it have a 4 7 star weight average rating over 422 review release in 2011 it cover all aspect of the machine learn workflow though it have a small scope than the original stanford class upon which it be base it still manage to cover a large number of technique and algorithm the estimate timeline be eleven week with two week dedicate to neural network and deep learning free and pay option be available ng be a dynamic yet gentle instructor with a palpable experience he inspire confidence especially when share practical implementation tip and warning about common pitfall a linear algebra refresher be provide and ng highlight the aspect of calculus most relevant to machine learning evaluation be automatic and be do via multiple choice quiz that follow each lesson and programming assignment the assignment there be eight of they can be complete in matlab or octave which be an open source version of matlab ng explain his language choice though python and r be likely more compelling choice in 2017 with the increase popularity of those language reviewer note that that shouldn t stop you from take the course a few prominent reviewer note the follow columbia university s machine learning be a relatively new offering that be part of their artificial intelligence micromaster on edx though it be new and doesn t have a large number of review the one that it do have be exceptionally strong professor john paisley be note as brilliant clear and clever it have a 4 8 star weight average rating over 10 review the course also cover all aspect of the machine learn workflow and more algorithm than the above stanford offer columbia s be a more advanced introduction with reviewer note that student should be comfortable with the recommend prerequisite calculus linear algebra statistic probability and code quiz 11 programming assignment 4 and a final exam be the mode of evaluation student can use either python octave or matlab to complete the assignment the course s total estimate timeline be eight to ten hour per week over twelve week it be free with a verify certificate available for purchase below be a few of the aforementioned sparkling review machine learn a ztm on udemy be an impressively detailed offering that provide instruction in both python and r which be rare and can t be say for any of the other top course it have a 4 5 star weight average rating over 8 119 review which make it the most review course of the one consider it cover the entire machine learn workflow and an almost ridiculous in a good way number of algorithm through 40 5 hour of on demand video the course take a more apply approach and be light math wise than the above two course each section start with an intuition video from eremenko that summarize the underlie theory of the concept be teach de ponteve then walk through implementation with separate video for both python and r as a bonus the course include python and r code template for student to download and use on their own project there be quiz and homework challenge though these aren t the strong point of the course eremenko and the superdatascience team be revere for their ability to make the complex simple also the prerequisite list be just some high school mathematic so this course might be a well option for those daunt by the stanford and columbia offering a few prominent reviewer note the follow our # 1 pick have a weight average rating of 4 7 out of 5 star over 422 review let s look at the other alternative sort by descend rating a reminder that deep learning only course be not include in this guide — you can find those here the analytic edge massachusetts institute of technology edx more focused on analytic in general though it do cover several machine learning topic use r strong narrative that leverage familiar real world example challenge ten to fifteen hour per week over twelve week free with a verify certificate available for purchase it have a 4 9 star weight average rating over 214 review python for datum science and machine learning bootcamp jose portilla udemy have large chunk of machine learning content but cover the whole data science process more of a very detailed intro to python amazing course though not ideal for the scope of this guide 21 5 hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 6 star weight average rating over 3316 review data science and machine learning bootcamp with r jose portilla udemy the comment for portilla s above course apply here as well except for r 17 5 hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 6 star weight average rating over 1317 review machine learning series lazy programmer inc udemy teach by a data scientist big data engineer full stack software engineer with an impressive resume lazy programmer currently have a series of 16 machine learning focus course on udemy in total the course have 5000 + rating and almost all of they have 4 6 star a useful course ordering be provide in each individual course s description use python cost varie depend on udemy discount which be frequent machine learn georgia tech udacity a compilation of what be three separate course supervise unsupervised and reinforcement learning part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms bite sized video as be udacity s style friendly professor estimate timeline of four month free it have a 4 56 star weight average rating over 9 review implement predictive analytic with spark in azure hdinsight microsoft edx introduce the core concept of machine learning and a variety of algorithms leverage several big data friendly tool include apache spark scala and hadoop use both python and r four hour per week over six week free with a verify certificate available for purchase it have a 4 5 star weight average rating over 6 review data science and machine learning with python — hand on frank kane udemy use python kane have nine year of experience at amazon and imdb nine hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 5 star weight average rating over 4139 review scala and spark for big datum and machine learning jose portilla udemy big datum focus specifically on implementation in scala and spark ten hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 5 star weight average rating over 607 review machine learning engineer nanodegree udacity udacity s flagship machine learning program which feature a good in class project review system and career support the program be a compilation of several individual udacity course which be free co create by kaggle estimate timeline of six month currently cost $ 199 usd per month with a 50 % tuition refund available for those who graduate within 12 month it have a 4 5 star weight average rating over 2 review learn from datum introductory machine learning california institute of technology edx enrollment be currently close on edx but be also available via caltech s independent platform see below it have a 4 49 star weight average rating over 42 review learn from datum introductory machine learn yaser abu mostafa california institute of technology a real caltech course not a water down version review note it be excellent for understand machine learning theory the professor yaser abu mostafa be popular among student and also write the textbook upon which this course be base video be tape lecture with lecture slide picture in picture upload to youtube homework assignment be pdf file the course experience for online student isn t as polished as the top three recommendation it have a 4 43 star weight average rating over 7 review mining massive dataset stanford university machine learn with a focus on big datum introduce modern distribute file system and mapreduce ten hour per week over seven week free it have a 4 4 star weight average rating over 30 review aws machine learn a complete guide with python chandra lingam udemy a unique focus on cloud base machine learning and specifically amazon web service use python nine hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 4 star weight average rating over 62 review introduction to machine learning & face detection in python holczer balazs udemy use python eight hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 4 star weight average rating over 162 review statlearne statistical learning stanford university base on the excellent textbook an introduction to statistical learning with application in r and teach by the professor who write it reviewer note that the mooc isn t as good as the book cite thin exercise and mediocre video five hour per week over nine week free it have a 4 35 star weight average rating over 84 review machine learning specialization university of washington coursera great course but last two class include the capstone project be cancel reviewer note that this series be more digestable read easy for those without strong technical background than other top machine learn course e g stanford s or caltech s be aware that the series be incomplete with recommend system deep learning and a summary miss free and pay option available it have a 4 31 star weight average rating over 80 review from 0 to 1 machine learn nlp & python cut to the chase loony corn udemy a down to earth shy but confident take on machine learning technique teach by four person team with decade of industry experience together use python cost varie depend on udemy discount which be frequent it have a 4 2 star weight average rating over 494 review principle of machine learn microsoft edx use r python and microsoft azure machine learn part of the microsoft professional program certificate in data science three to four hour per week over six week free with a verify certificate available for purchase it have a 4 09 star weight average rating over 11 review big data statistical inference and machine learning queensland university of technology futurelearn a nice brief exploratory machine learning course with a focus on big datum cover a few tool like r h2o flow and weka only three week in duration at a recommend two hour per week but one reviewer note that six hour per week would be more appropriate free and pay option available it have a 4 star weight average rating over 4 review genomic datum science and cluster bioinformatics v university of california san diego coursera for those interested in the intersection of computer science and biology and how it represent an important frontier in modern science focus on clustering and dimensionality reduction part of ucsd s bioinformatics specialization free and pay option available it have a 4 star weight average rating over 3 review intro to machine learn udacity prioritize topic breadth and practical tool in python over depth and theory the instructor sebastian thrun and katie malone make this class so fun consist of bite sized video and quiz follow by a mini project for each lesson currently part of udacity s data analyst nanodegree estimate timeline of ten week free it have a 3 95 star weight average rating over 19 review machine learning for data analysis wesleyan university coursera a brief intro machine learning and a few select algorithms cover decision tree random forest lasso regression and k mean cluster part of wesleyan s datum analysis and interpretation specialization estimate timeline of four week free and pay option available it have a 3 6 star weight average rating over 5 review program with python for data science microsoft edx produce by microsoft in partnership with code dojo use python eight hour per week over six week free and pay option available it have a 3 46 star weight average rating over 37 review machine learning for trade georgia tech udacity focus on apply probabilistic machine learning approach to trading decision use python part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree om estimate timeline of four month free it have a 3 29 star weight average rating over 14 review practical machine learning johns hopkins university coursera a brief practical introduction to a number of machine learning algorithm several one two star review express a variety of concern part of jhu s data science specialization four to nine hour per week over four week free and pay option available it have a 3 11 star weight average rating over 37 review machine learning for datum science and analytics columbia university edx introduce a wide range of machine learn topic some passionate negative review with concern include content choice a lack of programming assignment and uninspire presentation seven to ten hour per week over five week free with a verify certificate available for purchase it have a 2 74 star weight average rating over 36 review recommender system specialization university of minnesota coursera strong focus one specific type of machine learning — recommender system a four course specialization plus a capstone project which be a case study teach use lenskit an open source toolkit for recommender system free and pay option available it have a 2 star weight average rating over 2 review machine learning with big datum university of california san diego coursera terrible review that highlight poor instruction and evaluation some note it take they mere hour to complete the whole course part of ucsd s big datum specialization free and pay option available it have a 1 86 star weight average rating over 14 review practical predictive analytic model and method university of washington coursera a brief intro to core machine learning concept one reviewer note that there be a lack of quiz and that the assignment be not challenge part of uw s data science at scale specialization six to eight hour per week over four week free and pay option available it have a 1 75 star weight average rating over 4 review the follow course have one or no review as of may 2017 machine learn for musician and artist goldsmith university of london kadenze unique student learn algorithms software tool and machine learn good practice to make sense of human gesture musical audio and other real time datum seven session in length audit free and premium $ 10 usd per month option available it have one 5 star review apply machine learning in python university of michigan coursera teach use python and the scikit learn toolkit part of the apply data science with python specialization schedule to start may 29th free and pay option available apply machine learn microsoft edx teach use various tool include python r and microsoft azure machine learning note microsoft produce the course include hand on lab to reinforce the lecture content three to four hour per week over six week free with a verify certificate available for purchase machine learn with python big datum university teach use python target towards beginner estimate completion time of four hour big datum university be affiliate with ibm free machine learning with apache systemml big data university teach use apache systemml which be a declarative style language design for large scale machine learning estimate completion time of eight hour big datum university be affiliate with ibm free machine learning for data science university of california san diego edx doesn t launch until january 2018 programming example and assignment be in python use jupyter notebook eight hour per week over ten week free with a verify certificate available for purchase introduction to analytic model georgia tech edx the course advertise r as its primary programming tool five to ten hour per week over ten week free with a verify certificate available for purchase predictive analytic gain insight from big datum queensland university of technology futurelearn brief overview of a few algorithm use hewlett packard enterprise s vertica analytic platform as an apply tool start date to be announce two hour per week over four week free with a certificate of achievement available for purchase introducción al machine learning universita telefónica miríada x teach in spanish an introduction to machine learning that cover supervised and unsupervised learn a total of twenty estimate hour over four week machine learning path step dataquest teach in python use dataquest s interactive in browser platform multiple guide project and a plus project where you build your own machine learning system use your own datum subscription require the follow six course be offer by datacamp datacamp s hybrid teaching style leverage video and text base instruction with lot of example through an in browser code editor a subscription be require for full access to each course introduction to machine learn datacamp cover classification regression and clustering algorithm use r fifteen video and 81 exercise with an estimate timeline of six hour supervised learning with scikit learn datacamp use python and scikit learn cover classification and regression algorithms seventeen video and 54 exercise with an estimate timeline of four hour unsupervised learning in r datacamp provide a basic introduction to clustering and dimensionality reduction in r sixteen video and 49 exercise with an estimate timeline of four hour machine learn toolbox datacamp teach the big idea in machine learning use r 24 video and 88 exercise with an estimate timeline of four hour machine learn with the expert school budget datacamp a case study from a machine learning competition on drivendata involve build a model to automatically classify item in a school s budget datacamp s supervised learning with scikit learn be a prerequisite fifteen video and 51 exercise with an estimate timeline of four hour unsupervised learning in python datacamp cover a variety of unsupervised learning algorithm use python scikit learn and scipy the course end with student build a recommend system to recommend popular musical artist thirteen video and 52 exercise with an estimate timeline of four hour machine learning tom mitchell carnegie mellon university carnegie mellon s graduate introductory machine learning course a prerequisite to their second graduate level course statistical machine learning tape university lecture with practice problem homework assignment and a midterm all with solution post online a 2011 version of the course also exist cmu be one of the good graduate school for study machine learning and have a whole department dedicate to ml free statistical machine learn larry wasserman carnegie mellon university likely the most advanced course in this guide a follow up to carnegie mellon s machine learning course tape university lecture with practice problem homework assignment and a midterm all with solution post online free undergraduate machine learn nando de freitas university of british columbia an undergraduate machine learning course lecture be film and put on youtube with the slide post on the course website the course assignment be post as well no solution though de freita be now a full time professor at the university of oxford and receive praise for his teaching ability in various forum graduate version available see below machine learn nando de freitas university of british columbia a graduate machine learning course the comment in de freitas undergraduate course above apply here as well this be the fifth of a six piece series that cover the good online course for launch yourself into the data science field we cover programming in the first article statistic and probability in the second article intro to data science in the third article and datum visualization in the fourth the final piece will be a summary of those article plus the good online course for other key topic such as datum wrangle database and even software engineering if you re look for a complete list of data science online course you can find they on class central s data science and big data subject page if you enjoy read this check out some of class central s other piece if you have suggestion for course I miss let I know in the response if you find this helpful click the 💚 so more people will see it here on medium this be a condensed version of my original article publish on class central where I ve include detailed course syllabus from a quick cheer to a stand ovation clap to show how much you enjoy this story curriculum lead project @ datacamp I create my own data science master s program our community publish story worth read on development design and datum science
Vishal Maini,32K,10,https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12?source=tag_archive---------3----------------,a beginner s guide to ai ml 🤖 👶 machine learn for human medium,part 1 why machine learning matter the big picture of artificial intelligence and machine learning — past present and future part 2 1 supervised learning learn with an answer key introduce linear regression loss function overfitte and gradient descent part 2 2 supervise learn ii two method of classification logistic regression and svms part 2 3 supervise learn iii non parametric learner k near neighbor decision tree random forest introduce cross validation hyperparameter tuning and ensemble model part 3 unsupervised learning cluster k mean hierarchical dimensionality reduction principal component analysis pca singular value decomposition svd part 4 neural network & deep learning why where and how deep learning work draw inspiration from the brain convolutional neural network cnns recurrent neural network rnn real world application part 5 reinforcement learning exploration and exploitation markov decision process q learning policy learning and deep reinforcement learn the value learn problem appendix the good machine learning resource a curate list of resource for create your machine learning curriculum this guide be intend to be accessible to anyone basic concept in probability statistic program linear algebra and calculus will be discuss but it isn t necessary to have prior knowledge of they to gain value from this series artificial intelligence will shape our future more powerfully than any other innovation this century anyone who do not understand it will soon find themselves feel leave behind wake up in a world full of technology that feel more and more like magic the rate of acceleration be already astounding after a couple of ai winter and period of false hope over the past four decade rapid advance in datum storage and computer processing power have dramatically change the game in recent year in 2015 google train a conversational agent ai that could not only convincingly interact with human as a tech support helpdesk but also discuss morality express opinion and answer general fact base question the same year deepmind develop an agent that surpass human level performance at 49 atari game receive only the pixel and game score as input soon after in 2016 deepmind obsolete their own achievement by release a new state of the art gameplay method call a3c meanwhile alphago defeat one of the good human player at go — an extraordinary achievement in a game dominate by human for two decade after machine first conquer chess many master could not fathom how it would be possible for a machine to grasp the full nuance and complexity of this ancient chinese war strategy game with its 10170 possible board position there be only 1080atoms in the universe in march 2017 openai create agent that invent their own language to cooperate and more effectively achieve their goal soon after facebook reportedly successfully train agent to negotiate and even lie just a few day ago as of this writing on august 11 2017 openai reach yet another incredible milestone by defeat the world s top professional in 1v1 match of the online multiplayer game dota 2 much of our day to day technology be power by artificial intelligence point your camera at the menu during your next trip to taiwan and the restaurant s selection will magically appear in english via the google translate app today ai be use to design evidence base treatment plan for cancer patient instantly analyze result from medical test to escalate to the appropriate specialist immediately and conduct scientific research for drug discovery in everyday life it s increasingly commonplace to discover machine in role traditionally occupy by human really don t be surprised if a little housekeeping delivery bot show up instead of a human next time you call the hotel desk to send up some toothpaste in this series we ll explore the core machine learning concept behind these technology by the end you should be able to describe how they work at a conceptual level and be equip with the tool to start build similar application yourself artificial intelligence be the study of agent that perceive the world around they form plan and make decision to achieve their goal its foundation include mathematics logic philosophy probability linguistic neuroscience and decision theory many field fall under the umbrella of ai such as computer vision robotic machine learning and natural language processing machine learning be a subfield of artificial intelligence its goal be to enable computer to learn on their own a machine s learn algorithm enable it to identify pattern in observed datum build model that explain the world and predict thing without have explicit pre program rule and model the technology discuss above be example of artificial narrow intelligence ani which can effectively perform a narrowly define task meanwhile we re continue to make foundational advance towards human level artificial general intelligence agi also know as strong ai the definition of an agi be an artificial intelligence that can successfully perform any intellectual task that a human being can include learn planning and decision making under uncertainty communicate in natural language make joke manipulate people trade stock or reprogramme itself and this last one be a big deal once we create an ai that can improve itself it will unlock a cycle of recursive self improvement that could lead to an intelligence explosion over some unknown time period range from many decade to a single day you may have hear this point refer to as the singularity the term be borrow from the gravitational singularity that occur at the center of a black hole an infinitely dense one dimensional point where the law of physics as we understand they start to break down a recent report by the future of humanity institute survey a panel of ai researcher on timeline for agi and find that researcher believe there be a 50 % chance of ai outperform human in all task in 45 year grace et al 2017 we ve personally speak with a number of sane and reasonable ai practitioner who predict much long timeline the upper limit be never and other whose timeline be alarmingly short — as little as a few year the advent of great than human level artificial superintelligence asi could be one of the good or bad thing to happen to our specie it carry with it the immense challenge of specify what ais will want in a way that be friendly to human while it s impossible to say what the future hold one thing be certain 2017 be a good time to start understand how machine think to go beyond the abstraction of a philosopher in an armchair and intelligently shape our roadmap and policy with respect to ai we must engage with the detail of how machine see the world — what they want their potential bias and failure mode their temperamental quirk — just as we study psychology and neuroscience to understand how human learn decide act and feel machine learning be at the core of our journey towards artificial general intelligence and in the meantime it will change every industry and have a massive impact on our day to day live that s why we believe it s worth understand machine learning at least at a conceptual level — and we design this series to be the good place to start you don t necessarily need to read the series cover to cover to get value out of it here be three suggestion on how to approach it depend on your interest and how much time you have vishal most recently lead growth at upstart a lending platform that utilize machine learning to price credit automate the borrowing process and acquire user he spend his time think about startup apply cognitive science moral philosophy and the ethic of artificial intelligence samer be a master s student in computer science and engineering at ucsd and co founder of conigo lab prior to grad school he found tablescribe a business intelligence tool for smb and spend two year advise fortune 100 company at mckinsey samer previously study computer science and ethic politic and economic at yale most of this series be write during a 10 day trip to the united kingdom in a frantic blur of train plane cafe pub and wherever else we could find a dry place to sit our aim be to solidify our own understanding of artificial intelligence machine learning and how the method therein fit together — and hopefully create something worth share in the process and now without further ado let s dive into machine learning with part 2 1 supervised learning more from machine learning for human 🤖 👶 a special thank to jonathan eng edoardo conti grant schneider sunny kumar stephanie he tarun wadhwa and sachin maini series editor for their significant contribution and feedback from a quick cheer to a stand ovation clap to show how much you enjoy this story research comms @deepmindai previously @upstart @yale @trueventurestec demystify artificial intelligence & machine learning discussion on safe and intentional application of ai for positive social impact
Tim Anglade,7K,23,https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3?source=tag_archive---------4----------------,how hbo s silicon valley build not hotdog with mobile tensorflow keras & react native,the hbo show silicon valley release a real ai app that identify hotdog — and not hotdog — like the one show on season 4 s 4th episode the app be now available on android as well as io to achieve this we design a bespoke neural architecture that run directly on your phone and train it with tensorflow keras & nvidia gpus while the use case be farcical the app be an approachable example of both deep learning and edge computing all ai work be power 100 % by the user s device and image be process without ever leave their phone this provide user with a snappy experience no round trip to the cloud offline availability and well privacy this also allow we to run the app at a cost of $ 0 even under the load of a million user provide significant saving compare to traditional cloud base ai approach the app be develop in house by the show by a single developer run on a single laptop & attach gpu use hand curate datum in that respect it may provide a sense of what can be achieve today with a limited amount of time & resource by non technical company individual developer and hobbyist alike in that spirit this article attempt to give a detailed overview of step involve to help other build their own app if you haven t see the show or try the app you should the app let you snap a picture and then tell you whether it think that image be of a hotdog or not it s a straightforward use case that pay homage to recent ai research and application in particular imagenet while we ve probably dedicate more engineering resource to recognize hotdog than anyone else the app still fail in horrible and or subtle way conversely it s also sometimes able to recognize hotdog in complex situation accord to engadget it s incredible I ve have more success identify food with the app in 20 minute than I have have tag and identify song with shazam in the past two year have you ever find yourself read hacker news thinking they raise a 10 m series a for that I could build it in one weekend this app probably feel a lot like that and the initial prototype be indeed build in a single weekend use google cloud platform s vision api and react native but the final app we end up release on the app store require month of additional part time work to deliver meaningful improvement that would be difficult for an outsider to appreciate we spend week optimize overall accuracy training time inference time iterate on our setup & tooling so we could have a fast development iteration and spend a whole weekend optimize the user experience around ios & android permission don t even get I start on that one all too often technical blog post or academic paper skip over this part prefer to present the final choose solution in the interest of help other learn from our mistake & choice we will present an abridge view of the approach that didn t work for we before we describe the final architecture we end up shipping in the next section we choose react native to build the prototype as it would give we an easy sandbox to experiment with and would help we quickly support many device the experience end up be a good one and we keep react native for the remainder of the project it didn t always make thing easy and the design for the app be purposefully limited but in the end react native get the job do the other main component we use for the prototype — google cloud s vision api be quickly abandon there be 3 main factor for these reason we start experiment with what s trendily call edge computing which for our purpose mean that after train our neural network on our laptop we would export it and embe it directly into our mobile app so that the neural network execution phase or inference would run directly inside the user s phone through a chance encounter with pete warden of the tensorflow team we have become aware of its ability to run tensorflow directly embed on an ios device and start explore that path after react native tensorflow become the second fix part of our stack it only take a day of work to integrate tensorflow s objective c++ camera example in our react native shell it take slightly long to use their transfer learning script which help you retrain the inception architecture to deal with a more specific image problem inception be the name of a family of neural architecture build by google to deal with image recognition problem inception be available pre train which mean the training phase have be complete and the weight be set most often for image recognition network they have be train on imagenet a dataset contain over 20 000 different type of object hotdog be one of they however much like google cloud s vision api imagenet training reward breadth as much as depth here and out of the box accuracy on a single one of the 20 000 + category can be lack as such retraining also call transfer learning aim to take a full train neural net and retrain it to perform well on the specific problem you d like to handle this usually involve some degree of forget either by excise entire layer from the stack or by slowly erase the network s ability to distinguish a type of object e g chair in favor of well accuracy at recognize the one you care about I e hotdog while the network inception in this case may have be train on the 14 m image contain in imagenet we be able to retrain it on a just a few thousand hotdog image to get drastically enhance hotdog recognition the big advantage of transfer learning be you will get well result much fast and with less datum than if you train from scratch a full training might take month on multiple gpu and require million of image while retraining can conceivably be do in hour on a laptop with a couple thousand image one of the big challenge we encounter be understand exactly what should count as a hotdog and what should not define what a hotdog be end up be surprisingly difficult do cut up sausage count and if so which kind and subject to cultural interpretation similarly the open world nature of our problem mean we have to deal with an almost infinite number of inputs while certain computer vision problem have relatively limit input say x ray of bolt with or without a mechanical default we have to prepare the app to be feed selfie nature shot and any number of food suffice to say this approach be promise and do lead to some improved result however it have to be abandon for a couple of reason first the nature of our problem mean a strong imbalance in training datum there be many more example of thing that be not hotdog than thing that be hotdog in practice this mean that if you train your algorithm on 3 hotdog image and 97 non hotdog image and it recognize 0 % of the former but 100 % of the latter it will still score 97 % accuracy by default this be not straightforward to solve out of the box use tensorflow s retrain tool and basically necessitate set up a deep learning model from scratch import weight and train in a more control manner at this point we decide to bite the bullet and get something start with keras a deep learning library that provide nice easy to use abstraction on top of tensorflow include pretty awesome training tool and a class_weight option which be ideal to deal with this sort of dataset imbalance we be deal with we use that opportunity to try other popular neural architecture like vgg but one problem remain none of they could comfortably fit on an iphone they consume too much memory which lead to app crash and would sometime take up to 10 second to compute which be not ideal from a ux standpoint many thing be attempt to mitigate that but in the end it these architecture be just too big to run efficiently on mobile to give you a context out of time this be roughly the mid way point of the project by that time the ui be 90%+ do and very little of it be go to change but in hindsight the neural net be at well 20 % do we have a good sense of challenge & a good dataset but 0 line of the final neural architecture have be write none of our neural code could reliably run on mobile and even our accuracy be go to improve drastically in the week to come the problem directly ahead of we be simple if inception and vgg be too big be there a simple pre train neural network we could retrain at the suggestion of the always excellent jeremy p howard where have that guy be all our life we explore xception enet and squeezenet we quickly settle on squeezenet due to its explicit positioning as a solution for embed deep learning and the availability of a pre train keras model on github yay open source so how big of a difference do this make an architecture like vgg use about 138 million parameter essentially the number of number necessary to model the neuron and value between they inception be already a massive improvement require only 23 million parameter squeezenet in comparison only require 1 25 million this have two advantage there be tradeoff of course during this phase we start experiment with tune the neural network architecture in particular we start use batch normalization and try different activation function after add batch normalization and elu to squeezenet we be able to train neural network that achieve 90%+ accuracy when train from scratch however they be relatively brittle mean the same network would overfit in some case or underfit in other when confront to real life testing even add more example to the dataset and play with data augmentation fail to deliver a network that meet expectation so while this phase be promise and for the first time give we a function app that could work entirely on an iphone in less than a second we eventually move to our 4th & final architecture our final architecture be spur in large part by the publication on april 17 of google s mobilenet paper promise a new neural architecture with inception like accuracy on simple problem like ours with only 4 m or so parameter this mean it sit in an interesting sweet spot between a squeezenet that have maybe be overly simplistic for our purpose and the possibly overwrought elephant try to squeeze in a tutu of use inception or vgg on mobile the paper introduce some capacity to tune the size & complexity of network specifically to trade memory cpu consumption against accuracy which be very much top of mind for we at the time with less than a month to go before the app have to launch we endeavor to reproduce the paper s result this be entirely anticlimactic as within a day of the paper be publish a keras implementation be already offer publicly on github by refik can malli a student at istanbul technical university whose work we have already benefit from when we take inspiration from his excellent keras squeezenet implementation the depth & openness of the deep learning community and the presence of talente mind like r c be what make deep learning viable for application today — but they also make work in this field more thrilling than any tech trend we ve be involve with our final architecture end up make significant departure from the mobilenet architecture or from convention in particular so how do this stack work exactly deep learning often get a bad rap for be a black box and while it s true many component of it can be mysterious the network we use often leak information about how some of their magic work we can look at the layer of this stack and how they activate on specific input image give we a sense of each layer s ability to recognize sausage bun or other particularly salient hotdog feature data quality be of the utmost importance a neural network can only be as good as the datum that train it and improve training set quality be probably one of the top 3 thing we spend time on during this project the key thing we do to improve this be the final composition of our dataset be 150k image of which only 3k be hotdog there be only so many hotdog you can look at but there be many not hotdog to look at the 49 1 imbalance be deal with by say a keras class weight of 49 1 in favor of hotdog of the remain 147k image most be of food with just 3k photo of non food item to help the network generalize a bit more and not get trick into see a hotdog if present with an image of a human in a red outfit our data augmentation rule be as follow these number be derive intuitively base on experiment and our understanding of the real life usage of our app as oppose to careful experimentation the final key to our data pipeline be use patrick rodriguez s multiprocess image datum generator for keras while keras do have a build in multi thread and multiprocess implementation we find patrick s library to be consistently fast in our experiment for reason we do not have time to investigate this library cut our training time to a third of what it use to be the network be train use a 2015 macbook pro and attach external gpu egpu specifically an nvidia gtx 980 ti we d probably buy a 1080 ti if we be start today we be able to train the network on batch of 128 image at a time the network be train for a total of 240 epoch mean we run all 150k image through the network 240 time this take about 80 hour we train the network in 3 phase while learn rate be identify by run the linear experiment recommend by the clr paper they seem to intuitively make sense in that the max for each phase be within a factor of 2 of the previous minimum which be align with the industry standard recommendation of halve your learning rate if your accuracy plateaus during training in the interest of time we perform some training run on a paperspace p5000 instance run ubuntu in those case we be able to double the batch size and find that optimal learning rate for each phase be roughly double as well even have design a relatively compact neural architecture and have train it to handle situation it may find in a mobile context we have a lot of work leave to make it run properly try to run a top of the line neural net architecture out of the box can quickly burn hundred megabyte of ram which few mobile device can spare today beyond network optimization it turn out the way you handle image or even load tensorflow itself can have a huge impact on how quickly your network run how little ram it use and how crash free the experience will be for your user this be maybe the most mysterious part of this project relatively little information can be find about it possibly due to the dearth of production deep learning application run on mobile device as of today however we must commend the tensorflow team and particularly pete warden andrew harp and chad whipkey for the exist documentation and their kindness in answer our inquiry instead of use tensorflow on io we look at use apple s build in deep learning librarie instead bnn mpscnn and later on coreml we would have design the network in keras train it with tensorflow export all the weight value re implement the network with bnns or mpscnn or import it via coreml and load the parameter into that new implementation however the big obstacle be that these new apple library be only available on io 10 + and we want to support old version of io as io 10 + adoption and these framework continue to improve there may not be a case for use tensorflow on device in the near future if you think inject javascript into your app on the fly be cool try inject neural net into your app the last production trick we use be to leverage codepush and apple s relatively permissive term of service to live inject new version of our neural network after submission to the app store while this be mostly do to help we quickly deliver accuracy improvement to our user after release you could conceivably use this approach to drastically expand or alter the feature set of your app without go through an app store review again there be a lot of thing that didn t work or we didn t have time to do and these be the idea we d investigate in the future finally we d be remiss not to mention the obvious and important influence of user experience developer experience and build in bias in develop an ai app each probably deserve their own post or their own book but here be the very concrete impact of these 3 thing in our experience ux user experience be arguably more critical at every stage of the development of an ai app than for a traditional application there be no deep learning algorithm that will give you perfect result right now but there be many situation where the right mix of deep learning + ux will lead to result that be indistinguishable from perfect proper ux expectation be irreplaceable when it come to set developer on the right path to design their neural network set the proper expectation for user when they use the app and gracefully handle the inevitable ai failure building ai app without a ux first mindset be like train a neural net without stochastic gradient descent you will end up stuck in the local minima of the uncanny valley on your way to build the perfect ai use case dx developer experience be extremely important as well because deep learning training time be the new horsing around while wait for your program to compile we suggest you heavily favor dx first hence keras as it s always possible to optimize runtime for later run manual gpu parallelization multi process data augmentation tensorflow pipeline even re implement for caffe2 pytorch even project with relatively obtuse apis & documentation like tensorflow greatly improve dx by provide a highly test highly use well maintain environment for training & run neural network for the same reason it s hard to beat both the cost as well as the flexibility of have your own local gpu for development be able to look at edit image locally edit code with your prefer tool without delay greatly improve the development quality & speed of building ai project most ai app will hit more critical cultural bias than ours but as an example even our straightforward use case catch we flat foot with build in bias in our initial dataset that make the app unable to recognize french style hotdog asian hotdog and more oddity we do not have immediate personal experience with it s critical to remember that ai do not make well decision than human — they be infect by the same human bias we fall prey to via the training set human provide thank to mike judge alec berg clay tarver todd silverstein jonathan dotan lisa schomas amy solomon dorothy street & rich toyon and all the writer of the show — the app would simply not exist without they meaghan dana david jay and everyone at hbo scale venture partner & gitlab rachel thomas and jeremy howard & fast ai for all that they have teach I and for kindly review a draft of this post check out their free online deep learning course it s awesome jp simard for his help on io and finally the tensorflow team & r machinelearning for their help & inspiration and thank to everyone who use & share the app it make stare at picture of hotdog for month on end totally worth it 😅 from a quick cheer to a stand ovation clap to show how much you enjoy this story a i startup & hbo s silicon valley get in touch timanglade@gmail com
Sophia Ciocca,53K,9,https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe?source=tag_archive---------5----------------,how do spotify know you so well member feature story medium,member feature story a software engineer explain the science behind personalize music recommendation photo by studioeast getty image photo by studioeast getty image this monday — just like every monday before it — over 100 million spotify user find a fresh new playlist wait for they call discover weekly it s a custom mixtape of 30 song they ve never listen to before but will probably love and it s pretty much magic I m a huge fan of spotify and particularly discover weekly why it make I feel see it know my musical taste well than any person in my entire life ever have and I m consistently delight by how satisfyingly just right it be every week with track I probably would never have find myself or know I would like for those of you who live under a soundproof rock let I introduce you to my virtual good friend as it turn out I m not alone in my obsession with discover weekly the user base go crazy for it which have drive spotify to rethink its focus and invest more resource into algorithm base playlist ever since discover weekly debut in 2015 I ve be die to know how it work what s more I m a spotify fangirl so I sometimes like to pretend that I work there and research their product after three week of mad googling I feel like I ve finally get a glimpse behind the curtain so how do spotify do such an amazing job of choose those 30 song for each person each week let s zoom out for a second to look at how other music service have tackle music recommendation and how spotify s do it well back in the 2000 songza kick off the online music curation scene use manual curation to create playlist for user this mean that a team of music expert or other human curator would put together playlist that they just think sound good and then user would listen to those playlist later beat music would employ this same strategy manual curation work alright but it be base on that specific curator s choice and therefore couldn t take into account each listener s individual music taste like songza pandora be also one of the original player in digital music curation it employ a slightly more advanced approach instead manually tag attribute of song this mean a group of people listen to music choose a bunch of descriptive word for each track and tag the track accordingly then pandora s code could simply filter for certain tag to make playlist of similar sound music around that same time a music intelligence agency from the mit medium lab call the echo nest be bear which take a radical cutting edge approach to personalize music the echo nest use algorithm to analyze the audio and textual content of music allow it to perform music identification personalize recommendation playlist creation and analysis finally take another approach be last fm which still exist today and use a process call collaborative filtering to identify music its user might like but more on that in a moment so if that s how other music curation service have handle recommendation how do spotify s magic engine run how do it seem to nail individual user taste so much more accurately than any of the other service spotify doesn t actually use a single revolutionary recommendation model instead they mix together some of the good strategy use by other service to create their own uniquely powerful discovery engine to create discover weekly there be three main type of recommendation model that spotify employ let s dive into how each of these recommendation model work first some background when people hear the word collaborative filtering they generally think of netflix as it be one of the first company to use this method to power a recommendation model take user star base movie rating to inform its understanding of which movie to recommend to other similar user after netflix be successful the use of collaborative filtering spread quickly and be now often the starting point for anyone try to make a recommendation model unlike netflix spotify doesn t have a star base system with which user rate their music instead spotify s data be implicit feedback — specifically the stream count of the track and additional streaming datum such as whether a user save the track to their own playlist or visit the artist s page after listen to a song but what be collaborative filtering truly and how do it work here s a high level rundown explain in a quick conversation what s go on here each of these individual have track preference the one on the left like track p q r and s while the one on the right like track q r s and t collaborative filtering then use that datum to say hmmm you both like three of the same track — q r and s — so you be probably similar user therefore you re each likely to enjoy other track that the other person have listen to that you haven t hear yet therefore it suggest that the one on the right check out track p — the only track not mention but that his similar counterpart enjoy — and the one on the left check out track t for the same reasoning simple right but how do spotify actually use that concept in practice to calculate million of user suggest track base on million of other user preference with matrix math do with python library in actuality this matrix you see here be gigantic each row represent one of spotify s 140 million user — if you use spotify you yourself be a row in this matrix — and each column represent one of the 30 million song in spotify s database then the python library run this long complicated matrix factorization formula when it finish we end up with two type of vector represent here by x and y x be a user vector represent one single user s taste and y be a song vector represent one single song s profile now we have 140 million user vector and 30 million song vector the actual content of these vector be just a bunch of number that be essentially meaningless on their own but be hugely useful when compare to find out which user musical taste be most similar to mine collaborative filtering compare my vector with all of the other user vector ultimately spit out which user be the close match the same go for the y vector song you can compare a single song s vector with all the other and find out which song be most similar to the one in question collaborative filtering do a pretty good job but spotify know they could do even well by add another engine enter nlp the second type of recommendation model that spotify employ be natural language processing nlp model the source datum for these model as the name suggest be regular ol word track metadata news article blog and other text around the internet natural language processing which be the ability of a computer to understand human speech as it be speak be a vast field unto itself often harness through sentiment analysis api the exact mechanism behind nlp be beyond the scope of this article but here s what happen on a very high level spotify crawl the web constantly look for blog post and other write text about music to figure out what people be say about specific artist and song — which adjective and what particular language be frequently use in reference to those artist and song and which other artist and song be also be discuss alongside they while I don t know the specific of how spotify choose to then process this scrape datum I can offer some insight base on how the echo nest use to work with they they would bucket spotify s datum up into what they call cultural vector or top term each artist and song have thousand of top term that change on the daily each term have an associated weight which correlate to its relative importance — roughly the probability that someone will describe the music or artist with that term then much like in collaborative filter the nlp model use these term and weight to create a vector representation of the song that can be use to determine if two piece of music be similar cool right first a question you might be think first of all add a third model far improve the accuracy of the music recommendation service but this model also serve a secondary purpose unlike the first two type raw audio model take new song into account take for example a song your singer songwriter friend have put up on spotify maybe it only have 50 listen so there be few other listener to collaboratively filter it against it also isn t mention anywhere on the internet yet so nlp model win t pick it up luckily raw audio model don t discriminate between new track and popular track so with their help your friend s song could end up in a discover weekly playlist alongside popular song but how can we analyze raw audio datum which seem so abstract with convolutional neural network convolutional neural network be the same technology use in facial recognition software in spotify s case they ve be modify for use on audio datum instead of pixel here s an example of a neural network architecture this particular neural network have four convolutional layer see as the thick bar on the left and three dense layer see as the more narrow bar on the right the input be time frequency representation of audio frame which be then concatenate or link together to form the spectrogram the audio frame go through these convolutional layer and after pass through the last one you can see a global temporal pooling layer which pool across the entire time axi effectively compute statistic of the learn feature across the time of the song after process the neural network spit out an understanding of the song include characteristic like estimate time signature key mode tempo and loudness below be a plot of datum for a 30 second snippet of around the world by daft punk ultimately this reading of the song s key characteristic allow spotify to understand fundamental similarity between song and therefore which user might enjoy they base on their own listening history that cover the basic of the three major type of recommendation model feed spotify s recommendation pipeline and ultimately power the discover weekly playlist of course these recommendation model be all connect to spotify s large ecosystem which include giant amount of datum storage and use lot of hadoop cluster to scale recommendation and make these engine work on enormous matrix endless online music article and huge number of audio file I hope this be informative and pique your curiosity like it do mine for now I ll be work my way through my own discover weekly find my new favorite music while appreciate all the machine learning that s go on behind the scene 🎶 thank also to ladycollective for read this article and suggest edit software engineer writer and generally creative human interested in art feminism mindfulness and authenticity http sophiaciocca com welcome to a place where word matter on medium smart voice and original idea take center stage — with no ad in sight watch follow all the topic you care about and we ll deliver the good story for you to your homepage and inbox explore get unlimited access to the good story on medium — and support writer while you re at it just $ 5 month upgrade
Dhruv Parthasarathy,4.3K,12,https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4?source=tag_archive---------6----------------,a brief history of cnn in image segmentation from r cnn to mask r cnn,at athela we use convolutional neural network cnn for a lot more than just classification in this post we ll see how cnn can be use with great result in image instance segmentation ever since alex krizhevsky geoff hinton and ilya sutskever win imagenet in 2012 convolutional neural network cnn have become the gold standard for image classification in fact since then cnn have improve to the point where they now outperform human on the imagenet challenge while these result be impressive image classification be far simple than the complexity and diversity of true human visual understanding in classification there s generally an image with a single object as the focus and the task be to say what that image be see above but when we look at the world around we we carry out far more complex task we see complicated sight with multiple overlap object and different background and we not only classify these different object but also identify their boundary difference and relation to one another can cnn help we with such complex task namely give a more complicated image can we use cnn to identify the different object in the image and their boundary as have be show by ross girshick and his peer over the last few year the answer be conclusively yes through this post we ll cover the intuition behind some of the main technique use in object detection and segmentation and see how they ve evolve from one implementation to the next in particular we ll cover r cnn regional cnn the original application of cnn to this problem along with its descendant fast r cnn and fast r cnn finally we ll cover mask r cnn a paper release recently by facebook research that extend such object detection technique to provide pixel level segmentation here be the paper reference in this post inspire by the research of hinton s lab at the university of toronto a small team at uc berkeley lead by professor jitendra malik ask themselves what today seem like an inevitable question object detection be the task of find the different object in an image and classify they as see in the image above the team comprise of ross girshick a name we ll see again jeff donahue and trevor darrel find that this problem can be solve with krizhevsky s result by test on the pascal voc challenge a popular object detection challenge akin to imagenet they write let s now take a moment to understand how their architecture region with cnn r cnn work understand r cnn the goal of r cnn be to take in an image and correctly identify where the main object via a bounding box in the image but how do we find out where these bounding box be r cnn do what we might intuitively do as well propose a bunch of box in the image and see if any of they actually correspond to an object r cnn create these bounding box or region proposal use a process call selective search which you can read about here at a high level selective search show in the image above look at the image through window of different size and for each size try to group together adjacent pixel by texture color or intensity to identify object once the proposal be create r cnn warps the region to a standard square size and pass it through to a modified version of alexnet the win submission to imagenet 2012 that inspire r cnn as show above on the final layer of the cnn r cnn add a support vector machine svm that simply classify whether this be an object and if so what object this be step 4 in the image above improve the bounding box now have find the object in the box can we tighten the box to fit the true dimension of the object we can and this be the final step of r cnn r cnn run a simple linear regression on the region proposal to generate tight bounding box coordinate to get our final result here be the input and output of this regression model so to summarize r cnn be just the follow step r cnn work really well but be really quite slow for a few simple reason in 2015 ross girshick the first author of r cnn solve both these problem lead to the second algorithm in our short history fast r cnn let s now go over its main insight fast r cnn insight 1 roi region of interest pooling for the forward pass of the cnn girshick realize that for each image a lot of propose region for the image invariably overlap cause we to run the same cnn computation again and again ~2000 time his insight be simple — why not run the cnn just once per image and then find a way to share that computation across the ~2000 proposal this be exactly what fast r cnn do use a technique know as roipool region of interest pooling at its core roipool share the forward pass of a cnn for an image across its subregion in the image above notice how the cnn feature for each region be obtain by select a correspond region from the cnn s feature map then the feature in each region be pool usually use max pooling so all it take we be one pass of the original image as oppose to ~2000 fast r cnn insight 2 combine all model into one network the second insight of fast r cnn be to jointly train the cnn classifier and bounding box regressor in a single model where early we have different model to extract image feature cnn classify svm and tighten bounding box regressor fast r cnn instead use a single network to compute all three you can see how this be do in the image above fast r cnn replace the svm classifier with a softmax layer on top of the cnn to output a classification it also add a linear regression layer parallel to the softmax layer to output bounding box coordinate in this way all the output need come from one single network here be the input and output to this overall model even with all these advancement there be still one remain bottleneck in the fast r cnn process — the region proposer as we see the very first step to detect the location of object be generate a bunch of potential bounding box or region of interest to test in fast r cnn these proposal be create use selective search a fairly slow process that be find to be the bottleneck of the overall process in the middle 2015 a team at microsoft research compose of shaoqe ren kaime he ross girshick and jian sun find a way to make the region proposal step almost cost free through an architecture they creatively name fast r cnn the insight of fast r cnn be that region proposal depend on feature of the image that be already calculate with the forward pass of the cnn first step of classification so why not reuse those same cnn result for region proposal instead of run a separate selective search algorithm indeed this be just what the fast r cnn team achieve in the image above you can see how a single cnn be use to both carry out region proposal and classification this way only one cnn need to be train and we get region proposal almost for free the author write here be the input and output of their model how the region be generate let s take a moment to see how fast r cnn generate these region proposal from cnn feature fast r cnn add a fully convolutional network on top of the feature of the cnn create what s know as the region proposal network the region proposal network work by pass a slide window over the cnn feature map and at each window output k potential bounding box and score for how good each of those box be expect to be what do these k box represent intuitively we know that object in an image should fit certain common aspect ratio and size for instance we know that we want some rectangular box that resemble the shape of human likewise we know we win t see many box that be very very thin in such a way we create k such common aspect ratio we call anchor box for each such anchor box we output one bounding box and score per position in the image with these anchor box in mind let s take a look at the input and output to this region proposal network we then pass each such bounding box that be likely to be an object into fast r cnn to generate a classification and tightened bounding box so far we ve see how we ve be able to use cnn feature in many interesting way to effectively locate different object in an image with bounding box can we extend such technique to go one step far and locate exact pixel of each object instead of just bound box this problem know as image segmentation be what kaime he and a team of researcher include girshick explore at facebook ai use an architecture know as mask r cnn much like fast r cnn and fast r cnn mask r cnn s underlying intuition be straight forward give that fast r cnn work so well for object detection could we extend it to also carry out pixel level segmentation mask r cnn do this by add a branch to fast r cnn that output a binary mask that say whether or not a give pixel be part of an object the branch in white in the above image as before be just a fully convolutional network on top of a cnn base feature map here be its input and output but the mask r cnn author have to make one small adjustment to make this pipeline work as expect roialign realigning roipool to be more accurate when run without modification on the original fast r cnn architecture the mask r cnn author realize that the region of the feature map select by roipool be slightly misalign from the region of the original image since image segmentation require pixel level specificity unlike bound box this naturally lead to inaccuracy the author be able to solve this problem by cleverly adjust roipool to be more precisely align use a method know as roialign imagine we have an image of size 128x128 and a feature map of size 25x25 let s imagine we want feature the region correspond to the top leave 15x15 pixel in the original image see above how might we select these pixel from the feature map we know each pixel in the original image correspond to ~ 25 128 pixel in the feature map to select 15 pixel from the original image we just select 15 * 25 128 ~= 2 93 pixel in roipool we would round this down and select 2 pixel cause a slight misalignment however in roialign we avoid such round instead we use bilinear interpolation to get a precise idea of what would be at pixel 2 93 this at a high level be what allow we to avoid the misalignment cause by roipool once these mask be generate mask r cnn combine they with the classification and bounding box from fast r cnn to generate such wonderfully precise segmentation if you re interested in try out these algorithm yourself here be relevant repository fast r cnn mask r cnn in just 3 year we ve see how the research community have progress from krizhevsky et al s original result to r cnn and finally all the way to such powerful result as mask r cnn see in isolation result like mask r cnn seem like incredible leap of genius that would be unapproachable yet through this post I hope you ve see how such advancement be really the sum of intuitive incremental improvement through year of hard work and collaboration each of the idea propose by r cnn fast r cnn fast r cnn and finally mask r cnn be not necessarily quantum leap yet their sum product have lead to really remarkable result that bring we close to a human level understanding of sight what particularly excite I be that the time between r cnn and mask r cnn be just three year with continue funding focus and support how much far can computer vision improve over the next three year if you see any error or issue in this post please contact I at dhruv@getathela com and I ll immediately correct they if you re interested in apply such technique come join we at athela where we apply computer vision to blood diagnostic daily other post we ve write thank to bharath ramsundar pranav ramkrishnan tanay tandon and oliver cameron for help with this post from a quick cheer to a stand ovation clap to show how much you enjoy this story @dhruvp vp eng @athelas mit math and cs undergrad 13 mit cs master 14 previously director of ai program @ udacity blood diagnostic through deep learning http athela com
Andrej Karpathy,35K,8,https://medium.com/@karpathy/software-2-0-a64152b37c35?source=tag_archive---------7----------------,software 2 0 andrej karpathy medium,I sometimes see people refer to neural network as just another tool in your machine learning toolbox they have some pro and con they work here or there and sometimes you can use they to win kaggle competition unfortunately this interpretation completely miss the forest for the tree neural network be not just another classifier they represent the beginning of a fundamental shift in how we write software they be software 2 0 the classical stack of software 1 0 be what we re all familiar with — it be write in language such as python c++ etc it consist of explicit instruction to the computer write by a programmer by write each line of code the programmer identify a specific point in program space with some desirable behavior in contrast software 2 0 can be write in much more abstract human unfriendly language such as the weight of a neural network no human be involve in write this code because there be a lot of weight typical network might have million and code directly in weight be kind of hard I try instead our approach be to specify some goal on the behavior of a desirable program e g satisfy a dataset of input output pair of example or win a game of go write a rough skeleton of the code e g a neural net architecture that identify a subset of program space to search and use the computational resource at our disposal to search this space for a program that work in the specific case of neural network we restrict the search to a continuous subset of the program space where the search process can be make somewhat surprisingly efficient with backpropagation and stochastic gradient descent it turn out that a large portion of real world problem have the property that it be significantly easy to collect the datum or more generally identify a desirable behavior than to explicitly write the program in these case the programmer will often split into two the 2 0 programmer manually curate maintain massage clean and label dataset ; each label example literally program the final system because the dataset get compile into software 2 0 code via the optimization meanwhile the 1 0 programmer maintain the surround tool analytic visualization labeling interface infrastructure and the training code let s briefly examine some concrete example of this ongoing transition in each of these area we ve see improvement over the last few year when we give up on try to address a complex problem by write explicit code and instead transition the code into the 2 0 stack visual recognition use to consist of engineer feature with a bit of machine learning sprinkle on top at the end e g an svm since then we discover much more powerful visual feature by obtain large dataset e g imagenet and search in the space of convolutional neural network architecture more recently we don t even trust ourselves to hand code the architecture and we ve begin search over those as well speech recognition use to involve a lot of preprocesse gaussian mixture model and hide markov model but today consist almost entirely of neural net stuff a very relate often cite humorous quote attribute to fred jelinek from 1985 read every time I fire a linguist the performance of our speech recognition system go up speech synthesis have historically be approach with various stitching mechanism but today the state of the art model be large convnet e g wavenet that produce raw audio signal output machine translation have usually be approach with phrase base statistical technique but neural network be quickly become dominant my favorite architecture be train in the multilingual setting where a single model translate from any source language to any target language and in weakly supervised or entirely unsupervised setting game explicitly hand code go play program have be develop for a long while but alphago zero a convnet that look at the raw state of the board and play a move have now become by far the strong player of the game I expect we re go to see very similar result in other area e g dota 2 or starcraft database more traditional system outside of artificial intelligence be also see early hint of a transition for instance the case for learn index structure replace core component of a data management system with a neural network outperform cache optimize b tree by up to 70 % in speed while save an order of magnitude in memory you ll notice that many of my link above involve work do at google this be because google be currently at the forefront of re write large chunk of itself into software 2 0 code one model to rule they all provide an early sketch of what this might look like where the statistical strength of the individual domain be amalgamate into one consistent understanding of the world why should we prefer to port complex program into software 2 0 clearly one easy answer be that they work well in practice however there be a lot of other convenient reason to prefer this stack let s take a look at some of the benefit of software 2 0 think a convnet compare to software 1 0 think a production level c++ code base software 2 0 be computationally homogeneous a typical neural network be to the first order make up of a sandwich of only two operation matrix multiplication and thresholding at zero relu compare that with the instruction set of classical software which be significantly more heterogenous and complex because you only have to provide software 1 0 implementation for a small number of the core computational primitive e g matrix multiply it be much easy to make various correctness performance guarantee simple to bake into silicon as a corollary since the instruction set of a neural network be relatively small it be significantly easy to implement these network much close to silicon e g with custom asics neuromorphic chip and so on the world will change when low powered intelligence become pervasive around we e g small inexpensive chip could come with a pretraine convnet a speech recognizer and a wavenet speech synthesis network all integrate in a small protobrain that you can attach to stuff constant run time every iteration of a typical neural net forward pass take exactly the same amount of flop there be zero variability base on the different execution path your code could take through some sprawl c++ code base of course you could have dynamic compute graph but the execution flow be normally still significantly constrain this way we be also almost guarantee to never find ourselves in unintended infinite loop constant memory use relate to the above there be no dynamically allocate memory anywhere so there be also little possibility of swap to disk or memory leak that you have to hunt down in your code it be highly portable a sequence of matrix multiplie be significantly easy to run on arbitrary computational configuration compare to classical binary or script it be very agile if you have a c++ code and someone want you to make it twice as fast at cost of performance if need it would be highly non trivial to tune the system for the new spec however in software 2 0 we can take our network remove half of the channel retrain and there — it run exactly at twice the speed and work a bit bad it s magic conversely if you happen to get more datum compute you can immediately make your program work well just by add more channel and retrain module can meld into an optimal whole our software be often decompose into module that communicate through public function apis or endpoint however if two software 2 0 module that be originally train separately interact we can easily backpropagate through the whole think about how amazing it could be if your web browser could automatically re design the low level system instruction 10 stack down to achieve a high efficiency in loading web page with 2 0 this be the default behavior it be well than you finally and most importantly a neural network be a well piece of code than anything you or I can come up with in a large fraction of valuable vertical which currently at the very least involve anything to do with image video and sound speech the 2 0 stack also have some of its own disadvantage at the end of the optimization we re leave with large network that work well but it s very hard to tell how across many application area we ll be leave with a choice of use a 90 % accurate model we understand or 99 % accurate model we don t the 2 0 stack can fail in unintuitive and embarrassing way or bad they can silently fail e g by silently adopt bias in their training datum which be very difficult to properly analyze and examine when their size be easily in the million in most case finally we re still discover some of the peculiar property of this stack for instance the existence of adversarial example and attack highlight the unintuitive nature of this stack software 1 0 be code we write software 2 0 be code we do not write but seem to work well it be likely that any setting where the program be not obvious but one can repeatedly evaluate the performance of it e g — do you classify some image correctly do you win game of go will be subject to this transition because the optimization can find much well code than what we can write if you think of neural network as a new software stack and not just a pretty good classifier it quickly become apparent there be a lot of work to do for example from a system perspective in the 1 0 stack llvm ir form a middle layer between a number of front end language and back end architecture and provide an opportunity for optimization with neural network we re already see an explosion of front end for specify program subset to search over pytorch tf chainer mxnet etc and back end to run the training compilation and inference cpu gpu tpu ipu but what be a fitting ir and how we can optimize it halide like as another example we ve build up a vast amount of tooling that assist human in write 1 0 code like powerful ide with feature like syntax highlighting debugger profiler go to def git integration etc who be go to develop the first powerful software 2 0 ide which help with all of the workflow in accumulate visualize cleaning labeling and source dataset there be a lot of room for a layer of intelligence assist the 2 0 programmer e g perhaps the ide bubble up image that the network suspect be mislabele or assist in labeling or find example where the network be currently uncertain finally in the long term the future of software 2 0 be bright because it be increasingly clear to many that when we develop agi it will certainly be write in software 2 0 from a quick cheer to a stand ovation clap to show how much you enjoy this story director of ai at tesla previously research scientist at openai and phd student at stanford I like to train deep neural net on large dataset
Sebastian Heinz,4.4K,13,https://medium.com/mlreview/a-simple-deep-learning-model-for-stock-price-prediction-using-tensorflow-30505541d877?source=tag_archive---------8----------------,a simple deep learning model for stock price prediction use tensorflow,for a recent hackathon that we do at statworx some of our team member scrape minutely s&p 500 datum from the google finance api the datum consist of index as well as stock price of the s&p s 500 constituent have this datum at hand the idea of develop a deep learning model for predict the s&p 500 index base on the 500 constituent price one minute ago come immediately on my mind play around with the datum and build the deep learning model with tensorflow be fun and so I decide to write my first medium com story a little tensorflow tutorial on predict s&p 500 stock price what you will read be not an in depth tutorial but more a high level introduction to the important building block and concept of tensorflow model the python code I ve create be not optimize for efficiency but understandability the dataset I ve use can be download from here 40 mb our team export the scrape stock datum from our scrape server as a csv file the dataset contain n = 41266 minute of datum range from april to august 2017 on 500 stock as well as the total s&p 500 index price index and stock be arrange in wide format the datum be already clean and prepare meaning miss stock and index price be locf ed last observation carry forward so that the file do not contain any miss value a quick look at the s&p time series use pyplot plot datum sp500 note this be actually the lead of the s&p 500 index mean its value be shift 1 minute into the future this operation be necessary since we want to predict the next minute of the index and not the current minute the dataset be split into training and test datum the training datum contain 80 % of the total dataset the data be not shuffle but sequentially slice the training datum range from april to approx end of july 2017 the test data end end of august 2017 there be a lot of different approach to time series cross validation such as roll forecast with and without refit or more elaborate concept such as time series bootstrap resample the latter involve repeat sample from the remainder of the seasonal decomposition of the time series in order to simulate sample that follow the same seasonal pattern as the original time series but be not exact copy of its value most neural network architecture benefit from scale the input sometimes also the output why because most common activation function of the network s neuron such as tanh or sigmoid be define on the 1 1 or 0 1 interval respectively nowadays rectify linear unit relu activation be commonly use activation which be unbounded on the axis of possible activation value however we will scale both the input and target anyway scaling can be easily accomplish in python use sklearn s minmaxscaler remark caution must be undertake regard what part of the data be scale and when a common mistake be to scale the whole dataset before training and test split be be apply why be this a mistake because scale invoke the calculation of statistic e g the min max of a variable when perform time series forecasting in real life you do not have information from future observation at the time of forecast therefore calculation of scale statistic have to be conduct on training datum and must then be apply to the test datum otherwise you use future information at the time of forecasting which commonly bias forecasting metric in a positive direction tensorflow be a great piece of software and currently the lead deep learning and neural network computation framework it be base on a c++ low level backend but be usually control via python there be also a neat tensorflow library for r maintain by rstudio tensorflow operate on a graph representation of the underlie computational task this approach allow the user to specify mathematical operation as element in a graph of datum variable and operator since neural network be actually graph of datum and mathematical operation tensorflow be just perfect for neural network and deep learning check out this simple example steal from our deep learning introduction from our blog in the figure above two number be suppose to be add those number be store in two variable a and b the two value be flow through the graph and arrive at the square node where they be be add the result of the addition be store into another variable c actually a b and c can be consider as placeholder any number that be feed into a and b get add and be store into c this be exactly how tensorflow work the user define an abstract representation of the model neural network through placeholder and variable afterwards the placeholder get fill with real datum and the actual computation take place the follow code implement the toy example from above in tensorflow after have import the tensorflow library two placeholder be define use tf placeholder they correspond to the two blue circle on the left of the image above afterwards the mathematical addition be define via tf add the result of the computation be c = 9 with placeholder set up the graph can be execute with any integer value for a and b of course the former problem be just a toy example the require graph and computation in a neural network be much more complex as mention before it all start with placeholder we need two placeholder in order to fit our model x contain the network s input the stock price of all s&p 500 constituent at time t = t and y the network s output the index value of the s&p 500 at time t = t + 1 the shape of the placeholder correspond to none n_stock with none mean that the input be a 2 dimensional matrix and the output be a 1 dimensional vector it be crucial to understand which input and output dimension the neural net need in order to design it properly the none argument indicate that at this point we do not yet know the number of observation that flow through the neural net graph in each batch so we keep if flexible we will later define the variable batch_size that control the number of observation per training batch besides placeholder variable be another cornerstone of the tensorflow universe while placeholder be use to store input and target datum in the graph variable be use as flexible container within the graph that be allow to change during graph execution weight and bias be represent as variable in order to adapt during training variable need to be initialize prior to model training we will get into that a litte later in more detail the model consist of four hide layer the first layer contain 1024 neuron slightly more than double the size of the inputs subsequent hide layer be always half the size of the previous layer which mean 512 256 and finally 128 neuron a reduction of the number of neuron for each subsequent layer compress the information the network identify in the previous layer of course other network architecture and neuron configuration be possible but be out of scope for this introduction level article it be important to understand the require variable dimension between input hide and output layer as a rule of thumb in multilayer perceptron mlp the type of network use here the second dimension of the previous layer be the first dimension in the current layer for weight matrix this might sound complicated but be essentially just each layer pass its output as input to the next layer the bias dimension equal the second dimension of the current layer s weight matrix which correspond the number of neuron in this layer after definition of the require weight and bias variable the network topology the architecture of the network need to be specify hereby placeholder datum and variable weigh and bias need to be combine into a system of sequential matrix multiplication furthermore the hide layer of the network be transform by activation function activation function be important element of the network architecture since they introduce non linearity to the system there be dozen of possible activation function out there one of the most common be the rectified linear unit relu which will also be use in this model the image below illustrate the network architecture the model consist of three major building block the input layer the hide layer and the output layer this architecture be call a feedforward network feedforward indicate that the batch of datum solely flow from leave to right other network architecture such as recurrent neural network also allow datum flow backwards in the network the cost function of the network be use to generate a measure of deviation between the network s prediction and the actual observed training target for regression problem the mean square error mse function be commonly use mse compute the average squared deviation between prediction and target basically any differentiable function can be implement in order to compute a deviation measure between prediction and target however the mse exhibit certain property that be advantageous for the general optimization problem to be solve the optimizer take care of the necessary computation that be use to adapt the network s weight and bias variable during train those computation invoke the calculation of so call gradient that indicate the direction in which the weight and bias have to be change during training in order to minimize the network s cost function the development of stable and speedy optimizer be a major field in neural network an deep learning research here the adam optimizer be use which be one of the current default optimizer in deep learning development adam stand for adaptive moment estimation and can be consider as a combination between two other popular optimizer adagrad and rmsprop initializer be use to initialize the network s variable before training since neural network be train use numerical optimization technique the starting point of the optimization problem be one the key factor to find good solution to the underlying problem there be different initializer available in tensorflow each with different initialization approach here I use the tf variance_scaling_initializer which be one of the default initialization strategy note that with tensorflow it be possible to define multiple initialization function for different variable within the graph however in most case a unified initialization be sufficient after have define the placeholder variable initializer cost function and optimizer of the network the model need to be train usually this be do by minibatch training during minibatch train random data sample of n = batch_size be draw from the training datum and feed into the network the training dataset get divide into n batch_size batch that be sequentially feed into the network at this point the placeholder x and y come into play they store the input and target datum and present they to the network as input and target a sample data batch of x flow through the network until it reach the output layer there tensorflow compare the model prediction against the actual observed target y in the current batch afterwards tensorflow conduct an optimization step and update the network parameter correspond to the select learning scheme after have update the weight and bias the next batch be sample and the process repeat itself the procedure continue until all batch have be present to the network one full sweep over all batch be call an epoch the training of the network stop once the maximum number of epoch be reach or another stopping criterion define by the user apply during the training we evaluate the network prediction on the test set — the datum which be not learn but set aside — for every 5th batch and visualize it additionally the image be export to disk and later combine into a video animation of the training process see below the model quickly learn the shape und location of the time series in the test datum and be able to produce an accurate prediction after some epoch nice one can see that the network rapidly adapt to the basic shape of the time series and continue to learn finer pattern of the datum this also correspond to the adam learning scheme that lower the learning rate during model training in order not to overshoot the optimization minimum after 10 epoch we have a pretty close fit to the test datum the final test mse equal 0 00078 it be very low because the target be scale the mean absolute percentage error of the forecast on the test set be equal to 5 31 % which be pretty good note that this be just a fit to the test datum no actual out of sample metric in a real world scenario please note that there be ton of way of far improve this result design of layer and neuron choose different initialization and activation scheme introduction of dropout layer of neuron early stop and so on furthermore different type of deep learning model such as recurrent neural network might achieve well performance on this task however this be not the scope of this introductory post the release of tensorflow be a landmark event in deep learning research its flexibility and performance allow researcher to develop all kind of sophisticated neural network architecture as well as other ml algorithms however flexibility come at the cost of long time to model cycle compare to high level api such as keras or mxnet nonetheless I be sure that tensorflow will make its way to the de facto standard in neural network and deep learning development in research and practical application many of our customer be already use tensorflow or start develop project that employ tensorflow model also our data science consultant at statworx be heavily use tensorflow for deep learning and neural net research and development let s see what google have plan for the future of tensorflow one thing that be miss at least in my opinion be a neat graphical user interface for design and develop neural net architecture with tensorflow backend maybe this be something google be already work on ; if you have any comment or question on my first medium story feel free to comment below I will try to answer they also feel free to use my code or share this story with your peer on social platform of your choice update I ve add both the python script as well as a zip dataset to a github repository feel free to clone and fork lastly follow I on twitter | linkedin from a quick cheer to a stand ovation clap to show how much you enjoy this story ceo @ statworx do datum science stat and ml for over a decade food wine and cocktail enthusiast check our website https www statworx com highlight from machine learning research project and learn material from and for ml scientist engineer an enthusiast
Netflix Technology Blog,25K,13,https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76?source=tag_archive---------9----------------,artwork personalization at netflix netflix techblog medium,by ashok chandrashekar fernando amat justin basilico and tony jebara for many year the main goal of the netflix personalize recommendation system have be to get the right title in front each of our member at the right time with a catalog span thousand of title and a diverse member base span over a hundred million account recommend the title that be just right for each member be crucial but the job of recommendation do not end there why should you care about any particular title we recommend what can we say about a new and unfamiliar title that will pique your interest how do we convince you that a title be worth watch answer these question be critical in help our member discover great content especially for unfamiliar title one avenue to address this challenge be to consider the artwork or imagery we use to portray the title if the artwork represent a title capture something compelling to you then it act as a gateway into that title and give you some visual evidence for why the title might be good for you the artwork may highlight an actor that you recognize capture an exciting moment like a car chase or contain a dramatic scene that convey the essence of a movie or tv show if we present that perfect image on your homepage and as they say an image be worth a thousand word then maybe just maybe you will give it a try this be yet another way netflix differ from traditional medium offering we don t have one product but over a 100 million different product with one for each of our member with personalized recommendation and personalized visual in previous work we discuss an effort to find the single perfect artwork for each title across all our member through multi armed bandit algorithm we hunt for the good artwork for a title say strange thing that would earn the most play from the large fraction of our member however give the enormous diversity in taste and preference wouldn t it be well if we could find the good artwork for each of our member to highlight the aspect of a title that be specifically relevant to they as inspiration let we explore scenario where personalization of artwork would be meaningful consider the follow example where different member have different view history on the left be three title a member watch in the past to the right of the arrow be the artwork that a member would get for a particular movie that we recommend for they let we consider try to personalize the image we use to depict the movie good will hunt here we might personalize this decision base on how much a member prefer different genre and theme someone who have watch many romantic movie may be interested in good will hunt if we show the artwork contain matt damon and minnie driver whereas a member who have watch many comedy might be draw to the movie if we use the artwork contain robin williams a well know comedian in another scenario let s imagine how the different preference for cast member might influence the personalization of the artwork for the movie pulp fiction a member who watch many movie feature uma thurman would likely respond positively to the artwork for pulp fiction that contain uma meanwhile a fan of john travolta may be more interested in watch pulp fiction if the artwork feature john of course not all the scenario for personalize artwork be this clear and obvious so we don t enumerate such hand derive rule but instead rely on the datum to tell we what signal to use overall by personalize artwork we help each title put its good foot forward for every member and thus improve our member experience at netflix we embrace personalization and algorithmically adapt many aspect of our member experience include the row we select for the homepage the title we select for those row the gallery we display the message we send and so forth each new aspect that we personalize have unique challenge ; personalize the artwork we display be no exception and present different personalization challenge one challenge of image personalization be that we can only select a single piece of artwork to represent each title in each place we present it in contrast typical recommendation setting let we present multiple selection to a member where we can subsequently learn about their preference from the item a member select this mean that image selection be a chicken and egg problem operate in a closed loop if a member play a title it can only come from the image that we decide to present to that member what we seek to understand be when present a specific piece of artwork for a title influence a member to play or not to play a title and when a member would have play a title or not regardless of which image we present therefore artwork personalization sit on top of the traditional recommendation problem and the algorithm need to work in conjunction with each other of course to properly learn how to personalize artwork we need to collect a lot of datum to find signal that indicate when one piece of artwork be significantly well for a member another challenge be to understand the impact of change artwork that we show a member for a title between session do change artwork reduce recognizability of the title and make it difficult to visually locate the title again for example if the member think be interested before but have not yet watch it or do change the artwork itself lead the member to reconsider it due to an improve selection clearly if we find well artwork to present to a member we should probably use it ; but continuous change can also confuse people change image also introduce an attribution problem as it become unclear which image lead a member to be interested in a title next there be the challenge of understand how artwork perform in relation to other artwork we select in the same page or session maybe a bold close up of the main character work for a title on a page because it stand out compare to the other artwork but if every title have a similar image then the page as a whole may not seem as compelling look at each piece of artwork in isolation may not be enough and we need to think about how to select a diverse set of image across title on a page and across a session beyond the artwork for other title the effectiveness of the artwork for a title may depend on what other type of evidence and asset e g synopse trailer etc we also display for that title thus we may need a diverse selection where each can highlight complementary aspect of a title that may be compelling to a member to achieve effective personalization we also need a good pool of artwork for each title this mean that we need several asset where each be engage informative and representative of a title to avoid clickbait the set of image for a title also need to be diverse enough to cover a wide potential audience interested in different aspect of the content after all how engaging and informative a piece of artwork be truly depend on the individual see it therefore we need to have artwork that highlight not only different theme in a title but also different aesthetic our team of artist and designer strive to create image that be diverse across many dimension they also take into consideration the personalization algorithm which will select the image during their creative process for generating artwork finally there be engineering challenge to personalize artwork at scale one challenge be that our member experience be very visual and thus contain a lot of imagery so use personalize selection for each asset mean handle a peak of over 20 million request per second with low latency such a system must be robust fail to properly render the artwork in our ui bring a significantly degrade the experience our personalization algorithm also need to respond quickly when a title launch which mean rapidly learn to personalize in a cold start situation then after launch the algorithm must continuously adapt as the effectiveness of artwork may change over time as both the title evolve through its life cycle and member taste evolve much of the netflix recommendation engine be power by machine learning algorithm traditionally we collect a batch of datum on how our member use the service then we run a new machine learn algorithm on this batch of datum next we test this new algorithm against the current production system through an a b test an a b test help we see if the new algorithm be well than our current production system by try it out on a random subset of member member in group a get the current production experience while member in group b get the new algorithm if member in group b have high engagement with netflix then we roll out the new algorithm to the entire member population unfortunately this batch approach incur regret many member over a long period of time do not benefit from the well experience this be illustrate in the figure below to reduce this regret we move away from batch machine learning and consider online machine learning for artwork personalization the specific online learning framework we use be contextual bandit rather than wait to collect a full batch of datum wait to learn a model and then wait for an a b test to conclude contextual bandit rapidly figure out the optimal personalized artwork selection for a title for each member and context briefly contextual bandit be a class of online learning algorithm that trade off the cost of gather training datum require for learn an unbiased model on an ongoing basis with the benefit of apply the learn model to each member context in our previous unpersonalized image selection work we use non contextual bandit where we find the win image regardless of the context for personalization the member be the context as we expect different member to respond differently to the image a key property of contextual bandit be that they be design to minimize regret at a high level the training datum for a contextual bandit be obtain through the injection of control randomization in the learn model s prediction the randomization scheme can vary in complexity from simple epsilon greedy formulation with uniform randomness to closed loop scheme that adaptively vary the degree of randomization as a function of model uncertainty we broadly refer to this process as data exploration the number of candidate artwork that be available for a title along with the size of the overall population for which the system will be deploy inform the choice of the data exploration strategy with such exploration we need to log information about the randomization for each artwork selection this logging allow we to correct for skewed selection propensity and thereby perform offline model evaluation in an unbiased fashion as describe later exploration in contextual bandit typically have a cost or regret due to the fact that our artwork selection in a member session may not use the predict good image for that session what impact do this randomization have on the member experience and consequently on our metric with over a hundred million member the regret incur by exploration be typically very small and be amortize across our large member base with each member implicitly help provide feedback on artwork for a small portion of the catalog this make the cost of exploration per member negligible which be an important consideration when choose contextual bandit to drive a key aspect of our member experience randomization and exploration with contextual bandit would be less suitable if the cost of exploration be high under our online exploration scheme we obtain a training dataset that record for each member title image tuple whether that selection result in a play of the title or not furthermore we can control the exploration such that artwork selection do not change too often this give a clean attribution of the member s engagement to specific artwork we also carefully determine the label for each observation by look at the quality of engagement to avoid learn a model that recommend clickbait image one that entice a member to start play but ultimately result in low quality engagement in this online learning set we train our contextual bandit model to select the good artwork for each member base on their context we typically have up to a few dozen candidate artwork image per title to learn the selection model we can consider a simplification of the problem by rank image for a member independently across title even with this simplification we can still learn member image preference across title because for every image candidate we have some member who be present with it and engage with the title and some member who be present with it and do not engage these preference can be model to predict for each member title image tuple the probability that the member will enjoy a quality engagement these can be supervise learning model or contextual bandit counterpart with thompson sample linucb or bayesian method that intelligently balance make the good prediction with datum exploration in contextual bandit the context be usually represent as an feature vector provide as input to the model there be many signal we can use as feature for this problem in particular we can consider many attribute of the member the title they ve play the genre of the title interaction of the member with the specific title their country their language preference the device that the member be use the time of day and the day of week since our algorithm select image in conjunction with our personalized recommendation engine we can also use signal regard what our various recommendation algorithm think of the title irrespective of what image be use to represent it an important consideration be that some image be naturally well than other in the candidate pool we observe the overall take rate for all the image in our data exploration which be simply the number of quality play divide by the number of impression our previous work on unpersonalized artwork selection use overall difference in take rate to determine the single good image to select for a whole population in our new contextual personalized model the overall take rate be still important and personalization still recover selection that agree on average with the unpersonalized model s rank the optimal assignment of image artwork to a member be a selection problem to find the good candidate image from a title s pool of available image once the model be train as above we use it to rank the image for each context the model predict the probability of play for a give image in a give a member context we sort a candidate set of image by these probability and pick the one with the high probability that be the image we present to that particular member to evaluate our contextual bandit algorithm prior to deploy they online on real member we can use an offline technique know as replay 1 this method allow we to answer counterfactual question base on the log exploration datum figure 1 in other word we can compare offline what would have happen in historical session under different scenario if we have use different algorithm in an unbiased way replay allow we to see how member would have engage with our title if we have hypothetically present image that be select through a new algorithm rather than the algorithm use in production for image we be interested in several metric particularly the take fraction as describe above figure 2 show how contextual bandit approach help increase the average take fraction across the catalog compare to random selection or non contextual bandit after experiment with many different model offline and find one that have a substantial increase in replay we ultimately run an a b test to compare the most promising personalized contextual bandit against unpersonalized bandit as we suspect the personalization work and generate a significant lift in our core metric we also see a reasonable correlation between what we measure offline in replay and what we see online with the model the online result also produce some interesting insight for example the improvement of personalization be large in case where the member have no prior interaction with the title this make sense because we would expect that the artwork would be more important to someone when a title be less familiar with this approach we ve take our first step in personalize the selection of artwork for our recommendation and across our service this have result in a meaningful improvement in how our member discover new content so we ve roll it out to everyone this project be the first instance of personalize not just what we recommend but also how we recommend to our member but there be many opportunity to expand and improve this initial approach these opportunity include develop algorithm to handle cold start by personalize new image and new title as quickly as possible for example by use technique from computer vision another opportunity be extend this personalization approach across other type of artwork we use and other evidence that describe our title such as synopsis metadata and trailer there be also an even broad problem help artist and designer figure out what new imagery we should add to the set to make a title even more compelling and personalizable if these type of challenge interest you please let we know we be always look for great people to join our team and for these type of project we be especially excite by candidate with machine learning and or computer vision expertise 1 l li w chu j langford and x wang unbiased offline evaluation of contextual bandit base news article recommendation algorithm in proceeding of the fourth acm international conference on web search and data mining new york ny usa 2011 pp 297 306 from a quick cheer to a stand ovation clap to show how much you enjoy this story learn more about how netflix design build and operate our system and engineering organization learn about netflix s world class engineering effort company culture product development and more
Michael Jordan,34K,16,https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7?source=tag_archive---------0----------------,artificial intelligence — the revolution hasn t happen yet,artificial intelligence ai be the mantra of the current era the phrase be intone by technologist academician journalist and venture capitalist alike as with many phrase that cross over from technical academic field into general circulation there be significant misunderstanding accompany the use of the phrase but this be not the classical case of the public not understand the scientist — here the scientist be often as befuddle as the public the idea that our era be somehow see the emergence of an intelligence in silicon that rival our own entertain all of we — enthral we and frightening we in equal measure and unfortunately it distract we there be a different narrative that one can tell about the current era consider the follow story which involve human computer datum and life or death decision but where the focus be something other than intelligence in silicon fantasy when my spouse be pregnant 14 year ago we have an ultrasound there be a geneticist in the room and she point out some white spot around the heart of the fetus those be marker for down syndrome she note and your risk have now go up to 1 in 20 she far let we know that we could learn whether the fetus in fact have the genetic modification underlie down syndrome via an amniocentesis but amniocentesis be risky — the risk of kill the fetus during the procedure be roughly 1 in 300 be a statistician I determine to find out where these number be come from to cut a long story short I discover that a statistical analysis have be do a decade previously in the uk where these white spot which reflect calcium buildup be indeed establish as a predictor of down syndrome but I also notice that the imaging machine use in our test have a few hundred more pixel per square inch than the machine use in the uk study I go back to tell the geneticist that I believe that the white spot be likely false positive — that they be literally white noise she say ah that explain why we start see an uptick in down syndrome diagnose a few year ago ; it s when the new machine arrive we didn t do the amniocentesis and a healthy girl be bear a few month later but the episode trouble I particularly after a back of the envelope calculation convince I that many thousand of people have get that diagnosis that same day worldwide that many of they have opt for amniocentesis and that a number of baby have die needlessly and this happen day after day until it somehow get fix the problem that this episode reveal wasn t about my individual medical care ; it be about a medical system that measure variable and outcome in various place and time conduct statistical analysis and make use of the result in other place and time the problem have to do not just with datum analysis per se but with what database researcher call provenance — broadly where do datum arise what inference be draw from the datum and how relevant be those inference to the present situation while a train human might be able to work all of this out on a case by case basis the issue be that of design a planetary scale medical system that could do this without the need for such detailed human oversight I m also a computer scientist and it occur to I that the principle need to build planetary scale inference and decision make system of this kind blend computer science with statistic and take into account human utility be nowhere to be find in my education and it occur to I that the development of such principle — which will be need not only in the medical domain but also in domain such as commerce transportation and education — be at least as important as those of build ai system that can dazzle we with their game playing or sensorimotor skill whether or not we come to understand intelligence any time soon we do have a major challenge on our hand in bring together computer and human in way that enhance human life while this challenge be view by some as subservient to the creation of artificial intelligence it can also be view more prosaically — but with no less reverence — as the creation of a new branch of engineering much like civil engineering and chemical engineering in decade past this new discipline aim to corral the power of a few key idea bring new resource and capability to people and do so safely whereas civil engineering and chemical engineering be build on physics and chemistry this new engineering discipline will be build on idea that the precede century give substance to — idea such as information algorithm data uncertainty computing inference and optimization moreover since much of the focus of the new discipline will be on datum from and about human its development will require perspective from the social science and humanity while the building block have begin to emerge the principle for put these block together have not yet emerge and so the block be currently be put together in ad hoc way thus just as human build building and bridge before there be civil engineering human be proceed with the building of societal scale inference and decision make system that involve machine human and the environment just as early building and bridge sometimes fall to the ground — in unforeseen way and with tragic consequence — many of our early societal scale inference and decision make system be already expose serious conceptual flaw and unfortunately we be not very good at anticipate what the next emerge serious flaw will be what we re miss be an engineering discipline with its principle of analysis and design the current public dialog about these issue too often use ai as an intellectual wildcard one that make it difficult to reason about the scope and consequence of emerge technology let we begin by consider more carefully what ai have be use to refer to both recently and historically most of what be be call ai today particularly in the public sphere be what have be call machine learning ml for the past several decade ml be an algorithmic field that blend idea from statistic computer science and many other discipline see below to design algorithm that process datum make prediction and help make decision in term of impact on the real world ml be the real thing and not just recently indeed that ml would grow into massive industrial relevance be already clear in the early 1990 and by the turn of the century forward look company such as amazon be already use ml throughout their business solving mission critical back end problem in fraud detection and supply chain prediction and build innovative consumer facing service such as recommendation system as dataset and computing resource grow rapidly over the ensue two decade it become clear that ml would soon power not only amazon but essentially any company in which decision could be tie to large scale datum new business model would emerge the phrase datum science begin to be use to refer to this phenomenon reflect the need of ml algorithms expert to partner with database and distribute system expert to build scalable robust ml system and reflect the large social and environmental scope of the result system this confluence of idea and technology trend have be rebrande as ai over the past few year this rebranding be worthy of some scrutiny historically the phrase ai be coin in the late 1950 s to refer to the heady aspiration of realize in software and hardware an entity possess human level intelligence we will use the phrase human imitative ai to refer to this aspiration emphasize the notion that the artificially intelligent entity should seem to be one of we if not physically at least mentally whatever that might mean this be largely an academic enterprise while relate academic field such as operation research statistic pattern recognition information theory and control theory already exist and be often inspire by human intelligence and animal intelligence these field be arguably focus on low level signal and decision the ability of say a squirrel to perceive the three dimensional structure of the forest it live in and to leap among its branch be inspirational to these field ai be mean to focus on something different — the high level or cognitive capability of human to reason and to think sixty year later however high level reasoning and think remain elusive the development which be now be call ai arise mostly in the engineering field associate with low level pattern recognition and movement control and in the field of statistic — the discipline focus on find pattern in datum and on make well found prediction test of hypothesis and decision indeed the famous backpropagation algorithm that be rediscover by david rumelhart in the early 1980 and which be now view as be at the core of the so call ai revolution first arise in the field of control theory in the 1950 and 1960 one of its early application be to optimize the thrust of the apollo spaceship as they head towards the moon since the 1960s much progress have be make but it have arguably not come about from the pursuit of human imitative ai rather as in the case of the apollo spaceship these idea have often be hide behind the scene and have be the handiwork of researcher focus on specific engineering challenge although not visible to the general public research and system building in area such as document retrieval text classification fraud detection recommendation system personalize search social network analysis planning diagnostic and a b testing have be a major success — these be the advance that have power company such as google netflix facebook and amazon one could simply agree to refer to all of this as ai and indeed that be what appear to have happen such labeling may come as a surprise to optimization or statistic researcher who wake up to find themselves suddenly refer to as ai researcher but labeling of researcher aside the big problem be that the use of this single ill define acronym prevent a clear understanding of the range of intellectual and commercial issue at play the past two decade have see major progress — in industry and academia — in a complementary aspiration to human imitative ai that be often refer to as intelligence augmentation ia here computation and datum be use to create service that augment human intelligence and creativity a search engine can be view as an example of ia it augment human memory and factual knowledge as can natural language translation it augment the ability of a human to communicate computing base generation of sound and image serve as a palette and creativity enhancer for artist while service of this kind could conceivably involve high level reasoning and think currently they don t — they mostly perform various kind of string matching and numerical operation that capture pattern that human can make use of hope that the reader will tolerate one last acronym let we conceive broadly of a discipline of intelligent infrastructure ii whereby a web of computation datum and physical entity exist that make human environment more supportive interesting and safe such infrastructure be begin to make its appearance in domain such as transportation medicine commerce and finance with vast implication for individual human and society this emergence sometimes arise in conversation about an internet of thing but that effort generally refer to the mere problem of get thing onto the internet — not to the far grander set of challenge associate with these thing capable of analyze those datum stream to discover fact about the world and interact with human and other thing at a far high level of abstraction than mere bit for example return to my personal anecdote we might imagine live our life in a societal scale medical system that set up datum flow and datum analysis flow between doctor and device position in and around human body thereby able to aid human intelligence in make diagnosis and provide care the system would incorporate information from cell in the body dna blood test environment population genetic and the vast scientific literature on drug and treatment it would not just focus on a single patient and a doctor but on relationship among all human — just as current medical testing allow experiment do on one set of human or animal to be bring to bear in the care of other human it would help maintain notion of relevance provenance and reliability in the way that the current banking system focus on such challenge in the domain of finance and payment and while one can foresee many problem arise in such a system — involve privacy issue liability issue security issue etc — these problem should properly be view as challenge not show stopper we now come to a critical issue be work on classical human imitative ai the good or only way to focus on these large challenge some of the most herald recent success story of ml have in fact be in area associate with human imitative ai — area such as computer vision speech recognition game playing and robotic so perhaps we should simply await further progress in domain such as these there be two point to make here first although one would not know it from read the newspaper success in human imitative ai have in fact be limit — we be very far from realize human imitative ai aspiration unfortunately the thrill and fear of make even limited progress on human imitative ai give rise to level of over exuberance and medium attention that be not present in other area of engineering second and more importantly success in these domain be neither sufficient nor necessary to solve important ia and ii problem on the sufficiency side consider self drive car for such technology to be realize a range of engineering problem will need to be solve that may have little relationship to human competency or human lack of competency the overall transportation system an ii system will likely more closely resemble the current air traffic control system than the current collection of loosely couple forward face inattentive human driver it will be vastly more complex than the current air traffic control system specifically in its use of massive amount of datum and adaptive statistical modeling to inform fine grain decision it be those challenge that need to be in the forefront and in such an effort a focus on human imitative ai may be a distraction as for the necessity argument it be sometimes argue that the human imitative ai aspiration subsume ia and ii aspiration because a human imitative ai system would not only be able to solve the classical problem of ai as embody e g in the ture test but it would also be our good bet for solve ia and ii problem such an argument have little historical precedent do civil engineering develop by envisage the creation of an artificial carpenter or bricklayer should chemical engineering have be frame in term of create an artificial chemist even more polemically if our goal be to build chemical factory should we have first create an artificial chemist who would have then work out how to build a chemical factory a related argument be that human intelligence be the only kind of intelligence that we know and that we should aim to mimic it as a first step but human be in fact not very good at some kind of reasoning — we have our lapse bias and limitation moreover critically we do not evolve to perform the kind of large scale decision make that modern ii system must face nor to cope with the kind of uncertainty that arise in ii contexts one could argue that an ai system would not only imitate human intelligence but also correct it and would also scale to arbitrarily large problem but we be now in the realm of science fiction — such speculative argument while entertain in the setting of fiction should not be our principal strategy go forward in the face of the critical ia and ii problem that be begin to emerge we need to solve ia and ii problem on their own merit not as a mere corollary to a human imitative ai agenda it be not hard to pinpoint algorithmic and infrastructure challenge in ii system that be not central theme in human imitative ai research ii system require the ability to manage distribute repository of knowledge that be rapidly change and be likely to be globally incoherent such system must cope with cloud edge interaction in make timely distribute decision and they must deal with long tail phenomenon whereby there be lot of datum on some individual and little datum on most individual they must address the difficulty of share datum across administrative and competitive boundary finally and of particular importance ii system must bring economic idea such as incentive and pricing into the realm of the statistical and computational infrastructure that link human to each other and to value good such ii system can be view as not merely provide a service but as create market there be domain such as music literature and journalism that be cry out for the emergence of such market where datum analysis link producer and consumer and this must all be do within the context of evolve societal ethical and legal norm of course classical human imitative ai problem remain of great interest as well however the current focus on do ai research via the gathering of datum the deployment of deep learning infrastructure and the demonstration of system that mimic certain narrowly define human skill — with little in the way of emerge explanatory principle — tend to deflect attention from major open problem in classical ai these problem include the need to bring meaning and reasoning into system that perform natural language process the need to infer and represent causality the need to develop computationally tractable representation of uncertainty and the need to develop system that formulate and pursue long term goal these be classical goal in human imitative ai but in the current hubbub over the ai revolution it be easy to forget that they be not yet solve ia will also remain quite essential because for the foreseeable future computer will not be able to match human in their ability to reason abstractly about real world situation we will need well think out interaction of human and computer to solve our most pressing problem and we will want computer to trigger new level of human creativity not replace human creativity whatever that might mean it be john mccarthy while a professor at dartmouth and soon to take a position at mit who coin the term ai apparently to distinguish his bud research agenda from that of norbert wiener then an old professor at mit wiener have coin cybernetic to refer to his own vision of intelligent system — a vision that be closely tie to operation research statistic pattern recognition information theory and control theory mccarthy on the other hand emphasize the tie to logic in an interesting reversal it be wiener s intellectual agenda that have come to dominate in the current era under the banner of mccarthy s terminology this state of affair be surely however only temporary ; the pendulum swing more in ai than in most field but we need to move beyond the particular historical perspective of mccarthy and wiener we need to realize that the current public dialog on ai — which focus on a narrow subset of industry and a narrow subset of academia — risk blind we to the challenge and opportunity that be present by the full scope of ai ia and ii this scope be less about the realization of science fiction dream or nightmare of super human machine and more about the need for human to understand and shape technology as it become ever more present and influential in their daily life moreover in this understanding and shape there be a need for a diverse set of voice from all walk of life not merely a dialog among the technologically attune focus narrowly on human imitative ai prevent an appropriately wide range of voice from be hear while industry will continue to drive many development academia will also continue to play an essential role not only in provide some of the most innovative technical idea but also in bring researcher from the computational and statistical discipline together with researcher from other discipline whose contribution and perspective be sorely need — notably the social science the cognitive science and the humanity on the other hand while the humanity and the science be essential as we go forward we should also not pretend that we be talk about something other than an engineering effort of unprecedented scale and scope — society be aim to build new kind of artifact these artifact should be build to work as claim we do not want to build system that help we with medical treatment transportation option and commercial opportunity to find out after the fact that these system don t really work — that they make error that take their toll in term of human life and happiness in this regard as I have emphasize there be an engineering discipline yet to emerge for the datum focus and learn focus field as exciting as these latter field appear to be they can not yet be view as constitute an engineering discipline moreover we should embrace the fact that what we be witness be the creation of a new branch of engineer the term engineering be often invoke in a narrow sense — in academia and beyond — with overtone of cold affectless machinery and negative connotation of loss of control by human but an engineering discipline can be what we want it to be in the current era we have a real opportunity to conceive of something historically new — a human centric engineering discipline I will resist give this emerge discipline a name but if the acronym ai continue to be use as placeholder nomenclature go forward let s be aware of the very real limitation of this placeholder let s broaden our scope tone down the hype and recognize the serious challenge ahead michael i jordan from a quick cheer to a stand ovation clap to show how much you enjoy this story michael i jordan be a professor in the department of electrical engineering and computer science and the department of statistic at uc berkeley
Blaise Aguera y Arcas,8.7K,15,https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477?source=tag_archive---------1----------------,do algorithm reveal sexual orientation or just expose our stereotype,by blaise agüera y arcas alexander todorov and margaret mitchell a study claim that artificial intelligence can infer sexual orientation from facial image cause a medium uproar in the fall of 2017 the economist feature this work on the cover of their september 9th magazine ; on the other hand two major lgbtq organization the human right campaign and glaad immediately label it junk science michal kosinski who co author the study with fellow researcher yilun wang initially express surprise call the critique knee jerk reaction however he then proceed to make even bolder claim that such ai algorithm will soon be able to measure the intelligence political orientation and criminal inclination of people from their facial image alone kosinski s controversial claim be nothing new last year two computer scientist from china post a non peer review paper online in which they argue that their ai algorithm correctly categorize criminal with nearly 90 % accuracy from a government i d photo alone technology startup have also begin to crop up claim that they can profile people s character from their facial image these development have prompt the three of we to collaborate early in the year on a medium essay physiognomy s new clothe to confront claim that ai face recognition reveal deep character trait we describe how the junk science of physiognomy have root go back into antiquity with practitioner in every era resurrect belief base on prejudice use the new methodology of the age in the 19th century this include anthropology and psychology ; in the 20th genetic and statistical analysis ; and in the 21st artificial intelligence in late 2016 the paper motivate our physiognomy essay seem well outside the mainstream in tech and academia but as in other area of discourse what recently feel like a fringe position must now be address head on kosinski be a faculty member of stanford s graduate school of business and this new study have be accept for publication in the respected journal of personality and social psychology much of the ensue scrutiny have focus on ethic implicitly assume that the science be valid we will focus on the science the author train and test their sexual orientation detector use 35 326 image from public profile on a us date website composite image of the lesbian gay and straight man and woman in the sample reveal a great deal about the information available to the algorithm clearly there be difference between these four composite face wang and kosinski assert that the key difference be in physiognomy mean that a sexual orientation tend to go along with a characteristic facial structure however we can immediately see that some of these difference be more superficial for example the average straight woman appear to wear eyeshadow while the average lesbian do not glass be clearly visible on the gay man and to a less extent on the lesbian while they seem absent in the heterosexual composite might it be the case that the algorithm s ability to detect orientation have little to do with facial structure but be due rather to pattern in groom presentation and lifestyle we conduct a survey of 8 000 american use amazon s mechanical turk crowdsourcing platform to see if we could independently confirm these pattern ask 77 yes no question such as do you wear eyeshadow do you wear glass and do you have a beard as well as question about gender and sexual orientation the result show that lesbian indeed use eyeshadow much less than straight woman do gay man and woman do both wear glass more and young opposite sex attract man be considerably more likely to have prominent facial hair than their gay or same sex attract peer break down the answer by the age of the respondent can provide a rich and clear view of the datum than any single statistic in the follow figure we show the proportion of woman who answer yes to do you ever use makeup top and do you wear eyeshadow bottom average over 6 year age interval the blue curve represent strictly opposite sex attract woman a nearly identical set to those who answer yes to be you heterosexual or straight ; the cyan curve represent woman who answer yes to either or both of be you sexually attract to woman and be you romantically attract to woman ; and the red curve represent woman who answer yes to be you homosexual gay or lesbian 1 the shaded region around each curve show 68 % confidence interval 2 the pattern reveal here be intuitive ; it win t be break news to most that straight woman tend to wear more makeup and eyeshadow than same sex attract and even more so lesbian identify woman on the other hand these curve also show we how often these stereotype be violate that same sex attract man of most age wear glass significantly more than exclusively opposite sex attract man do might be a bit less obvious but this trend be equally clear 3 a proponent of physiognomy might be tempt to guess that this be somehow relate to difference in visual acuity between these population of man however ask the question do you like how you look in glass reveal that this be likely more of a stylistic choice same sex attract woman also report wear glass more as well as like how they look in glass more across a range of age one can also see how opposite sex attract woman under the age of 40 wear contact lense significantly more than same sex attract woman despite report that they have a vision defect at roughly the same rate far illustrate how the difference be drive by an aesthetic preference 4 similar analysis show that young same sex attract man be much less likely to have hairy face than opposite sex attract man serious facial hair in our plot be define as answer yes to have a goatee beard or moustache but no to stubble overall opposite sex attract man in our sample be 35 % more likely to have serious facial hair than same sex attract man and for man under the age of 31 who be overrepresente on date website this rise to 75 % wang and kosinski speculate in their paper that the faintness of the beard and moustache in their gay male composite might be connect with prenatal underexposure to androgen male hormone result in a feminize effect hence sparser facial hair the fact that we see a cohort of same sex attract man in their 40 who have just as much facial hair as opposite sex attract man suggest a different story in which fashion trend and cultural norm play the dominant role in choice about facial hair among man not differ exposure to hormone early in development the author of the paper additionally note that the heterosexual male composite appear to have dark skin than the other three composite our survey confirm that opposite sex attract man consistently self report have a tan face yes to be your face tan slightly more often than same sex attract man once again wang and kosinski reach for a hormonal explanation writing while the brightness of the facial image might be drive by many factor previous research find that testosterone stimulate melanocyte structure and function lead to a dark skin however a simple answer be suggest by the response to the question do you work outdoors overall opposite sex attract man be 29 % more likely to work outdoors and among man under 31 this rise to 39 % previous research have find that increase exposure to sunlight lead to dark skin 5 none of these result prove that there be no physiological basis for sexual orientation ; in fact ample evidence show we that orientation run much deep than a choice or a lifestyle in a critique aim in part at fraudulent conversion therapy program united states surgeon general david satcher write in a 2001 report sexual orientation be usually determine by adolescence if not early and there be no valid scientific evidence that sexual orientation can be change it follow that if we dig deeply enough into human physiology and neuroscience we will eventually find reliable correlate and maybe even the origin of sexual orientation in our survey we also find some evidence of outwardly visible correlate of orientation that be not cultural perhaps most strikingly very tall woman be overrepresente among lesbian identify respondent 6 however while this be interesting it s very far from a good predictor of woman s sexual orientation makeup and eyeshadow do much well the way wang and kosinski measure the efficacy of their ai gaydar be equivalent to choose a straight and a gay or lesbian face image both from datum hold out during the training process and ask how often the algorithm correctly guess which be which 50 % performance would be no well than random chance for woman guess that the taller of the two be the lesbian achieve only 51 % accuracy — barely above random chance this be because despite the statistically meaningful overrepresentation of tall woman among the lesbian population the great majority of lesbian be not unusually tall by contrast the performance measure in the paper 81 % for gay man and 71 % for lesbian woman seem impressive 7 consider however that we can achieve comparable result with trivial model base only on a handful of yes no survey question about presentation for example for pair of woman one of whom be lesbian the follow not exactly superhuman algorithm be on average 63 % accurate if neither or both woman wear eyeshadow flip a coin ; otherwise guess that the one who wear eyeshadow be straight and the other lesbian add six more yes no question about presentation do you ever use makeup do you have long hair do you have short hair do you ever use colored lipstick do you like how you look in glass and do you work outdoors as additional signal raise the performance to 70 % 8 give how many more detail about presentation be available in a face image 71 % performance no long seem so impressive several study include a recent one in the journal of sex research have show that human judge gaydar be no more reliable than a coin flip when the judgement be base on picture take under well control condition head pose lighting glass makeup etc it s well than chance if these variable be not control for because a person s presentation — especially if that person be out — involve social signal we signal our orientation and many other kind of status presumably in order to attract the kind of attention we want and to fit in with people like we 9 wang and kosinski argue against this interpretation on the ground that their algorithm work on facebook selfie of openly gay man as well as date website selfie the issue however be not whether the image come from a date website or facebook but whether they be self post or take under standardized condition most people present themselves in way that have be calibrate over many year of medium consumption observe other look in the mirror and gauge social reaction in one of the early gaydar study use social medium participant could categorize gay man with about 58 % accuracy ; but when the researcher use facebook image of gay and heterosexual man post by their friend still far from a perfect control the accuracy drop to 52 % if subtle bias in image quality expression and grooming can be pick up on by human these bias can also be detect by an ai algorithm while wang and kosinski acknowledge grooming and style they believe that the chief difference between their composite image relate to face shape argue that gay man s face be more feminine narrow jaw long nose large forehead while lesbian face be more masculine large jaw shorter nose small forehead as with less facial hair on gay man and dark skin on straight man they suggest that the mechanism be gender atypical hormonal exposure during development this echo a widely discredit 19th century model of homosexuality sexual inversion more likely heterosexual man tend to take selfie from slightly below which will have the apparent effect of enlarge the chin shorten the nose shrink the forehead and attenuate the smile see our selfie below this view emphasize dominance — or perhaps more benignly an expectation that the viewer will be short on the other hand as a wedding photographer note in her blog when you shoot from above your eye look big which be generally attractive — especially for woman this may be a heteronormative assessment when a face be photograph from below the nostril be prominent while high shooting angle de emphasize and eventually conceal they altogether look again at the composite image we can see that the heterosexual male face have more pronounced dark spot corresponding to the nostril than the gay male while the opposite be true for the female face this be consistent with a pattern of heterosexual man on average shooting from below heterosexual woman from above as the wedding photographer suggest and gay man and lesbian woman from directly in front a similar pattern be evident in the eyebrow shoot from above make they look more v shape but their apparent shape become flatter and eventually caret shape ^ as the camera be lower shoot from below also make the outer corner of the eye appear low in short the change in the average position of facial landmark be consistent with what we would expect to see from differ selfie angle the ambiguity between shoot angle and the real physical size of facial feature be hard to fully disentangle from a two dimensional image both for a human viewer and for an algorithm although the author be use face recognition technology design to try to cancel out all effect of head pose lighting grooming and other variable not intrinsic to the face we can confirm that this doesn t work perfectly ; that s why multiple distinct image of a person help when group photo by subject in google photo and why a person may initially appear in more than one group tom white a researcher at victoria university in new zealand have experiment with the same facial recognition engine kosinski and wang use vgg face and have find that its output vary systematically base on variable like smile and head pose when he train a classifier base on vgg face s output to distinguish a happy expression from a neutral one it get the answer right 92 % of the time — which be significant give that the heterosexual female composite have a much more pronounced smile change in head pose might be even more reliably detectable ; for 576 test image a classifier be able to pick out the one face to the right with 100 % accuracy in summary we have show how the obvious difference between lesbian or gay and straight face in selfie relate to groom presentation and lifestyle — that be difference in culture not in facial structure these difference include we ve demonstrate that just a handful of yes no question about these variable can do nearly as good a job at guess orientation as supposedly sophisticated facial recognition ai far the current generation of facial recognition remain sensitive to head pose and facial expression therefore — at least at this point — it s hard to credit the notion that this ai be in some way superhuman at out we base on subtle but unalterable detail of our facial structure this doesn t negate the privacy concern the author and various commentator have raise but it emphasize that such concern relate less to ai per se than to mass surveillance which be troubling regardless of the technology use even when as in the day of the stasi in east germany these be nothing but paper file and audiotape like computer or the internal combustion engine ai be a general purpose technology that can be use to automate a great many task include one that should not be undertake in the first place we be hopeful about the confluence of new powerful ai technology with social science but not because we believe in revive the 19th century research program of infer people s inner character from their outer appearance rather we believe ai be an essential tool for understand pattern in human culture and behavior it can expose stereotype inherent in everyday language it can reveal uncomfortable truth as in google s work with the geena davis institute where our face gender classifier establish that man be see and hear nearly twice as often as woman in hollywood movie yet female lead film outperform other at the box office make social progress and hold ourselves to account be more difficult without such hard evidence even when it only confirm our suspicion two of us margaret mitchell and blaise agüera y arca be research scientist specialize in machine learning and ai at google ; agüera y arcas lead a team that include deep learning apply to face recognition and power face group in google photo alex todorov be a professor in the psychology department at princeton where he direct the social perception lab he be the author of face value the irresistible influence of first impression 1 this wording be base on several large national survey which we be able to use to sanity check our number about 6 % of respondent identify as homosexual gay or lesbian and 85 % as heterosexual about 4 % of all gender be exclusively same sex attract of the man 10 % be either sexually or romantically same sex attract and of the woman 20 % just under 1 % of respondent be trans and about 2 % identify with both or neither of the pronoun she and he these number be broadly consistent with other survey especially when consider as a function of age the mechanical turk population skew somewhat young than the overall population of the us and consistent with other study our datum show that young people be far more likely to identify non heteronormatively 2 these be wide for same sex attract and lesbian woman because they be minority population result in a large sampling error the same hold for old people in our sample 3 for the remainder of the plot we stick to opposite sex attract and same sex attract as the count be high and the error bar therefore small ; these category be also somewhat less culturally freight since they rely on question about attraction rather than identity as with eyeshadow and makeup the effect be similar and often even large when compare heterosexual identifying with lesbian or gay identify people 4 although we didn t test this explicitly slightly different rate of laser correction surgery seem a likely cause of the small but grow disparity between opposite sex attract and same sex attract woman who answer yes to the vision defect question as they age 5 this finding may prompt the further question why do more opposite sex attract man work outdoors this be not address by any of our survey question but hopefully the other evidence present here will discourage an essentialist assumption such as straight man be just more outdoorsy without the evidence of a control study that can support the leap from correlation to cause such explanation be a form of logical fallacy sometimes call a just so story an unverifiable narrative explanation for a cultural practice 6 of the 253 lesbian identify woman in the sample 5 or 2 % be over six foot and 25 or 10 % be over 5 9 out of 3 333 heterosexual woman woman who answer yes to be you heterosexual or straight only 16 or 0 5 % be over six foot and 152 or 5 % be over 5 9 7 they note that these figure rise to 91 % for man and 83 % for woman if 5 image be consider 8 these result be base on the simple possible machine learning technique a linear classifier the classifier be train on a randomly choose 70 % of the datum with the remain 30 % of the datum hold out for test over 500 repetition of this procedure the error be 69 53 % ± 2 98 % with the same number of repetition and holdout base the decision on height alone give an error of 51 08 % ± 3 27 % and base it on eyeshadow alone yield 62 96 % ± 2 39 % 9 a longstanding body of work e g goffman s the presentation of self in everyday life 1959 and jones and pittman s toward a general theory of strategic self presentation 1982 delf more deeply into why we present ourselves the way we do both for instrumental reason status power attraction and because our presentation inform and be inform by how we conceive of our social self from a quick cheer to a stand ovation clap to show how much you enjoy this story blaise aguera y arcas lead google s ai group in seattle he found seadragon and be one of the creator of photosynth at microsoft
James Le,18.4K,11,https://towardsdatascience.com/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-dde4edffae11?source=tag_archive---------2----------------,a tour of the top 10 algorithm for machine learning newbie,in machine learn there s something call the no free lunch theorem in a nutshell it state that no one algorithm work well for every problem and it s especially relevant for supervised learning I e predictive modeling for example you can t say that neural network be always well than decision tree or vice versa there be many factor at play such as the size and structure of your dataset as a result you should try many different algorithm for your problem while use a hold out test set of datum to evaluate performance and select the winner of course the algorithm you try must be appropriate for your problem which be where pick the right machine learning task come in as an analogy if you need to clean your house you might use a vacuum a broom or a mop but you wouldn t bust out a shovel and start dig however there be a common principle that underlie all supervised machine learning algorithm for predictive modeling this be a general learning task where we would like to make prediction in the future y give new example of input variable x we don t know what the function f look like or its form if we do we would use it directly and we would not need to learn it from datum use machine learning algorithm the most common type of machine learning be to learn the mapping y = f x to make prediction of y for new x this be call predictive modeling or predictive analytic and our goal be to make the most accurate prediction possible for machine learning newbie who be eager to understand the basic of machine learning here be a quick tour on the top 10 machine learning algorithm use by datum scientist linear regression be perhaps one of the most well know and well understand algorithm in statistic and machine learn predictive modeling be primarily concerned with minimize the error of a model or make the most accurate prediction possible at the expense of explainability we will borrow reuse and steal algorithm from many different field include statistic and use they towards these end the representation of linear regression be an equation that describe a line that good fit the relationship between the input variable x and the output variable y by find specific weighting for the input variable call coefficient b for example y = b0 + b1 * x we will predict y give the input x and the goal of the linear regression learn algorithm be to find the value for the coefficient b0 and b1 different technique can be use to learn the linear regression model from datum such as a linear algebra solution for ordinary least square and gradient descent optimization linear regression have be around for more than 200 year and have be extensively study some good rule of thumb when use this technique be to remove variable that be very similar correlated and to remove noise from your datum if possible it be a fast and simple technique and good first algorithm to try logistic regression be another technique borrow by machine learning from the field of statistic it be the go to method for binary classification problem problem with two class value logistic regression be like linear regression in that the goal be to find the value for the coefficient that weight each input variable unlike linear regression the prediction for the output be transform use a non linear function call the logistic function the logistic function look like a big s and will transform any value into the range 0 to 1 this be useful because we can apply a rule to the output of the logistic function to snap value to 0 and 1 e g if less than 0 5 then output 1 and predict a class value because of the way that the model be learn the prediction make by logistic regression can also be use as the probability of a give data instance belong to class 0 or class 1 this can be useful for problem where you need to give more rationale for a prediction like linear regression logistic regression do work well when you remove attribute that be unrelated to the output variable as well as attribute that be very similar correlated to each other it s a fast model to learn and effective on binary classification problem logistic regression be a classification algorithm traditionally limit to only two class classification problem if you have more than two class then the linear discriminant analysis algorithm be the preferred linear classification technique the representation of lda be pretty straight forward it consist of statistical property of your datum calculate for each class for a single input variable this include prediction be make by calculate a discriminate value for each class and make a prediction for the class with the large value the technique assume that the data have a gaussian distribution bell curve so it be a good idea to remove outlier from your datum before hand it s a simple and powerful method for classification predictive modeling problem decision tree be an important type of algorithm for predictive modeling machinelearne the representation of the decision tree model be a binary tree this be your binary tree from algorithm and data structure nothing too fancy each node represent a single input variable x and a split point on that variable assume the variable be numeric the leaf node of the tree contain an output variable y which be use to make a prediction prediction be make by walk the split of the tree until arrive at a leaf node and output the class value at that leaf node tree be fast to learn and very fast for make prediction they be also often accurate for a broad range of problem and do not require any special preparation for your datum naive baye be a simple but surprisingly powerful algorithm for predictive modeling the model be comprise of two type of probability that can be calculate directly from your training data 1 the probability of each class ; and 2 the conditional probability for each class give each x value once calculate the probability model can be use to make prediction for new datum use baye theorem when your data be real value it be common to assume a gaussian distribution bell curve so that you can easily estimate these probability naive baye be call naive because it assume that each input variable be independent this be a strong assumption and unrealistic for real datum nevertheless the technique be very effective on a large range of complex problem the knn algorithm be very simple and very effective the model representation for knn be the entire training dataset simple right prediction be make for a new data point by search through the entire training set for the k most similar instance the neighbor and summarize the output variable for those k instance for regression problem this might be the mean output variable for classification problem this might be the mode or most common class value the trick be in how to determine the similarity between the data instance the simple technique if your attribute be all of the same scale all in inch for example be to use the euclidean distance a number you can calculate directly base on the difference between each input variable knn can require a lot of memory or space to store all of the datum but only perform a calculation or learn when a prediction be need just in time you can also update and curate your training instance over time to keep prediction accurate the idea of distance or closeness can break down in very high dimension lot of input variable which can negatively affect the performance of the algorithm on your problem this be call the curse of dimensionality it suggest you only use those input variable that be most relevant to predict the output variable a downside of k near neighbor be that you need to hang on to your entire training dataset the learn vector quantization algorithm or lvq for short be an artificial neural network algorithm that allow you to choose how many training instance to hang onto and learn exactly what those instance should look like the representation for lvq be a collection of codebook vector these be select randomly in the beginning and adapt to well summarize the training dataset over a number of iteration of the learning algorithm after learn the codebook vector can be use to make prediction just like k near neighbor the most similar neighbor good matching codebook vector be find by calculate the distance between each codebook vector and the new data instance the class value or real value in the case of regression for the good matching unit be then return as the prediction good result be achieve if you rescale your datum to have the same range such as between 0 and 1 if you discover that knn give good result on your dataset try use lvq to reduce the memory requirement of store the entire training dataset support vector machine be perhaps one of the most popular and talk about machine learning algorithm a hyperplane be a line that split the input variable space in svm a hyperplane be select to well separate the point in the input variable space by their class either class 0 or class 1 in two dimension you can visualize this as a line and let s assume that all of our input point can be completely separate by this line the svm learn algorithm find the coefficient that result in the good separation of the class by the hyperplane the distance between the hyperplane and the close datum point be refer to as the margin the good or optimal hyperplane that can separate the two class be the line that have the large margin only these point be relevant in define the hyperplane and in the construction of the classifier these point be call the support vector they support or define the hyperplane in practice an optimization algorithm be use to find the value for the coefficient that maximize the margin svm might be one of the most powerful out of the box classifier and worth try on your dataset random forest be one of the most popular and most powerful machine learning algorithm it be a type of ensemble machine learning algorithm call bootstrap aggregation or bag the bootstrap be a powerful statistical method for estimate a quantity from a data sample such as a mean you take lot of sample of your data calculate the mean then average all of your mean value to give you a well estimation of the true mean value in bag the same approach be use but instead for estimate entire statistical model most commonly decision tree multiple sample of your training datum be take then model be construct for each datum sample when you need to make a prediction for new datum each model make a prediction and the prediction be average to give a well estimate of the true output value random forest be a tweak on this approach where decision tree be create so that rather than select optimal split point suboptimal split be make by introduce randomness the model create for each sample of the datum be therefore more different than they otherwise would be but still accurate in their unique and different way combine their prediction result in a well estimate of the true underlying output value if you get good result with an algorithm with high variance like decision tree you can often get well result by bag that algorithm boost be an ensemble technique that attempt to create a strong classifier from a number of weak classifier this be do by build a model from the training datum then create a second model that attempt to correct the error from the first model model be add until the training set be predict perfectly or a maximum number of model be add adaboost be the first really successful boost algorithm develop for binary classification it be the good starting point for understand boost modern boost method build on adaboost most notably stochastic gradient boost machine adaboost be use with short decision tree after the first tree be create the performance of the tree on each training instance be use to weight how much attention the next tree that be create should pay attention to each training instance training datum that be hard to predict be give more weight whereas easy to predict instance be give less weight model be create sequentially one after the other each update the weight on the training instance that affect the learning perform by the next tree in the sequence after all the tree be build prediction be make for new datum and the performance of each tree be weight by how accurate it be on training datum because so much attention be put on correct mistake by the algorithm it be important that you have clean datum with outlier remove a typical question ask by a beginner when face a wide variety of machine learning algorithm be which algorithm should I use the answer to the question vary depend on many factor include 1 the size quality and nature of datum ; 2 the available computational time ; 3 the urgency of the task ; and 4 what you want to do with the datum even an experienced data scientist can not tell which algorithm will perform the good before try different algorithm although there be many other machine learning algorithm these be the most popular one if you re a newbie to machine learn these would be a good starting point to learn — — if you enjoy this piece I d love it if you hit the clap button 👏 so other might stumble upon it you can find my own code on github and more of my writing and project at https jameskle com you can also follow I on twitter email I directly or find I on linkedin from a quick cheer to a stand ovation clap to show how much you enjoy this story blue ocean thinker https jameskle com sharing concept idea and code
Emmanuel Ameisen,12.8K,13,https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e?source=tag_archive---------3----------------,how to solve 90 % of nlp problem a step by step guide,for more content like this follow insight and emmanuel on twitter whether you be an establish company or work to launch a new service you can always leverage text datum to validate improve and expand the functionality of your product the science of extract meaning and learn from text datum be an active topic of research call natural language processing nlp nlp produce new and exciting result on a daily basis and be a very large field however have work with hundred of company the insight team have see a few key practical application come up much more frequently than any other while many nlp paper and tutorial exist online we have find it hard to find guideline and tip on how to approach these problem efficiently from the ground up after lead hundred of project a year and gain advice from top team all over the united states we write this post to explain how to build machine learning solution to solve problem like the one mention above we ll begin with the simple method that could work and then move on to more nuanced solution such as feature engineering word vector and deep learning after read this article you ll know how to we write this post as a step by step guide ; it can also serve as a high level overview of highly effective standard approach this post be accompany by an interactive notebook demonstrate and apply all these technique feel free to run the code and follow along every machine learning problem start with datum such as a list of email post or tweet common source of textual information include disaster on social medium dataset for this post we will use a dataset generously provide by crowdflower call disaster on social medium where our task will be to detect which tweet be about a disastrous event as oppose to an irrelevant topic such as a movie why a potential application would be to exclusively notify law enforcement official about urgent emergency while ignore review of the most recent adam sandler film a particular challenge with this task be that both class contain the same search term use to find the tweet so we will have to use subtler difference to distinguish between they in the rest of this post we will refer to tweet that be about disaster as disaster and tweet about anything else as irrelevant we have label datum and so we know which tweet belong to which category as richard socher outline below it be usually fast simple and cheap to find and label enough datum to train a model on rather than try to optimize a complex unsupervised method one of the key skill of a data scientist be know whether the next step should be work on the model or the datum a good rule of thumb be to look at the datum first and then clean it up a clean dataset will allow a model to learn meaningful feature and not overfit on irrelevant noise here be a checklist to use to clean your datum see the code for more detail after follow these step and check for additional error we can start use the clean label datum to train model machine learning model take numerical value as input model work on image for example take in a matrix represent the intensity of each pixel in each color channel our dataset be a list of sentence so in order for our algorithm to extract pattern from the datum we first need to find a way to represent it in a way that our algorithm can understand I e as a list of number a natural way to represent text for computer be to encode each character individually as a number ascii for example if we be to feed this simple representation into a classifier it would have to learn the structure of word from scratch base only on our datum which be impossible for most dataset we need to use a high level approach for example we can build a vocabulary of all the unique word in our dataset and associate a unique index to each word in the vocabulary each sentence be then represent as a list that be as long as the number of distinct word in our vocabulary at each index in this list we mark how many time the give word appear in our sentence this be call a bag of word model since it be a representation that completely ignore the order of word in our sentence this be illustrate below we have around 20 000 word in our vocabulary in the disaster of social medium example which mean that every sentence will be represent as a vector of length 20 000 the vector will contain mostly 0s because each sentence contain only a very small subset of our vocabulary in order to see whether our embedding be capture information that be relevant to our problem I e whether the tweet be about disaster or not it be a good idea to visualize they and see if the class look well separate since vocabulary be usually very large and visualize datum in 20 000 dimension be impossible technique like pca will help project the datum down to two dimension this be plot below the two class do not look very well separate which could be a feature of our embedding or simply of our dimensionality reduction in order to see whether the bag of word feature be of any use we can train a classifier base on they when first approach a problem a general good practice be to start with the simple tool that could solve the job whenever it come to classify datum a common favorite for its versatility and explainability be logistic regression it be very simple to train and the result be interpretable as you can easily extract the most important coefficient from the model we split our datum in to a training set use to fit our model and a test set to see how well it generalize to unseen datum after training we get an accuracy of 75 4 % not too shabby guess the most frequent class irrelevant would give we only 57 % however even if 75 % precision be good enough for our need we should never ship a model without try to understand it a first step be to understand the type of error our model make and which kind of error be least desirable in our example false positive be classify an irrelevant tweet as a disaster and false negative be classify a disaster as an irrelevant tweet if the priority be to react to every potential event we would want to lower our false negative if we be constrain in resource however we might prioritize a lower false positive rate to reduce false alarm a good way to visualize this information be use a confusion matrix which compare the prediction our model make with the true label ideally the matrix would be a diagonal line from top leave to bottom right our prediction match the truth perfectly our classifier create more false negative than false positive proportionally in other word our model s most common error be inaccurately classify disaster as irrelevant if false positive represent a high cost for law enforcement this could be a good bias for our classifier to have to validate our model and interpret its prediction it be important to look at which word it be use to make decision if our datum be bias our classifier will make accurate prediction in the sample datum but the model would not generalize well in the real world here we plot the most important word for both the disaster and irrelevant class plot word importance be simple with bag of word and logistic regression since we can just extract and rank the coefficient that the model use for its prediction our classifier correctly pick up on some pattern hiroshima massacre but clearly seem to be overfitte on some meaningless term heyoo x1392 right now our bag of word model be deal with a huge vocabulary of different word and treat all word equally however some of these word be very frequent and be only contribute noise to our prediction next we will try a way to represent sentence that can account for the frequency of word to see if we can pick up more signal from our datum in order to help our model focus more on meaningful word we can use a tf idf score term frequency inverse document frequency on top of our bag of word model tf idf weigh word by how rare they be in our dataset discount word that be too frequent and just add to the noise here be the pca projection of our new embedding we can see above that there be a clear distinction between the two color this should make it easy for our classifier to separate both group let s see if this lead to well performance train another logistic regression on our new embedding we get an accuracy of 76 2 % a very slight improvement have our model start pick up on more important word if we be get a well result while prevent our model from cheat then we can truly consider this model an upgrade the word it pick up look much more relevant although our metric on our test set only increase slightly we have much more confidence in the term our model be use and thus would feel more comfortable deploy it in a system that would interact with customer our late model manage to pick up on high signal word however it be very likely that if we deploy this model we will encounter word that we have not see in our training set before the previous model will not be able to accurately classify these tweet even if it have see very similar word during training to solve this problem we need to capture the semantic meaning of word mean we need to understand that word like good and positive be close than apricot and continent the tool we will use to help we capture meaning be call word2vec use pre train word word2vec be a technique to find continuous embedding for word it learn from read massive amount of text and memorizing which word tend to appear in similar context after be train on enough datum it generate a 300 dimension vector for each word in a vocabulary with word of similar meaning be close to each other the author of the paper open source a model that be pre train on a very large corpus which we can leverage to include some knowledge of semantic meaning into our model the pre train vector can be find in the repository associate with this post a quick way to get a sentence embed for our classifier be to average word2vec score of all word in our sentence this be a bag of word approach just like before but this time we only lose the syntax of our sentence while keep some semantic information here be a visualization of our new embedding use previous technique the two group of color look even more separate here our new embedding should help our classifier find the separation between both class after train the same model a third time a logistic regression we get an accuracy score of 77 7 % our good result yet time to inspect our model since our embedding be not represent as a vector with one dimension per word as in our previous model it s hard to see which word be the most relevant to our classification while we still have access to the coefficient of our logistic regression they relate to the 300 dimension of our embedding rather than the index of word for such a low gain in accuracy lose all explainability seem like a harsh trade off however with more complex model we can leverage black box explainer such as lime in order to get some insight into how our classifier work lime lime be available on github through an open source package a black box explainer allow user to explain the decision of any classifier on one particular example by perturb the input in our case remove word from the sentence and see how the prediction change let s see a couple explanation for sentence from our dataset however we do not have time to explore the thousand of example in our dataset what we ll do instead be run lime on a representative sample of test case and see which word keep come up as strong contributor use this approach we can get word importance score like we have for previous model and validate our model s prediction look like the model pick up highly relevant word imply that it appear to make understandable decision these seem like the most relevant word out of all previous model and therefore we re more comfortable deploying in to production we ve cover quick and efficient approach to generate compact sentence embedding however by omit the order of word we be discard all of the syntactic information of our sentence if these method do not provide sufficient result you can utilize more complex model that take in whole sentence as input and predict label without the need to build an intermediate representation a common way to do that be to treat a sentence as a sequence of individual word vector use either word2vec or more recent approach such as glove or cove this be what we will do below convolutional neural network for sentence classification train very quickly and work well as an entry level deep learn architecture while convolutional neural network cnn be mainly know for their performance on image datum they have be provide excellent result on text relate task and be usually much quick to train than most complex nlp approach e g lstms and encoder decoder architecture this model preserve the order of word and learn valuable information on which sequence of word be predictive of our target class contrary to previous model it can tell the difference between alex eat plant and plant eat alex training this model do not require much more work than previous approach see code for detail and give we a model that be much well than the previous one get 79 5 % accuracy as with the model above the next step should be to explore and explain the prediction use the method we describe to validate that it be indeed the good model to deploy to user by now you should feel comfortable tackle this on your own here be a quick recap of the approach we ve successfully use these approach be apply to a particular example case use model tailor towards understanding and leverage short text such as tweet but the idea be widely applicable to a variety of problem I hope this help you we d love to hear your comment and question feel free to comment below or reach out to @emmanuelameisen here or on twitter want to learn apply artificial intelligence from top professional in silicon valley or new york learn more about the artificial intelligence program be you a company work in ai and would like to get involve in the insight ai fellow program feel free to get in touch from a quick cheer to a stand ovation clap to show how much you enjoy this story ai lead at insight ai @emmanuelameisen insight fellow program your bridge to a career in datum
Mybridge,10.1K,6,https://medium.mybridge.co/30-amazing-machine-learning-projects-for-the-past-year-v-2018-b853b8621ac7?source=tag_archive---------4----------------,30 amazing machine learning project for the past year v 2018,for the past year we ve compare nearly 8 800 open source machine learning project to pick top 30 0 3 % chance this be an extremely competitive list and it carefully pick the good open source machine learning librarie dataset and app publish between january and december 2017 mybridge ai evaluate the quality by consider popularity engagement and recency to give you an idea about the quality the average number of github star be 3 558 open source project can be useful for datum scientist you can learn by read the source code and build something on top of the exist project give a plenty of time to play around with machine learning project you may have miss for the past year < recommend learn > a neural network deep learn a ztm hand on artificial neural network 68 745 recommend 4 5 5 star b tensorflow complete guide to tensorflow for deep learning with python 17 834 recommend 4 6 5 star click the number below credit give to the big contributor fasttext library for fast text representation and classification 11786 star on github courtesy of facebook research muse multilingual unsupervised or supervise word embedding base on fast text 695 star on github deep photo styletransfer code and datum for paper deep photo style transfer 9747 star on github courtesy of fujun luan ph d at cornell university the world s simple facial recognition api for python and the command line 8672 star on github courtesy of adam geitgey magenta music and art generation with machine intelligence 8113 star on github sonnet tensorflow base neural network library 5731 star on github courtesy of malcolm reynold at deepmind deeplearn j a hardware accelerate machine intelligence library for the web 5462 star on github courtesy of nikhil thorat at google brain fast style transfer in tensorflow 4843 star on github courtesy of logan engstrom at mit pysc2 starcraft ii learning environment 3683 star on github courtesy of timo ewald at deepmind airsim open source simulator base on unreal engine for autonomous vehicle from microsoft ai & research 3861 star on github courtesy of shital shah at microsoft facet visualization for machine learning dataset 3371 star on github courtesy of google brain style2paint ai colorization of image 3310 star on github tensor2tensor a library for generalize sequence to sequence model — google research 3087 star on github courtesy of ryan sepassi at google brain image to image translation in pytorch e g horse2zebra edges2cat and more 2847 star on github courtesy of jun yan zhu ph d at berkeley faiss a library for efficient similarity search and clustering of dense vector 2629 star on github courtesy of facebook research fashion mnist a mnist like fashion product database 2780 star on github courtesy of han xiao research scientist zalando tech parlai a framework for training and evaluating ai model on a variety of openly available dialog dataset 2578 star on github courtesy of alexander miller at facebook research fairseq facebook ai research sequence to sequence toolkit 2571 star on github pyro deep universal probabilistic programming with python and pytorch 2387 star on github courtesy of uber ai labs igan interactive image generation power by gan 2369 star on github deep image prior image restoration with neural network but without learn 2188 star on github courtesy of dmitry ulyanov ph d at skoltech face_classification real time face detection and emotion gender classification use fer2013 imdb dataset with a keras cnn model and opencv 1967 star on github speech to text wavenet end to end sentence level english speech recognition use deepmind s wavenet and tensorflow 1961 star on github courtesy of namju kim at kakao brain stargan unified generative adversarial network for multi domain image to image translation 1954 star on github courtesy of yunjey choi at korea university ml agent unity machine learning agent 1658 star on github courtesy of arthur juliani deep learning at unity3d deepvideoanalytic a distribute visual search and visual datum analytic platform 1494 star on github courtesy of akshay bhat ph d at cornell university opennmt open source neural machine translation in torch 1490 star on github pix2pixhd synthesize and manipulate 2048x1024 image with conditional gan 1283 star on github courtesy of ming yu liu at ai research scientist at nvidia horovod distribute training framework for tensorflow 1188 star on github courtesy of uber engineering ai block a powerful and intuitive wysiwyg interface that allow anyone to create machine learning model 899 star on github deep neural network for voice conversion voice style transfer in tensorflow 845 star on github courtesy of dabi ahn ai research at kakao brain that s it for machine learn open source project if you like this curation read good daily article base on your programming skill on our website from a quick cheer to a stand ovation clap to show how much you enjoy this story we rank article for professional read more and achieve more
David Foster,12.8K,11,https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188?source=tag_archive---------5----------------,how to build your own alphazero ai use python and kera,in this article I ll attempt to cover three thing in march 2016 deepmind s alphago beat 18 time world champion go player lee sedol 4 1 in a series watch by over 200 million people a machine have learn a super human strategy for playing go a feat previously think impossible or at the very least at least a decade away from be accomplish this in itself be a remarkable achievement however on 18th october 2017 deepmind take a giant leap far the paper master the game of go without human knowledge unveil a new variant of the algorithm alphago zero that have defeat alphago 100 0 incredibly it have do so by learn solely through self play start tabula rasa blank state and gradually find strategy that would beat previous incarnation of itself no long be a database of human expert game require to build a super human ai a mere 48 day later on 5th december 2017 deepmind release another paper master chess and shogi by self play with a general reinforcement learning algorithm show how alphago zero could be adapt to beat the world champion program stockfish and elmo at chess and shogi the entire learning process from be show the game for the first time to become the good computer program in the world have take under 24 hour with this alphazero be bear — the general algorithm for get good at something quickly without any prior knowledge of human expert strategy there be two amazing thing about this achievement it can not be overstate how important this be this mean that the underlie methodology of alphago zero can be apply to any game with perfect information the game state be fully know to both player at all time because no prior expertise be require beyond the rule of the game this be how it be possible for deepmind to publish the chess and shogi paper only 48 day after the original alphago zero paper quite literally all that need to change be the input file that describe the mechanic of the game and to tweak the hyper parameter relate to the neural network and monte carlo tree search if alphazero use super complex algorithm that only a handful of people in the world understand it would still be an incredible achievement what make it extraordinary be that a lot of the idea in the paper be actually far less complex than previous version at its heart lie the follow beautifully simple mantra for learn doesn t that sound a lot like how you learn to play game when you play a bad move it s either because you misjudge the future value of result position or you misjudge the likelihood that your opponent would play a certain move so didn t think to explore that possibility these be exactly the two aspect of gameplay that alphazero be train to learn firstly check out the alphago zero cheat sheet for a high level understanding of how alphago zero work it s worth have that to refer to as we walk through each part of the code there s also a great article here that explain how alphazero work in more detail clone this git repository which contain the code I ll be reference to start the learning process run the top two panel in the run ipynb jupyter notebook once it s build up enough game position to fill its memory the neural network will begin train through additional self play and train it will gradually get well at predict the game value and next move from any position result in well decision making and smart overall play we ll now have a look at the code in more detail and show some result that demonstrate the ai get strong over time n b — this be my own understanding of how alphazero work base on the information available in the paper reference above if any of the below be incorrect apology and I ll endeavour to correct it the game that our algorithm will learn to play be connect4 or four in a row not quite as complex as go but there be still 4 531 985 219 092 game position in total the game rule be straightforward player take it in turn to enter a piece of their colour in the top of any available column the first player to get four of their colour in a row — each vertically horizontally or diagonally win if the entire grid be fill without a four in a row be create the game be draw here s a summary of the key file that make up the codebase this file contain the game rule for connect4 each square be allocate a number from 0 to 41 as follow the game py file give the logic behind move from one game state to another give a choose action for example give the empty board and action 38 the takeaction method return a new game state with the start player s piece at the bottom of the centre column you can replace the game py file with any game file that conform to the same api and the algorithm will in principal learn strategy through self play base on the rule you have give it this contain the code that start the learning process it load the game rule and then iterate through the main loop of the algorithm which consist of three stage there be two agent involve in this loop the best_player and the current_player the best_player contain the good perform neural network and be use to generate the self play memory the current_player then retrain its neural network on these memory and be then pitch against the best_player if it win the neural network inside the best_player be switch for the neural network inside the current_player and the loop start again this contain the agent class a player in the game each player be initialise with its own neural network and monte carlo search tree the simulate method run the monte carlo tree search process specifically the agent move to a leaf node of the tree evaluate the node with its neural network and then backfill the value of the node up through the tree the act method repeat the simulation multiple time to understand which move from the current position be most favourable it then return the choose action to the game to enact the move the replay method retrain the neural network use memory from previous game this file contain the residual_cnn class which define how to build an instance of the neural network it use a condensed version of the neural network architecture in the alphagozero paper — I e a convolutional layer follow by many residual layer then split into a value and policy head the depth and number of convolutional filter can be specify in the config file the keras library be use to build the network with a backend of tensorflow to view individual convolutional filter and densely connect layer in the neural network run the following inside the the run ipynb notebook this contain the node edge and mct class that constitute a monte carlo search tree the mct class contain the movetoleaf and backfill method previously mention and instance of the edge class store the statistic about each potential move this be where you set the key parameter that influence the algorithm adjust these variable will affect that run time neural network accuracy and overall success of the algorithm the above parameter produce a high quality connect4 player but take a long time to do so to speed the algorithm up try the follow parameter instead contain the playmatche and playmatchesbetweenversion function that play match between two agent to play against your creation run the follow code it s also in the run ipynb notebook when you run the algorithm all model and memory file be save in the run folder in the root directory to restart the algorithm from this checkpoint later transfer the run folder to the run_archive folder attach a run number to the folder name then enter the run number model version number and memory version number into the initialise py file corresponding to the location of the relevant file in the run_archive folder run the algorithm as usual will then start from this checkpoint an instance of the memory class store the memory of previous game that the algorithm use to retrain the neural network of the current_player this file contain a custom loss function that mask prediction from illegal move before pass to the cross entropy loss function the location of the run and run_archive folder log file be save to the log folder inside the run folder to turn on log set the value of the logger_disable variable to false inside this file view the log file will help you to understand how the algorithm work and see inside its mind for example here be a sample from the logg mct file equally from the logg tourney file you can see the probability attach to each move during the evaluation phase training over a couple of day produce the follow chart of loss against mini batch iteration number the top line be the error in the policy head the cross entropy of the mct move probability against the output from the neural network the bottom line be the error in the value head the mean squared error between the actual game value and the neural network predict of the value the middle line be an average of the two clearly the neural network be get well at predict the value of each game state and the likely next move to show how this result in strong and strong play I run a league between 17 player range from the 1st iteration of the neural network up to the 49th each pairing play twice with both player have a chance to play first here be the final standing clearly the later version of the neural network be superior to the early version win most of their game it also appear that the learn hasn t yet saturate — with further training time the player would continue to get strong learning more and more intricate strategy as an example one clear strategy that the neural network have favour over time be grab the centre column early observe the difference between the first version of the algorithm and say the 30th version 1st neural network version 30th neural network version this be a good strategy as many line require the centre column — claim this early ensure your opponent can not take advantage of this this have be learn by the neural network without any human input there be a game py file for a game call metasquare in the game folder this involve place x and o marker in a grid to try to form square of different size large square score more point than small square and the player with the most point when the grid be full win if you switch the connect4 game py file for the metasquare game py file the same algorithm will learn how to play metasquare instead hopefully you find this article useful — let I know in the comment below if you find any typo or have question about anything in the codebase or article and I ll get back to you as soon as possible if you would like to learn more about how our company apply data science develop innovative datum science solution for business feel free to get in touch through our website or directly through linkedin and if you like this feel free to leave a few hearty clap apply datum science be a london base consultancy that implement end to end datum science solution for business deliver measurable value if you re look to do more with your datum let s talk from a quick cheer to a stand ovation clap to show how much you enjoy this story co founder of apply data science cut edge data science news and project
George Seif,11.4K,11,https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68?source=tag_archive---------6----------------,the 5 clustering algorithms datum scientist need to know,clustering be a machine learning technique that involve the grouping of data point give a set of data point we can use a cluster algorithm to classify each datum point into a specific group in theory datum point that be in the same group should have similar property and or feature while datum point in different group should have highly dissimilar property and or feature cluster be a method of unsupervised learning and be a common technique for statistical datum analysis use in many field in data science we can use cluster analysis to gain some valuable insight from our datum by see what group the data point fall into when we apply a cluster algorithm today we re go to look at 5 popular cluster algorithm that datum scientist need to know and their pro and con k means be probably the most well know cluster algorithm it s teach in a lot of introductory datum science and machine learning class it s easy to understand and implement in code check out the graphic below for an illustration k means have the advantage that it s pretty fast as all we re really do be compute the distance between point and group center ; very few computation it thus have a linear complexity o n on the other hand k means have a couple of disadvantage firstly you have to select how many group class there be this isn t always trivial and ideally with a cluster algorithm we d want it to figure those out for we because the point of it be to gain some insight from the datum k mean also start with a random choice of cluster center and therefore it may yield different cluster result on different run of the algorithm thus the result may not be repeatable and lack consistency other cluster method be more consistent k medians be another cluster algorithm relate to k mean except instead of recompute the group center point use the mean we use the median vector of the group this method be less sensitive to outlier because of use the median but be much slow for large dataset as sort be require on each iteration when compute the median vector mean shift clustering be a slide window base algorithm that attempt to find dense area of data point it be a centroid base algorithm mean that the goal be to locate the center point of each group class which work by update candidate for center point to be the mean of the point within the slide window these candidate window be then filter in a post processing stage to eliminate near duplicate form the final set of center point and their correspond group check out the graphic below for an illustration an illustration of the entire process from end to end with all of the slide window be show below each black dot represent the centroid of a slide window and each gray dot be a data point in contrast to k mean cluster there be no need to select the number of cluster as mean shift automatically discover this that s a massive advantage the fact that the cluster center converge towards the point of maximum density be also quite desirable as it be quite intuitive to understand and fit well in a naturally data drive sense the drawback be that the selection of the window size radius r can be non trivial dbscan be a density base cluster algorithm similar to mean shift but with a couple of notable advantage check out another fancy graphic below and let s get start dbscan pose some great advantage over other cluster algorithm firstly it do not require a pe set number of cluster at all it also identify outlier as noise unlike mean shift which simply throw they into a cluster even if the data point be very different additionally it be able to find arbitrarily sized and arbitrarily shape cluster quite well the main drawback of dbscan be that it doesn t perform as well as other when the cluster be of vary density this be because the setting of the distance threshold ε and minpoint for identify the neighborhood point will vary from cluster to cluster when the density vary this drawback also occur with very high dimensional datum since again the distance threshold ε become challenge to estimate one of the major drawback of k means be its naive use of the mean value for the cluster center we can see why this isn t the good way of do thing by look at the image below on the left hand side it look quite obvious to the human eye that there be two circular cluster with different radius center at the same mean k mean can t handle this because the mean value of the cluster be a very close together k mean also fail in case where the cluster be not circular again as a result of use the mean as cluster center gaussian mixture model gmms give we more flexibility than k mean with gmms we assume that the data point be gaussian distribute ; this be a less restrictive assumption than say they be circular by use the mean that way we have two parameter to describe the shape of the cluster the mean and the standard deviation take an example in two dimension this mean that the cluster can take any kind of elliptical shape since we have standard deviation in both the x and y direction thus each gaussian distribution be assign to a single cluster in order to find the parameter of the gaussian for each cluster e g the mean and standard deviation we will use an optimization algorithm call expectation maximization em take a look at the graphic below as an illustration of the gaussian be fit to the cluster then we can proceed on to the process of expectation maximization cluster use gmms there be really 2 key advantage to use gmms firstly gmms be a lot more flexible in term of cluster covariance than k mean ; due to the standard deviation parameter the cluster can take on any ellipse shape rather than be restrict to circle k means be actually a special case of gmm in which each cluster s covariance along all dimension approach 0 secondly since gmms use probability they can have multiple cluster per data point so if a data point be in the middle of two overlap cluster we can simply define its class by say it belong x percent to class 1 and y percent to class 2 I e gmms support mixed membership hierarchical cluster algorithm actually fall into 2 category top down or bottom up bottom up algorithm treat each datum point as a single cluster at the outset and then successively merge or agglomerate pair of cluster until all cluster have be merge into a single cluster that contain all datum point bottom up hierarchical clustering be therefore call hierarchical agglomerative clustering or hac this hierarchy of cluster be represent as a tree or dendrogram the root of the tree be the unique cluster that gather all the sample the leave be the cluster with only one sample check out the graphic below for an illustration before move on to the algorithm step hierarchical clustering do not require we to specify the number of cluster and we can even select which number of cluster look well since we be build a tree additionally the algorithm be not sensitive to the choice of distance metric ; all of they tend to work equally well whereas with other cluster algorithm the choice of distance metric be critical a particularly good use case of hierarchical clustering method be when the underlie datum have a hierarchical structure and you want to recover the hierarchy ; other cluster algorithm can t do this these advantage of hierarchical clustering come at the cost of low efficiency as it have a time complexity of o n3 unlike the linear complexity of k mean and gmm there be your top 5 cluster algorithm that a data scientist should know we ll end off with an awesome visualization of how well these algorithm and a few other perform courtesy of scikit learn from a quick cheer to a stand ovation clap to show how much you enjoy this story certify nerd ai machine learning engineer sharing concept idea and code
Mybridge,6.6K,6,https://medium.mybridge.co/30-amazing-python-projects-for-the-past-year-v-2018-9c310b04cdb3?source=tag_archive---------7----------------,30 amazing python project for the past year v 2018,for the past year we ve compare nearly 15 000 open source python project to pick top 30 0 2 % chance this be an extremely competitive list and it carefully pick the good open source python librarie tool and program publish between january and december 2017 mybridge ai evaluate the quality by consider popularity engagement and recency to give you an idea about the quality the average number of github star be 3 707 open source project can be useful for programmer you can learn by read the source code and build something on top of the exist project give a plenty of time to play around with python project you may have miss for the past year < recommend learn > a beginner the python bible build 11 project and go from beginner to pro 27 672 recommend 4 7 5 star b datum science python for datum science and machine learning bootcamp use numpy panda seaborn matplotlib plotly 90 212 recommend 4 6 5 star click the number below credit give to the big contributor home assistant v0 6 + open source home automation platform run on python 3 11357 star on github courtesy of paulus schoutsen pytorch tensor and dynamic neural network in python with strong gpu acceleration 11019 star on github courtesy of adam paszke and other at pytorch team grumpy a python to go source code transcompiler and runtime 8367 star on github courtesy of dylan trotter and other at google sanic async python 3 5 + web server that s write to go fast 8028 star on github courtesy of channel cat and eli uriegas python fire a library for automatically generate command line interface cli from absolutely any python object 7775 star on github courtesy of david bieber and other at google brain spacy v2 0 industrial strength natural language processing nlp with python and cython 7633 star on github courtesy of matthew honnibal pipenv python development workflow for human 7273 star on github courtesy of kenneth reitz micropython a lean and efficient python implementation for microcontroller and constrained system 5728 star on github prophet tool for produce high quality forecast for time series datum that have multiple seasonality with linear or non linear growth 4369 star on github courtesy of facebook serpentai game agent framework in python help you create ais bot to play any game 3411 star on github courtesy of nicholas brochu dash interactive reactive web app in pure python 3281 star on github courtesy of chris p instapy instagram bot like comment follow automation script 3179 star on github courtesy of timg apistar a fast and expressive api framework for python 3024 star on github courtesy of tom christie faiss a library for efficient similarity search and clustering of dense vector 2717 star on github courtesy of matthijs douze and other at facebook research mechanicalsoup a python library for automate interaction with website 2244 star on github well exception pretty and useful exception in python automatically 2121 star on github courtesy of qix flashtext extract keyword from sentence or replace keyword in sentence 2019 star on github courtesy of vikash singh maya datetime for human in python 1828 star on github kenneth reitz mimesis v1 0 python library which help generate mock datum in different language for various purpose these datum can be especially useful at various stage of software development and test 1732 star on github courtesy of líkið geimfari open paperless scan index and archive all of your paper document a document management system 1717 star on github courtesy of tina zhou fsociety hacking tool pack a penetration testing framework 1585 star on github courtesy of manis manisso livepython visually trace python code in real time 1577 star on github courtesy of anastasis germanidi hatch a modern project package and virtual env manager for python 1537 star on github courtesy of ofek lev tangent source to source debuggable derivative in pure python 1433 star on github courtesy of alex wiltschko and other at google brain clairvoyant a python program that identify and monitor historical cue for short term stock movement 1159 star on github courtesy of anthony federico monkeytype a system for python that generate static type annotation by collect runtime type 1143 star on github courtesy of carl meyer at instagram engineering eel a little python library for make simple electron like html js gui app 1137 star on github surprise v1 0 a python scikit for building and analyze recommend system 1103 star on github gain web crawling framework for everyone 1009 star on github courtesy of 高久力 pdftabextract a set of tool for extract table from pdf file help to do data mining on scan document 722 star on github that s it for python open source of the year if you like this curation read good daily article base on your programming skill on our website from a quick cheer to a stand ovation clap to show how much you enjoy this story we rank article for professional read more and achieve more
Simon Greenman,10.2K,16,https://towardsdatascience.com/who-is-going-to-make-money-in-ai-part-i-77a2f30b8cef?source=tag_archive---------8----------------,who be go to make money in ai part I towards data science,we be in the midst of a gold rush in ai but who will reap the economic benefit the mass of startup who be all gold pan the corporate who have massive gold mining operation the technology giant who be supply the pick and shovel and which nation have the rich seam of gold we be currently experience another gold rush in ai billion be be invest in ai startup across every imaginable industry and business function google amazon microsoft and ibm be in a heavyweight fight invest over $ 20 billion in ai in 2016 corporate be scramble to ensure they realise the productivity benefit of ai ahead of their competitor while look over their shoulder at the startup china be put its considerable weight behind ai and the european union be talk about a $ 22 billion ai investment as it fear lose ground to china and the us ai be everywhere from the 3 5 billion daily search on google to the new apple iphone x that use facial recognition to amazon alexa that cutely answer our question media headline tout the story of how ai be help doctor diagnose disease bank well assess customer loan risk farmer predict crop yield marketer target and retain customer and manufacturer improve quality control and there be think tank dedicate to study the physical cyber and political risk of ai ai and machine learning will become ubiquitous and weave into the fabric of society but as with any gold rush the question be who will find gold will it just be the brave the few and the large or can the snappy upstart grab their nugget will those provide the pick and shovel make most of the money and who will hit pay dirt as I start think about who be go to make money in ai I end up with seven question who will make money across the 1 chip maker 2 platform and infrastructure provider 3 enable model and algorithm provider 4 enterprise solution provider 5 industry vertical solution provider 6 corporate user of ai and 7 nation while there be many way to skin the cat of the ai landscape hopefully below provide a useful explanatory framework — a value chain of sort the company note be representative of large player in each category but in no way be this list intend to be comprehensive or predictive even though the price of computational power have fall exponentially demand be rise even fast ai and machine learning with its massive dataset and its trillion of vector and matrix calculation have a ferocious and insatiable appetite bring on the chip nvidia s stock be up 1500 % in the past two year benefit from the fact that their graphical processing unit gpu chip that be historically use to render beautiful high speed flow game graphic be perfect for machine learning google recently launch its second generation of tensor processing unit tpus and microsoft be build its own brainwave ai machine learning chip at the same time startup such as graphcore who have raise over $ 110 m be look to enter the market incumbent chip provider such as ibm intel qualcomm and amd be not stand still even facebook be rumour to be build a team to design its own ai chip and the chinese be emerge as serious chip player with cambricon technology announce the first cloud ai chip this past week what be clear be that the cost of design and manufacturing chip then sustain a position as a global chip leader be very high it require extremely deep pocket and a world class team of silicon and software engineer this mean that there will be very few new winner just like the gold rush day those that provide the cheap and most widely use pick and shovel will make a lot of money the ai race be now also take place in the cloud amazon realise early that startup would much rather rent computer and software than buy it and so it launch amazon web service aws in 2006 today ai be demand so much compute power that company be increasingly turn to the cloud to rent hardware through infrastructure as a service iaas and platform as a service paas offering the fight be on among the tech giant microsoft be offer their hybrid public and private azure cloud service that allegedly have over one million computer and in the past few week they announce that their brainwave hardware solutionsdramatically accelerate machine learning with their own bing search engine performance improve by a factor of ten google be rush to play catchup with its own googlecloud offering and we be see the chinese alibaba start to take global share amazon — microsoft — google and ibm be go to continue to duke this one out and watch out for the massively scale cloud player from china the big pick and shovel guy will win again today google be the world s large ai company attract the good ai mind spend small country size gdp budget on r&d and sit on the good dataset gleam from the billion of user of their service ai be power google s search autonomous vehicle speech recognition intelligent reasoning massive search and even its own work on drug discovery and disease diangosis and the incredible ai machine learning software and algorithm that be power all of google s ai activity — tensorflow — be now be give away for free yes for free tensorflow be now an open source software project available to the world and why be they do this as jeff dean head of google brain recently say there be 20 million organisation in the world that could benefit from machine learning today if million of company use this good in class free ai software then they be likely to need lot of computing power and who be well serve to offer that well google cloud be of course optimise for tensorflow and related ai service and once you become reliant on their software and their cloud you become a very sticky customer for many year to come no wonder it be a brutal race for global ai algorithm dominance with amazon — microsoft — ibm also offer their own cheap or free ai software service we be also see a fight for not only machine learning algorithm but cognitive algorithm that offer service for conversational agent and bot speech natural language processing nlp and semantic vision and enhance core algorithm one startup in this increasingly contest space be clarifai who provide advanced image recognition system for business to detect near duplicate and visual search it have raise nearly $ 40 m over the past three year the market for vision relate algorithm and service be estimate to be a cumulative $ 8 billion in revenue between 2016 and 2025 the giant be not stand still ibm for example be offer its watson cognitive product and service they have twenty or so apis for chatbots vision speech language knowledge management and empathy that can be simply be plug into corporate software to create ai enable application cognitive api be everywhere kdnugget list here over 50 of the top cognitive service from the giant and startup these service be be put into the cloud as ai as a service aiaas to make they more accessible just recently microsoft s ceo satya nadella claim that a million developer be use their ai apis service and tool for building ai power app and nearly 300 000 developer be use their tool for chatbot I wouldn t want to be a startup compete with these goliath the winner in this space be likely to favour the heavyweight again they can hire the good research and engineering talent spend the most money and have access to the large dataset to flourish startup be go to have to be really well fund support by lead researcher with a whole battery of ip patent and publish paper deep domain expertise and have access to quality dataset and they should have excellent navigational skill to sail ahead of the giant or sail different race there will many startup casualty but those that can scale will find themselves as global enterprise or quickly acquire by the heavyweight and even if a startup have not find a path to commercialisation then they could become acquihire company buy for their talent if they be work on enable ai algorithm with a strong research orient team we see this in 2014 when deepmind a two year old london base company that develop unique reinforcement machine learning algorithm be acquire by google for $ 400 m enterprise software have be dominate by giant such as salesforce ibm oracle and sap they all recognise that ai be a tool that need to be integrate into their enterprise offering but many startup be rush to become the next generation of enterprise service fill in gap where the incumbent don t currently tread or even attempt to disrupt they we analyse over two hundred use case in the enterprise space range from customer management to market to cybersecurity to intelligence to hr to the hot area of cognitive robotic process automation rpa the enterprise field be much more open than previous space with a veritable medley of startup provide point solution for these use case today there be over 200 ai powered company just in the recruitment space many of they ai startups cybersecurity leader darktrace and rpa leader uipathhave war chest in the $ 100 million the incumbent also want to make sure their ecosystem stay on the forefront and be invest in startup that enhance their offering salesforce have invest in digital genius a customer management solution and similarly unbable that offer enterprise translation service incumbent also often have more press problem sap for example be rush to play catchup in offer a cloud solution let alone catchup in ai we be also see tool provider try to simplify the task require to create deploy and manage ai service in the enterprise machine learning training for example be a messy business where 80 % of time can be spend on datum wrangling and an inordinate amount of time be spend on testing and tuning of what be call hyperparameter petuum a tool provider base in pittsburgh in the us have raise over $ 100 m to help accelerate and optimise the deployment of machine learning model many of these enterprise startup provider can have a healthy future if they quickly demonstrate that they be solve and scale solution to meet real world enterprise need but as always happen in software gold rush there will be a handful of winner in each category and for those ai enterprise category winner they be likely to be snap up along with the good in class tool provider by the giant if they look too threatening ai be drive a race for the good vertical industry solution there be a wealth of new ai power startup provide solution to corporate use case in the healthcare financial service agriculture automative legal and industrial sector and many startup be take the ambitious path to disrupt the incumbent corporate player by offer a service directly to the same customer it be clear that many startup be provide valuable point solution and can succeed if they have access to 1 large and proprietary data training set 2 domain knowledge that give they deep insight into the opportunity within a sector 3 a deep pool of talent around apply ai and 4 deep pocket of capital to fund rapid growth those startup that be do well generally speak the corporate commercial language of customer business efficiency and roi in the form of well develop go to market plan for example zestfinance have raise nearly $ 300 m to help improve credit decision making that will provide fair and transparent credit to everyone they claim they have the world s good data scientist but they would wouldn t they for those startup that be look to disrupt exist corporate player they need really deep pocket for example affirm that offer loan to consumer at the point of sale have raise over $ 700 m these company quickly need to create a defensible moat to ensure they remain competitive this can come from datum network effect where more datum beget well ai base service and product that get more revenue and customer that get more datum and so the flywheel effect continue and while corporate might look to new vendor in their industry for ai solution that could enhance their top and bottom line they be not go to sit back and let upstart muscle in on their customer and they be not go to sit still and let their corporate competitor gain the first advantage through ai there be currently a massive race for corporate innovation large company have their own venture group invest in startup run accelerator and build their own startup to ensure that they be leader in ai drive innovation large corporate be in a strong position against the startup and small company due to their data asset datum be the fuel for ai and machine learning who be well place to take advantage of ai than the insurance company that have ream of historic datum on underwriting claim the financial service company that know everything about consumer financial product buy behaviour or the search company that see more user search for information than any other corporate large and small be well positioned to extract value from ai in fact gartner research predict ai derive business value be project to reach up to $ 3 9 trillion by 2022 there be hundred if not thousand of valuable use case that ai can address across organisation corporate can improve their customer experience save cost low price drive revenue and sell well product and service power by ai ai will help the big get big often at the expense of small company but they will need to demonstrate strong visionary leadership an ability to execute and a tolerance for not always get technology enable project right on the first try country be also also in a battle for ai supremacy china have not be shy about its call to arm around ai it be invest massively in grow technical talent and develop startup its more lax regulatory environment especially in datum privacy help china lead in ai sector such as security and facial recognition just recently there be an example of chinese police pick out one most wanted face in a crowd of 50 000 at a music concert and sensetime group ltd that analyse face and image on a massive scale report it raise $ 600 m become the most valuable global ai startup the chinese point out that their mobile market be 3x the size of the we and there be 50x more mobile payment take place — this be a massive datum advantage the european focus on datum privacy regulation could put they at a disadvantage in certain area of ai even if the union be talk about a $ 22b investment in ai the uk germany france and japan have all make recent announcement about their nation state ai strategy for example president macron say the french government will spend $ 1 85 billion over the next five year to support the ai ecosystem include the creation of large public dataset company such as google s deepmind and samsung have commit to open new paris labs and fujitsu be expand its paris research centre the british just announce a $ 1 4 billion push into ai include funding of 1000 ai phds but while nation be invest in ai talent and the ecosystem the question be who will really capture the value will france and the uk simply be subsidise phds who will be hire by google and while payroll and income taxis will be healthy on those six figure machine learning salarie the bulk of the economic value create could be with this american company its shareholder and the smile american treasury ai will increase productivity and wealth in company and country but how will that wealth be distribute when the headline suggest that 30 to 40 % of our job will be take by the machine economist can point to lesson from hundred of year of increase technology automation will there be net job creation or net job loss the public debate often cite geoffrey hinton the godfather of machine learning who suggest radiologist will lose their job by the dozen as machine diagnose disease from medical image but then we can look to the chinese who be use ai to assist radiologist in manage the overwhelming demand to review 1 4 billion ct scan annually for lung cancer the result be not job loss but an expand market with more efficient and accurate diagnosis however there be likely to be a period of upheaval when much of the value will go to those few company and country that control ai technology and datum and low skilled country whose wealth depend on job that be target of ai automation will likely suffer ai will favour the large and the technologically skilled in examine the landscape of ai it have become clear that we be now enter a truly golden era for ai and there be few key theme appear as to where the economic value will migrate in short it look like the ai gold rush will favour the company and country with control and scale over the good ai tool and technology the datum the well technical worker the most customer and the strong access to capital those with scale will capture the lion s share of the economic value from ai in some way plus ça change plus c est la même choose but there will also be large golden nugget that will be find by a few choice brave startup but like any gold rush many startup will hit pay dirt and many individual and society will likely feel like they have not see the benefit of the gold rush this be the first part in a series of article I intend to write on the topic of the economic of ai I welcome your feedback write by simon greenman I be a lover of technology and how it can be apply in the business world I run my own advisory firm good practice ai help executive of enterprise and startup accelerate the adoption of roi base ai application please get in touch to discuss this if you enjoy this piece I d love it if you hit the clap button 👏 so other might stumble upon it and please post your comment or you can email I directly or find I on linkedin or twitter or follow I at simon greenman from a quick cheer to a stand ovation clap to show how much you enjoy this story ai guy mapquest guy grow innovate and transform company with tech start up investor mentor and geek sharing concept idea and code
Eugenio Culurciello,6.4K,8,https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0?source=tag_archive---------9----------------,the fall of rnn lstm towards data science,we fall for recurrent neural network rnn long short term memory lstm and all their variant now it be time to drop they it be the year 2014 and lstm and rnn make a great come back from the dead we all read colah s blog and karpathy s ode to rnn but we be all young and unexperienced for a few year this be the way to solve sequence learn sequence translation seq2seq which also result in amazing result in speech to text comprehension and the raise of siri cortana google voice assistant alexa also let we not forget machine translation which result in the ability to translate document into different language or neural machine translation but also translate image into text text into image and captioning video and well you get the idea then in the follow year 2015 16 come resnet and attention one could then well understand that lstm be a clever bypass technique also attention show that mlp network could be replace by average network influence by a context vector more on this later it only take 2 more year but today we can definitely say but do not take our word for it also see evidence that attention base network be use more and more by google facebook salesforce to name a few all these company have replace rnn and variant for attention base model and it be just the beginning rnn have the day count in all application because they require more resource to train and run than attention base model see this post for more info remember rnn and lstm and derivative use mainly sequential processing over time see the horizontal arrow in the diagram below this arrow mean that long term information have to sequentially travel through all cell before get to the present processing cell this mean it can be easily corrupt by be multiply many time by small number < 0 this be the cause of vanish gradient to the rescue come the lstm module which today can be see as multiple switch gate and a bit like resnet it can bypass unit and thus remember for long time step lstm thus have a way to remove some of the vanish gradient problem but not all of it as you can see from the figure above still we have a sequential path from old past cell to the current one in fact the path be now even more complicated because it have additive and forget branch attach to it no question lstm and gru and derivative be able to learn a lot of long term information see result here ; but they can remember sequence of 100s not 1000 or 10 000 or more and one issue of rnn be that they be not hardware friendly let I explain it take a lot of resource we do not have to train these network fast also it take much resource to run these model in the cloud and give that the demand for speech to text be grow rapidly the cloud be not scalable we will need to process at the edge right into the amazon echo see note below for more detail if sequential processing be to be avoid then we can find unit that look ahead or well look back since most of the time we deal with real time causal datum where we know the past and want to affect future decision not so in translate sentence or analyze record video for example where we have all datum and can reason on it more time such look back ahead unit be neural attention module which we previously explain here to the rescue and combine multiple neural attention module come the hierarchical neural attention encoder show in the figure below a well way to look into the past be to use attention module to summarize all past encode vector into a context vector ct notice there be a hierarchy of attention module here very similar to the hierarchy of neural network this be also similar to temporal convolutional network tcn report in note 3 below in the hierarchical neural attention encoder multiple layer of attention can look at a small portion of recent past say 100 vector while layer above can look at 100 of these attention module effectively integrate the information of 100 x 100 vector this extend the ability of the hierarchical neural attention encoder to 10 000 past vector but more importantly look at the length of the path need to propagate a representation vector to the output of the network in hierarchical network it be proportional to log n where n be the number of hierarchy layer this be in contrast to the t step that a rnn need to do where t be the maximum length of the sequence to be remember and t > > n this architecture be similar to a neural ture machine but let the neural network decide what be read out from memory via attention this mean an actual neural network will decide which vector from the past be important for future decision but what about store to memory the architecture above store all previous representation in memory unlike neural turning machine this can be rather inefficient think about store the representation of every frame in a video — most time the representation vector do not change frame to frame so we really be store too much of the same what can we do be add another unit to prevent correlated datum to be store for example by not store vector too similar to previously store one but this be really a hack the good would be to be let the application guide what vector should be save or not this be the focus of current research study stay tune for more information tell your friend it be very surprising to we to see so many company still use rnn lstm for speech to text many unaware that these network be so inefficient and not scalable please tell they about this post about training rnn lstm rnn and lstm be difficult to train because they require memory bandwidth bind computation which be the bad nightmare for hardware designer and ultimately limit the applicability of neural network solution in short lstm require 4 linear layer mlp layer per cell to run at and for each sequence time step linear layer require large amount of memory bandwidth to be compute in fact they can not use many compute unit often because the system have not enough memory bandwidth to feed the computational unit and it be easy to add more computational unit but hard to add more memory bandwidth note enough line on a chip long wire from processor to memory etc as a result rnn lstm and variant be not a good match for hardware acceleration and we talk about this issue before here and here a solution will be compute in memory device like the one we work on at fwdnxt see this repository for a simple example of these technique note 1 hierarchical neural attention be similar to the idea in wavenet but instead of a convolutional neural network we use hierarchical attention module also hierarchical neural attention can be also bi directional note 2 rnn and lstm be memory bandwidth limited problem see this for detail the processing unit s need as much memory bandwidth as the number of operation s they can provide make it impossible to fully utilize they the external bandwidth be never go to be enough and a way to slightly ameliorate the problem be to use internal fast cache with high bandwidth the good way be to use technique that do not require large amount of parameter to be move back and forth from memory or that can be re use for multiple computation per byte transfer high arithmetic intensity note 3 here be a paper compare cnn to rnn temporal convolutional network tcn outperform canonical recurrent network such as lstms across a diverse range of task and dataset while demonstrate long effective memory note 4 relate to this topic be the fact that we know little of how our human brain learn and remember sequence we often learn and recall long sequence in small segment such as a phone number 858 534 22 30 memorize as four segment behavioral experiment suggest that human and some animal employ this strategy of break down cognitive or behavioral sequence into chunk in a wide variety of task — these chunk remind I of small convolutional or attention like network on small sequence that then be hierarchically string together like in the hierarchical neural attention encoder and temporal convolutional network tcn more study make I think that work memory be similar to rnn network that use recurrent real neuron network and their capacity be very low on the other hand both the cortex and hippocampus give we the ability to remember really long sequence of step like where do I park my car at airport 5 day ago suggest that more parallel pathway may be involve to recall long sequence where attention mechanism gate important chunk and force hop in part of the sequence that be not relevant to the final goal or task note 5 the above evidence show we do not read sequentially in fact we interpret character word and sentence as a group an attention base or convolutional module perceive the sequence and project a representation in our mind we would not be misread this if we process this information sequentially we would stop and notice the inconsistency I have almost 20 year of experience in neural network in both hardware and software a rare combination see about I here medium webpage scholar linkedin and more if you find this article useful please consider a donation to support more tutorial and blog any contribution can make a difference from a quick cheer to a stand ovation clap to show how much you enjoy this story I dream and build new technology sharing concept idea and code
WiseWolf Fund,14.2K,8,https://medium.com/@wisewolf_fund/unique-trends-to-look-out-for-with-artificial-intelligence-1db3de178463?source=---------0----------------,game change trend to look out for with ai wisewolf fund medium,artificial intelligence be a state of the art technological trend that many company be try to integrate into their business a recent report by mckinsey state that baidu the chinese equivalent of alphabet invest $ 20 billion in ai last year at the same time alphabet invest roughly $ 30 billion in develop ai technology the chinese government have be actively pursue ai technology in an attempt to control a future cornerstone innovation company in the we be also invest time money and energy into advance ai technology the reason for such interest towards artificial intelligence be that artificial intelligence can enhance any product or function this be why company and government make considerable investment in the research and development of this technology its role in increase the production performance while simultaneously reduce the cost can not be underestimate since some of the large entity in the world be focus on promote the ai technology it would be wise to understand and follow the trend ai be already shape the economy and in the near future its effect may be even more significant ignore the new technology and its influence on the global economic situation be a recipe for failure despite the huge public interest and attention towards ai its evolution be still somewhat halt by the objective cause as any new and fast develop industry ai be quickly outgrow its environment accord to adam temper an author of many creative research on artificial intelligence the development of ai be mostly limit by the lack of employee with relevant expertise very few mature standard industry tool limited high quality training material available few option for easy access to preconfigured machine learning environment and the general focus in the industry on implementation rather than design with any new complex technology the learning curve be steep our educational institution be several step behind the commercial application of this technology it be important that ai scientist work collaboratively share knowledge and good practice to address this deficiency ai be rapidly increase its impact on society ; we need to ensure that the power of ai doesn t remain with the elite few another factor that may be hinder the progress of ai be the cautious stance that people tend to take towards it artificial intelligence be still too sci fi too strange and therefore sometimes scary when people learn to trust ai it will make a true quantum leap in the way of general adoption and application adam temper support this point too describe the possible way for ai technology to gain public trust as at the same time if we analyze the primary purpose of ai we will see it for what it really be — a tool to perform the routine task relieve human for something more creative or innovative when ask about the current trend and opportunity of ai aaron edell ceo and co founder of machine box and one of the top writer on ai describe they as follow ai have also become a political talking point in recent year there have be argument that ai will help to create job but that it will also cause certain worker to lose their job for example estimation prove that self drive vehicle will cause 25 000 truck driver to lose their job each month also as much as 1 million picker and packer work in us warehouse could be out of a job this be due to the fact that by implement ai factory can operate with as few as a dozen of worker naturally company gladly implement artificial intelligence as it ensure considerable saving at the same time government be concern about the current employment situation as well as the short term and long term prediction some country have already begin to plan measure about the new ai technology that be intend to keep the economy stable in fact it would not be fair to say that artificial intelligence cause people to lose job true the whole point of automation be make machine do what people use to do before however it would be more correct if we say that artificial intelligence reshape the employment situation together with take over human function it create other job force people to master new skill encourage worker to increase productivity but it be obvious that ai be go to turn the regular sequence of event upside down therefore the good approach be not to wait until ai leave you unemployed but rather proactively embrace it and learn to live with it as we say already ai can also create job so a wise move would be to learn to manage ai base tool with the advance of ai product learn to work with they may secure you a job and even promote your career your future largely depend on your current and expect income however another important factor be the way you manage your finance of course invest in your own or your child s knowledge be one of the good investment you can ever make at the same time if you need some financial cushion to secure your family s welfare you should look at the available investment opportunity and this be where artificial intelligence may become your good friend professional consultant and investment manager in the recent year in addition to the traditional bank and financial institution we have witness the appearance of a totally new and innovative investment system we be talk about the blockchain technology and the cryptocurrencie that it support million of people all over the world have already appreciate the transparency and flexibility of the blockchain network by watch the cryptocurrency trend carefully and trade wisely individual investor have make fortune within a very short time nowadays the cryptocurrency opportunity be open for everyone not only for the industry expert there be investment fund run on artificial intelligence that be available for individual investor with such fund you be on one hand protect by the blockchain technology it ensure proper safety of your fund and the security of your transaction on the other hand you do not need to be an investment expert to make wise decision this be where artificial intelligence be at your service it analyze the exist trend on the extremely volatile cryptocurrency market and show you the good opportunity the main point be that we should not regard ai as a threat to our career and a danger to our well be instead we should analyze the investment opening create by ai technology that can secure our prosperity for example wolf coin be use ai technology to create a seamless investment channel for savvy individual this robust channel open great opportunity that investor can use to become new rich kid on the block most noteworthy the low entry cost of $ 10 have make it one offer that will enjoy a huge buzz the focus on this new market opening will help people build a solid financial nest egg that will keep they safe even in the face of the storm wisewolf fund launch the wolf coin focus its effort on create a great opportunity for people who wish to benefit from cryptocurrency trading but be new to this trend with artificial intelligence and advanced analytical algorithm the fund arrange the most favorable condition for individual investor mainstream manufacturer company and factory be embrace ai technology to change the mode of their operation therefore it be critical to keep tab on this reality as it can bring many benefit that can not be find elsewhere ai be one of the hot topic of discussion however it be now clear that ai be here to stay so people should accept the obvious in order to create the future that they desire the wise strategy be to embrace artificial intelligence and let it work to maintain our well be from a quick cheer to a stand ovation clap to show how much you enjoy this story the wisewolf crypto fund provide an easy way to enter the cryptocurrency market even for non techie
Justin Lee,8.3K,11,https://medium.com/swlh/chatbots-were-the-next-big-thing-what-happened-5fc49dd6fa61?source=---------1----------------,chatbots be the next big thing what happen the startup medium,oh how the headline blare chatbot be the next big thing our hope be sky high bright eyed and bushy tail the industry be ripe for a new era of innovation it be time to start socialize with machine and why wouldn t they be all the road sign point towards insane success at the mobile world congress 2017 chatbot be the main headliner the conference organizer cite an overwhelming acceptance at the event of the inevitable shift of focus for brand and corporate to chatbot in fact the only significant question around chatbot be who would monopolize the field not whether chatbot would take off in the first place one year on we have an answer to that question no because there isn t even an ecosystem for a platform to dominate chatbot weren t the first technological development to be talk up in grandiose term and then slump spectacularly the age old hype cycle unfold in familiar fashion expectation build build and then it all kind of fizzle out the predict paradim shift didn t materialize and app be tellingly still alive and well we look back at our breathless optimism and turn to each other slightly baffled be that it that be the chatbot revolution we be promise digit s ethan bloch sum up the general consensus accord to dave feldman vice president of product design at heap chatbots didn t just take on one difficult problem and fail they take on several and fail all of they bot can interface with user in different way the big divide be text vs speech in the beginning of computer interface be the write word user have to type command manually into a machine to get anything do then graphical user interface guis come along and save the day we become entrance by windows mouse click icon and hey we eventually get color too meanwhile a bunch of research scientist be busily develop natural language nl interface to database instead of have to learn an arcane database query language another bunch of scientist be develop speech processing software so that you could just speak to your computer rather than have to type this turn out to be a whole lot more difficult than anyone originally realise the next item on the agenda be hold a two way dialog with a machine here s an example dialog date back to the 1990 with vcr setup system pretty cool right the system take turn in collaborative way and do a smart job of figure out what the user want it be carefully craft to deal with conversation involve vcrs and could only operate within strict limitation modern day bot whether they use type or speak input have to face all these challenge but also work in an efficient and scalable way on a variety of platform basically we re still try to achieve the same innovation we be 30 year ago here s where I think we re go wrong an oversized assumption have be that app be over and would be replace by bot by pit two such disparate concept against one another instead of see they as separate entity design to serve different purpose we discourage bot development you might remember a similar war cry when app first come onto the scene ten year ago but do you remember when app replace the internet it s say that a new product or service need to be two of the follow well cheap or fast be chatbot cheap or fast than app no — not yet at least whether they re well be subjective but I think it s fair to say that today s good bot isn t comparable to today s good app plus nobody think that use lyft be too complicated or that it s too hard to order food or buy a dress on an app what be too complicated be try to complete these task with a bot — and have the bot fail a great bot can be about as useful as an average app when it come to rich sophisticated multi layer app there s no competition that s because machine let we access vast and complex information system and the early graphical information system be a revolutionary leap forward in help we locate those system modern day app benefit from decade of research and experimentation why would we throw this away but if we swap the word replace with extend thing get much more interesting today s most successful bot experience take a hybrid approach incorporate chat into a broad strategy that encompass more traditional element the next wave will be multimodal app where you can say what you want like with siri and get back information as a map text or even a spoken response another problematic aspect of the sweeping nature of hype be that it tend to bypass essential question like these for plenty of company bot just aren t the right solution the past two year be litter with case of bot be blindly apply to problem where they aren t need build a bot for the sake of it let it loose and hope for the good will never end well the vast majority of bot be build use decision tree logic where the bot s can response rely on spot specific keyword in the user input the advantage of this approach be that it s pretty easy to list all the case that they be design to cover and that s precisely their disadvantage too that s because these bot be purely a reflection of the capability fastidiousness and patience of the person who create they ; and how many user need and input they be able to anticipate problem arise when life refuse to fit into those box accord to recent report 70 % of the 100 000 + bot on facebook messenger be fail to fulfil simple user request this be partly a result of developer fail to narrow their bot down to one strong area of focus when we be build growthbot we decide to make it specific to sale and marketer not an all rounder despite the temptation to get overexcite about potential capabiltie remember a bot that do one thing well be infinitely more helpful than a bot that do multiple thing poorly a competent developer can build a basic bot in minute — but one that can hold a conversation that s another story despite the constant hype around ai we re still a long way from achieve anything remotely human like in an ideal world the technology know as nlp natural language processing should allow a chatbot to understand the message it receive but nlp be only just emerge from research lab and be very much in its infancy some platform provide a bit of nlp but even the good be at toddler level capacity for example think about siri understand your word but not their meaning as matt asay outline this result in another issue failure to capture the attention and creativity of developer and conversation be complex they re not linear topic spin around each other take random turn restart or abruptly finish today s rule base dialogue system be too brittle to deal with this kind of unpredictability and statistical approach use machine learning be just as limit the level of ai require for human like conversation just isn t available yet and in the meantime there be few high quality example of trailblaze bot to lead the way as dave feldman remark once upon a time the only way to interact with computer be by type arcane command to the terminal visual interface use window icon or a mouse be a revolution in how we manipulate information there s a reason compute move from text base to graphical user interface guis on the input side it s easy and fast to click than it be to type tap or selecting be obviously preferable to type out a whole sentence even with predictive often error prone text on the output side the old adage that a picture be worth a thousand word be usually true we love optical display of information because we be highly visual creature it s no accident that kid love touch screen the pioneer who dream up graphical interface be inspire by cognitive psychology the study of how the brain deal with communication conversational uis be mean to replicate the way human prefer to communicate but they end up require extra cognitive effort essentially we re swap something simple for a more complex alternative sure there be some concept that we can only express use language show I all the way of get to a museum that give I 2000 step but don t take long than 35 minute but most task can be carry out more efficiently and intuitively with guis than with a conversational ui aim for a human dimension in business interaction make sense if there s one thing that s break about sale and market it s the lack of humanity brand hide behind ticket number feedback form do not reply email automate response and gate contact we form facebook s goal be that their bot should pass the so call ture test mean you can t tell whether you be talk to a bot or a human but a bot isn t the same as a human it never will be a conversation encompasse so much more than just text human can read between the line leverage contextual information and understand double layer like sarcasm bot quickly forget what they re talk about mean it s a bit like converse with someone who have little or no short term memory as hubspot team pinpoint people aren t easily fool and pretend a bot be a human be guarantee to diminish return not to mention the fact that you re lie to your user and even those rare bot that be power by state of the art nlp and excel at processing and produce content will fall short in comparison and here s the other thing conversational uis be build to replicate the way human prefer to communicate — with other human but be that how human prefer to interact with machine not necessarily at the end of the day no amount of witty quip or human like mannerism will save a bot from conversational failure in a way those early adopter weren t entirely wrong people be yell at google home to play their favorite song order pizza from the domino s bot and get makeup tip from sephora but in term of consumer response and developer involvement chatbots haven t live up to the hype generate circa 2015 16 not even close computer be good at be computer search for datum crunch number analyze opinion and condense that information computer aren t good at understand human emotion the state of nlp mean they still don t get what we re ask they never mind how we feel that s why it s still impossible to imagine effective customer support sale or marketing without the essential human touch empathy and emotional intelligence for now bot can continue to help we with automate repetitive low level task and query ; as cog in a large more complex system and we do they and ourselves a disservice by expect so much so soon but that s not the whole story yes our industry massively overestimate the initial impact chatbot would have emphasis on initial as bill gate once say the hype be over and that s a good thing now we can start examine the middle ground grey area instead of the hyper inflate frantic black and white zone I believe we re at the very beginning of explosive growth this sense of anti climax be completely normal for transformational technology messaging will continue to gain traction chatbot aren t go away nlp and ai be become more sophisticated every day developer app and platform will continue to experiment with and heavily invest in conversational marketing and I can t wait to see what happen next from a quick cheer to a stand ovation clap to show how much you enjoy this story head of growth for growthbot messaging & conversational strategy @hubspot medium s large publication for maker subscribe to receive our top story here → https goo gl zhclji
Michael Solana,680,5,https://medium.com/s/story/artificial-intelligence-is-humanitys-rorschach-test-6fb1ef9c0ce4?source=---------2----------------,artificial intelligence be humanity s rorschach test,member feature story slime sunday founder fund slime sunday founder fund I don t fear artificial intelligence I fear people who fear artificial intelligence it s the 1960s a psychologist stare at his patient — a balding middle aged foreman with a cigarette in his hand and a curl of smoke around he like a halo on an acid trip the psychologist hold up an inkblot an ambiguous black splatter on a white flashcard and ask his patient what he see the thinking be his patient not willing or otherwise able to express his feeling his thought his motivation might inadvertently reveal some piece of his inner self while describe the ambiguous the foreman doesn t see a nondescript swiggle or stain he see a man and woman make love perhaps violently he see a mother hold her child he see a grisly murder while the description of these inkblot reveal very little about the world they reveal a great deal about the man describe they because when face with an inscrutable abstract he project himself onto the ambiguous let s look at this in the context of artificial intelligence I m not talk about self drive car or algorithm serve ad for wallpaper and nice leather boot on gmail I m not talk about the stuff we call artificial intelligence to raise money from bewildered venture capitalist on sand hill road I m talk about general artificial intelligence which be a computer that want stuff and chiefly to live I m talk about build a conscious machine just smart enough to make itself smart from here the thought experiment run like this the conscious machine do make itself smarter and once it s smart it learn how to make itself smart which it do for good measure the smart the machine become the fast this pattern repeat itself and the intelligence of the machine begin to increase exponentially in this way a conscious artificial intelligence bear on a tuesday morning might be twice as smart as the smart man who ever live by wednesday afternoon and omnipotent by friday this be how we invent the thing that invent god in nerd lore it s know as the singularity the question — the only question that could possibly matter to a human no long at the top of the intellectual food chain — be what do an exponential intelligence want conventional wisdom it extremely want to murder you the dystopian version of superintelligence be illustrate with frequency by leader in the technology industry and be famously depict by hollywood in film like terminator or more recently ex machina and even the avenger the angry god a I be a story you know because it be the story you be constantly tell we build the thinking machine it surpass our ability in every way and it destroy we for one of any number of reason maybe it perceive we as a threat maybe we re just in its way and it hardly perceive we at all — humanity a disposable insect race there be of course many argument in opposition to the now ubiquitous concept of our apocalypse by artificial intelligence I myself have call into question the logic of such dystopian argument in anatomy of next but our subject here be less pertain to the nature of the conscious machine than it be to the way we talk about this subject and what it mean first consider that most of the artificial intelligence depict in culture look human a representation with no basis in technological reality then the true scope of the singularity be almost impossible to predict which beg a question where be these opinion about the broadly unknowable come from there s an obvious difficulty in try to understand the hypothetical motivation of a hypothetically god like intelligence to your beloved labradoodle you be a being of immense magic with near unfathomable motivation you summon light and sound from inanimate matter soar through the street on angry metal cast fire from your hand the labradoodle s conception of man be distort because there be a vast difference between the intelligence of a dog and the intelligence of a human let we name this difference x now as we try and understand the difference between the most intelligent human who have ever live and a hypothetical god like intelligence bear of the singularity let we set our difference in intelligence at a conservative 1000x how do one even begin to conceive of a be this smart here we approach our inscrutable abstract and our robot rorschach test but in this contemporary version of the famous psychological prompt what we be observe be not even entirely ambiguous we be attempt to imagine a greatly amplified mind here each of we have a particularly relevant data point — our own in try to imagine the amplify intelligence it be natural to imagine our own intelligence amplify in imagine the motivation of this amplify intelligence we naturally imagine ourselves if as you try to conceive of a future with machine intelligence a monster come to mind it be likely you aren t afraid of something alien at all you re afraid of something exactly like you what would you do with unlimited power psychological projection seem to work in several context outside of general artificial intelligence in the technology industry the concept of meritocracy be now hotly debate how much of your life be determine by luck and how much by chance there s no answer here we know for sure but have there ever be a well rorschach test for separate high achiever from people who be give what they have question pertain to human nature be almost open self reflection be we basically good with some exception or be human basically beast with an animal nature just barely contain by a set of slowly erode story we tell ourselves — law faith society the inner working of a mind can t be fully share and they can t be observe by a neutral party we therefore do not — can not currently — know anything of the inner working of people in general but we can know ourselves so in the face of large abstraction concern intelligence we hold up a mirror not everyone who fear general artificial intelligence would cause harm to other there be many people who haven t think deeply about these question at all they look to their neighbor for cue on what to think and there be no shortage of people willing to tell they the medium have ad to sell after all and historically they have find great success in do this with horror story but as we try to understand the people who have think about these question with some depth — with the depth require of a thoughtful screenplay for example or a book or a company — it s worth consider the inkblot technology liberty teenager with superpower vp @foundersfund creator + producer # anatomyofnext welcome to a place where word matter on medium smart voice and original idea take center stage — with no ad in sight watch follow all the topic you care about and we ll deliver the good story for you to your homepage and inbox explore get unlimited access to the good story on medium — and support writer while you re at it just $ 5 month upgrade
Emmanuel Ameisen,935,11,https://blog.insightdatascience.com/reinforcement-learning-from-scratch-819b65f074d8?source=---------3----------------,reinforcement learning from scratch insight datum,want to learn about apply artificial intelligence from lead practitioner in silicon valley new york or toronto learn more about the insight artificial intelligence fellow program be you a company work in ai and would like to get involve in the insight ai fellow program feel free to get in touch recently I give a talk at the o reilly ai conference in beijing about some of the interesting lesson we ve learn in the world of nlp while there I be lucky enough to attend a tutorial on deep reinforcement learning deep rl from scratch by unity technology I think that the session lead by arthur juliani be extremely informative and want to share some big takeaway below in our conversation with company we ve see a rise of interesting deep rl application tool and result in parallel the inner working and application of deep rl such as alphago picture above can often seem esoteric and hard to understand in this post I will give an overview of core aspect of the field that can be understand by anyone many of the visual be from the slide of the talk and some be new the explanation and opinion be mine if anything be unclear reach out to I here deep rl be a field that have see vast amount of research interest include learn to play atari game beat pro player at dota 2 and defeat go champion contrary to many classical deep learning problem that often focus on perception do this image contain a stop sign deep rl add the dimension of action that influence the environment what be the goal and how do I get there in dialog system for example classical deep learning aim to learn the right response for a give query on the other hand deep reinforcement learning focus on the right sequence of sentence that will lead to a positive outcome for example a happy customer this make deep rl particularly attractive for task that require planning and adaptation such as manufacturing or self drive however industry application have trail behind the rapidly advance result come out of the research community a major reason be that deep rl often require an agent to experiment million of time before learn anything useful the good way to do this rapidly be by use a simulation environment this tutorial will be use unity to create environment to train agent in for this workshop lead by arthur juliani and leon chen their goal be to get every participant to successfully train multiple deep rl algorithm in 4 hour a tall order below be a comprehensive overview of many of the main algorithm that power deep rl today for a more complete set of tutorial arthur juliani write an 8 part series start here deep rl can be use to best the top human player at go but to understand how that s do you first need to understand a few simple concept start with much easy problem 1 it all start with slot machine let s imagine you be face with 4 chest that you can pick from at each turn each of they have a different average payout and your goal be to maximize the total payout you receive after a fix number of turn this be a classic problem call multi armed bandit and be where we will start the crux of the problem be to balance exploration which help we learn about which state be good and exploitation where we now use what we know to pick the good slot machine here we will utilize a value function that map our action to an estimate reward call the q function first we ll initialize all q value at equal value then we ll update the q value of each action pick each chest base on how good the payout be after choose this action this allow we to learn a good value function we will approximate our q function use a neural network start with a very shallow one that learn a probability distribution by use a softmax over the 4 potential chest while the value function tell we how good we estimate each action to be the policy be the function that determine which action we end up take intuitively we might want to use a policy that pick the action with the high q value this perform poorly in practice as our q estimate will be very wrong at the start before we gather enough experience through trial and error this be why we need to add a mechanism to our policy to encourage exploration one way to do that be to use epsilon greedy which consist of take a random action with probability epsilon we start with epsilon be close to 1 always choose random action and low epsilon as we go along and learn more about which chest be good eventually we learn which chest be good in practice we might want to take a more subtle approach than either take the action we think be the good or a random action a popular method be boltzmann exploration which adjust probability base on our current estimate of how good each chest be add in a randomness factor 2 add different state the previous example be a world in which we be always in the same state wait to pick from the same 4 chest in front of we most real word problem consist of many different state that be what we will add to our environment next now the background behind chest alternate between 3 color at each turn change the average value of the chest this mean we need to learn a q function that depend not only on the action the chest we pick but the state what the color of the background be this version of the problem be call contextual multi armed bandit surprisingly we can use the same approach as before the only thing we need to add be an extra dense layer to our neural network that will take in as input a vector represent the current state of the world 3 learn about the consequence of our action there be another key factor that make our current problem simple than most in most environment such as in the maze depict above the action that we take have an impact on the state of the world if we move up on this grid we might receive a reward or we might receive nothing but the next turn we will be in a different state this be where we finally introduce a need for plan first we will define our q function as the immediate reward in our current state plus the discount reward we be expect by take all of our future action this solution work if our q estimate of state be accurate so how can we learn a good estimate we will use a method call temporal difference td learn to learn a good q function the idea be to only look at a limited number of step in the future td 1 for example only use the next 2 state to evaluate the reward surprisingly we can use td 0 which look at the current state and our estimate of the reward the next turn and get great result the structure of the network be the same but we need to go through one forward step before receive the error we then use this error to back propagate gradient like in traditional deep learning and update our value estimate 3 + introduce monte carlo another method to estimate the eventual success of our action be monte carlo estimate this consist of play out the entire episode with our current policy until we reach an end success by reach a green block or failure by reach a red block in the image above and use that result to update our value estimate for each traverse state this allow we to propagate value efficiently in one batch at the end of an episode instead of every time we make a move the cost be that we be introduce noise to our estimate since we attribute very distant reward to they 4 the world be rarely discrete the previous method be use neural network to approximate our value estimate by mapping from a discrete number of state and action to a value in the maze for example there be 49 state square and 4 action move in each adjacent direction in this environment we be try to learn how to balance a ball on a 2 dimensional paddle by decide at each time step whether we want to tilt the paddle leave or right here the state space become continuous the angle of the paddle and the position of the ball the good news be we can still use neural network to approximate this function a note about off policy vs on policy learn the method we use previously be off policy method mean we can generate datum with any strategy use epsilon greedy for example and learn from it on policy method can only learn from action that be take follow our policy remember a policy be the method we use to determine which action to take this constrain our learning process as we have to have an exploration strategy that be build in to the policy itself but allow we to tie result directly to our reasoning and enable we to learn more efficiently the approach we will use here be call policy gradient and be an on policy method previously we be first learn a value function q for each action in each state and then build a policy on top in vanilla policy gradient we still use monte carlo estimate but we learn our policy directly through a loss function that increase the probability of choose reward action since we be learn on policy we can not use method such as epsilon greedy which include random choice to get our agent to explore the environment the way that we encourage exploration be by use a method call entropy regularization which push our probability estimate to be wide and thus will encourage we to make risky choice to explore the space 4 + leverage deep learning for representation in practice many state of the art rl method require learn both a policy and value estimate the way we do this with deep learning be by have both be two separate output of the same backbone neural network which will make it easy for our neural network to learn good representation one method to do this be advantage actor critic a2c we learn our policy directly with policy gradient define above and learn a value function use something call advantage instead of update our value function base on reward we update it base on our advantage which measure how much well or bad an action be than our previous value function estimate it to be this help make learn more stable compare to simple q learning and vanilla policy gradient 5 learn directly from the screen there be an additional advantage to use deep learning for these method which be that deep neural network excel at perceptive task when a human play a game the information receive be not a list of state but an image usually of a screen or a board or the surround environment image base learning combine a convolutional neural network cnn with rl in this environment we pass in a raw image instead of feature and add a 2 layer cnn to our architecture without change anything else we can even inspect activation to see what the network pick up on to determine value and policy in the example below we can see that the network use the current score and distant obstacle to estimate the value of the current state while focus on nearby obstacle for determine action neat as a side note while toy around with the provide implementation I ve find that visual learning be very sensitive to hyperparameter change the discount rate slightly for example completely prevent the neural network from learn even on a toy application this be a widely know problem but it be interesting to see it first hand 6 nuanced action so far we ve play with environment with continuous and discrete state space however every environment we study have a discrete action space we could move in one of four direction or tilt the paddle to the left or right ideally for application such as self drive car we would like to learn continuous action such as turn the steering wheel between 0 and 360 degree in this environment call 3d ball world we can choose to tilt the paddle to any value on each of its axis this give we more control as to how we perform action but make the action space much large we can approach this by approximate our potential choice with gaussian distribution we learn a probability distribution over potential action by learn the mean and variance of a gaussian distribution and our policy we sample from that distribution simple in theory 7 next step for the brave there be a few concept that separate the algorithm describe above from state of the art approach it s interesting to see that conceptually the good robotic and game playing algorithm be not that far away from the one we just explore that s it for this overview I hope this have be informative and fun if you be look to dive deeply into the theory of rl give arthur s post a read or diving deeply by follow david silver s ucl course if you be look to learn more about the project we do at insight or how we work with company please check we out below or reach out to I here want to learn about apply artificial intelligence from lead practitioner in silicon valley new york or toronto learn more about the insight artificial intelligence fellow program be you a company work in ai and would like to get involve in the insight ai fellow program feel free to get in touch from a quick cheer to a stand ovation clap to show how much you enjoy this story ai lead at insight ai @emmanuelameisen insight fellow program your bridge to a career in datum
Irhum Shafkat,2K,15,https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1?source=---------4----------------,intuitively understand convolution for deep learning,the advent of powerful and versatile deep learning framework in recent year have make it possible to implement convolution layer into a deep learning model an extremely simple task often achievable in a single line of code however understand convolution especially for the first time can often feel a bit unnerving with term like kernels filter channel and so on all stack onto each other yet convolution as a concept be fascinatingly powerful and highly extensible and in this post we ll break down the mechanic of the convolution operation step by step relate it to the standard fully connect network and explore just how they build up a strong visual hierarchy make they powerful feature extractor for image the 2d convolution be a fairly simple operation at heart you start with a kernel which be simply a small matrix of weight this kernel slide over the 2d input datum perform an elementwise multiplication with the part of the input it be currently on and then sum up the result into a single output pixel the kernel repeat this process for every location it slide over convert a 2d matrix of feature into yet another 2d matrix of feature the output feature be essentially the weighted sum with the weight be the value of the kernel itself of the input feature locate roughly in the same location of the output pixel on the input layer whether or not an input feature fall within this roughly same location get determine directly by whether it s in the area of the kernel that produce the output or not this mean the size of the kernel directly determine how many or few input feature get combine in the production of a new output feature this be all in pretty stark contrast to a fully connect layer in the above example we have 5×5=25 input feature and 3×3=9 output feature if this be a standard fully connect layer you d have a weight matrix of 25×9 = 225 parameter with every output feature be the weighted sum of every single input feature convolution allow we to do this transformation with only 9 parameter with each output feature instead of look at every input feature only get to look at input feature come from roughly the same location do take note of this as it ll be critical to our later discussion before we move on it s definitely worth look into two technique that be commonplace in convolution layer padding and stride padding do something pretty clever to solve this pad the edge with extra fake pixel usually of value 0 hence the oft use term zero padding this way the kernel when slide can allow the original edge pixel to be at its center while extend into the fake pixel beyond the edge produce an output the same size as the input the idea of the stride be to skip some of the slide location of the kernel a stride of 1 mean to pick slide a pixel apart so basically every single slide act as a standard convolution a stride of 2 mean pick slide 2 pixel apart skip every other slide in the process downsize by roughly a factor of 2 a stride of 3 mean skip every 2 slide downsize roughly by factor 3 and so on more modern network such as the resnet architecture entirely forgo pooling layer in their internal layer in favor of strided convolution when need to reduce their output size of course the diagram above only deal with the case where the image have a single input channel in practicality most input image have 3 channel and that number only increase the deep you go into a network it s pretty easy to think of channel in general as be a view of the image as a whole emphasise some aspect de emphasise other so this be where a key distinction between term come in handy whereas in the 1 channel case where the term filter and kernel be interchangeable in the general case they re actually pretty different each filter actually happen to be a collection of kernel with there be one kernel for every single input channel to the layer and each kernel be unique each filter in a convolution layer produce one and only one output channel and they do it like so each of the kernel of the filter slide over their respective input channel produce a process version of each some kernel may have strong weight than other to give more emphasis to certain input channel than other eg a filter may have a red kernel channel with strong weight than other and hence respond more to difference in the red channel feature than the other each of the per channel process version be then sum together to form one channel the kernel of a filter each produce one version of each channel and the filter as a whole produce one overall output channel finally then there s the bias term the way the bias term work here be that each output filter have one bias term the bias get add to the output channel so far to produce the final output channel and with the single filter case down the case for any number of filter be identical each filter process the input with its own different set of kernel and a scalar bias with the process describe above produce a single output channel they be then concatenate together to produce the overall output with the number of output channel be the number of filter a nonlinearity be then usually apply before pass this as input to another convolution layer which then repeat this process even with the mechanic of the convolution layer down it can still be hard to relate it back to a standard feed forward network and it still doesn t explain why convolution scale to and work so much well for image datum suppose we have a 4×4 input and we want to transform it into a 2×2 grid if we be use a feedforward network we d reshape the 4×4 input into a vector of length 16 and pass it through a densely connect layer with 16 input and 4 output one could visualize the weight matrix w for a layer and although the convolution kernel operation may seem a bit strange at first it be still a linear transformation with an equivalent transformation matrix if we be to use a kernel k of size 3 on the reshaped 4×4 input to get a 2×2 output the equivalent transformation matrix would be note while the above matrix be an equivalent transformation matrix the actual operation be usually implement as a very different matrix multiplication 2 the convolution then as a whole be still a linear transformation but at the same time it s also a dramatically different kind of transformation for a matrix with 64 element there s just 9 parameter which themselves be reuse several time each output node only get to see a select number of input the one inside the kernel there be no interaction with any of the other input as the weight to they be set to 0 it s useful to see the convolution operation as a hard prior on the weight matrix in this context by prior I mean predefine network parameter for example when you use a pretraine model for image classification you use the pretraine network parameter as your prior as a feature extractor to your final densely connect layer in that sense there s a direct intuition between why both be so efficient compare to their alternative transfer learning be efficient by order of magnitude compare to random initialization because you only really need to optimize the parameter of the final fully connect layer which mean you can have fantastic performance with only a few dozen image per class here you don t need to optimize all 64 parameter because we set most of they to zero and they ll stay that way and the rest we convert to share parameter result in only 9 actual parameter to optimize this efficiency matter because when you move from the 784 input of mnist to real world 224×224×3 image that s over 150 000 input a dense layer attempt to halve the input to 75 000 input would still require over 10 billion parameter for comparison the entirety of resnet 50 have some 25 million parameter so fix some parameter to 0 and tie parameter increase efficiency but unlike the transfer learning case where we know the prior be good because it work on a large general set of image how do we know this be any good the answer lie in the feature combination the prior lead the parameter to learn early on in this article we discuss that so with backpropagation come in all the way from the classification node of the network the kernel have the interesting task of learn weight to produce feature only from a set of local input additionally because the kernel itself be apply across the entire image the feature the kernel learn must be general enough to come from any part of the image if this be any other kind of data eg categorical datum of app install this would ve be a disaster for just because your number of app install and app type column be next to each other doesn t mean they have any local share feature common with app install date and time use sure the four may have an underlie high level feature eg which app people want most that can be find but that give we no reason to believe the parameter for the first two be exactly the same as the parameter for the latter two the four could ve be in any consistent order and still be valid pixel however always appear in a consistent order and nearby pixel influence a pixel e g if all nearby pixel be red it s pretty likely the pixel be also red if there be deviation that s an interesting anomaly that could be convert into a feature and all this can be detect from compare a pixel with its neighbor with other pixel in its locality and this idea be really what a lot of early computer vision feature extraction method be base around for instance for edge detection one can use a sobel edge detection filter a kernel with fix parameter operate just like the standard one channel convolution for a non edge contain grid eg the background sky most of the pixel be the same value so the overall output of the kernel at that point be 0 for a grid with an vertical edge there be a difference between the pixel to the left and right of the edge and the kernel compute that difference to be non zero activating and reveal the edge the kernel only work only a 3×3 grid at a time detect anomaly on a local scale yet when apply across the entire image be enough to detect a certain feature on a global scale anywhere in the image so the key difference we make with deep learning be ask this question can useful kernel be learn for early layer operate on raw pixel we could reasonably expect feature detector of fairly low level feature like edge line etc there s an entire branch of deep learning research focus on make neural network model interpretable one of the most powerful tool to come out of that be feature visualization use optimization 3 the idea at core be simple optimize a image usually initialize with random noise to activate a filter as strongly as possible this do make intuitive sense if the optimize image be completely fill with edge that s strong evidence that s what the filter itself be look for and be activate by use this we can peek into the learn filter and the result be stunning one important thing to notice here be that convolve image be still image the output of a small grid of pixel from the top left of an image will still be on the top leave so you can run another convolution layer on top of another such as the two on the left to extract deep feature which we visualize yet however deep our feature detector get without any further change they ll still be operate on very small patch of the image no matter how deep your detector be you can t detect face from a 3×3 grid and this be where the idea of the receptive field come in a essential design choice of any cnn architecture be that the input size grow small and small from the start to the end of the network while the number of channel grow deep this as mention early be often do through stride or pool layer locality determine what input from the previous layer the output get to see the receptive field determine what area of the original input to the entire network the output get to see the idea of a strided convolution be that we only process slide a fix distance apart and skip the one in the middle from a different point of view we only keep output a fix distance apart and remove the rest 1 we then apply a nonlinearity to the output and per usual then stack another new convolution layer on top and this be where thing get interesting even if be we to apply a kernel of the same size 3×3 have the same local area to the output of the strided convolution the kernel would have a large effective receptive field this be because the output of the strided layer still do represent the same image it be not so much cropping as it be resize only thing be that each single pixel in the output be a representative of a large area of whose other pixel be discard from the same rough location from the original input so when the next layer s kernel operate on the output it s operate on pixel collect from a large area note if you re familiar with dilated convolution note that the above be not a dilated convolution both be method of increase the receptive field but dilated convolution be a single layer while this take place on a regular convolution follow a strided convolution with a nonlinearity inbetween this expansion of the receptive field allow the convolution layer to combine the low level feature line edge into high level feature curve texture as we see in the mixed3a layer follow by a pooling stride layer the network continue to create detector for even high level feature part pattern as we see for mixed4a the repeat reduction in image size across the network result in by the 5th block on convolution input size of just 7×7 compare to input of 224×224 at this point each single pixel represent a grid of 32×32 pixel which be huge compare to early layer where an activation mean detect an edge here an activation on the tiny 7×7 grid be one for a very high level feature such as for bird the network as a whole progress from a small number of filter 64 in case of googlenet detect low level feature to a very large number of filter 1024 in the final convolution each look for an extremely specific high level feature follow by a final pooling layer which collapse each 7×7 grid into a single pixel each channel be a feature detector with a receptive field equivalent to the entire image compare to what a standard feedforward network would have do the output here be really nothing short of awe inspire a standard feedforward network would have produce abstract feature vector from combination of every single pixel in the image require intractable amount of datum to train the cnn with the prior impose on it start by learn very low level feature detector and as across the layer as its receptive field be expand learn to combine those low level feature into progressively high level feature ; not an abstract combination of every single pixel but rather a strong visual hierarchy of concept by detect low level feature and use they to detect high level feature as it progress up its visual hierarchy it be eventually able to detect entire visual concept such as face bird tree etc and that s what make they such powerful yet efficient with image datum with the visual hierarchy cnn build it be pretty reasonable to assume that their vision system be similar to human and they re really great with real world image but they also fail in way that strongly suggest their vision system aren t entirely human like the most major problem adversarial example 4 example which have be specifically modify to fool the model adversarial example would be a non issue if the only tamper one that cause the model to fail be one that even human would notice the problem be the model be susceptible to attack by sample which have only be tamper with ever so slightly and would clearly not fool any human this open the door for model to silently fail which can be pretty dangerous for a wide range of application from self drive car to healthcare robustness against adversarial attack be currently a highly active area of research the subject of many paper and even competition and solution will certainly improve cnn architecture to become safe and more reliable cnn be the model that allow computer vision to scale from simple application to power sophisticated product and service range from face detection in your photo gallery to make well medical diagnosis they might be the key method in computer vision go forward or some other new breakthrough might just be around the corner regardless one thing be for sure they re nothing short of amazing at the heart of many present day innovative application and be most certainly worth deeply understanding hope you enjoy this article if you d like to stay connected you ll find I on twitter here if you have a question comment be welcome — I find they to be useful to my own learning process as well from a quick cheer to a stand ovation clap to show how much you enjoy this story curious programmer tinker around in python and deep learning sharing concept idea and code
Sam Drozdov,2.3K,6,https://uxdesign.cc/an-intro-to-machine-learning-for-designers-5c74ba100257?source=---------5----------------,an intro to machine learning for designer ux collective,there be an ongoing debate about whether or not designer should write code wherever you fall on this issue most people would agree that designer should know about code this help designer understand constraint and empathize with developer it also allow designer to think outside of the pixel perfect box when problem solve for the same reason designer should know about machine learning put simply machine learning be a field of study that give computer the ability to learn without be explicitly program arthur samuel 1959 even though arthur samuel coin the term over fifty year ago only recently have we see the most exciting application of machine learning — digital assistant autonomous driving and spam free email all exist thank to machine learning over the past decade new algorithm well hardware and more datum have make machine learn an order of magnitude more effective only in the past few year company like google amazon and apple have make some of their powerful machine learning tool available to developer now be the good time to learn about machine learning and apply it to the product you be build since machine learning be now more accessible than ever before designer today have the opportunity to think about how machine learning can be apply to improve their product designer should be able to talk with software developer about what be possible how to prepare and what outcome to expect below be a few example application that should serve as inspiration for these conversation machine learning can help create user centric product by personalize experience to the individual who use they this allow we to improve thing like recommendation search result notification and ad machine learning be effective at find abnormal content credit card company use this to detect fraud email provider use this to detect spam and social medium company use this to detect thing like hate speech machine learning have enable computer to begin to understand the thing we say natural language processing and the thing we see computer vision this allow siri to understand siri set a reminder google photo to create album of your dog and facebook to describe a photo to those visually impair machine learning be also helpful in understand how user be group this insight can then be use to look at analytic on a group by group basis from here different feature can be evaluate across group or be roll out to only a particular group of user machine learning allow we to make prediction about how a user might behave next know this we can help prepare for a user s next action for example if we can predict what content a user be plan on view we can preload that content so it s immediately ready when they want it depend on the application and what datum be available there be different type of machine learning algorithm to choose from I ll briefly cover each of the follow supervised learning allow we to make prediction use correctly label datum label datum be a group of example that have informative tag or output for example photo with associate hashtag or a house s feature eq number of bedroom location and its price by use supervised learning we can fit a line to the label datum that either split the datum into category or represent the trend of the datum use this line we be able to make prediction on new datum for example we can look at new photo and predict hashtag or look at a new house s feature and predict its price if the output we be try to predict be a list of tag or value we call it classification if the output we be try to predict be a number we call it regression unsupervised learning be helpful when we have unlabele datum or we be not exactly sure what output like an image s hashtag or a house s price be meaningful instead we can identify pattern among unlabeled datum for example we can identify related item on an e commerce website or recommend item to someone base on other who make similar purchase if the pattern be a group we call it a cluster if the pattern be a rule e q if this then that we call it an association reinforcement learning doesn t use an exist datum set instead we create an agent to collect its own datum through trial and error in an environment where it be reinforce with a reward for example an agent can learn to play mario by receive a positive reward for collect coin and a negative reward for walk into a goomba reinforcement learning be inspire by the way that human learn and have turn out to be an effective way to teach computer specifically reinforcement have be effective at training computer to play game like go and dota understand the problem you be try to solve and the available datum will constrain the type of machine learning you can use e q identify object in an image with supervised learning require a label datum set of image however constraint be the fruit of creativity in some case you can set out to collect datum that be not already available or consider other approach even though machine learning be a science it come with a margin of error it be important to consider how a user s experience might be impact by this margin of error for example when an autonomous car fail to recognize its surrounding people can get hurt even though machine learning have never be as accessible as it be today it still require additional resource developer and time to be integrate into a product this make it important to think about whether the result impact justify the amount of resource need to implement we have barely cover the tip of the iceberg but hopefully at this point you feel more comfortable thinking about how machine learning can be apply to your product if you be interested in learn more about machine learning here be some helpful resource thank for read chat with I on twitter @samueldrozdov from a quick cheer to a stand ovation clap to show how much you enjoy this story digital product designer samueldrozdov com curate story on user experience usability and product design by @fabriciot and @caioab
Conor Dewey,252,10,https://towardsdatascience.com/the-big-list-of-ds-ml-interview-resources-2db4f651bd63?source=---------6----------------,the big list of ds ml interview resource towards data science,data science interview certainly aren t easy I know this first hand I ve participate in over 50 individual interview and phone screen while apply for competitive internship over the last calendar year through this exciting and somewhat at time very painful process I ve accumulate a plethora of useful resource that help I prepare for and eventually pass data science interview long story short I ve decide to sort through all my bookmark and note in order to deliver a comprehensive list of data science resource with this list by your side you should have more than enough effective tool at your disposal next time you re preppe for a big interview it s worth note that many of these resource be naturally go to geared towards entry level and intern datum science position as that s where my expertise lie keep that in mind and enjoy here s some of the more general resource cover data science as a whole specifically I highly recommend check out the first two link regard 120 datum science interview question while the ebook itself be a couple buck out of pocket the answer themselves be free on quora these be some of my favorite full coverage question to practice with right before an interview even data scientist can not escape the dreaded algorithmic code interview in my experience this isn t the case 100 % of the time but chance be you ll be ask to work through something similar to an easy or medium question on leetcode or hackerrank as far as language go most company will let you use whatever language you want personally I do almost all of my algorithmic coding in java even though the position be target at python and r programmer if I have to recommend one thing it s to break out your wallet and invest in crack the code interview it absolutely live up to the hype I plan to continue use it for year to come once the interviewer know that you can think through problem and code effectively chance be that you ll move onto some more data science specific application depend on the interviewer and the position you will likely be able to choose between python and r as your tool of choice since I m partial to python my resource below will primarily focus on effectively use panda and numpy for datum analysis a data science interview typically isn t complete without check your knowledge of sql this can be do over the phone or through a live code question more likely the latter I ve find that the difficulty level of these question can vary a good bit range from be painfully easy to require complex join and obscure function our good friend statistic be still crucial for datum scientist and it s reflect as such in interview I have many interview begin by see if I can explain a common statistic or probability concept in simple and concise term as position get more experienced I suspect this happen less and less as traditional statistical question begin to take the more practical form of a b testing scenario cover later in the post you ll notice that I ve compile a few more resource here than in other section this isn t a mistake machine learning be a complex field that be a virtual guarantee in data science interview today the way that you ll be test on this be no guarantee however it may come up as a conceptual question regard cross validation or bias variance tradeoff or it may take the form of a take home assignment with a dataset attach I ve see both several time so you ve get to be prepare for anything specifically check out the machine learn flashcard below they re only a couple buck and be my by far my favorite way to quiz myself on any conceptual ml stuff this win t be cover in every single data science interview but it s certainly not uncommon most interview will have atleast one section solely dedicate to product thinking which often lend itself to a b testing of some sort make sure your familiar with the concept and statistical background necessary in order to be prepare when it come up if you have time to spare I take the free online course by udacity and overall I be pretty impressed lastly I want to call out all of the post relate to data science job and interview that I read over and over again to understand not only how to prepare but what to expect as well if you only check out one section here this be the one to focus on this be the layer that sit on top of all the technical skill and application don t overlook it I hope you find these resource useful during your next interview or job search I know I do truthfully I m just glad that I save these link somewhere lastly this post be part of an ongoing initiative to open source my experience apply and interview at data science position so if you enjoy this content then be sure to follow I for more stuff like this if you re interested in receive my weekly rundown of interesting article and resource focus on data science machine learning and artificial intelligence then subscribe to self drive data science use the form below if you enjoy this post feel free to hit the clap button and if you re interested in post to come make sure to follow I on medium at the link below — I ll be write and shipping every day this month as part of a 30 day challenge this article be originally publish on conordewey com from a quick cheer to a stand ovation clap to show how much you enjoy this story data scientist & writer | www conordewey com sharing concept idea and code
Abhishek Parbhakar,937,6,https://towardsdatascience.com/must-know-information-theory-concepts-in-deep-learning-ai-e54a5da9769d?source=---------7----------------,must know information theory concept in deep learning ai,information theory be an important field that have make significant contribution to deep learning and ai and yet be unknown to many information theory can be see as a sophisticated amalgamation of basic building block of deep learning calculus probability and statistic some example of concept in ai that come from information theory or related field in the early 20th century scientist and engineer be struggle with the question how to quantify the information be there a analytical way or a mathematical measure that can tell we about the information content for example consider below two sentence it be not difficult to tell that the second sentence give we more information since it also tell that bruno be big and brown in addition to be a dog how can we quantify the difference between two sentence can we have a mathematical measure that tell we how much more information second sentence have as compare to the first scientist be struggle with these question semantic domain and form of datum only add to the complexity of the problem then mathematician and engineer claude shannon come up with the idea of entropy that change our world forever and mark the beginning of digital information age shannon propose that the semantic aspect of datum be irrelevant and nature and meaning of data doesn t matter when it come to information content instead he quantify information in term of probability distribution and uncertainty shannon also introduce the term bit that he humbly credit to his colleague john tukey this revolutionary idea not only lay the foundation of information theory but also open new avenue for progress in field like artificial intelligence below we discuss four popular widely use and must know information theoretic concept in deep learning and datum science also call information entropy or shannon entropy entropy give a measure of uncertainty in an experiment let s consider two experiment if we compare the two experiment in exp 2 it be easy to predict the outcome as compare to exp 1 so we can say that exp 1 be inherently more uncertain unpredictable than exp 2 this uncertainty in the experiment be measure use entropy therefore if there be more inherent uncertainty in the experiment then it have high entropy or less the experiment be predictable more be the entropy the probability distribution of experiment be use to calculate the entropy a deterministic experiment which be completely predictable say toss a coin with p h = 1 have entropy zero an experiment which be completely random say roll fair dice be least predictable have maximum uncertainty and have the high entropy among such experiment another way to look at entropy be the average information gain when we observe outcome of an random experiment the information gain for a outcome of an experiment be define as a function of probability of occurrence of that outcome more the rarer be the outcome more be the information gain from observe it for example in an deterministic experiment we always know the outcome so no new information gain be here from observe the outcome and hence entropy be zero for a discrete random variable x with possible outcome state x_1 x_n the entropy in unit of bit be define as where p x_i be the probability of i^th outcome of x cross entropy be use to compare two probability distribution it tell we how similar two distribution be cross entropy between two probability distribution p and q define over same set of outcome be give by mutual information be a measure of mutual dependency between two probability distribution or random variable it tell we how much information about one variable be carry by the another variable mutual information capture dependency between random variable and be more generalized than vanilla correlation coefficient which capture only the linear relationship mutual information of two discrete random variable x and y be define as where p x y be the joint probability distribution of x and y and p x and p y be the marginal probability distribution of x and y respectively also call relative entropy kl divergence be another measure to find similarity between two probability distribution it measure how much one distribution diverge from the other suppose we have some datum and true distribution underlie it be p but we don t know this p so we choose a new distribution q to approximate this datum since q be just an approximation it win t be able to approximate the datum as good as p and some information loss will occur this information loss be give by kl divergence kl divergence between p and q tell we how much information we lose when we try to approximate datum give by p with q kl divergence of a probability distribution q from another probability distribution p be define as kl divergence be commonly use in unsupervised machine learning technique variational autoencoder information theory be originally formulate by mathematician and electrical engineer claude shannon in his seminal paper a mathematical theory of communication in 1948 note term experiment random variable & ai machine learn deep learning data science have be use loosely above but have technically different meaning in case you like the article do follow I abhishek parbhakar for more article relate to ai philosophy and economic from a quick cheer to a stand ovation clap to show how much you enjoy this story finding equilibria among ai philosophy and economic sharing concept idea and code
Aman Dalmia,2.3K,17,https://blog.usejournal.com/what-i-learned-from-interviewing-at-multiple-ai-companies-and-start-ups-a9620415e4cc?source=---------8----------------,what I learn from interview at multiple ai company and start up,over the past 8 month I ve be interview at various company like google s deepmind wadhwani institute of ai microsoft ola fractal analytic and a few other primarily for the role — data scientist software engineer & research engineer in the process not only do I get an opportunity to interact with many great mind but also have a peek at myself along with a sense of what people really look for when interview someone I believe that if I d have this knowledge before I could have avoid many mistake and have prepare in a much well manner which be what the motivation behind this post be to be able to help someone bag their dream place of work this post arise from a discussion with one of my junior on the lack of really fulfil job opportunity offer through campus placement for people work in ai also when I be prepare I notice people use a lot of resource but as per my experience over the past month I realise that one can do away with a few minimal one for most role in ai all of which I m go to mention at the end of the post I begin with how to get notice a k a the interview then I provide a list of company and start up to apply which be follow by how to ace that interview base on whatever experience I ve have I add a section on what we should strive to work for I conclude with minimal resource you need for preparation note for people who be sit for campus placement there be two thing I d like to add firstly most of what I m go to say except for the last one maybe be not go to be relevant to you for placement but and this be my second point as I mention before opportunity on campus be mostly in software engineering role have no intersection with ai so this post be specifically mean for people who want to work on solve interesting problem use ai also I want to add that I haven t clear all of these interview but I guess that s the essence of failure — it s the great teacher the thing that I mention here may not all be useful but these be thing that I do and there s no way for I to know what might have end up make my case strong to be honest this step be the most important one what make off campus placement so tough and exhausting be get the recruiter to actually go through your profile among the plethora of application that they get have a contact inside the organisation place a referral for you would make it quite easy but in general this part can be sub divide into three key step a do the regulatory preparation and do that well so with regulatory preparation I mean — a linkedin profile a github profile a portfolio website and a well polished cv firstly your cv should be really neat and concise follow this guide by udacity for clean up your cv — resume revamp it have everything that I intend to say and I ve be use it as a reference guide myself as for the cv template some of the in build format on overleaf be quite nice I personally use deedy resume here s a preview as it can be see a lot of content can be fit into one page however if you really do need more than that then the format link above would not work directly instead you can find a modified multi page format of the same here the next most important thing to mention be your github profile a lot of people underestimate the potential of this just because unlike linkedin it doesn t have a who view your profile option people do go through your github because that s the only way they have to validate what you have mention in your cv give that there s a lot of noise today with people associate all kind of buzzword with their profile especially for data science open source have a big role to play too with majority of the tool implementation of various algorithms list of learn resource all be open source I discuss the benefit of get involve in open source and how one can start from scratch in an early post here the bare minimum for now should be • create a github account if you don t already have one • create a repository for each of the project that you have do • add documentation with clear instruction on how to run the code• add documentation for each file mention the role of each function the meaning of each parameter proper format e g pep8 for python along with a script to automate the previous step optional move on the third step be what most people lack which be have a portfolio website demonstrate their experience and personal project make a portfolio indicate that you be really serious about get into the field and add a lot of point to the authenticity factor also you generally have space constraint on your cv and tend to miss out on a lot of detail you can use your portfolio to really delve deep into the detail if you want to and it s highly recommend to include some sort of visualisation or demonstration of the project idea it s really easy to create one too as there be a lot of free platform with drag and drop feature make the process really painless I personally use weebly which be a widely use tool it s well to have a reference to begin with there be a lot of awesome one out there but I refer to deshraj yadav s personal website to begin with make mine finally a lot of recruiter and start up have nowadays start use linkedin as their go to platform for hire a lot of good job get post there apart from recruiter the people work at influential position be quite active there as well so if you can grab their attention you have a good chance of get in too apart from that maintain a clean profile be necessary for people to have the will to connect with you an important part of linkedin be their search tool and for you to show up you must have the relevant keyword intersperse over your profile it take I a lot of iteration and re evaluation to finally have a decent one also you should definitely ask people with or under whom you ve work with to endorse you for your skill and add a recommendation talk about their experience of work with you all of this increase your chance of actually get notice I ll again point towards udacity s guide for linkedin and github profile all this might seem like a lot but remember that you don t need to do it in a single day or even a week or a month it s a process it never end set up everything at first would definitely take some effort but once it s there and you keep update it regularly as event around you keep happen you ll not only find it to be quite easy but also you ll be able to talk about yourself anywhere anytime without have to explicitly prepare for it because you become so aware about yourself b stay authentic I ve see a lot of people do this mistake of present themselves as per different job profile accord to I it s always well to first decide what actually interest you what would you be happy do and then search for relevant opportunity ; not the other way round the fact that the demand for ai talent surpass the supply for the same give you this opportunity spending time on your regulatory preparation mention above would give you an all around perspective on yourself and help make this decision easy also you win t need to prepare answer to various kind of question that you get ask during an interview most of they would come out naturally as you d be talk about something you really care about c network once you re do with a figure out b networking be what will actually help you get there if you don t talk to people you miss out on hear about many opportunity that you might have a good shot at it s important to keep connect with new people each day if not physically then on linkedin so that upon compound it after many day you have a large and strong network networking be not message people to place a referral for you when I be start off I do this mistake way too often until I stumble upon this excellent article by mark meloon where he talk about the importance of build a real connection with people by offer our help first another important step in networking be to get your content out for example if you re good at something blog about it and share that blog on facebook and linkedin not only do this help other it help you as well once you have a good enough network your visibility increase multi fold you never know how one person from your network like or comment on your post may help you reach out to a much broad audience include people who might be look for someone of your expertise I m present this list in alphabetical order to avoid the misinterpretation of any specific preference however I do place a * on the one that I d personally recommend this recommendation be base on either of the following mission statement people personal interaction or scope of learn more than 1 * be purely base on the 2nd and 3rd factor your interview begin the moment you have enter the room and a lot of thing can happen between that moment and the time when you re ask to introduce yourself — your body language and the fact that you re smile while greet they play a big role especially when you re interview for a start up as culture fit be something that they extremely care about you need to understand that as much as the interviewer be a stranger to you you re a stranger to he she too so they re probably just as nervous as you be it s important to view the interview as more of a conversation between yourself and the interviewer both of you be look for a mutual fit — you be look for an awesome place to work at and the interviewer be look for an awesome person like you to work with so make sure that you re feel good about yourself and that you take the charge of make the initial moment of your conversation pleasant for they and the easy way I know how to make that happen be to smile there be mostly two type of interview — one where the interviewer have come with come prepare set of question and be go to just ask you just that irrespective of your profile and the second where the interview be base on your cv I ll start with the second one this kind of interview generally begin with a can you tell I a bit about yourself at this point 2 thing be a big no — talk about your gpa in college and talk about your project in detail an ideal statement should be about a minute or two long should give a good idea on what have you be do till now and it s not restrict to academic you can talk about your hobby like read book play sport meditation etc — basically anything that contribute to define you the interviewer will then take something that you talk about here as a cue for his next question and then the technical part of the interview begin the motive of this kind of interview be to really check whether whatever you have write on your cv be true or not there would be a lot of question on what could be do differently or if x be use instead of y what would have happen at this point it s important to know the kind of trade off that be usually make during implementation for e g if the interviewer say that use a more complex model would have give well result then you might say that you actually have less datum to work with and that would have lead to overfitte in one of the interview I be give a case study to work on and it involve designing algorithm for a real world use case I ve notice that once I ve be give the green flag to talk about a project the interviewer really like it when I talk about it in the follow flow problem > 1 or 2 previous approach > our approach > result > intuition the other kind of interview be really just to test your basic knowledge don t expect those question to be too hard but they would definitely scratch every bit of the basic that you should be have mainly base around linear algebra probability statistic optimisation machine learning and or deep learn the resource mention in the minimal resource you need for preparation section should suffice but make sure that you don t miss out one bit among they the catch here be the amount of time you take to answer those question since these cover the basic they expect that you should be answer they almost instantly so do your preparation accordingly throughout the process it s important to be confident and honest about what you know and what you don t know if there s a question that you re certain you have no idea about say it upfront rather than make aah um sound if some concept be really important but you be struggle with answer it the interviewer would generally depend on how you do in the initial part be happy to give you a hint or guide you towards the right solution it s a big plus if you manage to pick their hint and arrive at the correct solution try to not get nervous and the good way to avoid that is by again smile now we come to the conclusion of the interview where the interviewer would ask you if you have any question for they it s really easy to think that your interview be do and just say that you have nothing to ask I know many people who get reject just because of fail at this last question as I mention before it s not only you who be be interview you be also look for a mutual fit with the company itself so it s quite obvious that if you really want to join a place you must have many question regard the work culture there or what kind of role be they see you in it can be as simple as be curious about the person interview you there s always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you re truly interested in be a part of their team a final question that I ve start ask all my interviewer be for a feedback on what they might want I to improve on this have help I tremendously and I still remember every feedback that I ve get which I ve incorporate into my daily life that s it base on my experience if you re just honest about yourself be competent truly care about the company you re interview for and have the right mindset you should have tick all the right box and should be get a congratulatory mail soon 😄 we live in an era full of opportunity and that apply to anything that you love you just need to strive to become the good at it and you will find a way to monetise it as gary vaynerchuk just follow he already say this be a great time to be work in ai and if you re truly passionate about it you have so much that you can do with ai you can empower so many people that have always be under represent we keep nagging about the problem surround we but there s be never such a time where common people like we can actually do something about those problem rather than just complain jeffrey hammerbacher founder cloudera have famously say we can do so much with ai than we can ever imagine there be many extremely challenging problem out there which require incredibly smart people like you to put your head down on and solve you can make many life well time to let go of what be cool or what would look good think and choose wisely any data science interview comprise of question mostly of a subset of the follow four category computer science math statistic and machine learning if you re not familiar with the math behind deep learning then you should consider go over my last post for resource to understand they however if you be comfortable I ve find that the chapter 2 3 and 4 of the deep learning book be enough to prepare revise for theoretical question during such interview I ve be prepare summary for a few chapter which you can refer to where I ve try to even explain a few concept that I find challenge to understand at first in case you be not willing to go through the entire chapter and if you ve already do a course on probability you should be comfortable answer a few numerical as well for stat cover these topic should be enough now the range of question here can vary depend on the type of position you be apply for if it s a more traditional machine learning base interview where they want to check your basic knowledge in ml you can complete any one of the follow course machine learning by andrew ng — cs 229 machine learning course by caltech professor yas abu mostafa important topic be supervise learn classification regression svm decision tree random forest logistic regression multi layer perceptron parameter estimation bayes decision rule unsupervise learn k mean cluster gaussian mixture model dimensionality reduction pca now if you re apply for a more advanced position there s a high chance that you might be question on deep learning in that case you should be very comfortable with convolutional neural network cnn and or depend upon what you ve work on recurrent neural network rnn and their variant and by be comfortable you must know what be the fundamental idea behind deep learning how cnns rnns actually work what kind of architecture have be propose and what have be the motivation behind those architectural change now there s no shortcut for this either you understand they or you put enough time to understand they for cnn the recommend resource be stanford s cs 231n and cs 224n for rnn I find this neural network class by hugo larochelle to be really enlighten too refer this for a quick refresher too udacity come to the aid here too by now you should have figure out that udacity be a really important place for an ml practitioner there be not a lot of place work on reinforcement learning rl in india and I too be not experience in rl as of now so that s one thing to add to this post sometime in the future get place off campus be a long journey of self realisation I realise that this have be another long post and I m again extremely grateful to you for value my thought I hope that this post find a way of be useful to you and that it help you in some way to prepare for your next datum science interview well if it do I request you to really think about what I talk about in what we should strive to work for I m very thankful to my friend from iit guwahati for their helpful feedback especially ameya godbole kothapalli vignesh and prabal jain a majority of what I mention here like view an interview as a conversation and seek feedback from our interviewer arise from multiple discussion with prabal who have be advise I constantly on how I can improve my interview skill this story be publish in noteworthy where thousand come every day to learn about the people & idea shape the product we love follow our publication to see more product & design story feature by the journal team from a quick cheer to a stand ovation clap to show how much you enjoy this story ai fanatic • math lover • dreamer the official journal blog
Lance Ulanoff,15.1K,5,https://medium.com/@LanceUlanoff/did-google-duplex-just-pass-the-turing-test-ffcfe6868b02?source=---------9----------------,do google duplex just pass the ture test lance ulanoff medium,I think it be the first um that be the moment when I realize I be hear something extraordinary a computer carry out a completely natural and very human sound conversation with a real person and it wasn t just a random talk this conversation have a purpose a destination to make an appointment at a hair salon the entity make the call and appointment be google assistant run duplex google s still experimental ai voice system and the venue be google I o google s yearly developer conference which this year focus heavily on the late development in ai machine and deep learning google ceo sundar pichai explain that what we be hear be a real phone call make to a hair salon that didn t know it be part of an experiment or that they be talk to a computer he launch duplex by ask google assistant to book a haircut appointment for tuesday morning the ai do the rest duplex make the call and when someone at the salon pick up the voice ai start the conversation with hi I m call to book a woman s hair cut appointment for a client um I m look for something on may third when the attendant ask duplex to give she one second duplex respond with mmm hmm the conversation continue as the salon representative present various date and time and the ai ask about other option eventually the ai and the salon worker agree on an appointment date and time what I hear be so convincing I have trouble discern who be the salon worker and who what be the duplex ai it be stunning and somewhat disconcert I liken it to the feeling you d get if a store mannequin suddenly smile at you it be easily the most remarkable human computer conversation I d ever hear and the close thing I ve see a voice ai pass the turing test which be the ai threshold suggest by computer scientist alan ture in the 1950 ture posit that by 2000 computer would be able to fool human into thinking they be converse with other human at least 30 % of the time he be right in 2014 a chatbot name eugene goostman successfully impersonate a wise ass 14 year old programmer during lengthy text base chat with unsuspecting human ture however hadn t necessarily consider voice base system and for obvious reason talk computer be somewhat less adept at fool human spend a few minute converse with your voice assistant of choice and you ll soon discover their limitation their speech can be stilte pronunciation off and response time can be slow especially if they re try to access a cloud base server and forget about conversation most can handle two consecutive query at most and they virtually all require a trigger phrase like alexa or hey siri google be work on remove unnecessary okay google in short back and forth convos with the digital assistant google assistant run duplex didn t exhibit any of those short coming it sound like a young female assistant carefully schedule her boss s haircut in addition to the natural cadence google add speech disfluencie the verbal tick um uhs and mm hmms and latency or pause that naturally occur when people be speak the result be a perfectly human voice produce entirely by a computer the second call demonstration where a male voice duplex try to make restaurant reservation be even more remarkable the human call participant didn t entirely understand duplex s verbal request and then tell duplex that for the number of people it want to bring to the restaurant they didn t need a reservation duplex handle all this without miss a beat the amazing thing be that the assistant can actually understand the nuance of conversation say pichai during the keynote that ability come by way of neural network technology and intensive machine learning for as accomplished as duplex be in make hair appointment and restaurant reservation it might stumble in deep or more abstract conversation in a blog post on duplex development google engineer explain that they constrain duplex s training to closed domain or well define topic like dinner reservation and hair appointment this give they the ability to perform intense exploration of the topic and focus training duplex be guide during training within the domain by experienced operator who could keep track of mistake and work with engineer to improve response in short this mean that while duplex have your hair and dine out option cover it could stumble in movie reservation and negotiation with your cable provider even so duplex fool two human I hear no hesitation or confusion in the hair salon call there be no indication that the salon worker think something be amiss she want to help this young woman make an appointment what will she think when she learn she be dupe by duplex obviously duplex s conversation be also short each last less than a minute put they well short of the ture test benchmark I would ve enjoy hear the conversation devolve as they extend a few minute or more I m sure duplex will soon tackle more domain and long conversation and it will someday pass the turing test it s only a matter of time before duplex be handle other mundane or difficult call for we like call our parent with our own voice see wavenet technology eventually we ll have our duplex voice call each other handle pleasantry and make plan which google assistant can then drop in our google calendar but that s the future for now duplex s performance stand as a powerful proof of concept for our long imagine future of conversational ai s capable of help entertaining and engage with we it s the first major step on the path to the ai depict in the movie she where joaquin phoenix star as a man who fall in love with his chatty voice assistant play by the disembodied voice of scarlett johansson so no duplex didn t pass the ture test but I do wonder what alan ture would think of it from a quick cheer to a stand ovation clap to show how much you enjoy this story tech expert journalist social medium commentator amateur cartoonist and robotic fan
Sophia Arakelyan,7,4,https://buzzrobot.com/from-ballerina-to-ai-researcher-part-i-46fce67f809b?source=---------2----------------,from ballerina to ai researcher part I buzzrobot,last year I publish the article from ballerina to ai writer where I describe how I embrace the technical part of ai without a technical background but have love and passion for ai I educate myself and be able to build a neural net classifier and do project in deep rl recently I ve become a participant in the openai scholarship program openai be a non profit that gather top ai researcher to ensure the safety of ai to benefit humanity every week for the next three month I ll publish blog post share my story of transformation from a person dedicate to 15 year of professional dancing and then write about tech and ai to actually conduct ai research find your true calling — the key component of happiness my primary goal with the series of blog post from ballerina to ai researcher be to show that it s never too late to embrace a new field start over again and find your true calling find work you love be one of the most important component of happiness — something that you do every day and invest your time in to grow ; that make you feel fulfil give you energy ; something that be a refuge for your soul great thing never come easy we have to be able to fight to make great thing happen but you can t fight for something you don t believe in especially if you don t feel like it s really important for you and humanity find that thing be a real challenge I feel lucky that I find my true passion — ai to I the technology itself and the ai community — researcher scientist people who dedicate their life to build the most powerful technology of all time with the mission to benefit humanity and make it safe for we — be a great source of energy the structure of the blog post series today I m give an overall intro of what I m go to cover in my from ballerina to ai researcher series I ll dedicate the sequence of blog post during the openai scholar program to several aspect of ai technology I ll cover those area that concern I a lot like ai and automation bias in ml dual use of ai etc also the structure of my post will include some insight on what I m work on right now the final technical project will be available by the end of august and will be open source I feel very lucky to have alec radford an experienced researcher as my mentor who guide I in the nlp and nlu research area first week of my scholarship I ve dedicate my first week within the program to learn about the transformer architecture that perform much well on sequential datum compare to rnns lstms the novelty of the architecture be its multi head self attention mechanism accord to the original paper experiment with the transformer on two machine translation task show the model to be superior in quality while be more parallelizable and require significantly less time to train more concretely when rnns or cnn take a sequence as an input it go through sentence word by word which be a huge obstacle toward parallelization of the process take more time to train model moreover if sequence be too long the model tend to forget the content of distant position in sequence or mix it with the follow position content — this be the fundamental problem in deal with sequential datum the transformer architecture reduce this problem thank to the multi head self attention mechanism I digge into rnn lstm model to catch up with the background information to that end I ve find andrew ng s course on deep learning along with the paper extremely useful to develop insight regard the transformer I go through the follow resource the video by łukasz kaiser from google brain one of the model s creator ; a blog post with very well elaborate content re the model run the code tensor2tensor and the code use the pytorch framework from this paper to feel the difference between the tf and pytorch framework overall the goal within the program be to develop deep comprehension of the nlu research area challenge current state of the art ; and to formulate and test hypothesis that tackle the most important problem of the field I ll share more on what I m work on in my future article meanwhile if you have question feedback please leave a comment if you want to learn more about I here be my facebook and twitter account I d appreciate your feedback on my post such as what topic be most interesting to you that I should consider further coverage on from a quick cheer to a stand ovation clap to show how much you enjoy this story former ballerina turn ai writer fan of sci fi astrophysic consciousness be the key founder of buzzrobot com the publication aim to cover practical aspect of ai technology use case along with interview with notable people in the ai field
Matt Schlicht,5K,11,https://chatbotsmagazine.com/the-complete-beginner-s-guide-to-chatbots-8280b7b906ca?source=tag_archive---------3----------------,the complete beginner s guide to chatbots chatbots magazine,what be chatbot why be they such a big opportunity how do they work how can I build one how can I meet other people interested in chatbot these be the question we re go to answer for you right now ready let s do this do you work in ecommerce stop read and click here we make something for you p s here be where I believe the future of bot be head you will probably disagree with I at first p p s my new guide about conversational commerce be up I think you ll find it super interesting a chatbot be a service power by rule and sometimes artificial intelligence that you interact with via a chat interface the service could be any number of thing range from functional to fun and it could live in any major chat product facebook messenger slack telegram text message etc if you haven t wrap your head around it yet don t worry here s an example to help you visualize a chatbot if you want to buy shoe from nordstrom online you would go to their website look around until you find the shoe you want and then you would purchase they if nordstrom make a bot which I m sure they will you would simply be able to message nordstrom on facebook it would ask you what you re look for and you would simply tell it instead of browse a website you will have a conversation with the nordstrom bot mirror the type of experience you would get when you go into the retail store watch this video from facebook s recent f8 conference where they make their major announcement at the 7 30 mark david marcus the vice president of message product at facebook explain what it look like to buy shoe in a facebook messenger bot buying shoe isn t the only thing chatbot can be use for here be a couple of other example see with bot the possibility be endless you can build anything imaginable and I encourage you to do just that but why make a bot sure it look cool it s use some super advanced technology but why should someone spend their time and energy on it it s a huge opportunity huge scroll down and I ll explain you be probably wonder why do anyone care about chatbot they look like simple text base service what s the big deal great question I ll tell you why people care about chatbot it s because for the first time ever people be use messenger app more than they be use social network let that sink in for a second people be use messenger app more than they be use social network so logically if you want to build a business online you want to build where the people be that place be now inside messenger app this be why chatbot be such a big deal it s potentially a huge business opportunity for anyone willing to jump headfirst and build something people want but how do these bot work how do they know how to talk to people and answer question isn t that artificial intelligence and isn t that insanely hard to do yes you be correct it be artificial intelligence but it s something that you can totally do yourself let I explain there be two type of chatbot one function base on a set of rule and the other more advanced version use machine learn what do this mean chatbot that function base on rule chatbot that function use machine learning bot be create with a purpose a store will likely want to create a bot that help you purchase something where someone like comcast might create a bot that can answer customer support question you start to interact with a chatbot by send it a message click here to try send a message to the cnn chatbot on facebook so if these bot use artificial intelligence to make they work well isn t that really hard to do don t I need to be an expert at artificial intelligence to be able to build something that have artificial intelligence short answer no you don t have to be an expert at artificial intelligence to create an awesome chatbot that have artificial intelligence just make sure to not over promise on your application s ability if you can t make the product good with artificial intelligence right now it might be good to not put it in yet however over the past decade quite a bit of advancement have be make in the area of artificial intelligence so much in fact that anyone who know how to code can incorporate some level of artificial intelligence into their product how do you build artificial intelligence into your bot don t worry I ve get you cover I ll tell you how to do it in the next section of this post build a chatbot can sound daunting but it s totally doable you ll be create an artificial intelligence power chat machine in no time or of course you can always build a basic chat bot that doesn t have a fancy ai brain and strictly follow rule you will need to figure out what problem you be go to solve with your bot choose which platform your bot will live on facebook slack etc set up a server to run your bot from and choose which service you will use to build your bot here be a ton of resource to get you start platform documentation other resource don t want to build your own now that you ve get your chatbot and artificial intelligence resource maybe it s time you meet other people who be also interested in chatbots chatbot have be around for decade but because of the recent advancement in artificial intelligence and machine learning there be a big opportunity for people to create bot that be well fast and strong if you re read this you probably fall into one of these category wouldn t it be awesome if you have a place to meet learn and share information with other people interested in chatbot yeah we think so too that s why I create a forum call chatbot news and it have quickly become the large community relate to chatbot the member of the chatbots group be investor who manage well over $ 2 billion in capital employee at facebook instagram fitbit nike and ycombinator company and hacker from around the world we would love if you join click here to request an invite private chatbot community I have also create the silicon valley chatbots meetup register here to be notify when we schedule our first event from a quick cheer to a stand ovation clap to show how much you enjoy this story ceo of octane ai founder of chatbots magazine yc alum forbes 30 under 30 product at ustream for 4 year sell for $ 130mil do digital for lil wayne chatbots ai nlp facebook messenger slack telegram and more
Gil Fewster,3.3K,5,https://medium.freecodecamp.org/the-mind-blowing-ai-announcement-from-google-that-you-probably-missed-2ffd31334805?source=tag_archive---------4----------------,the mind blow ai announcement from google that you probably miss,disclaimer I m not an expert in neural network or machine learning since originally write this article many people with far more expertise in these field than myself have indicate that while impressive what google have achieve be evolutionary not revolutionary in the very least it s fair to say that I m guilty of anthropomorphise in part of the text I ve leave the article s content unchanged because I think it s interesting to compare the gut reaction I have with the subsequent comment of expert in the field I strongly encourage reader to browse the comment after read the article for some perspective more sober and inform than my own in the closing week of 2016 google publish an article that quietly sail under most people s radar which be a shame because it may just be the most astonishing article about machine learning that I read last year don t feel bad if you miss it not only be the article compete with the pre christmas rush that most of we be navigate — it be also tuck away on google s research blog beneath the geektastic headline zero shot translation with google s multilingual neural machine translation system this doesn t exactly scream must read do it especially when you ve get project to wind up gift to buy and family feud to be resolve — all while the advent calendar relentlessly count down the day until christmas like some kind of chocolate fill yuletide doomsday clock luckily I m here to bring you up to speed here s the deal up until september of last year google translate use phrase base translation it basically do the same thing you and I do when we look up key word and phrase in our lonely planet language guide it s effective enough and blisteringly fast compare to awkwardly thumb your way through a bunch of page look for the french equivalent of please bring I all of your cheese and don t stop until I fall over but it lack nuance phrase base translation be a blunt instrument it do the job well enough to get by but map roughly equivalent word and phrase without an understanding of linguistic structure can only produce crude result this approach be also limit by the extent of an available vocabulary phrase base translation have no capacity to make educated guess at word it doesn t recognize and can t learn from new input all that change in september when google give their translation tool a new engine the google neural machine translation system gnmt this new engine come fully load with all the hot 2016 buzzword like neural network and machine learn the short version be that google translate get smart it develop the ability to learn from the people who use it it learn how to make educated guess about the content tone and meaning of phrase base on the context of other word and phrase around they and — here s the bit that should make your brain explode — it get creative google translate invent its own language to help it translate more effectively what s more nobody tell it to it didn t develop a language or interlingua as google call it because it be code to it develop a new language because the software determine over time that this be the most efficient way to solve the problem of translation stop and think about that for a moment let it sink in a neural computing system design to translate content from one human language into another develop its own internal language to make the task more efficient without be tell to do so in a matter of week I ve add a correction retraction of this paragraph in the note to understand what s go on we need to understand what zero shot translation capability be here s google s mike schuster nikhil thorat and melvin johnson from the original blog post here you can see an advantage of google s new neural machine over the old phrase base approach the gmnt be able to learn how to translate between two language without be explicitly teach this wouldn t be possible in a phrase base model where translation be dependent upon an explicit dictionary to map word and phrase between each pair of language be translate and this lead the google engineer onto that truly astonishing discovery of creation so there you have it in the last week of 2016 as journos around the world start pen their be this the bad year in living memory thinkpiece google engineer be quietly document a genuinely astonishing breakthrough in software engineering and linguistic I just think maybe you d want to know ok to really understand what s go on we probably need multiple computer science and linguistic degree I m just barely scrape the surface here if you ve get time to get a few degree or if you ve already get they please drop I a line and explain it all I to slowly update 1 in my excitement it s fair to say that I ve exaggerate the idea of this as an intelligent system — at least so far as we would think about human intelligence and decision making make sure you read chris mcdonald s comment after the article for a more sober perspective update 2 nafrondel s excellent detailed reply be also a must read for an expert explanation of how neural network function from a quick cheer to a stand ovation clap to show how much you enjoy this story a tinkerer our community publish story worth read on development design and datum science
Adam Geitgey,10.4K,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=tag_archive---------5----------------,machine learning be fun part 2 adam geitgey medium,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in italiano español français türkçe русский 한국어 português فارسی tiếng việt or 普通话 in part 1 we say that machine learning be use generic algorithm to tell you something interesting about your datum without write any code specific to the problem you be solve if you haven t already read part 1 read it now this time we be go to see one of these generic algorithm do something really cool — create video game level that look like they be make by human we ll build a neural network feed it exist super mario level and watch new one pop out just like part 1 this guide be for anyone who be curious about machine learning but have no idea where to start the goal be be accessible to anyone — which mean that there s a lot of generalization and we skip lot of detail but who care if this get anyone more interested in ml then mission accomplish back in part 1 we create a simple algorithm that estimate the value of a house base on its attribute give datum about a house like this we end up with this simple estimation function in other word we estimate the value of the house by multiply each of its attribute by a weight then we just add those number up to get the house s value instead of use code let s represent that same function as a simple diagram however this algorithm only work for simple problem where the result have a linear relationship with the input what if the truth behind house price isn t so simple for example maybe the neighborhood matter a lot for big house and small house but doesn t matter at all for medium sized house how could we capture that kind of complicated detail in our model to be more clever we could run this algorithm multiple time with different of weight that each capture different edge case now we have four different price estimate let s combine those four price estimate into one final estimate we ll run they through the same algorithm again but use another set of weight our new super answer combine the estimate from our four different attempt to solve the problem because of this it can model more case than we could capture in one simple model let s combine our four attempt to guess into one big diagram this be a neural network each node know how to take in a set of input apply weight to they and calculate an output value by chain together lot of these node we can model complex function there s a lot that I m skip over to keep this brief include feature scaling and the activation function but the most important part be that these basic idea click it s just like lego we can t model much with one single lego block but we can model anything if we have enough basic lego block to stick together the neural network we ve see always return the same answer when you give it the same input it have no memory in programming term it s a stateless algorithm in many case like estimate the price of house that s exactly what you want but the one thing this kind of model can t do be respond to pattern in datum over time imagine I hand you a keyboard and ask you to write a story but before you start my job be to guess the very first letter that you will type what letter should I guess I can use my knowledge of english to increase my odd of guess the right letter for example you will probably type a letter that be common at the beginning of word if I look at story you write in the past I could narrow it down far base on the word you usually use at the beginning of your story once I have all that datum I could use it to build a neural network to model how likely it be that you would start with any give letter our model might look like this but let s make the problem hard let s say I need to guess the next letter you be go to type at any point in your story this be a much more interesting problem let s use the first few word of ernest hemingway s the sun also rise as an example what letter be go to come next you probably guess n — the word be probably go to be box we know this base on the letter we ve already see in the sentence and our knowledge of common word in english also the word middleweight give we an extra clue that we be talk about box in other word it s easy to guess the next letter if we take into account the sequence of letter that come right before it and combine that with our knowledge of the rule of english to solve this problem with a neural network we need to add state to our model each time we ask our neural network for an answer we also save a set of our intermediate calculation and re use they the next time as part of our input that way our model will adjust its prediction base on the input that it have see recently keep track of state in our model make it possible to not just predict the most likely first letter in the story but to predict the most likely next letter give all previous letter this be the basic idea of a recurrent neural network we be update the network each time we use it this allow it to update its prediction base on what it see most recently it can even model pattern over time as long as we give it enough of a memory predict the next letter in a story might seem pretty useless what s the point one cool use might be auto predict for a mobile phone keyboard but what if we take this idea to the extreme what if we ask the model to predict the next most likely character over and over — forever we d be ask it to write a complete story for we we see how we could guess the next letter in hemingway s sentence let s try generate a whole story in the style of hemingway to do this we be go to use the recurrent neural network implementation that andrej karpathy write andrej be a deep learning researcher at stanford and he write an excellent introduction to generate text with rnn you can view all the code for the model on github we ll create our model from the complete text of the sun also rise — 362 239 character use 84 unique letter include punctuation uppercase lowercase etc this datum set be actually really small compare to typical real world application to generate a really good model of hemingway s style it would be much well to have at several time as much sample text but this be good enough to play around with as an example as we just start to train the rnn it s not very good at predict letter here s what it generate after a 100 loop of training you can see that it have figure out that sometimes word have space between they but that s about it after about 1000 iteration thing be look more promising the model have start to identify the pattern in basic sentence structure it s add period at the end of sentence and even quote dialog a few word be recognizable but there s also still a lot of nonsense but after several thousand more training iteration it look pretty good at this point the algorithm have capture the basic pattern of hemingway s short direct dialog a few sentence even sort of make sense compare that with some real text from the book even by only look for pattern one character at a time our algorithm have reproduce plausible looking prose with proper formatting that be kind of amazing we don t have to generate text completely from scratch either we can seed the algorithm by supply the first few letter and just let it find the next few letter for fun let s make a fake book cover for our imaginary book by generate a new author name and a new title use the seed text of er he and the s not bad but the really mind blow part be that this algorithm can figure out pattern in any sequence of datum it can easily generate real looking recipe or fake obama speech but why limit ourselves human language we can apply this same idea to any kind of sequential datum that have a pattern in 2015 nintendo release super mario makertm for the wii u gaming system this game let you draw out your own super mario brother level on the gamepad and then upload they to the internet so you friend can play through they you can include all the classic power up and enemy from the original mario game in your level it s like a virtual lego set for people who grow up play super mario brother can we use the same model that generate fake hemingway text to generate fake super mario brother level first we need a datum set for train our model let s take all the outdoor level from the original super mario brothers game release in 1985 this game have 32 level and about 70 % of they have the same outdoor style so we ll stick to those to get the design for each level I take an original copy of the game and write a program to pull the level design out of the game s memory super mario bros be a 30 year old game and there be lot of resource online that help you figure out how the level be store in the game s memory extract level datum from an old video game be a fun programming exercise that you should try sometime here s the first level from the game which you probably remember if you ever play it if we look closely we can see the level be make of a simple grid of object we could just as easily represent this grid as a sequence of character with one character represent each object we ve replace each object in the level with a letter and so on use a different letter for each different kind of object in the level I end up with text file that look like this look at the text file you can see that mario level don t really have much of a pattern if you read they line by line the pattern in a level really emerge when you think of the level as a series of column so in order for the algorithm to find the pattern in our datum we need to feed the datum in column by column figure out the most effective representation of your input datum call feature selection be one of the key of use machine learning algorithm well to train the model I need to rotate my text file by 90 degree this make sure the character be feed into the model in an order where a pattern would more easily show up just like we see when create the model of hemingway s prose a model improve as we train it after a little training our model be generate junk it sort of have an idea that s and = s should show up a lot but that s about it it hasn t figure out the pattern yet after several thousand iteration it s start to look like something the model have almost figure out that each line should be the same length it have even start to figure out some of the logic of mario the pipe in mario be always two block wide and at least two block high so the p s in the datum should appear in 2x2 cluster that s pretty cool with a lot more training the model get to the point where it generate perfectly valid datum let s sample an entire level s worth of datum from our model and rotate it back horizontal this data look great there be several awesome thing to notice finally let s take this level and recreate it in super mario maker play it yourself if you have super mario maker you can play this level by bookmarke it online or by look it up use level code 4ac9 0000 0157 f3c3 the recurrent neural network algorithm we use to train our model be the same kind of algorithm use by real world company to solve hard problem like speech detection and language translation what make our model a toy instead of cut edge be that our model be generate from very little datum there just aren t enough level in the original super mario brothers game to provide enough datum for a really good model if we could get access to the hundred of thousand of user create super mario maker level that nintendo have we could make an amazing model but we can t — because nintendo win t let we have they big company don t give away their datum for free as machine learning become more important in more industry the difference between a good program and a bad program will be how much datum you have to train your model that s why company like google and facebook need your datum so badly for example google recently open source tensorflow its software toolkit for build large scale machine learning application it be a pretty big deal that google give away such important capable technology for free this be the same stuff that power google translate but without google s massive trove of datum in every language you can t create a competitor to google translate data be what give google its edge think about that the next time you open up your google map location history or facebook location history and notice that it store every place you ve ever be in machine learn there s never a single way to solve a problem you have limitless option when decide how to pre process your datum and which algorithm to use often combine multiple approach will give you well result than any single approach reader have send I link to other interesting approach to generate super mario level if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 3 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
David Venturi,10.6K,20,https://medium.freecodecamp.org/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0?source=tag_archive---------6----------------,every single machine learning course on the internet rank by your review,a year and a half ago I drop out of one of the good computer science program in canada I start create my own data science master s program use online resource I realize that I could learn everything I need through edx coursera and udacity instead and I could learn it fast more efficiently and for a fraction of the cost I m almost finish now I ve take many data science relate course and audit portion of many more I know the option out there and what skill be need for learner prepare for a data analyst or data scientist role so I start create a review drive guide that recommend the good course for each subject within data science for the first guide in the series I recommend a few code class for the beginner datum scientist then it be statistic and probability class then introduction to data science also data visualization for this guide I spend a dozen hour try to identify every online machine learning course offer as of may 2017 extract key bit of information from their syllabus and review and compile their rating my end goal be to identify the three good course available and present they to you below for this task I turn to none other than the open source class central community and its database of thousand of course rating and review since 2011 class central founder dhawal shah have keep a close eye on online course than arguably anyone else in the world dhawal personally help I assemble this list of resource each course must fit three criterion we believe we cover every notable course that fit the above criterion since there be seemingly hundred of course on udemy we choose to consider the most reviewed and high rate one only there s always a chance that we miss something though so please let we know in the comment section if we leave a good course out we compile average rating and number of review from class central and other review site to calculate a weighted average rating for each course we read text review and use this feedback to supplement the numerical rating we make subjective syllabus judgment call base on three factor a popular definition originate from arthur samuel in 1959 machine learning be a subfield of computer science that give computer the ability to learn without be explicitly program in practice this mean develop computer program that can make prediction base on datum just as human can learn from experience so can computer where datum = experience a machine learn workflow be the process require for carry out a machine learning project though individual project can differ most workflow share several common task problem evaluation datum exploration datum preprocesse model training testing deployment etc below you ll find helpful visualization of these core step the ideal course introduce the entire process and provide interactive example assignment and or quiz where student can perform each task themselves first off let s define deep learning here be a succinct description as would be expect portion of some of the machine learn course contain deep learning content I choose not to include deep learning only course however if you be interested in deep learning specifically we ve get you cover with the follow article my top three recommendation from that list would be several course list below ask student to have prior programming calculus linear algebra and statistic experience these prerequisite be understandable give that machine learning be an advanced discipline miss a few subject good news some of this experience can be acquire through our recommendation in the first two article program statistic of this data science career guide several top rank course below also provide gentle calculus and linear algebra refresher and highlight the aspect most relevant to machine learning for those less familiar stanford university s machine learning on coursera be the clear current winner in term of rating review and syllabus fit teach by the famous andrew ng google brain founder and former chief scientist at baidu this be the class that spark the founding of coursera it have a 4 7 star weight average rating over 422 review release in 2011 it cover all aspect of the machine learn workflow though it have a small scope than the original stanford class upon which it be base it still manage to cover a large number of technique and algorithm the estimate timeline be eleven week with two week dedicate to neural network and deep learning free and pay option be available ng be a dynamic yet gentle instructor with a palpable experience he inspire confidence especially when share practical implementation tip and warning about common pitfall a linear algebra refresher be provide and ng highlight the aspect of calculus most relevant to machine learning evaluation be automatic and be do via multiple choice quiz that follow each lesson and programming assignment the assignment there be eight of they can be complete in matlab or octave which be an open source version of matlab ng explain his language choice though python and r be likely more compelling choice in 2017 with the increase popularity of those language reviewer note that that shouldn t stop you from take the course a few prominent reviewer note the follow columbia university s machine learning be a relatively new offering that be part of their artificial intelligence micromaster on edx though it be new and doesn t have a large number of review the one that it do have be exceptionally strong professor john paisley be note as brilliant clear and clever it have a 4 8 star weight average rating over 10 review the course also cover all aspect of the machine learn workflow and more algorithm than the above stanford offer columbia s be a more advanced introduction with reviewer note that student should be comfortable with the recommend prerequisite calculus linear algebra statistic probability and code quiz 11 programming assignment 4 and a final exam be the mode of evaluation student can use either python octave or matlab to complete the assignment the course s total estimate timeline be eight to ten hour per week over twelve week it be free with a verify certificate available for purchase below be a few of the aforementioned sparkling review machine learn a ztm on udemy be an impressively detailed offering that provide instruction in both python and r which be rare and can t be say for any of the other top course it have a 4 5 star weight average rating over 8 119 review which make it the most review course of the one consider it cover the entire machine learn workflow and an almost ridiculous in a good way number of algorithm through 40 5 hour of on demand video the course take a more apply approach and be light math wise than the above two course each section start with an intuition video from eremenko that summarize the underlie theory of the concept be teach de ponteve then walk through implementation with separate video for both python and r as a bonus the course include python and r code template for student to download and use on their own project there be quiz and homework challenge though these aren t the strong point of the course eremenko and the superdatascience team be revere for their ability to make the complex simple also the prerequisite list be just some high school mathematic so this course might be a well option for those daunt by the stanford and columbia offering a few prominent reviewer note the follow our # 1 pick have a weight average rating of 4 7 out of 5 star over 422 review let s look at the other alternative sort by descend rating a reminder that deep learning only course be not include in this guide — you can find those here the analytic edge massachusetts institute of technology edx more focused on analytic in general though it do cover several machine learning topic use r strong narrative that leverage familiar real world example challenge ten to fifteen hour per week over twelve week free with a verify certificate available for purchase it have a 4 9 star weight average rating over 214 review python for datum science and machine learning bootcamp jose portilla udemy have large chunk of machine learning content but cover the whole data science process more of a very detailed intro to python amazing course though not ideal for the scope of this guide 21 5 hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 6 star weight average rating over 3316 review data science and machine learning bootcamp with r jose portilla udemy the comment for portilla s above course apply here as well except for r 17 5 hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 6 star weight average rating over 1317 review machine learning series lazy programmer inc udemy teach by a data scientist big data engineer full stack software engineer with an impressive resume lazy programmer currently have a series of 16 machine learning focus course on udemy in total the course have 5000 + rating and almost all of they have 4 6 star a useful course ordering be provide in each individual course s description use python cost varie depend on udemy discount which be frequent machine learn georgia tech udacity a compilation of what be three separate course supervise unsupervised and reinforcement learning part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms bite sized video as be udacity s style friendly professor estimate timeline of four month free it have a 4 56 star weight average rating over 9 review implement predictive analytic with spark in azure hdinsight microsoft edx introduce the core concept of machine learning and a variety of algorithms leverage several big data friendly tool include apache spark scala and hadoop use both python and r four hour per week over six week free with a verify certificate available for purchase it have a 4 5 star weight average rating over 6 review data science and machine learning with python — hand on frank kane udemy use python kane have nine year of experience at amazon and imdb nine hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 5 star weight average rating over 4139 review scala and spark for big datum and machine learning jose portilla udemy big datum focus specifically on implementation in scala and spark ten hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 5 star weight average rating over 607 review machine learning engineer nanodegree udacity udacity s flagship machine learning program which feature a good in class project review system and career support the program be a compilation of several individual udacity course which be free co create by kaggle estimate timeline of six month currently cost $ 199 usd per month with a 50 % tuition refund available for those who graduate within 12 month it have a 4 5 star weight average rating over 2 review learn from datum introductory machine learning california institute of technology edx enrollment be currently close on edx but be also available via caltech s independent platform see below it have a 4 49 star weight average rating over 42 review learn from datum introductory machine learn yaser abu mostafa california institute of technology a real caltech course not a water down version review note it be excellent for understand machine learning theory the professor yaser abu mostafa be popular among student and also write the textbook upon which this course be base video be tape lecture with lecture slide picture in picture upload to youtube homework assignment be pdf file the course experience for online student isn t as polished as the top three recommendation it have a 4 43 star weight average rating over 7 review mining massive dataset stanford university machine learn with a focus on big datum introduce modern distribute file system and mapreduce ten hour per week over seven week free it have a 4 4 star weight average rating over 30 review aws machine learn a complete guide with python chandra lingam udemy a unique focus on cloud base machine learning and specifically amazon web service use python nine hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 4 star weight average rating over 62 review introduction to machine learning & face detection in python holczer balazs udemy use python eight hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 4 star weight average rating over 162 review statlearne statistical learning stanford university base on the excellent textbook an introduction to statistical learning with application in r and teach by the professor who write it reviewer note that the mooc isn t as good as the book cite thin exercise and mediocre video five hour per week over nine week free it have a 4 35 star weight average rating over 84 review machine learning specialization university of washington coursera great course but last two class include the capstone project be cancel reviewer note that this series be more digestable read easy for those without strong technical background than other top machine learn course e g stanford s or caltech s be aware that the series be incomplete with recommend system deep learning and a summary miss free and pay option available it have a 4 31 star weight average rating over 80 review from 0 to 1 machine learn nlp & python cut to the chase loony corn udemy a down to earth shy but confident take on machine learning technique teach by four person team with decade of industry experience together use python cost varie depend on udemy discount which be frequent it have a 4 2 star weight average rating over 494 review principle of machine learn microsoft edx use r python and microsoft azure machine learn part of the microsoft professional program certificate in data science three to four hour per week over six week free with a verify certificate available for purchase it have a 4 09 star weight average rating over 11 review big data statistical inference and machine learning queensland university of technology futurelearn a nice brief exploratory machine learning course with a focus on big datum cover a few tool like r h2o flow and weka only three week in duration at a recommend two hour per week but one reviewer note that six hour per week would be more appropriate free and pay option available it have a 4 star weight average rating over 4 review genomic datum science and cluster bioinformatics v university of california san diego coursera for those interested in the intersection of computer science and biology and how it represent an important frontier in modern science focus on clustering and dimensionality reduction part of ucsd s bioinformatics specialization free and pay option available it have a 4 star weight average rating over 3 review intro to machine learn udacity prioritize topic breadth and practical tool in python over depth and theory the instructor sebastian thrun and katie malone make this class so fun consist of bite sized video and quiz follow by a mini project for each lesson currently part of udacity s data analyst nanodegree estimate timeline of ten week free it have a 3 95 star weight average rating over 19 review machine learning for data analysis wesleyan university coursera a brief intro machine learning and a few select algorithms cover decision tree random forest lasso regression and k mean cluster part of wesleyan s datum analysis and interpretation specialization estimate timeline of four week free and pay option available it have a 3 6 star weight average rating over 5 review program with python for data science microsoft edx produce by microsoft in partnership with code dojo use python eight hour per week over six week free and pay option available it have a 3 46 star weight average rating over 37 review machine learning for trade georgia tech udacity focus on apply probabilistic machine learning approach to trading decision use python part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree om estimate timeline of four month free it have a 3 29 star weight average rating over 14 review practical machine learning johns hopkins university coursera a brief practical introduction to a number of machine learning algorithm several one two star review express a variety of concern part of jhu s data science specialization four to nine hour per week over four week free and pay option available it have a 3 11 star weight average rating over 37 review machine learning for datum science and analytics columbia university edx introduce a wide range of machine learn topic some passionate negative review with concern include content choice a lack of programming assignment and uninspire presentation seven to ten hour per week over five week free with a verify certificate available for purchase it have a 2 74 star weight average rating over 36 review recommender system specialization university of minnesota coursera strong focus one specific type of machine learning — recommender system a four course specialization plus a capstone project which be a case study teach use lenskit an open source toolkit for recommender system free and pay option available it have a 2 star weight average rating over 2 review machine learning with big datum university of california san diego coursera terrible review that highlight poor instruction and evaluation some note it take they mere hour to complete the whole course part of ucsd s big datum specialization free and pay option available it have a 1 86 star weight average rating over 14 review practical predictive analytic model and method university of washington coursera a brief intro to core machine learning concept one reviewer note that there be a lack of quiz and that the assignment be not challenge part of uw s data science at scale specialization six to eight hour per week over four week free and pay option available it have a 1 75 star weight average rating over 4 review the follow course have one or no review as of may 2017 machine learn for musician and artist goldsmith university of london kadenze unique student learn algorithms software tool and machine learn good practice to make sense of human gesture musical audio and other real time datum seven session in length audit free and premium $ 10 usd per month option available it have one 5 star review apply machine learning in python university of michigan coursera teach use python and the scikit learn toolkit part of the apply data science with python specialization schedule to start may 29th free and pay option available apply machine learn microsoft edx teach use various tool include python r and microsoft azure machine learning note microsoft produce the course include hand on lab to reinforce the lecture content three to four hour per week over six week free with a verify certificate available for purchase machine learn with python big datum university teach use python target towards beginner estimate completion time of four hour big datum university be affiliate with ibm free machine learning with apache systemml big data university teach use apache systemml which be a declarative style language design for large scale machine learning estimate completion time of eight hour big datum university be affiliate with ibm free machine learning for data science university of california san diego edx doesn t launch until january 2018 programming example and assignment be in python use jupyter notebook eight hour per week over ten week free with a verify certificate available for purchase introduction to analytic model georgia tech edx the course advertise r as its primary programming tool five to ten hour per week over ten week free with a verify certificate available for purchase predictive analytic gain insight from big datum queensland university of technology futurelearn brief overview of a few algorithm use hewlett packard enterprise s vertica analytic platform as an apply tool start date to be announce two hour per week over four week free with a certificate of achievement available for purchase introducción al machine learning universita telefónica miríada x teach in spanish an introduction to machine learning that cover supervised and unsupervised learn a total of twenty estimate hour over four week machine learning path step dataquest teach in python use dataquest s interactive in browser platform multiple guide project and a plus project where you build your own machine learning system use your own datum subscription require the follow six course be offer by datacamp datacamp s hybrid teaching style leverage video and text base instruction with lot of example through an in browser code editor a subscription be require for full access to each course introduction to machine learn datacamp cover classification regression and clustering algorithm use r fifteen video and 81 exercise with an estimate timeline of six hour supervised learning with scikit learn datacamp use python and scikit learn cover classification and regression algorithms seventeen video and 54 exercise with an estimate timeline of four hour unsupervised learning in r datacamp provide a basic introduction to clustering and dimensionality reduction in r sixteen video and 49 exercise with an estimate timeline of four hour machine learn toolbox datacamp teach the big idea in machine learning use r 24 video and 88 exercise with an estimate timeline of four hour machine learn with the expert school budget datacamp a case study from a machine learning competition on drivendata involve build a model to automatically classify item in a school s budget datacamp s supervised learning with scikit learn be a prerequisite fifteen video and 51 exercise with an estimate timeline of four hour unsupervised learning in python datacamp cover a variety of unsupervised learning algorithm use python scikit learn and scipy the course end with student build a recommend system to recommend popular musical artist thirteen video and 52 exercise with an estimate timeline of four hour machine learning tom mitchell carnegie mellon university carnegie mellon s graduate introductory machine learning course a prerequisite to their second graduate level course statistical machine learning tape university lecture with practice problem homework assignment and a midterm all with solution post online a 2011 version of the course also exist cmu be one of the good graduate school for study machine learning and have a whole department dedicate to ml free statistical machine learn larry wasserman carnegie mellon university likely the most advanced course in this guide a follow up to carnegie mellon s machine learning course tape university lecture with practice problem homework assignment and a midterm all with solution post online free undergraduate machine learn nando de freitas university of british columbia an undergraduate machine learning course lecture be film and put on youtube with the slide post on the course website the course assignment be post as well no solution though de freita be now a full time professor at the university of oxford and receive praise for his teaching ability in various forum graduate version available see below machine learn nando de freitas university of british columbia a graduate machine learning course the comment in de freitas undergraduate course above apply here as well this be the fifth of a six piece series that cover the good online course for launch yourself into the data science field we cover programming in the first article statistic and probability in the second article intro to data science in the third article and datum visualization in the fourth the final piece will be a summary of those article plus the good online course for other key topic such as datum wrangle database and even software engineering if you re look for a complete list of data science online course you can find they on class central s data science and big data subject page if you enjoy read this check out some of class central s other piece if you have suggestion for course I miss let I know in the response if you find this helpful click the 💚 so more people will see it here on medium this be a condensed version of my original article publish on class central where I ve include detailed course syllabus from a quick cheer to a stand ovation clap to show how much you enjoy this story curriculum lead project @ datacamp I create my own data science master s program our community publish story worth read on development design and datum science
Michael Jordan,34K,16,https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7?source=tag_archive---------7----------------,artificial intelligence — the revolution hasn t happen yet,artificial intelligence ai be the mantra of the current era the phrase be intone by technologist academician journalist and venture capitalist alike as with many phrase that cross over from technical academic field into general circulation there be significant misunderstanding accompany the use of the phrase but this be not the classical case of the public not understand the scientist — here the scientist be often as befuddle as the public the idea that our era be somehow see the emergence of an intelligence in silicon that rival our own entertain all of we — enthral we and frightening we in equal measure and unfortunately it distract we there be a different narrative that one can tell about the current era consider the follow story which involve human computer datum and life or death decision but where the focus be something other than intelligence in silicon fantasy when my spouse be pregnant 14 year ago we have an ultrasound there be a geneticist in the room and she point out some white spot around the heart of the fetus those be marker for down syndrome she note and your risk have now go up to 1 in 20 she far let we know that we could learn whether the fetus in fact have the genetic modification underlie down syndrome via an amniocentesis but amniocentesis be risky — the risk of kill the fetus during the procedure be roughly 1 in 300 be a statistician I determine to find out where these number be come from to cut a long story short I discover that a statistical analysis have be do a decade previously in the uk where these white spot which reflect calcium buildup be indeed establish as a predictor of down syndrome but I also notice that the imaging machine use in our test have a few hundred more pixel per square inch than the machine use in the uk study I go back to tell the geneticist that I believe that the white spot be likely false positive — that they be literally white noise she say ah that explain why we start see an uptick in down syndrome diagnose a few year ago ; it s when the new machine arrive we didn t do the amniocentesis and a healthy girl be bear a few month later but the episode trouble I particularly after a back of the envelope calculation convince I that many thousand of people have get that diagnosis that same day worldwide that many of they have opt for amniocentesis and that a number of baby have die needlessly and this happen day after day until it somehow get fix the problem that this episode reveal wasn t about my individual medical care ; it be about a medical system that measure variable and outcome in various place and time conduct statistical analysis and make use of the result in other place and time the problem have to do not just with datum analysis per se but with what database researcher call provenance — broadly where do datum arise what inference be draw from the datum and how relevant be those inference to the present situation while a train human might be able to work all of this out on a case by case basis the issue be that of design a planetary scale medical system that could do this without the need for such detailed human oversight I m also a computer scientist and it occur to I that the principle need to build planetary scale inference and decision make system of this kind blend computer science with statistic and take into account human utility be nowhere to be find in my education and it occur to I that the development of such principle — which will be need not only in the medical domain but also in domain such as commerce transportation and education — be at least as important as those of build ai system that can dazzle we with their game playing or sensorimotor skill whether or not we come to understand intelligence any time soon we do have a major challenge on our hand in bring together computer and human in way that enhance human life while this challenge be view by some as subservient to the creation of artificial intelligence it can also be view more prosaically — but with no less reverence — as the creation of a new branch of engineering much like civil engineering and chemical engineering in decade past this new discipline aim to corral the power of a few key idea bring new resource and capability to people and do so safely whereas civil engineering and chemical engineering be build on physics and chemistry this new engineering discipline will be build on idea that the precede century give substance to — idea such as information algorithm data uncertainty computing inference and optimization moreover since much of the focus of the new discipline will be on datum from and about human its development will require perspective from the social science and humanity while the building block have begin to emerge the principle for put these block together have not yet emerge and so the block be currently be put together in ad hoc way thus just as human build building and bridge before there be civil engineering human be proceed with the building of societal scale inference and decision make system that involve machine human and the environment just as early building and bridge sometimes fall to the ground — in unforeseen way and with tragic consequence — many of our early societal scale inference and decision make system be already expose serious conceptual flaw and unfortunately we be not very good at anticipate what the next emerge serious flaw will be what we re miss be an engineering discipline with its principle of analysis and design the current public dialog about these issue too often use ai as an intellectual wildcard one that make it difficult to reason about the scope and consequence of emerge technology let we begin by consider more carefully what ai have be use to refer to both recently and historically most of what be be call ai today particularly in the public sphere be what have be call machine learning ml for the past several decade ml be an algorithmic field that blend idea from statistic computer science and many other discipline see below to design algorithm that process datum make prediction and help make decision in term of impact on the real world ml be the real thing and not just recently indeed that ml would grow into massive industrial relevance be already clear in the early 1990 and by the turn of the century forward look company such as amazon be already use ml throughout their business solving mission critical back end problem in fraud detection and supply chain prediction and build innovative consumer facing service such as recommendation system as dataset and computing resource grow rapidly over the ensue two decade it become clear that ml would soon power not only amazon but essentially any company in which decision could be tie to large scale datum new business model would emerge the phrase datum science begin to be use to refer to this phenomenon reflect the need of ml algorithms expert to partner with database and distribute system expert to build scalable robust ml system and reflect the large social and environmental scope of the result system this confluence of idea and technology trend have be rebrande as ai over the past few year this rebranding be worthy of some scrutiny historically the phrase ai be coin in the late 1950 s to refer to the heady aspiration of realize in software and hardware an entity possess human level intelligence we will use the phrase human imitative ai to refer to this aspiration emphasize the notion that the artificially intelligent entity should seem to be one of we if not physically at least mentally whatever that might mean this be largely an academic enterprise while relate academic field such as operation research statistic pattern recognition information theory and control theory already exist and be often inspire by human intelligence and animal intelligence these field be arguably focus on low level signal and decision the ability of say a squirrel to perceive the three dimensional structure of the forest it live in and to leap among its branch be inspirational to these field ai be mean to focus on something different — the high level or cognitive capability of human to reason and to think sixty year later however high level reasoning and think remain elusive the development which be now be call ai arise mostly in the engineering field associate with low level pattern recognition and movement control and in the field of statistic — the discipline focus on find pattern in datum and on make well found prediction test of hypothesis and decision indeed the famous backpropagation algorithm that be rediscover by david rumelhart in the early 1980 and which be now view as be at the core of the so call ai revolution first arise in the field of control theory in the 1950 and 1960 one of its early application be to optimize the thrust of the apollo spaceship as they head towards the moon since the 1960s much progress have be make but it have arguably not come about from the pursuit of human imitative ai rather as in the case of the apollo spaceship these idea have often be hide behind the scene and have be the handiwork of researcher focus on specific engineering challenge although not visible to the general public research and system building in area such as document retrieval text classification fraud detection recommendation system personalize search social network analysis planning diagnostic and a b testing have be a major success — these be the advance that have power company such as google netflix facebook and amazon one could simply agree to refer to all of this as ai and indeed that be what appear to have happen such labeling may come as a surprise to optimization or statistic researcher who wake up to find themselves suddenly refer to as ai researcher but labeling of researcher aside the big problem be that the use of this single ill define acronym prevent a clear understanding of the range of intellectual and commercial issue at play the past two decade have see major progress — in industry and academia — in a complementary aspiration to human imitative ai that be often refer to as intelligence augmentation ia here computation and datum be use to create service that augment human intelligence and creativity a search engine can be view as an example of ia it augment human memory and factual knowledge as can natural language translation it augment the ability of a human to communicate computing base generation of sound and image serve as a palette and creativity enhancer for artist while service of this kind could conceivably involve high level reasoning and think currently they don t — they mostly perform various kind of string matching and numerical operation that capture pattern that human can make use of hope that the reader will tolerate one last acronym let we conceive broadly of a discipline of intelligent infrastructure ii whereby a web of computation datum and physical entity exist that make human environment more supportive interesting and safe such infrastructure be begin to make its appearance in domain such as transportation medicine commerce and finance with vast implication for individual human and society this emergence sometimes arise in conversation about an internet of thing but that effort generally refer to the mere problem of get thing onto the internet — not to the far grander set of challenge associate with these thing capable of analyze those datum stream to discover fact about the world and interact with human and other thing at a far high level of abstraction than mere bit for example return to my personal anecdote we might imagine live our life in a societal scale medical system that set up datum flow and datum analysis flow between doctor and device position in and around human body thereby able to aid human intelligence in make diagnosis and provide care the system would incorporate information from cell in the body dna blood test environment population genetic and the vast scientific literature on drug and treatment it would not just focus on a single patient and a doctor but on relationship among all human — just as current medical testing allow experiment do on one set of human or animal to be bring to bear in the care of other human it would help maintain notion of relevance provenance and reliability in the way that the current banking system focus on such challenge in the domain of finance and payment and while one can foresee many problem arise in such a system — involve privacy issue liability issue security issue etc — these problem should properly be view as challenge not show stopper we now come to a critical issue be work on classical human imitative ai the good or only way to focus on these large challenge some of the most herald recent success story of ml have in fact be in area associate with human imitative ai — area such as computer vision speech recognition game playing and robotic so perhaps we should simply await further progress in domain such as these there be two point to make here first although one would not know it from read the newspaper success in human imitative ai have in fact be limit — we be very far from realize human imitative ai aspiration unfortunately the thrill and fear of make even limited progress on human imitative ai give rise to level of over exuberance and medium attention that be not present in other area of engineering second and more importantly success in these domain be neither sufficient nor necessary to solve important ia and ii problem on the sufficiency side consider self drive car for such technology to be realize a range of engineering problem will need to be solve that may have little relationship to human competency or human lack of competency the overall transportation system an ii system will likely more closely resemble the current air traffic control system than the current collection of loosely couple forward face inattentive human driver it will be vastly more complex than the current air traffic control system specifically in its use of massive amount of datum and adaptive statistical modeling to inform fine grain decision it be those challenge that need to be in the forefront and in such an effort a focus on human imitative ai may be a distraction as for the necessity argument it be sometimes argue that the human imitative ai aspiration subsume ia and ii aspiration because a human imitative ai system would not only be able to solve the classical problem of ai as embody e g in the ture test but it would also be our good bet for solve ia and ii problem such an argument have little historical precedent do civil engineering develop by envisage the creation of an artificial carpenter or bricklayer should chemical engineering have be frame in term of create an artificial chemist even more polemically if our goal be to build chemical factory should we have first create an artificial chemist who would have then work out how to build a chemical factory a related argument be that human intelligence be the only kind of intelligence that we know and that we should aim to mimic it as a first step but human be in fact not very good at some kind of reasoning — we have our lapse bias and limitation moreover critically we do not evolve to perform the kind of large scale decision make that modern ii system must face nor to cope with the kind of uncertainty that arise in ii contexts one could argue that an ai system would not only imitate human intelligence but also correct it and would also scale to arbitrarily large problem but we be now in the realm of science fiction — such speculative argument while entertain in the setting of fiction should not be our principal strategy go forward in the face of the critical ia and ii problem that be begin to emerge we need to solve ia and ii problem on their own merit not as a mere corollary to a human imitative ai agenda it be not hard to pinpoint algorithmic and infrastructure challenge in ii system that be not central theme in human imitative ai research ii system require the ability to manage distribute repository of knowledge that be rapidly change and be likely to be globally incoherent such system must cope with cloud edge interaction in make timely distribute decision and they must deal with long tail phenomenon whereby there be lot of datum on some individual and little datum on most individual they must address the difficulty of share datum across administrative and competitive boundary finally and of particular importance ii system must bring economic idea such as incentive and pricing into the realm of the statistical and computational infrastructure that link human to each other and to value good such ii system can be view as not merely provide a service but as create market there be domain such as music literature and journalism that be cry out for the emergence of such market where datum analysis link producer and consumer and this must all be do within the context of evolve societal ethical and legal norm of course classical human imitative ai problem remain of great interest as well however the current focus on do ai research via the gathering of datum the deployment of deep learning infrastructure and the demonstration of system that mimic certain narrowly define human skill — with little in the way of emerge explanatory principle — tend to deflect attention from major open problem in classical ai these problem include the need to bring meaning and reasoning into system that perform natural language process the need to infer and represent causality the need to develop computationally tractable representation of uncertainty and the need to develop system that formulate and pursue long term goal these be classical goal in human imitative ai but in the current hubbub over the ai revolution it be easy to forget that they be not yet solve ia will also remain quite essential because for the foreseeable future computer will not be able to match human in their ability to reason abstractly about real world situation we will need well think out interaction of human and computer to solve our most pressing problem and we will want computer to trigger new level of human creativity not replace human creativity whatever that might mean it be john mccarthy while a professor at dartmouth and soon to take a position at mit who coin the term ai apparently to distinguish his bud research agenda from that of norbert wiener then an old professor at mit wiener have coin cybernetic to refer to his own vision of intelligent system — a vision that be closely tie to operation research statistic pattern recognition information theory and control theory mccarthy on the other hand emphasize the tie to logic in an interesting reversal it be wiener s intellectual agenda that have come to dominate in the current era under the banner of mccarthy s terminology this state of affair be surely however only temporary ; the pendulum swing more in ai than in most field but we need to move beyond the particular historical perspective of mccarthy and wiener we need to realize that the current public dialog on ai — which focus on a narrow subset of industry and a narrow subset of academia — risk blind we to the challenge and opportunity that be present by the full scope of ai ia and ii this scope be less about the realization of science fiction dream or nightmare of super human machine and more about the need for human to understand and shape technology as it become ever more present and influential in their daily life moreover in this understanding and shape there be a need for a diverse set of voice from all walk of life not merely a dialog among the technologically attune focus narrowly on human imitative ai prevent an appropriately wide range of voice from be hear while industry will continue to drive many development academia will also continue to play an essential role not only in provide some of the most innovative technical idea but also in bring researcher from the computational and statistical discipline together with researcher from other discipline whose contribution and perspective be sorely need — notably the social science the cognitive science and the humanity on the other hand while the humanity and the science be essential as we go forward we should also not pretend that we be talk about something other than an engineering effort of unprecedented scale and scope — society be aim to build new kind of artifact these artifact should be build to work as claim we do not want to build system that help we with medical treatment transportation option and commercial opportunity to find out after the fact that these system don t really work — that they make error that take their toll in term of human life and happiness in this regard as I have emphasize there be an engineering discipline yet to emerge for the datum focus and learn focus field as exciting as these latter field appear to be they can not yet be view as constitute an engineering discipline moreover we should embrace the fact that what we be witness be the creation of a new branch of engineer the term engineering be often invoke in a narrow sense — in academia and beyond — with overtone of cold affectless machinery and negative connotation of loss of control by human but an engineering discipline can be what we want it to be in the current era we have a real opportunity to conceive of something historically new — a human centric engineering discipline I will resist give this emerge discipline a name but if the acronym ai continue to be use as placeholder nomenclature go forward let s be aware of the very real limitation of this placeholder let s broaden our scope tone down the hype and recognize the serious challenge ahead michael i jordan from a quick cheer to a stand ovation clap to show how much you enjoy this story michael i jordan be a professor in the department of electrical engineering and computer science and the department of statistic at uc berkeley
Milo Spencer-Harper,7.8K,6,https://medium.com/technology-invention-and-more/how-to-build-a-simple-neural-network-in-9-lines-of-python-code-cc8f23647ca1?source=tag_archive---------8----------------,how to build a simple neural network in 9 line of python code,as part of my quest to learn about ai I set myself the goal of build a simple neural network in python to ensure I truly understand it I have to build it from scratch without use a neural network library thank to an excellent blog post by andrew trask I achieve my goal here it be in just 9 line of code in this blog post I ll explain how I do it so you can build your own I ll also provide a long but more beautiful version of the source code but first what be a neural network the human brain consist of 100 billion cell call neuron connect together by synapsis if sufficient synaptic input to a neuron fire that neuron will also fire we call this process think we can model this process by create a neural network on a computer it s not necessary to model the biological complexity of the human brain at a molecular level just its high level rule we use a mathematical technique call matrix which be grid of number to make it really simple we will just model a single neuron with three input and one output we re go to train the neuron to solve the problem below the first four example be call a training set can you work out the pattern should the be 0 or 1 you might have notice that the output be always equal to the value of the leftmost input column therefore the answer be the should be 1 training process but how do we teach our neuron to answer the question correctly we will give each input a weight which can be a positive or negative number an input with a large positive weight or a large negative weight will have a strong effect on the neuron s output before we start we set each weight to a random number then we begin the training process eventually the weight of the neuron will reach an optimum for the training set if we allow the neuron to think about a new situation that follow the same pattern it should make a good prediction this process be call back propagation formula for calculate the neuron s output you might be wonder what be the special formula for calculate the neuron s output first we take the weighted sum of the neuron s input which be next we normalise this so the result be between 0 and 1 for this we use a mathematically convenient function call the sigmoid function if plot on a graph the sigmoid function draw an s shape curve so by substitute the first equation into the second the final formula for the output of the neuron be you might have notice that we re not use a minimum firing threshold to keep thing simple formula for adjust the weight during the training cycle diagram 3 we adjust the weight but how much do we adjust the weight by we can use the error weight derivative formula why this formula first we want to make the adjustment proportional to the size of the error secondly we multiply by the input which be either a 0 or a 1 if the input be 0 the weight isn t adjust finally we multiply by the gradient of the sigmoid curve diagram 4 to understand this last one consider that the gradient of the sigmoid curve can be find by take the derivative so by substitute the second equation into the first equation the final formula for adjust the weight be there be alternative formulae which would allow the neuron to learn more quickly but this one have the advantage of be fairly simple construct the python code although we win t use a neural network library we will import four method from a python mathematics library call numpy these be for example we can use the array method to represent the training set show early the t function transpose the matrix from horizontal to vertical so the computer be store the number like this ok I think we re ready for the more beautiful version of the source code once I ve give it to you I ll conclude with some final thought I have add comment to my source code to explain everything line by line note that in each iteration we process the entire training set simultaneously therefore our variable be matrix which be grid of number here be a complete work example write in python also available here https github com miloharper simple neural network final thought try run the neural network use this terminal command python main py you should get a result that look like we do it we build a simple neural network use python first the neural network assign itself random weight then train itself use the training set then it consider a new situation 1 0 0 and predict 0 99993704 the correct answer be 1 so very close traditional computer program normally can t learn what s amazing about neural network be that they can learn adapt and respond to new situation just like the human mind of course that be just 1 neuron perform a very simple task but what if we hook million of these neuron together could we one day create something conscious I ve be inspire by the huge response this article have receive I m consider create an online course click here to tell I what topic to cover i d love to hear your feedback from a quick cheer to a stand ovation clap to show how much you enjoy this story study economic at oxford university founder of www moju io interested in politic and ai technology trend and new invention follow this collection to update the late trend update as a collection editor I don t have any permission to add your article in the wild please submit your article and I will approve also follow this collection please
Greg Fish,1,4,https://worldofweirdthings.com/looking-for-a-ghost-in-the-machine-4c997c4da45b?source=tag_archive---------0----------------,look for a ghost in the machine weird thing,a short while ago I write about some of the challenge involve in create artificial intelligence and raise the question of how exactly a machine would spontaneously attain self awareness while I ve get plenty of feedback about how far technology have come so far and how it s imminent that machine will become much smart than we I never get any specific as to how exactly this would happen to I it s not a philosophical question because I m use to look at technology from a design and development standpoint when I ask for specific I m talk about functional requirement so far the close thing to outline the requirement for a super intelligent computer be a paper by university of oxford philosopher and futurist nick bostrom the first thing bostrom try to do be to establish a benchmark by how to grade what he call a super intellect and qualify his definition accord to he this super intellect would be smart than any human mind in every capacity from the scientific to the creative it s a pretty lofty goal because design something smart than yourself require that you build something you don t fully understand you might have a sudden stroke of luck and succeed but it s more than likely that you ll build a defective product instead imagine build a dna helix from scratch and with no detailed manual to go by even if you have all the tool and know where to find some bit of information to guide you when you don t know exactly what you re do the task become very challenging and you end up make a lot of mistake along the way there s also the question of how exactly we evaluate what the term smart mean in bostrom s projection when you have an intelligent machine become fully proficient in a certain area of expertise like say medicine it could combine with another machine which have an excellent understanding of physics and so on until all this consolidation lead to a device that know all that we know and can use all that cross disciplinary knowledge to gain insight we just don t have yet technologically that should be possible but the question be whether a machine like that would really be smart than human per se it would be far more knowledgeable than any individual human grant but it s not as if there aren t expert in particular field come together to make all sort of cross disciplinary connection and discovery what bostrom call a super intellect be actually just a massive knowledge base that can mine itself for information the paper be last revise in 1998 when we didn t have the enormous digital library we take for grant in today s world those library seem a fair bit like bostrom s super intellect in their function and if we be to combine they to mine their depth with sophisticated algorithm which look for cross disciplinary potential we d bring his concept to life but there s not a whole lot of intelligence there just a lot of datum much of which would be subject to change or revision as research and discovery continue just like bostrom say it would be a very useful tool for scientist and researcher however it wouldn t be think on its own and give the human advice even if we put all this datum on supercomputer which could live up to the paper s ambitious hardware requirement rev it up to match the estimate capacity of our brain it say and watch a new kind of intellect start wake up and take shape with the proper software accord to bostrom the human brain operate at 100 teraflop or 100 trillion float point operation per second now as he predict computer have reach this speed by 2004 and go far beyond that in fact we have supercomputer which be as much as ten time fast supposedly at these operate speed we should be able to write software which allow supercomputer to learn by interact with human and sift through our digitized knowledge but the reality be that we d be try to teach an intimate object make of metal and plastic how to think and solve problem something we re already bear with and hone over our lifetime you can teach someone how to ride a bike and how to balance but how exactly would you teach someone to understand the purpose of ride a bike how would you tell someone with no emotion no desire no want and no need why he should go anywhere that deep layer of motivation and wiring have take several billion year to appear and be hone over a 600 million additional year of evolution when we start try to make an ai system comparable to ours we re effectively way behind from the get go to truly create an intelligent computer which doesn t just act as if it s think or do mechanical action which be easy to predict and program we d need to impart in all that information in trillion of line of code and trick circuitry into deduce it need to behave like a live being and that s a job that couldn t be do in less than century much less in the next 20 to 30 year as project by ray kurzweil and his fan eerie illustration by neil blevin from a quick cheer to a stand ovation clap to show how much you enjoy this story techie rantt staff writer and editor computer lobotomist science tech and other oddity
Oliver Lindberg,1,7,https://medium.com/the-lindberg-interviews/interview-with-googles-alfred-spector-on-voice-search-hybrid-intelligence-and-more-2f6216aa480c?source=tag_archive---------1----------------,interview with google s alfred spector on voice search hybrid intelligence and more,google s a pretty good search engine right well you ain t see nothing yet vp of research alfred spector talk to oliver lindberg about the technology emerge from google labs — from voice search to hybrid intelligence and beyond this article originally appear in issue 198 of net magazine in 2010 and be republish at www techradar com google have always be tight lip about product that haven t launch yet it s no secret however that thank to the company s bottom up culture its engineer be work on ton of new project at the same time follow the mantra of release early release often the speed at which the search engine giant be churn out tool be staggering at the heart of it all be alfred spector google s vice president of research and special initiative one of the area google be make significant advance in be voice search spector be astounded by how rapidly it s come along the google mobile app feature search by voice capability that be available for the iphone blackberry window mobile and android all version understand english include we uk australian and indian english accent but the late addition for nokia s60 phone even introduce mandarin speech recognition which — because of its many different accent and tonal characteristic — pose a huge engineering challenge it s the most spoken language in the world but as it isn t exactly keyboard friendly voice search could become immensely popular in china voice be one of these grand technology challenge in computer science spector explain can a computer understand the human voice it s be work on for many decade and what we ve realise over the last couple of year be that search particularly on handheld device be amenable to voice as an import mechanism it s very valuable to be able to use voice all of we know that no matter how good the keyboard it s tricky to type exactly the right thing into a searchbar while hold your backpack and everything else to get a computer to take account of your voice be no mean feat of course one idea be to take all of the voice that the system hear over time into one huge pan human voice model so on the one hand we have a voice that s high and with an english accent and on the other hand my voice which be deep and with an american accent they both go into one model or it just become personalise to the individual ; voice scientist be a little unclear as to which be the good approach the research department be also make progress in machine translation google translate already feature 51 language include swahili and yiddish the late version introduce instant real time translation phonetic input and text to speech support in english we re able to go from any language to any of the other and there be 51 time 50 so 2 550 possibility spector explain we re focus on increase the number of language because we d like to handle even those language where there s not an enormous volume of usage it will make the web far more valuable to more people if they can access the english or chinese language web for example but we also continue to focus on quality because almost always the translation be valuable but imperfect sometimes it come from train our translation system over more raw datum so we have say eu document in english and french and can compare they and learn rule for translation the other approach be to bring more knowledge into translation for example we re use more syntactic knowledge today and do automate parsing with language it s be a grand challenge of the field since the late 1950 now it s finally achieve mass usage the team lead by scientist franz josef och have be collect datum for more than 100 language and the google translator toolkit which make use of the wisdom of the crowd now even support 345 language many of which be minority language the editor enable user to translate text correct the automatic translation and publish it spector think that this approach be the future as computer become even fast handle more and more datum — a lot of it in the cloud — machine learn from user and thus become smart he call this concept hybrid intelligence it s very difficult to solve these technological problem without human input he say it s hard to create a robot that s as clever smart and knowledgeable of the world as we human be but it s not as tough to build a computational system like google which extend what we do greatly and gradually learn something about the world from we but that require our interpretation to make it really successful we need to get computer and people communicate in both direction so the computer learn from the human and make the human more effective example of hybrid intelligence be google suggest which instantly offer popular search as you type a search query and the do you mean feature in google search which correct you when you misspell a query in the search bar the more you use it the well the system get training computer to become seemingly more intelligent pose major hurdle for google s engineer computer don t train as efficiently as people do spector explain let s take the chess example if a kasparov be the educator we could count on almost anything he say as be accurate but if you try to learn from a million chess player you learn from my child as well who play chess but they re 10 and eight they ll be right sometimes and not right other time there s noise in that and some of the noise be spam one also have to have careful regard for privacy issue by collect enormous amount of datum google hope to create a powerful database that eventually will understand the relationship between word for example a dog be an animal and a dog have four leg the challenge be to try to establish these relationship automatically use ton of information instead of have expert teach the system this database would then improve search result and language translation because it would have a well understanding of the meaning of the word there s also a lot of research around conceptual search let s take a video of a couple in front of the empire state building we watch the video and it s clear they re on their honeymoon but what be the video about be it about love or honeymoon or be it about rent office space it s a fundamentally challenging problem one example of conceptual search be google image swirl which be add to lab in november enter a keyword and you get a list of 12 image ; click on each one bring up a cluster of related picture click on any of they to expand the wonder wheel further google note that they re not just the most relevant image ; the algorithm determine the most relevant group of image with similar appearance and meaning to improve the world s datum google continue to focus on the importance of the open internet another lab project google fusion table facilitate data management in the cloud it enable user to create table filter and aggregate datum merge it with other datum source and visualise it with google map or the google visualisation api the data set can then be publish share or keep private and comment on by people around the world it s an example of open collaboration spector say if it s public we can crawl it to make it searchable and easily visible to people we hire one of the good database researcher in the world alon halevy to lead it google be aim to make more information available more easily across multiple device whether it s image video speech or map no matter which language we re use spector call the impact totally transparent processing — it revolutionise the role of computation in day today life the computer can break down all these barrier to communication and knowledge no matter what device we re use we have access to thing we can do translation there be book or government document and some day we hope to have medical record whatever you want no matter where you be you can find it spector retire in early 2015 and now serve as the cto of two sigma investment this article originally appear in issue 198 of net magazine in 2010 and be republish at www techradar com photography by andy short from a quick cheer to a stand ovation clap to show how much you enjoy this story independent editor and content consultant founder and captain of @pixelpioneers co founder and curator of www generateconf com former editor of @netmag interview with lead tech entrepreneur and web designer conduct by @oliverlindberg at @netmag
Greg Fish,1,4,https://worldofweirdthings.com/the-technical-trouble-with-humanoid-robots-2c712649f3c5?source=tag_archive---------5----------------,the technical trouble with humanoid robot weird thing,if you ve be read this blog long enough you may recall that I m not a big fan of humanoid robot there s no need to invoke the uncanny valley effect even though some attempt to build humanoid robot manage to produce rather creepy entity which try to look as human as possible to goad future user into some kind of social bond with they presumably to gain their trust and get into a perfect position to kill the inferior thing make of flesh no the reason why I m not sure that humanoid robot will be invaluable to we in the future be a very pragmatic one simply put emulate bipedalism be a huge computational overhead as well as a major and unavoidable engineering and maintenance headache and with the limit on size and weight of would be robot butler as well as the patience of its user humanoid bot designer may be aim a bit too high we walk run and perform complicated task with our hand and foot so easily we only notice the amount of effort and coordination this take after an injury that limit our mobility the reason why we can do that lie in a small squishy mass of neuron coordinate a firestorm of constant activity unlike old standing urban myth imply we actually use all of our brainpower and we need it to help coordinate and execute the same motion that robot struggle to repeat of course our brain be cheat when compare to a computer because with ten of billion of neuron and trillion of synapsis our brain be like scream fast supercomputer they can calculate what it will take to catch a ball in mid air in less than a few hundred millisecond and make the most minute adjustment to our muscle in order to keep we balanced and upright just as quickly likewise our body can heal the constant wear and tear on our joint wear and tear we will accumulate from walk run and bump into thing bipedal robot navigate our world wouldn t have these asset humanoid machine would need to be constantly maintain just to keep up with we in a mechanical sense and carry the equivalent of red storm in their head or at least be link to something like it to even hope to coordinate themselves as quickly as we do cognitively and physically academically this be a lofty goal which could yield new algorithm and robotic design practically not so much while last month s feature in pop sci bemoan the lack of interest in humanoid robot in the u s it also fail to demonstrate why such an incredibly complicated machine would be need for basic household chore that could be do by robotic system function independently and without the need to move on two leg instead we get the standard baby boomer caretaker argument which go somewhat like this or alternatively a computer could book your appointment via e mail or a system that let patient make an appointment with their doctor on the web a smart dispenser that give you the right amount of pill check for potential interaction base on public medical database and beep to remind you to take your medicine and a programmable walker with actuator and a few button could do these job while cost far less than the ten of million a humanoid robot would cost by 2025 and require much less coordination or learning than a programmable humanoid why wouldn t we want to pursue immediate fix to what s be describe as a loom caretaker shortage choose instead to invest billion of dollar into e jeeve which may take an entire decade or two just to learn how to go about daily human life ready to tackle the problem only after it be no long an issue even if we start right now if anything harp on the need for a robotic hand for baby boomer future medical woe would only prompt more r&d cash into immediate solution and rule base intelligent agent we already employ rather than long term academic research there s a huge gap between human ability and machinery because we have the benefit of have evolve over hundred of million of year of trial and error machine even though they re advance at an ever fast pace only have a few decade by comparison it will take decade more to build self repair machine and computer chip that can boast the same performance as a supercomputer while be small enough to fit in human sized robot head before robotic butler become practical and feasible and even then we might go with distinctly robotic version because they d be cheap to maintain and operate from a quick cheer to a stand ovation clap to show how much you enjoy this story techie rantt staff writer and editor computer lobotomist science tech and other oddity
Frank Diana,50,10,https://medium.com/@frankdiana/the-evolving-role-of-business-analytics-76818e686e39?source=tag_archive---------2----------------,the evolve role of business analytic frank diana medium,an old post that seem to be get a lot of attention appreciation for analytic rise business analytic refer to the skill technology application and practice for the continuous exploration of datum to gain insight that drive business decision business analytic be multi faceted it combine multiple form of analytic and apply the right method to deliver expect result it focus on develop new insight use technique include data mining predictive analytic natural language process artificial intelligence statistical analysis and quantitative analysis in addition domain knowledge be a key component of the business analytic portfolio business analytic can then be view as the combination of domain knowledge and all form of analytic in a way that create analytic application focus on enable specific business outcome analytic application have a set of business outcome that they must enable for fraud its reduce loss for quality & safety it might be avoid expensive recall understand how to enable these outcome be the first step in determine the make up of each specific application for example in the case of insurance fraud it s not enough to use statistical analysis to predict fraud you need a strong focus on text domain expertise and the ability to visually portray organize crime ring insight gain through this analysis may be use as input for human decision or may drive fully automate decision database capacity processor speed and software enhancement will continue to drive even more sophisticated analytic application the key component of business analytic be there be a massive explosion of datum occur on a number of level the notion of data overload be echo in a previous 2010 ibm ceo study title capitalize on complexity in this study a large number of ceo describe their organization as datum rich but insight poor many voice frustration over their inability to transform available datum into feasible action plan this notion of turn datum into insight and insight to action be a common and grow theme accord to pricewaterhouse cooper there be approximately 75 to 100 million blog and 10 20 million internet discussion board and forum in the english language alone as the forrester diagram describe more consumer be move up the ladder and become creator of content in addition estimate show the volume of unstructured datum email audio video web page etc double every three month effectively manage and harness this vast amount of information present both a great challenge and a great opportunity datum be flow through medical device scientific device sensor monitor detector other supply chain device instrument car and road instrument domestic appliance etc everything will be instrument and from this instrumentation come datum this datum will be analyze to find insight that drive smart decision the utility sector provide a great example of the grow need for analytic the smart grid and the gradual installation of intelligent endpoint smart meter and other device will generate volume of data smart grid utility be evolve into broker of information the datum tsunami that will wash over utility in the come year be a formidable it challenge but it be also a huge opportunity to move beyond simple meter to cash function and into real time optimization of their operation this type of instrumentation be play out in many industry as this occur industry player will be challenge to leverage the datum generate by these device inside the enterprise consider the increase volume of email word document pdf excel worksheet and free form text field that contain everything from budget and forecast to customer proposal contract call center note and expense report outside the enterprise the growth of web base content which be primarily unstructured continue to accelerate everything from social medium comment in blog forum and social network to survey verbatim and wiki page most industry analyst estimate more than 80 % of the intelligence require to make smart decision be contain in unstructured datum or text the survey result in a recent mit sloan report support both an aggressive adoption of analytic and a shift in the analytic footprint accord to the report many traditional form of analytic will be surpass in the next 24 month the author produce a very effective visual that show this shift from today s analytic footprint to the future footprint although list as number one the author describe visualization as dashboard and scorecard — the traditional method of visualization new and emerge method help accelerate time to insight these new approach help we absorb insight from large volume of datum in rapid fashion the analytic identify as create the most value in 24 month be company and organization continue to invest million of dollar capture store and maintain all type of business datum to drive sale and revenue optimize operation manage risk and ensure compliance most of this investment have be in technology and application that manage structured datum — code information reside in relational data base management system in the form of row and column current method such as traditional business intelligence bi be more about query and reporting and focus on answer question such as what happen how many how often and what action be need new form of advanced analytic be require to address the business imperative describe early business analytic focus on answer question such as why be this happen what if these trend continue what will happen next predict what be the good that can happen optimize there be a grow view that prescribe outcome be the ultimate role of analytic ; that be identify those action that deliver the right business outcome organization should first define the insight need to meet business objective and then identify datum that provide that insight too often company start with datum the previously mention ibm study also reveal that analytic drive organization have 33 percent more revenue growth with 32 percent more return on capital invest organization expect value from emerge analytic technique to soar the growth of innovative analytic application will serve as a mean to help individual across the organization consume and act upon insight derive through complex analysis some example of innovative use a recent mit sloan report effectively use the maturity model concept to describe how organization typically evolve to analytic excellence the author point out that organization begin with efficiency goal and then address growth objective after experience be gain the author believe this be a common practice but not necessarily a good practice they see the traditional analytic adoption path start in datum intensive area like financial management operation and sale and marketing as company move up the maturity curve they branch out into new function such as strategy product research customer service and customer experience in the opinion of the author these pattern suggest that success in one area stimulate adoption in other they suggest that this allow organization to increase their level of sophistication the author of the mit sloan special report through their analysis of survey result have create three level of analytic capability the report provide a very nice matrix that describe these level in the context of a maturity model in review business challenge outline in the matrix there be one very interesting dynamic the transition from cost and efficiency to revenue growth customer retention and customer acquisition the author of the report find that as the value of analytic grow organization be likely to seek a wide range of capability — and more advanced use of exist one the survey indicate that this dynamic be lead some organization to create a centralized analytic unit that make it possible to share analytic resource efficiently and effectively these centralized enterprise unit be the primary source of analytic provide a home for more advanced skill within the organization this same dynamic will lead to the appointment of chief analytic officer cao start in 2011 the availability of strong business focus analytical talent will be the great constraint on a company s analytical capability the outsourcing of analytic will become an attractive alternative as the need for specialized skill will lead organization to look for outside help outsource analytic allow a company to focus on take action base on insight deliver by the outsourcer the outsourcer can leverage these specialized resource across multiple client as the importance of analytic grow organization will have an option to outsource expect to see more of this in 2011 we will see more organization establish enterprise datum management function to coordinate datum across business unit we will also see smart approach such as information lifecycle management as oppose to the common approach of throw more hardware at the grow data problem the information management challenge will grow as million of next generation tech savvy user use feed and mash up to bring datum together into usable part so they can answer their own question this give rise to new challenge include data security and governance originally publish at frankdiana net on march 20 2011 from a quick cheer to a stand ovation clap to show how much you enjoy this story tcs executive focus on the rapid evolution of society and business fascinate by the view of the world in the next decade and beyond https frankdiana net
Paul Christiano,43,31,https://ai-alignment.com/a-formalization-of-indirect-normativity-7e44db640160?source=tag_archive---------0----------------,formalize indirect normativity ai alignment,this post outline a formalization of what nick bostrom call indirect normativity I don t think it s an adequate solution to the ai control problem ; but to my knowledge it be the first precise specification of a goal that meet the not terrible bar I e which do not lead to terrible consequence if pursue without any caveat or restriction the proposal outline here be sketch in early 2012 while I be visit fhi and be my first serious foray into ai control when face with the challenge of write down precise moral principle adhere to the standard demand in mathematics moral philosopher encounter two serious difficulty in light of these difficulty a moral philosopher might simply declare it be not my place to aspire to mathematical standard of precision ethic as a project inherently require shared language understanding and experience ; it become impossible or meaningless without they this may be a defensible philosophical position but unfortunately the issue be not entirely philosophical in the interest of build institution or machine which reliably pursue what we value we may one day be force to describe precisely what we value in a way that do not depend on charitable or common sense interpretation in the same way that we today must describe what we want do precisely to computer often with considerable effort if some aspect of our value can not be describe formally then it may be more difficult to use institution or machine to reliably satisfy they this be not to say that describe our value formally be necessary to satisfy they merely that it might make it easy since we be focus on find any precise and satisfactory moral theory rather than resolve dispute in moral philosophy we will adopt a consequentialist approach without justification and focus on axiology moreover we will begin from the standpoint of expect utility maximization and leave aside question about how or over what space the maximization be perform we aim to mathematically define a utility function u such that we would be willing to build a hypothetical machine which exceptionlessly maximize u possibly at the catastrophic expense of any other value we will assume that the machine have an ability to reason which at least rival that of human and be willing to tolerate arbitrarily complex definition of u within its ability to reason about they we adopt an indirect approach rather than specify what exactly we want we specify a process for determine what we want this process be extremely complex so that any computationally limited agent will always be uncertain about the process output however by reasoning about the process it be possible to make judgment about which action have the high expect utility in light of this uncertainty for example I might adopt the principle a state of affair be valuable to the extent that I would judge it valuable after a century of reflection in general I will be uncertain about what I would say after a century but I can act on the basis of my good guess after a century I will probably prefer world with more happiness and so today I should prefer world with more happiness after a century I have only a small probability of value tree feeling and so today I should go out of my way to avoid hurt they if it be either instrumentally useful or extremely easy as I spend more time think my belief about what I would say after a century may change and I will start to pursue different state of affair even though the formal definition of my value be static similarly I might desire to think about the value of tree feeling if I expect that my opinion be unstable if I spend a month think about tree my current view will then be a much well predictor of my view after a hundred year and if I know well whether or not tree feeling be valuable I can make well decision this example be quite informal but it communicate the main idea of the approach we stress that the value of our contribution if any be in the possibility of a precise formulation our proposal itself will be relatively informal ; instead it be a description of how you would arrive at a precise formulation the use of indirection seem to be necessary to achieve the desire level of precision our proposal contain only two explicit step each of these step require substantial elaboration but we must also specify what we expect the human to do with these tool this proposal be well understand in the context of other fantastic seeming proposal such as my utility be whatever I would write down if I reflect for a thousand year without interruption or biological decay the counterfactual event which take place within the definition be far beyond the realm our intuition recognize as realistic and have no place except in thought experiment but to the extent that we can reason about these counterfactual and change our behavior on the basis of that reasoning if so motivated we can already see how such fantastic situation could affect our more prosaic reality the remainder of this document consist of brief elaboration of some of these step and a few argument about why this be a desirable process the first step of our proposal be a high fidelity mathematical model of human cognition we will set aside philosophical trouble and assume that the human brain be a purely physical system which may be characterize mathematically even grant this it be not clear how we can realistically obtain such a characterization the most obvious approach to characterize a brain be to combine measurement of its behavior or architecture with an understanding of biology chemistry and physic this project represent a massive engineering effort which be currently just begin most pessimistically our proposal could be postpone until this project s completion this could still be long before the mathematical characterization of the brain become useful for run experiment or automate human activity because we be interested only in a definition we do not care about have the computational resource necessary to simulate the brain an impractical mathematical definition however may be much easy to obtain we can define a model of a brain in term of exhaustive search which could never be practically carry out for example give some observation of a neuron we can formally define a brute force search for a model of that neuron similarly give model of individual neuron we may be able to specify a brute force search over all way of connect those neuron which account for our observation of the brain say some datum acquire through functional neuroimaging it may be possible to carry out this definition without exploit any structural knowledge about the brain beyond what be necessary to measure it effectively by collect image datum for a human expose to a wide variety of stimulus we can recover a large corpus of datum which must be explain by any model of a human brain moreover by use our explicit knowledge of human cognition we can algorithmically generate an extensive range of test which identify a successful simulation by probe response to question or performance on game or puzzle in fact this project may be possible use exist resource the complexity of the human brain be not as unapproachable as it may at first appear though it may contain 1014synapse each describe by many parameter it can be specify much more compactly a newborn s brain can be specify by about 109bits of genetic information together with a recipe for a physical simulation of development the human brain appear to form new long term memory at a rate of 1 2 bit per second suggest that it may be possible to specify an adult brain use 109additional bit of experiential information this suggest that it may require only about 1010bit of information to specify a human brain which be at the limit of what can be reasonably collect by exist technology for functional neuroimage this discussion have gloss over at least one question what do we mean by brain emulation human cognition do not reside in a physical system with sharp boundary and it be not clear how you would define or use a simulation of the input output behavior of such an object we will focus on some system which do have precisely define input output behavior and which capture the important aspect of human cognition consider a system contain a human a keyboard a monitor and some auxiliary instrument well insulate from the environment except for some wire carry input to the monitor and output from the keyboard and auxiliary instrument and wire carry power the input to this system be simply screen to be display on the monitor say deliver as a sequence to be display one after another at 30 frame per second while the output be the information convey from the keyboard and the other measuring apparatus also deliver as a sequence of datum dump each recording activity from the last 30th of a second this human in a box system can be easily formally define if a precise description of a human brain and coarse description of the human body and the environment be available alternatively the input output behavior of the human in a box can be directly observe and a computational model construct for the entire system let h be a mathematical definition of the result randomized function from input sequence in 1 in 2 in k to the next output out k h be by design a good approximation to what the human would output if present with any particular input sequence use h we can mathematically define what would happen if the human interact with a wide variety of system for example if we deliver out k as the input to an abstract computer run some arbitrary software and then define in k+1 as what the screen would next display we can mathematically define the distribution over transcript which would have arise if the human have interact with the abstract computer this computer could be run an interactive shell a video game or a message client note that h reflect the behavior of a particular human in a particular mental state this state be determine by the process use to design h or the datum use to learn it in general we can control h by choose an appropriate human and provide appropriate instruction train more emulation could be produce by similar measure if necessary use only a single human may seem problematic but we will not rely on this lone individual to make all relevant ethical judgment instead we will try to select a human with the motivational stability to carry out the subsequent step faithfully which will define u use the judgment of a community consist of many human this discussion have be brief and have necessarily gloss over several important difficulty one difficulty be the danger of use computationally unbounded brute force search give the possibility of short program which exhibit goal orient behavior another difficulty be that unless the emulation project be extremely conservative the model it produce be not likely to be fully functional human their thought may be blur in various way they may be miss many memory or skill and they may lack important functionality such as long term memory formation or emotional expression the scope of these issue depend on the availability of datum from which to learn the relevant aspect of human cognition realistic proposal along these line will need to accommodate these shortcoming rely on distorted emulation as a tool to construct increasingly accurate model for any idealized software with a distinguished instruction return we can use h to mathematically define the distribution over return value which would result if the human be to interact with that software we will informally define a particular program t which provide a rich environment in which the remainder of our proposal can be implement from a technical perspective this will be the last step of our proposal the remain step will be reflect only in the intention and behavior of the human being simulate in h fix a convenient and adequately expressive language say a dialect of python design to run on an abstract machine t implement a standard interface for an interactive shell in this language the user can look through all of the past instruction that have be execute and their return value render as string or execute a new instruction we also provide symbol represent h and t themselves as function from sequence of k inputs to a value for the kth output we also provide some useful information such as a snapshot of the internet and some information about the process use to create h and t which we encode as a bit string and store in a single environment variable datum we assume that our language of choice have a return instruction and we have t return whenever the user execute this instruction some care need to be take to define the behavior if t enter an infinite loop we want to minimize the probability that the human accidentally hang the terminal with catastrophic consequence but we can not provide a complete safety net without run into unresolvable issue with self reference we define u to be the value return by h interact with t if h represent an unfortunate mental state then this interaction could be short and unproductive the simulated human could just decide to type return 0 and be do with it however by choose an appropriate human to simulate and inculcate an appropriate mental state we can direct the process far we intend for h to use the resource in t to initiate a large deliberative process for example the first step of this process may be to instantiate many copy of h interact with variant of message client which be in contact with each other the return value from the original process could then be define as the value return by a designate leader from this community or as a majority vote amongst the copy of h or so on another step might be to create appropriate realistic virtual environment for simulated brain rather than confine they to box for motivational stability it may be helpful to design various coordination mechanism involve framework for interaction cache mental state which be frequently re instantiate or sanity check whereby one copy of h monitor the behavior of another the result community of simulated brain then engage in a protract planning process ensure that subsequent step can be carry out safely or develop alternative approach the main priority of this community be to reduce the probability of error as far as possible exactly what constitute an error will be discuss at more length later at the end of this process we obtain a formal definition of a new protocol h+ which submit its input for consideration to a large community and then produce its output use some deliberation mechanism democratic vote one leader use the rest of the community as advisor etc the next step require our community of simulate brain to construct a detailed simulation of earth which they can observe and manipulate once they have such a simulation they have access to all of the datum which would have be available on earth in particular they can now explore many possible future and construct simulation for each live human in order to locate earth we will again leverage an exhaustive search first h+ decide on informal desiderata for an earth simulation these be likely to be as follow once h+ have decide on the desiderata it use a brute force search to find a simulation satisfy they for each possible program it instantiate a new copy of h+ task with evaluate whether that program be an acceptable simulation we then define e to be a uniform distribution over program which pass this evaluation we might have doubt about whether this process produce the real earth perhaps even once we have verify that it be identical accord to a laundry list of measure it may still be different in other important way there be two reason why we might care about such difference first if the simulated earth have a substantially different set of people than the real earth then a different set of people will be involve in the subsequent decision make if we care particularly about the opinion of the people who actually exist which the reader might well be amongst such people then this may be unsatisfactory second if event transpire significantly differently on the simulated earth than the real earth value judgment design to guide behavior appropriately in the simulated earth may lead to less appropriate behavior in the real earth this will not be a problem if our ultimate definition of u consist of universalizable ethical principle but we will see that u might take other form these concern be address by a few broad argument first check a detailed but arbitrary laundry list actually provide a very strong guarantee for example if this laundry list include verify a snapshot of the internet then every event or person document on the internet must exist unchanged and every keystroke of every person compose a document on the internet must not be disturb if the world be well interconnected then it may be very difficult to modify part of the world without have substantial effect elsewhere and so if a long enough arbitrary list of property be fix we expect nearly all of the world to be the same as well second if the essential character of the world be fix but detailed be varied we should expect the sort of moral judgment reach by consensus to be relatively constant finally if the system whose behavior depend on these moral judgment be identical between the real and simulated world then output a u which cause that system to behave a certain way in the simulated world will also cause that system to behave that way in the real world once h+ have define a simulation of the world which permit inspection and intervention by careful trial and error h+ can inspect a variety of possible future in particular they can find intervention which cause the simulated human society to conduct a real brain emulation project and produce high fidelity brain scan for all live human once these scan have be obtain h+ can use they to define u as the output of a new community h++ which draw on the expertise of all live human operate under ideal condition there be two important degree of flexibility how to arrange the community for efficient communication and deliberation and how to delegate the authority to define u in term of organization the distinction between different approach be probably not very important for example it would probably be perfectly satisfactory to start from a community of human interact with each other over something like the exist internet but on abstract secure infrastructure more important be the safety measure which would be in place and the mechanism for resolve difference of value between different simulate human the basic approach to resolve dispute be to allow each human to independently create a utility function u each bound in the interval 0 1 and then to return their average this average can either be unweighte or can be weight by a measure of each individual s influence in the real world in accordance with a game theoretic notion like the shapley value apply to abstract game or simulation of the original world more sophisticated mechanism be also possible and may be desirable of course these question can and should be address in part by h+ during its deliberation in the previous step after all h+ have access to an unlimited length of time to deliberate and have infinitely powerful computational aid the role of our reasoning at this stage be simply to suggest that we can reasonably expect h+ to discover effective solution as when discuss discover a brain simulation by brute force we have skip over some critical issue in this section in general brute force search particularly over program which we would like to run be quite dangerous because such search will discover many program with destructive goal orient behavior to deal with these issue in both case we must rely on patience and powerful safety measure once we have a formal description of a community of interact human give as much time as necessary to deliberate and equip with infinitely powerful computational aid it become increasingly difficult to make coherent prediction about their behavior critically though we can also become increasingly confident that the outcome of their behavior will reflect their intention we sketch some possibility to illustrate the degree of flexibility available perhaps the most natural possibility be for this community to solve some outstanding philosophical problem and to produce a utility function which directly capture their preference however even if they quickly discover a formulation which appear to be attractive they would still be wise to spend a great length of time and to leverage some of these other technique to ensure that their propose solution be really satisfactory another natural possibility be to eschew a comprehensive theory of ethic and define value in term of the community s judgment we can define a utility function in term of the hypothetical judgment of astronomical number of simulated human collaboratively evaluate the goodness of a state of affair by examine its history at the atomic level understand the relevant high order structure and apply human intuition it seem quite likely that the community will gradually engage in self modification enlarge their cognitive capacity along various dimension as they come to understand the relevant aspect of cognition and judge such modification to preserve their essential character either independently or as an outgrowth of this process they may gradually or abruptly pass control to machine intelligence which they be suitably confident express their value this process could be use to acquire the power necessary to define a utility function in one of the above framework or understanding value preserve self modification or machine intelligence may itself prove an important ingredient in formalizing what it be we value any of these operation would be perform only after considerable analysis when the original simulated human be extremely confident in the desirability of the result whatever path they take and whatever coordination mechanism they use eventually they will output a utility function u we then define u = 0 if u < 0 u = 1 if u > 1 and u = u otherwise at this point we have offer a proposal for formally define a function u we have make some general observation about what this definition entail but now we may wonder to what extent u reflect our value or more relevantly to what extent our value be serve by the creation of u maximizer concern may be divide into a few natural category we respond to each of these objection in turn if the process work as intend we will reach a stage in which a large community of human reflect on their value undergo a process of discovery and potentially self modification and then output its result we may be concern that this dynamic do not adequately capture what we value for example we may believe that some other extrapolation dynamic capture our value or that it be morally desirable to act on the basis of our current belief without further reflection or that the presence of realistic disruption such as the threat of catastrophe have an important role in shape our moral deliberation the important observation in the defense of our proposal be that whatever objection we could think of today we could think of within the simulation if upon reflection we decide that too much reflection be undesirable we can simply change our plan appropriately if we decide that realistic interference be important for moral deliberation we can construct a simulation in which such interference occur or determine our moral principle by observe moral judgment in our own world s possible future there be some chance that this proposal be inadequate for some reason which win t be apparent upon reflection but then by definition this be a fact which we can not possibly hope to learn by deliberate now it therefore seem quite difficult to maintain objection to the proposal along these line one aspect of the proposal do get lock in however after be consider by only one human rather than by a large civilization the distribution of authority amongst different human and the nature of mechanism for resolve differ value judgment here we have two possible defense one be that the mechanism for resolve such disagreement can be reflect on at length by the individual simulate in h this individual can spend generation of subjective time and greatly expand her own cognitive capacity while attempt to determine the appropriate way to resolve such disagreement however this defense be not completely satisfactory we may be able to rely on this individual to produce a very technically sound and generally efficient proposal but the proposal itself be quite value laden and rely on one individual to make such a judgment be in some sense beg the question a second more compelling defense be that the structure of our world have already provide a mechanism for resolve value disagreement by assign decision make weight in a way that depend on current influence for example as determine by the simulated ability of various coalition to achieve various goal we can generate a class of proposal which be at a minimum no bad than the status quo of course these consideration will also be shape by the condition surround the creation or maintenance of system which will be guide by u for example if a nation be to create a u maximizer they might first adopt an internal policy for assign influence on u by perform this decision making in an idealized environment we can also reduce the likelihood of destructive conflict and increase the opportunity for mutually beneficial bargaining we may have moral objection to codify this sort of might make right policy favor a more democratic proposal or something else entirely but as a matter of empirical fact a more cosmopolitan proposal will be adopt only if it be support by those with the appropriate form of influence a situation which be unchanged by precisely codify exist power structure finally the value of the simulation in this process may diverge from the value of the original human model for one reaosn or another for example the simulated human may predictably disagree with the original model about ethical question by virtue of probably have no physical instantiation that be the output of this process be define in term of what a particular human would do in a situation which that human know will never come to pass if I ask what would I do if I be to wake up in a featureless room and tell that the future of humanity depend on my action the answer might begin with become distressed that I be clearly inhabit a hypothetical situation and adjust my ethical view to take into account the fact that people in hypothetical situation apparently have relevant first person experience set aside the question of whether such adjustment be justify they at least raise the possibility that our value may diverge from those of the simulation in this process these change might be minimize by understand their nature in advance and treat they on a case by case basis if we can become convinced that our understanding be exhaustive for example we could try and use human who robustly employ updateless decision theory which never undergo such predictable change or we could attempt to engineer a situation in which all of the human be emulate do have physical instantiation and naive self interest for those emulation align roughly with the desire behavior for example by allow the early emulation to write themselves into our world we can imagine many way in which this process can fail to work as intend the original brain emulation may accurately model human behavior the original subject may deviate from the intend plan or simulate human can make an error when interact with their virtual environment which cause the process to get hijack by some unintended dynamic we can argue that the proposal be likely to succeed and can bolster the argument in various way by reduce the number of assumption necessary for succee build in fault tolerance justify each assumption more rigorously and so on however we be unlikely to eliminate the possibility of error therefore we need to argue that if the process fail with some small probability the result value will only be slightly disturb this be the reason for require u to lie in the interval 0 1 we will see that this restriction bound the damage which may be do by an unlikely failure if the process fail with some small probability ε then we can represent the result utility function as u = 1 — ε u1 + ε u2 where u1 be the intended utility function and u2 be a utility function produce by some arbitrary error process now consider two possible state of affair a and b such that u1 a > u1 b + ε 1 — ε ≈ u1 b + ε then since 0 ≤ u2 ≤ 1 we have u a = 1 — ε u1 a + ε u2 a > 1 — ε u1 b + ε ≥ 1 — ε u1 b + ε u2 b = u b thus if a be substantially well than b accord to u1 then a be well than b accord to u this show that a small probability of error whether come from the stochasticity of our process or an agent s uncertainty about the process output have only a small effect on the result value moreover the process contain a human who have access to a simulation of our world this imply in particular that they have access to a simulation of whatever u maximize agent exist in the world and they have knowledge of those agent belief about u this allow they to choose u with perfect knowledge of the effect of error in these agent judgment in some case this will allow they to completely negate the effect of error term for example if the randomness in our process cause a perfectly cooperate community of simulated human to control u with probability 2⁄3 and cause an arbitrary adversary to control it with probability 1⁄3 then the simulated human can spend half of their mass outputting a utility function which exactly counter the effect of the adversary in general the situation be not quite so simple the fraction of mass control by any particular coalition will vary as the system s uncertainty about u varie and so it will be impossible to counteract the effect of an error term in a way which be time independent instead we will argue later that an appropriate choice of a bounded and noisy u can be use to achieve a very wide variety of effective behavior of u maximizer overcome the limitation both of bound utility maximization and of noisy specification of utility function many possible problem with this scheme be describe or implicitly address above but that discussion be not exhaustive and there be some class of error that fall through the crack one interesting class of failure concern change in the value of the hypothetical human h this human be in a very strange situation and it seem quite possible that the physical universe we know contain extremely few instance of that situation especially as the process unfold and become more exotic so h s first person experience of this situation may lead to significant change in h s view for example our intuition that our own universe be valuable seem to be derive substantially from our judgment that our own first person experience be valuable if hypothetically we find ourselves in a very alien universe it seem quite plausible that we would judge the experience within that universe to be morally valuable as well depend perhaps on our initial philosophical inclination another example concern our self interest much of individual human value seem to depend on their own anticipation about what will happen to they especially when face with the prospect of very negative outcome if hypothetically we wake up in a completely non physical situation it be not exactly clear what we would anticipate and this may distort our behavior would we anticipate the plan thought experiment occur as plan would we focus our attention on those location in the universe where a simulation of the thought experiment might be occur this possibility be particularly troubling in light of the incentive our scheme create — anyone who can manipulate h s behavior can have a significant effect on the future of our world and so many may be motivate to create simulation of h a realistic u maximizer will not be able to carry out the process describe in the definition of u in fact this process probably require immensely more computing resource than be available in the universe it may even involve the reaction of a simulated human to watch a simulation of the universe to what extent can we make robust guarantee about the behavior of such an agent we have already touch on this difficulty when discuss the maxim a state of affair be valuable to the extent I would judge it valuable after a century of reflection we can not generally predict our own judgment in a hundred year time but we can have well found belief about those judgment and act on the basis of those belief we can also have belief about the value of further deliberation and can strike a balance between such deliberation and act on our current good guess a u maximizer face a similar set of problem it can not understand the exact form of u but it can still have well found belief about u and about what sort of action be good accord to u for example if we suppose that the u maximizer can carry out any reasoning that we can carry out then the u maximizer know to avoid anything which we suspect would be bad accord to u for example torture human even if the u maximizer can not carry out this reasoning as long as it can recognize that human have powerful predictive model for other human it can simply appropriate those model either by carry out reasoning inspire by human model or by simply ask moreover the community of human be simulate in our process have access to a simulation of whatever u maximizer be operate under this uncertainty and have a detailed understanding of that uncertainty this allow the community to shape their action in a way with predictable to the u maximizer consequence it be easily conceivable that our value can not be capture by a bound utility function easiest to imagine be the possibility that some state of the world be much well than other in a way that require unbounded utility function but it be also conceivable that the framework of utility maximization be fundamentally not an appropriate one for guide such an agent s action or that the notion of utility maximization hide subtlety which we do not yet appreciate we will argue that it be possible to transform bound utility maximization into an arbitrary alternative system of decision making by design a utility function which reward world in which the u maximizer replace itself with an alternative decision maker it be straightforward to design a utility function which be maximize in world where any particular u maximizer convert itself into a non u maximizer even if no simple characterization can be find for the desire act we can simply instantiate many community of human to look over a world history and decide whether or not they judge the u maximizer to have act appropriately the more complicated question be whether a realistic u maximizer can be make to convert itself into a non u maximizer give that it be logically uncertain about the nature of u it be at least conceivable that it couldn t if the desirability of some other behavior be only reveal by philosophical consideration which be too complex to ever be discover by physically limit agent then we should not expect any physically limited u maximizer to respond to those consideration of course in this case we could also not expect normal human deliberation to correctly capture our value the relevant question be whether a u maximizer could switch to a different normative framework if an ordinary investment of effort by human society reveal that a different normative framework be more appropriate if a u maximizer do not spend any time investigate this possibility than it may not be expect to act on it but to the extent that we assign a significant probability to the simulated human decide that a different normative framework be more appropriate and to the extent that the u maximizer be able to either emulate or accept our reasoning it will also assign a significant probability to this possibility unless it be able to rule it out by more sophisticated reasoning if we and the u maximizer expect the simulation to output a u which reward a switch to a different normative framework and this possibility be consider seriously then u maximization entail explore this possibility if these exploration suggest that the simulate human probably do recommend some particular alternative framework and will output a u which assign high value to world in which this framework be adopt and low value to world in which it isn t then a u maximizer will change framework such a change of framework may involve sweeping action in the world for example the u maximizer may have create many other agent which be pursue activity instrumentally useful to maximize u these agent may then need to be destroy or alter ; anticipate this possibility the u maximizer be likely to take action to ensure that its current good guess about u do not get lock in this argument suggest that a u maximizer could adopt an arbitrary alternative framework if it be feasible to conclude that human would endorse that framework upon reflection our proposal appear to be something of a cop out in that it decline to directly take a stance on any ethical issue indeed not only do we fail to specify a utility function ourselves but we expect the simulation to which we have delegate the problem to in turn delegate it at least a few more time clearly at some point this process must bottom out with actual value judgment and we may be concern that this sort of pass the buck be just obscure deep problem which will arise when the process do bottom out as observe above whatever such concern we might have can also be discover by the simulation we create if there be some fundamental difficulty which always arise when try to assign value then we certainly have not exacerbate this problem by delegation nevertheless there be at least two coherent objection one might raise both of these objection can be meet with a single response in the current world we face a broad range of difficult and often urgent problem by pass the buck the first time we delegate resolution of ethical challenge to a civilization which do not have to deal with some of these difficulty in particular it face no urgent existential threat this allow we to divert as much energy as possible to deal with practical problem today while still capture most of the benefit of nearly arbitrarily extensive ethical deliberation this process be define in term of the behavior of unthinkably many hypothetical brain emulation it be conceivable that the moral status of these emulation may be significant we must make a distinction between two possible source of moral value it could be the case that a u maximizer carry out simulation on physical hardware in order to well understand u and these simulation have moral value or it could be the case that the hypothetical emulation themselves have moral value in the first case we can remark that the moral value of such simulation be itself incorporate into the definition of u therefore a u maximizer will be sensitive to the possible suffering of simulation it run while try to learn about u as long as it believe that we may might be concern about the simulation welfare upon reflection it can rely as much as possible on approach which do not involve run simulation which deprive simulation of the first person experience of discomfort or which estimate outcome by run simulation in more pleasant circumstance if the u maximizer be able to foresee that we will consider certain sacrifice in simulation welfare worthwhile then it will make those sacrifice in general in the same way that we can argue that estimate of u reflect our value over state of affair we can argue that estimate of u reflect our value over process for learn about u in the second case a u maximizer in our world may have little ability to influence the welfare of hypothetical simulation invoke in the definition of u however the possible disvalue of these simulation experience be probably seriously diminish in general the moral value of such hypothetical simulation experience be somewhat dubious if we simply write down the definition of u these simulation seem to have no more reality than story book character whose activity we describe the good argument for their moral relevance come from the great causal significance of their decision if the action of a powerful u maximizer depend on its belief about what a particular simulation would do in a particular situation include for example that simulation s awareness of discomfort or fear or confusion at the absurdity of the hypothetical situation in which they find themselves then it may be the case that those emotional response be grant moral significance however although we may define astronomical number of hypothetical simulation the detailed emotional response of very view of these simulation will play an important role in the definition of u moreover for the most part the existence of the hypothetical simulation we define be extremely well control by those simulation themselves and may be expect to be count as unusually happy by the light of the simulation themselves the early simulation who have less such control be create from an individual who have provide consent and be select to find such situation particularly non distress finally we observe that u can exert control over the experience of even hypothetical simulation if the early simulation would experience morally relevant suffering because of their causal significance but the later simulation they generate robustly disvalue this suffer the later simulation can simulate each other and ensure that they all take the same action eliminate the causal significance of the early simulation originally publish at ordinaryideas wordpress com on april 21 2012 from a quick cheer to a stand ovation clap to show how much you enjoy this story openai align ai system with human interest
Robbie Tilton,3,15,https://medium.com/@robbietilton/emotional-computing-with-ai-3513884055fa?source=tag_archive---------1----------------,emotional computing robbie tilton medium,investigate the human to computer relationship through reverse engineer the ture test human be get close to create a computer with the ability to feel and think although the process of the human brain be at large unknown computer scientist have be work to simulate the human capacity to feel and understand emotion this paper explore what it mean to live in an age where computer can have emotional depth and what this mean for the future of human to computer interaction in an experiment between a human and a human disguise as a computer the ture test be reverse engineer in order to understand the role computer will play as they become more adept to the process of the human mind implication for this study be discuss and the direction for future research suggest the computer be a gateway technology that have open up new way of creation communication and expression computer in first world country be a standard household item approximately 70 % of americans own one as of 2009 us census bereau and be utilize as a tool to achieve a diverse range of goal as this product continue to become more globalize transistor be become small processor be become fast hard drive be hold information in new networked pattern and human be adapt to the method of interaction expect of machine at the same time with more powerful computer and quick mean of communication — many researcher be explore how a computer can serve as a tool to simulate the brain cognition if a computer be able to achieve the same intellectual and emotional property as the human brain — we could potentially understand how we ourselves think and feel coin by mit the term affective computing relate to computation of emotion or the affective phenomenon and be a study that break down complex process of the brain relate they to machine like activity marvin minsky rosalind picard clifford nass and scott brave — along with many other — have contribute to this field and what it would mean to have a computer that could fully understand its user in their research it be very clear that human have the capacity to associate human emotion and personality trait with a machine nass and brave 2005 but can a human ever truly treat machine as a person in this paper we will uncover what it mean for human to interact with machine of great intelligence and attempt to predict the future of human to computer interaction the human to computer relationship be continuously evolve and be dependent on the software interface user interact with with regard to current wide scale interface — osx window linux ios and android — the tool and ability that a computer provide remain to be the central focus of computational advancement for commercial purpose this relationship to software be drive by utilitarian need and human do not expect emotional comprehension or intellectually equivalent thought in their household device as face track eye tracking speech recognition and kinetic recognition be advance in their experimental laboratory it be anticipate that these technology will eventually make their way to the mainstream market to provide a new relationship to what a computer can understand about its user and how a user can interact with a computer this paper be not about if a computer will have the ability to feel and love its user but ask the question — to what capacity will human be able to reciprocate feeling to a machine how do intelligence quotient iq differ from emotional quotient eq an iq be a representational relationship of intelligence that measure cognitive ability like learn understanding and deal with new situation an eq be a method of measure emotional intelligence and the ability to both use emotion and cognitive skill cherry advance in computer iq have be astonishing and have prove that machine be capable of answer difficult question accurately be able to hold a conversation with human like understanding and allow for emotional connection between a human and machine the turing test in particular have show the machine ability to think and even fool a person into believe that it be a human ture test explain in detail in section 4 machine like deep blue watson eliza svetlana cleverbot and many more — have all expand the perception of what a computer be and can be if an increase computational iq can allow a human to computer relationship to feel more like a human to human interaction what would the advancement of computational eq bring we peter robinson a professor at the university of cambridge state that if a computer understand its user feeling that it can then respond with an interaction that be more intuitive for its user robinson in essence eq advocate feel that it can facilitate a more natural interaction process where collaboration can occur with a computer in alan ture s computing machinery and intelligence ture 1950 a variant on the classic british parlor imitation game be propose the original game revolve around three player a man a a woman b and an interrogator © the interrogator stay in a room apart from a and b and only can communicate to the participant through text base communication a typewriter or instant messenger style interface when the game begin one contestant a or b be ask to pretend to be the opposite gender and to try and convince the interrogator © of this at the same time the oppose participant be give full knowledge that the other contestant be try to fool the interrogator with alan ture s computational background he take this imitation game one step far by replace one of the participant a or b with a machine — thus make the investigator try and depict if he she be speak to a human or machine in 1950 turing propose that by 2000 the average interrogator would not have more than a 70 percent chance of make the right identification after five minute of question the ture test be first pass in 1966 with eliza by joseph weizenbaum a chat robot program to act like a rogerian psychotherapist weizenbaum 1966 in 1972 kenneth colby create a similar bot call parry that incorporate more personality than eliza and be program to act like a paranoid schizophrenic bowden 2006 since these initial victory for the test the 21st century have prove to continue to provide machine with more human like quality and trait that have make people fall in love with they convince they of be human and have human like reasoning brian christian the author of the most human human argue that the problem with design artificial intelligence with great ability be that even though these machine be capable of learn and speak that they have no self they be mere accumulation of identity and thought that be foreign to the machine and have no central identity of their own he also argue that people be begin to idealize the machine and admire machine capabilitie more than their fellow human — in essence — he argue human be evolve to become more like machine with less of a notion of self christian 2011 ture state we like to believe that man be in some subtle way superior to the rest of creation and it be likely to be quite strong in intellectual people since they value the power of think more highly than other and be more inclined to base their belief in the superiority of man on this power if this be true will human idealize the future of the machine for its intelligence or will they remain an inferior being as an object of our creation reverse the ture test allow we to understand how human will treat machine when machine provide an equivalent emotional and intellectual capacity this also hit directly on jefferson lister s quote not until a machine can write a sonnet or compose a concerto because of thought and emotion feel and not by the chance fall of symbol could we agree that machine equal brain that be not only write it but know that it have write it participant be give a chat room simulation between two participant a a human interrogator and b a human disguise as a computer in this simulation a and b be both place in different room to avoid influence and communicate through a text base interface a be inform that b be an advanced computer chat bot with the capacity to feel understand learn and speak like a human b be inform to be his or herself text base communication be choose to follow ture s argument that a computer voice should not help an interrogator determine if it s a human or computer pairing of participant be choose to participate in the interaction one at a time to avoid influence from other participant each experiment be five minute in length to replicate ture s time restraint twenty eight graduate student be recruit from the nyu interactive telecommunication program to participate in the study — 50 % male and 50 % female the experiment be evenly distribute across man and woman after be recruit in person participant be direct to a website that give instruction and run the experiment upon enter the website a participant be tell that we be in the process of evaluate an advanced cloud base computing system that have the capacity to feel emotion understand learn and converse like a human b participant be instruct that they would be communicate with another person through text and to be themselves they be also tell that participant a think they be a computer but that they shouldn t act like a computer or pretend to be one in any way this allow a to explicitly understand that they be talk to a computer while b know a perspective and explicitly be not go to play the role of a computer participant be then direct to communicate with the bot or human freely without restriction after five minute of conversation the participant be ask to stop and then fill out a questionnaire participant be ask to rate iq and eq of the person they be converse with a participant perceive the following of b iq 0 % — not good 0 % — barely acceptable 21 4 % — okay 50 % — great 28 6 % excellent iq average rating 81 4 % eq 0 % — not good 7 1 % — barely acceptable 50 % — okay 14 3 % — great 28 6 % — excellent eq average rating 72 8 % ability to hold a conversation 0 % — not good 0 % — barely acceptable 28 6 % — okay 35 7 % — great 35 7 % — excellent ability to hold a conversation average 81 4 % b participant perceive the following of a iq 0 % — not good 21 4 % — barely acceptable 35 7 % — okay 28 6 % — great 14 3 % excellent iq average rating 67 % eq 7 1 % — not good 14 3 % — barely acceptable 28 6 % — okay 35 7 % — great 14 3 % — excellent eq average rating 67 % ability to hold a conversation 7 1 % — not good 28 6 % — barely acceptable 35 7 % — okay 0 % — great 28 6 % — excellent ability to hold a conversation average 62 8 % overall a participant give the perceive chabot high rating than b participant give a in particular the high rating be in regard to the chat bot s iq this data state that people view the chat bot to be more intellectually competent it also imply that people talk with bot decrease their iq eq and conversation ability when communicate with computer a participant be allow to decide their username within the chat system to well reflect how they want to portray themselves to the machine b participant be designate the gender neutral name bot in an attempt to ganger gender perception for the machine the male to female ratio be divide evenly with all participant 50 % be male and 50 % be female a participant 50 % of the time think b be a male 7 1 % a female and 42 9 % gender neutral on the other hand b participant 28 6 % of the time think a be a male 57 1 % a female and 14 3 % gender neutral the username a chose be as follow hihi inessah somade3 willzing jihyun g ann divagrrl93 thisdoug jono minion10 p 123 itslynnburke from these result it be clear that people associate the male gender and gender neutrality with machine it also demonstrate that people modify their identity when speak with machine b participant be ask if they would like to pursue a friendship with the person they chat with 50 % of participant respond affirmatively that they would indeed like to pursue a friendship while 50 % say maybe or no one response state I would like to continue the conversation but I don t think I would be entice to pursue a friendship another respond maybe I like people who be intellectually curious but I worry that the person might be a bit of a smart ass overall the participant disguise as a machine may or may not pursue a friendship after five minute of text base conversation b participant be also ask if they feel a care about their feeling 21 4 % state that a indeed do care about their feeling 21 4 % state that they weren t sure if a care about their feeling and 57 2 % state that a do not care about their feeling these result indicate a user s lack of attention to b s emotional state a participant be ask what they feel could be improve about the b participant the follow improvement be note should be funny give it a well sense of humor it can be well if he know about my friend or preference the response be inconsistent and too slow it should share more about itself your algorithm be prime prude just like that letdown siri well I guess I like it well but it should be more engaged and human consistency not after the first cold prompt it push I on too many question I feel that it give up on answering and the response time be a bit slow outsource the chatbot to fluent english speaker elsewhere and pretend they be bot — if the response be this slow to this many inquiry then it should be about the same experience I be very impressed with its parse ability so far not as much with its reasoning I think some parameter for the conversation would help like ask a question maybe make the response fast I be confuse at first because I ask a question wait a bit then ask another question wait and then get a response from the bot the response from this indicate that even if a computer be a human that its user may not necessarily be fully satisfied with its performance the response imply that each user would like the machine to accommodate his or her need in order to cause less personality and cognitive friction with several participant comment incorporate response time it also indicate people expect machine to have consistent response time human clearly vary in speed when listen thinking and respond but it be expect of machine to act in a rhythmic fashion it also suggest that there be an expectation that a machine will answer all question ask and will not ask its user more question than perceive necessary a participant be ask if they feel b s artificial intelligence could improve their relationship to computer if integrate in their daily product 57 1 % of participant respond affirmatively that they feel this could improve their relationship well I think I prefer talk to a person well but yes for ipod smart phone etc would be very handy for everyday use product yes especially iphone be always with I so it can track my daily behavior that make the algorithm smarter possibly I should have query it for information that would have be more relevant to I absolutely yes the 42 9 % which respond negatively have doubt that it would be necessary or desirable not sure it might creep I out if it be I like siri as much as the next gal but honestly we re approach the uncanny valley now its not clear to I why this type of relationship need to improve I think human relationship still need a lot of work nope I still prefer flesh sack no the finding of the paper be relevant to the future of affective computation whether a super computer with a human like iq and eq can improve the human to computer interaction the uncertainty of computational equivalency that turing bring forth be indeed an interesting starting point to understand what we want out of the future of computer the response from the experiment affirm gender perception of machine and show how we display ourselves to machine it seem that we limit our intelligence limit our emotion and obscure our identity when communicate to a machine this lead we to question if we would want to give our true self to a computer if it doesn t have a self of its own it also could indicate that people censor themselves for machine because they lack a similarity that bond human to human or that there s a stigma associate with place information in a digital device the inverse relationship be also show through the datum that people perceive a bot iq eq and discussion ability to be high even though the chat bot be indeed a human this data can imply human perceive bot to not have restriction and to be competent at certain procedure the result also imply that human aren t really sure what they want out of artificial intelligence in the future and that we be not certain that an affective computer would even enjoy a user company and or conversation the result also state that we currently think of computer as a very personal device that should be passive not active but reactive when interact with it suggest a consistent reliability we expect upon machine and that we expect to take more information from a machine than it take from we a major limitation of this experiment be the sample size and sample diversity the sample size of twenty eight student be too small to fully understand and gather a stable result set it be also only conduct with nyu interactive telecommunication student who all have extensive experience with computer and technology to get a more accurate assessment of emotion a more diverse sample range need to be take five minute be a short amount of time to create an emotional connection or friendship to stay true to the ture test limitation this be enforce but further relational understanding could be understand if more time be grant beside the visual interface of the chat window it would be important to show the emotion of participant b through a virtual avatar not have this visual feedback could have limit emotional resonance with participant a time be also a limitation people aren t use to speak to inquisitive machine yet and even through a familiar interface a chat room many participant haven t hold conversation with machine previously perhaps if chat bot become more active conversational participant in commercial application user will feel less censor to give themselves to the conversation in addition to the refinement note in the limitation describe above there be several other experiment for possible future study for example investigate a long term human to bot relationship this would provide a well understanding toward the emotion a human can share with a machine and how a machine can reciprocate these emotion it would also well allow computer scientist to understand what really create a significant relationship when physical limitation be present future study should attempt to push these result far by understand how a large sample react to a computer algorithm with high intellectual and emotional understanding it should also attempt to understand the boundary of emotional computing and what be ideal for the user and what be ideal for the machine without compromise either party capacity this paper demonstrate the diverse range of emotion that people can feel for affective computation and indicate that we be not in a time where computational equivalency be fully desire or accept positive reaction indicate that there be optimism for more adept artificial intelligence and that there be interest in the field for commercial use it also provide insight that human limit themselves when communicate with machine and that inversely machine don t limit themselves when communicate with human books & articlesbowden m 2006 mind as machine a history of cognitive science oxford university press christian b 2011 the most human human marvin m 2006 the emotion machine commonsense think artificial intelligence and the future of the human mind simon & schuster paperback nass c brave s 2005 wire for speech how voice activate and advance the human computer relationship mit press nass c brave s 2005 hutchinson k computer that care investigate the effect of orientation of emotion exhibit by an embody computer agent human computer study 161 178 elsevi picard r 1997 affective computing mit press searle j 1980 mind brain and program cambridge university press 417 457 ture a 1950 computing machinery and intelligence mind stor 59 433 460 wilson r keil f 2001 the mit encyclopedia of the cognitive science mit press weizenbaum j 1966 eliza — a computer program for the study of natural language communication between man and machine communication of the acm 36 45 website cherry k what be emotional intelligence http psychology about com od personalitydevelopment a emotionalintell htm epstein r 2006 clever bot radio lab http www radiolab org 2011 may 31 clever bot ibm 1977 deep blue ibm http www research ibm com deepblue ibm 2011 watson ibm http www 03 ibm com innovation us watson index html leavitt d 2011 I take the ture test new york times http www nytimes com 2011 03 20 book review book review the most human human by brian christian html personal robotic group 2008 nexi mit http robotic medium mit edu robinson p the emotional computer camrbidge idea http www cam ac uk research news the emotional computer us census bereau 2009 household with a computer and internet use 1984 to 2009 http www census gov hhe computer 1960 s eliza mit http www manifestation com neurotoy eliza php3 from a quick cheer to a stand ovation clap to show how much you enjoy this story
Wildcat2030,5,5,https://becominghuman.ai/becoming-a-cyborg-should-be-taken-gently-of-modern-bio-paleo-machines-cyborgology-b6c65436e416?source=tag_archive---------3----------------,become a cyborg should be take gently of modern bio paleo machine — cyborgology,we be on the edge of a paleolithic machine intelligence world a world oscillate between that which be already historical and that which be barely recognizable some of we teeter on this bio electronic borderline have this ghostly sensation that a new horizon be on the verge of be reveal still misty yet glow with some inner light eerie but compelling the metaphor I use for bridge seemingly contrast on first sight paradoxical between such a futuristic concept as machine intelligence and the paleolithic age be apt I think for though advance in computation with fractional ai appear almost everywhere be become nearly casual the truth of the matter be that machine be still tribal and disperse it be a dawn all right but a dawn be still only a hint of the day that be about to shine a dawn of hyperconnecte machine interweave with biological organism cybernetically info relate and semi independent the modern paleo machine do not recognize border ; do not concern themselves with value and morality and do not philosophize about the meaning of it all not yet that be as in our own paleo past the need of the machine do not yet contain passion for individuation desire for emotional recognition or indeed feeling of dismay or despair uncontrollable urge or dream of far world also this will change eventually but not yet the paleo machinic world be in its experimentation stage probe it boundary survey the landscape of the infoverse mapping the hyperconnected situation chart a trajectory for its own evolution all this unconsciously we the biological part of the machine be provide the tool for its uplift we embed camera everywhere so it can see we implant sensor all over the planet so it may feel but above all we nudge and we push towards a great connectivity all this unaware together we form a weird cohabitation of biomechanical electro organic planetary os that be change its environment no more human not mechanical but a combine interactive intelligence that journey on oblivious to its past blind to its future irreverent to the moment of its conception already lose to its parenthood agreement and yet it evolve unconscious on the machine part unaware on the biological part the almost sentient operating system of the global planetary infosphere be emerge wild eyed complex in its arrangement of co existence it reach to comprehend its unexpected growth the quid pro quo we give the machine the platform to evolve ; the machine in turn give we advantage of fitness and manipulation we give the machine a space to turn our dream into reality ; the machine in turn serve our need and acquire sapience in the process in this hypercomplex state of affair there be no judgment and no inherent morality ; there be motion inevitable inexorable inescapable and mesmerize the embodiment be cybernetic though there be no pilot cyborgian and enhance we play the game not of throne but of the common connect and network the machine follow in our footstep catalyze our universality provide for we in turn a meaning we can not yet understand or realize the hybridization process be in full swing reach to cohere tribe of machine with tribe of human each provide for the other a non design direction for which neither have a plan or project outcome ; both mingle and weave a reality for which there be no ontos expect no telo all this lead we to remember that only retrospectively do we recognize the move from the paleo tribe to the neolithic status we do not know that it happen then and have no control over the motion on the same token we scarcely see the motion now and have no control over its directionality there be however a small difference some will say it be insignificant I do not think it so for we be some of we to some extent at least aware of the motion and we can embed it with a meaning of our choice we can if we muster our cognitive reason our amazing skill of abstraction and simulation whisper sweet utopia into the probability process of emergence we can if we so desire passionate the operating system to beautify the process of evolution and eliminate or mitigate the danger of inchoate blind walking we can if we manage to control our own paleo urge to destroy ourselves allow the combine interactive intelligence of man and machine to shine forth into a bright future of expand subjectivity we can sing to the machine cuddle they ; caress their circuit accept their electronic flaw so they can accept our bio flaw we can merge aesthetically not with conquest but with understanding we can become wise that be the difference this time around be wise in this context imply a new form of discourse an intersubjective cross pollination of a wide array of discipline the very trans disciplinarily nature of the process of cyborgization inform the discourse of subjectivity the discourse on subjectivity not unlike the move from paleo to neolithic societal structure demand of we a re assessment of the relation between man and machine for this re assessment to take place coherently the nascent re organization of the hyperconnecte machinic infosphere need be understand as a ground for the expansion of subjectivity in a sense the motion into the new hyperconnected infosphere be not unlike the move of the neolithic to domestication of plant and animal this time around however the domestication can be see as the adoption of technology for the furtherance of subjectivity into the world understand this process be difficult and far from obvious it be a perspective however that might allow we a wide context of appreciation of the current upheaval happen all around we * * * a writer futurist and a polytopian tyger a c a k a @wildcat2030 be the founder and editor of the polytopia project at space collective he also write at reality augment and urbnfutr as well as contribute to h+ magazine his passion and love for science fiction lead he to initiate the sci fi ultrashort project * * * photo credit for baby with ipad photo illumination by amanda tipton originally publish at thesocietypage org on november 22 2012 from a quick cheer to a stand ovation clap to show how much you enjoy this story futurist writer polytopia philosophy science science fiction late news info and tutorial on artificial intelligence machine learn deep learn big datum and what it mean for humanity
Greg Fish,1,4,https://worldofweirdthings.com/why-you-just-cant-black-box-an-a-i-d7c41e7d9123?source=tag_archive---------5----------------,why you just can t black box an a i weird thing,singularitarian generally believe two thing about artificial intelligence first and foremost they say it s just a matter of time before we have an ai system that will quickly become superhumanly intelligent secondly and a lot more ominously they believe that this system can sweep away humanity not because it will be evil by nature but because it win t care about human or what happen to they and one of the big priority for a researcher in the field should be figure out how to build a friendly artificial intelligence training it like one would train a pet with a mix of operant conditioning and software while the first point be one I ve cover before and point out again and again that superhuman be a very relative term and that computer be in many way already superhuman without be intelligent the second point be one that I haven t yet give a proper examination and neither have vocal singularitarian why because if you read any of the paper on their version of friendly ai yo ll soon discover how quickly they begin to describe the system they re try to tame as a black box with mostly know input and measurable output hardly a confident and lucid description of how an artificial intelligence function and ultimately what rule will govern it no problem there say the singularitarian the system will be so advanced by the time this happen that we ll be very unlikely to know exactly how it function anyway it will modify its own source code optimize how well it perform and generally be all but inscrutable to computer scientist sound great for comic book but when we re talk about real artificially intelligent system this approach sound more like surrender to robot artificial neural network and bayesian classifier to come up with whatever intelligence they want and send all the researcher and programmer out for coffee in the meantime artificial intelligence will not grow from a vacuum it will come together from system use to tackle discrete task and govern by several if not one common framework that exchange information between these system I say this because the only form of intelligence we can readily identify be find in live thing which use a brain to perform cognitive task and since brain seem to be wire this way and we re try to emulate the basic function of the brain it wouldn t be all that much of a stretch to assume that we d want to combine system good at related task and build on the accomplishment of exist system and to combine they we ll have to know how to build they conceiving of an ai in a black box be a good approach if we want to test how a particular system should react when work with the ai and focus on the system we re try to test by mock the ai s response down the chain of event think of it as dependency injection with an ai interface system but by abstract the ai away what we ve also do be make it impossible to test the inner working of the ai system no wonder then that the singularitarian fellow have to bring in operant conditioning or social training to basically housebreak the synthetic mind into do what they need it to do they have no other choice in their framework we can not simply debug the system or reset its configuration file to limit its action but why have they resign to such an odd notion and why do they assume that computer scientist be create something they win t be able to control even more bizarrely why do they think that an intelligence that can t be control by its creator could be control by a module they ll attach to the black box to regulate how nice or malevolent towards human it would be wouldn t it just find away around that module too if it s superhumanly smart wouldn t it make a lot more sense for its creator to build it to act in cooperation with human by watch what human say or do treat each reaction or command as a trigger for carry out a useful action it be train to perform and that bring we back full circle to train machine to do something we have to lay out a neural network and some high level logic to coordinate what the network output mean we ll need to confirm that the training be successful before we employ it for any specific task therefore we ll know how it learn what it learn and how it make its decision because all machine work on propositional logic and hence would make the same choice or set of choice at any give time if it didn t we wouldn t use it so of what use be a black box ai here when we can just lay out the logical diagram and figure out how it s make decision and how we alter its cognitive process if need be again we could isolate the component and mock their behavior to test how individual sub system function on their own eliminate the dependency for each set of test beyond that this block box be either a hindrance to a researcher or a vehicle for someone who doesn t know how to build a synthetic mind but really really want to talk about what he imagine it will be like and how to harness its raw cognitive power and that s ok really but let s not pretend that we know that an artificial intelligence beyond its creator understanding will suddenly emerge form the digital aether when the odd of that be similar to my toaster come to life and bark at I when it think I want to feed it some bread from a quick cheer to a stand ovation clap to show how much you enjoy this story techie rantt staff writer and editor computer lobotomist science tech and other oddity
Greg Fish,2,3,https://worldofweirdthings.com/why-do-we-want-to-build-a-fully-fledged-a-g-i-1658afc3f758?source=tag_archive---------6----------------,why do we want to build a fully fledge a g I weird thing,undoubtedly the most ambitious idea in the world of artificial intelligence be create an entity comparable to a human in cognitive ability the so call agi we could debate how it may come about whether it will want to be your friend or not whether it will settle the metaphysical question of whet make human who they be or open new door in the discussion but for a second let s think like software architect and ask the question we should always tackle first before design anything why would we want to build it what will we gain a sapient friend or partner we don t know that will we figure out what make human tick maybe maybe not since what work in the propositional logic of artificial neural network doesn t necessarily always apply to an organic human brain will we settle the question of how an intellect emerge not really since we would only be provide one example and a fairly controversial one at that and what exactly will a g in agi entail will we need to embody it for it to work and if not how would we develop the intellectual capacity of an entity extant in only abstract space will we have anything in common with it and could we understand what it want and there s more to it than that even though I just ask some fairly heavy question be we to build an agi not by accident but by design we would effectively be make the choice to experiment on a sapient entity and that s something that may have to be clear by an ethic committee otherwise we re implicitly say that an artificial cognitive entity have no right to self determination and that may be fine if it doesn t really care about they but what if it do what if the drive for freedom evolve from a cognitive routine mean for self defense and self perpetuation if we steer an ai model away from sapience by design be we in effect snuff out an opportunity or protect ourselves we can always suspend the model debug it and see what s go on in its mind but again the ethical consideration will play a significant part and very importantly while we will get to know what such an agi think and how we may not know how it will first emerge the whole agi concept be a very ambiguous effort at define intelligence and hence doesn t give we enough to objectively determine an intelligent artificial entity when we make one because we can always find an argument for and against how to interpret the result of an experiment mean to design one we barely even know where to start now I could see major advantage to fuse with machine and become cyborgs in the near future as we d swap irreparably damage part and piece for 3d print titanium tungsten carbide and carbon nanotube to overcome crippling injury or treat an otherwise terminal disease I could also see a huge upside to have direct interface to the machine around we to speed up our work and make life more convenient but when it come to such an abstract and all consume technological experiment as agi the benefit seem to be very very nebulous at good and the investment necessary seem extremely uncertain to pay off since we can t even define what will make our agi a true agi rather than another example of a large expert system whereas with wetware and expert system we can measure our return on investment with life save or significant gain in efficiency how do we justify create another intelligent entity after many decade of work especially if it turn out that we actually can t make one or it turn out to be completely different than what we hope it would be as it near completion but maybe I m wrong maybe there s a benefit to an agi that I m overlook and if that be the case enlighten I in the comment because this be a serious question why peruse an agi from a quick cheer to a stand ovation clap to show how much you enjoy this story techie rantt staff writer and editor computer lobotomist science tech and other oddity
James Faghmous ,187,6,https://medium.com/@nomadic_mind/new-to-machine-learning-avoid-these-three-mistakes-73258b3848a4?source=tag_archive---------0----------------,new to machine learning avoid these three mistake,machine learning ml be one of the hot field in data science as soon as ml enter the mainstream through amazon netflix and facebook people have be giddy about what they can learn from their datum however modern machine learn I e not the theoretical statistical learning that emerge in the 70 be very much an evolve field and despite its many success we be still learn what exactly can ml do for datum practitioner I give a talk on this topic early this fall at northwestern university and I want to share these cautionary tale with a wide audience machine learning be a field of computer science where algorithm improve their performance at a certain task as more datum be observe to do so algorithm select a hypothesis that good explain the datum at hand with the hope that the hypothesis would generalize to future unseen datum take the left panel in the figure in the header the crosse denote the observe datum project in a two dimensional space — in this case house price and their corresponding size in square meter the blue line be the algorithm s good hypothesis to explain the observed datum it state there be a linear relationship between the price and size of a house as the house s size increase so do its price in linear increment now use this hypothesis I can predict the price of an unseen datapoint base on its size as the dimension of the datum increase the hypothesis that explain the datum become more complex however give that we be use a finite sample of observation to learn our hypothesis find an adequate hypothesis that generalize to unseen datum be nontrivial there be three major pitfall one can fall into that will prevent you from have a generalizable model and hence the conclusion of your hypothesis will be in doubt occam s razor be a principle attribute to william of occam a 14th century philosopher occam s razor advocate for choose the simple hypothesis that explain your datum yet no simple while this notion be simple and elegant it be often misunderstood to mean that we must select the simple hypothesis possible regardless of performance in their 2008 paper in nature johan nyberg and colleague use a 4 level artificial neural network to predict seasonal hurricane count use two or three environmental variable the author report stellar accuracy in predict seasonal north atlantic hurricane count however their model violate occam s razor and most certainly doesn t generalize to unseen datum the razor be violate when the hypothesis or model select to describe the relationship between environmental datum and seasonal hurricane count be generate use a four layer neural network a four layer neural network can model virtually any function no matter how complex and could fit a small dataset very well but fail to generalize to unseen datum the rightmost panel in the top figure show such incident the hypothesis select by the algorithm the blue curve to explain the data be so complex that it fit through every single data point that be for any give house size in the training datum I can give you with pinpoint accuracy the price it would sell for it doesn t take much to observe that even a human couldn t be that accurate we could give you a very close estimate of the price but to predict the selling price of a house within a few dollar every single time be impossible the pitfall of select too complex a hypothesis be know as overfitte think of overfitte as memorizing as oppose to learn if you be a child and you be memorize how to add number you may memorize the sum of any pair of integer between 0 and 10 however when ask to calculate 11 + 12 you will be unable to because you have never see 11 or 12 and therefore couldn t memorize their sum that s what happen to an overfitte model it get too lazy to learn the general principle that explain the datum and instead memorize the datum data leakage occur when the datum you be use to learn a hypothesis happen to have the information you be try to predict the most basic form of data leakage would be to use the same datum that we want to predict as input to our model e g use the price of a house to predict the price of the same house however most often data leakage occur subtly and inadvertently for example one may wish to learn for anomaly as oppose to raw datum that be a deviation from a long term mean however many fail to remove the test datum before compute the anomaly and hence the anomaly carry some information about the datum you want to predict since they influence the mean and standard deviation before be remove the be several way to avoid data leakage as outline by claudia perlich in her great paper on the subject however there be no silver bullet — sometimes you may inherit a corrupt dataset without even realize it one way to spot data leakage be if you be do very poorly on unseen independent datum for example say you get a dataset from someone that span 2000 2010 but you start collect you own datum from 2011 onward if your model s performance be poor on the newly collect datum it may be a sign of data leakage you must resist the urge to retrain the model with both the potentially corrupt and new datum instate either try to identify the cause of poor performance on the new datum or well yet independently reconstruct the entire dataset as a rule of thumb your good defense be to always be mindful of the possibility of data leakage in any dataset sampling bias be the case when you shortchange your model by train it on a biased or non random dataset which result in a poorly generalizable hypothesis in the case of housing price sample bias occur if for some reason all the house price size you collect be of huge mansion however when it be time to test your model and the first price you need to predict be that of a 2 bedroom apartment you couldn t predict it sample bias happen very frequently mainly because as human we be notorious for be bias nonrandom sampler one of the most common example of this bias happen in startup and invest if you attend any business school course they will use all these case study of how to build a successful company such case study actually depict the anomaly and not the norm as most company fail — for every apple that become a success there be 1000 other startup that die try so to build an automate data drive investment strategy you would need sample from both successful and unsuccessful company the figure above figure 13 be a concrete example of sample bias say you want to predict whether a tornado be go to originate at certain location base on two environmental condition wind shear and convective available potential energy cape we don t have to worry about what these variable actually mean but figure 13 show the wind shear and cape associate with 242 tornado case we can fit a model to these datum but it will certainly not generalize because we fail to include shear and cape value when tornado do not occur in order for our model to separate between positive tornado and negative no tornado event we must train it use both population there you have it be mindful of these limitation do not guarantee that your ml algorithm will solve all your problem but it certainly reduce the risk of be disappoint when your model doesn t generalize to unseen datum now go on young jedi train your model you must from a quick cheer to a stand ovation clap to show how much you enjoy this story @nomadic_mind sometimes the difference between success and failure be the same as between = and = = living be in the detail
Datafiniti,3,5,https://blog.datafiniti.co/classifying-websites-with-neural-networks-39123a464055?source=tag_archive---------1----------------,classify website with neural network knowledge from datum the datafiniti blog,at datafiniti we have a strong need for convert unstructured web content into structured datum for example we d like to find a page like and do the following both of these be hard thing for a computer to do in an automate manner while it s easy for you or I to realize that the above web page be sell some jean a computer would have a hard time make the distinction from the above page from either of the follow web page or both of these page share many similarity to the actual product page but also have many key difference the real challenge though be that if we look at the entire set of possible web page those similarity and difference become somewhat blurred which mean hard and fast rule for classification will fail often in fact we can t even rely on just look at the underlie html since there be huge variation in how product page be lay out in html while we could try and develop a complicated set of rule to account for all the condition that perfectly identify a product page do so would be extremely time consume and frankly incredibly boring work instead we can try use a classical technique out of the artificial intelligence handbook neural network here s a quick primer on neural network let s say we want to know whether any particular mushroom be poisonous or not we re not entirely sure what determine this but we do have a record of mushroom with their diameter and height along with which of these mushroom be poisonous to eat for sure in order to see if we could use diameter and height to determine poisonous ness we could set up the follow equation a * diameter + b * height = 0 or 1 for not poisonous poisonous we would then try various combination of a and b for all possible diameter and height until we find a combination that correctly determine poisonous ness for as many mushroom as possible neural network provide a structure for use the output of one set of input datum to adjust a and b to the most likely good value for the next set of input datum by constantly adjust a and b this way we can quickly get to the good possible value for they in order to introduce more complex relationship in our datum we can introduce hidden layer in this model which would end up look something like for a more detailed explanation of neural network you can check out the follow link in our product page classifier algorithm we setup a neural network with 1 input layer with 27 node 1 hide layer with 25 node and 1 output layer with 3 output node our input layer model several feature include our output layer have the follow our algorithm for the neural network take the follow step the ultimate output be two set of input layer t1 and t2 that we can use in a matrix equation to predict page type for any give web page this work like so so how do we do in order to determine how successful we be in our prediction we need to determine how to measure success in general we want to measure how many true positive tp result as compare to false positive fp and false negative fn conventional measurement for these be our implementation have the follow result these score be just over our training set of course the actual score on real life datum may be a bit low but not by much this be pretty good we should have an algorithm on our hand that can accurately classify product page about 90 % of the time of course identify product page isn t enough we also want to pull out the actual structured datum in particular we re interested in product name price and any unique identifier e g upc ean & isbn this information would help we fill out our product search we don t actually use neural network for do this neural network be well suit toward classification problem and extract datum from a web page be a different type of problem instead we use a variety of heuristic specific to each attribute we re try to extract for example for product name we look at the < h1 > and < h2 > tag and use a few metric to determine the good choice we ve be able to achieve around a 80 % accuracy here we may go into the actual metric and methodology for develop they in a separate post we feel pretty good about our ability to classify and extract product datum the extraction part could be well but it s steadily be improve in the meantime we re also work on classify other type of page such as business datum company team page event datum and more as we roll out these classifier and datum extractor we re include each one in our crawl of the entire internet this mean that we can scan the entire internet and pull out any available datum that exist out there exciting stuff you can connect with we and learn more about our business people product and property api and dataset by select one of the option below from a quick cheer to a stand ovation clap to show how much you enjoy this story instant access to web datum build the world s large database of web datum — follow our journey
Theo,3,4,https://becominghuman.ai/is-there-a-future-for-innovation-18b4d5ab168f?source=tag_archive---------4----------------,be there a future for innovation become human artificial intelligence magazine,have you notice how tech savvy child have become but be no long streetwise I read a friend s thought on his own site last week and there be a slight pang of regret in where technology and innovation seem to be lead we all and so I start to worry about where the concept of innovation be go for future generation there s an increase reliance on technology for the sake of convenience child be become self reliant too quickly but gadget be replace people as the mentor the human bonding of parenthood be a prime example of where it s take a toll I ve see parent hand over idevice to pacify a child numerous time now the lullaby and bedtime reading session have be replace with cut the rope and automate storybook app I know a child who have develop speech difficulty because he s be bring up on cable tv and a ds lite pronounce word as he have hear they from a tiny speaker and not by watch how his parent pronounce they and I start to worry about how the concept of innovation be be redefine for future generation I use my imagination constantly as a child and it s still as active now as it be then but I didn t use technology to spoon feed I the next generation expect innovation to happen at their fingertip with little to no real stimulus steve job say stay hungry stay foolish and he be right innovation come from a keenness it s a starvation and hunger that drive people forward to spark and create it come from grab what little there be from the ether and turn it into something spectacular it s the big bang of human thought creation and I start to worry about what the concept of innovation mean for future generation technology be take away the power to think for ourselves and from our child everything must be there and in real time for instant consumption it s junk food for the mind and we re get fat on it and that breed lazy innovation we ve become satiate before we reach the point of real creativity nobody want to bother take the time to put it all together themselves any more it have to be ready for we and we re happy to throw it away if it doesn t work first time use it or lose it there s less sweat and toil involve if we don t persevere with failure remember see the human race depict in wall e that s where innovation be head and because of this we risk so many thing disappear for the sake of convenience we re all guilty of it I m guilty of it I be ask once what would become absurd in ten year think about it I realize we re on the cusp of put book on the endanger species list real book book bind in hard and paperback not digital copy from a kindle store and that scare I because the next generation of kid may grow up never see one or experience sit with their father as he read an old batter copy of the hobbit because he ll be sit there hand over an ipad with the hobbit read along app teed up and it ll be an actor voice not his father s voice pretend to be a bunch of troll about to eat a company of dwarfs innovation be a magical crazy concept it stem from a combination of crazy imagination human interaction and creativity not convenient manufacture technology can aid collaboration in way we ve never experience before but it can t run crazy for we and for the sake of future generation don t let it here s to the crazy one indeed from a quick cheer to a stand ovation clap to show how much you enjoy this story founder and ceo @ rawshark studio late news info and tutorial on artificial intelligence machine learn deep learn big datum and what it mean for humanity
x.ai,1,2,https://medium.com/@xdotai/i-scheduled-1-019-meetings-in-2012-and-that-doesnt-count-reschedules-x-ai-278d7e824eb3?source=tag_archive---------5----------------,I schedule 1 019 meeting in 2012 — and that doesn t count reschedule — x ai,the number of meeting that I schedule in 2012 might seem astronomical put in context it s less so I be a startup founder at the time and that year my company visual revenue take series a funding double revenue and start discuss a possible exit I like the number though as a startup romantic one could turn it into a nifty malcolm gladwell type rule of thumb call the 1 000 meeting rule gladwell s claim that greatness require an enormous time sacrifice ring true to I — whether that mean invest 10 000 hour into a subject matter to become an expert or conduct a 1 000 meeting per year be another question more interesting though be the impact of this 1 019 figure and a related one of those more than one thousand meeting I schedule 672 be reschedule that be painful but these number be among the early piece of datum that inspire I to start x ai * a meeting be define as an event in my calendar which be marginally flawed in both direction give some event would be travel to jfk which be obviously a task and not a meeting where other would be interview sale director candidate which be really 4 meeting in 1 originally publish at x ai on october 14 2013 from a quick cheer to a stand ovation clap to show how much you enjoy this story magically schedule meeting
Arjan Haring 🔮🔨,1,5,https://medium.com/@happybandits/website-morphing-and-more-revolutions-in-marketing-8a5cabc60576?source=tag_archive---------6----------------,website morphing and more revolution in market arjan haring 🔮 🔨 medium,john r hauser be the kirin professor of marketing at m I t s sloan school of management where he teach new product development marketing management and statistical and research methodology he have serve mit as head of the mit marketing group head of the management science area research director of the center for innovation in product development and co director of the international center for research on the management of technology he be the co author of two textbook design and marketing of new product and essential of new product management and a former editor of marketing science now on the advisory board I think it wouldn t be smart to start this interview with something as dull and complex as a definition or be I the only one that like to read light weight and short article let s just get it over with website morphing match the look and feel of a website to each customer so that over a series of customer revenue or profit be maximize that didn t hurt as much as I expect I actually love the idea it sound completely logical but be we talk about a completely new idea there be tremendous variety in the way customer process and use information some prefer simple recommendation while other like to dig into the detail some customer think verbally or holistically other prefer picture and graph what be new be that we now have good algorithm to identify how customer think from the choice they make as they explore website their clickstream but once we identify the way they think we still need an automatic way to learn which website look and feel will lead to the most sale this be a very complex problem which fortunately have a relative simple solution base on fundamental research by john gittin our contribution be to combine the identification algorithm with the learning algorithm and develop an automate system that be feasible and practical once we develop the technology to morph website we be only limit by our imagination in our first application we match the look and feel to customer cognitive style in our second we match to cognitive and cultural style we then use the algorithm to morph banner advertisement to achieve almost a 100 % lift in click through rate our late project use both cognitive style and the customer search history to match the automotive banner advertising to enhance click consideration and purchase likelihood I also love the fact that you combine technology with behavioral science on the psychology side of thing you be have be busy with cognitive style cognitive switching and cognitive simplicity can you tell we a little bit more about these theory and why you choose to use they customer be smart they know when to use simple decision rule cognitive simplicity and when to use more complicated rule our research have be two fold 1 website morphing and banner morphing figure out how customer think and provide information in the format that help they think the way they prefer to think 2 we have also focus on identify consideration heuristic typically customer seriously consider only a small fraction of available product to do so they use simple rule that balance thinking and search cost with the completeness of information by understand these simple rule manager can develop well product and well marketing strategy we can now identify these decision rule quickly with machine learning method but a caveat — customer do not always use cognitively simple rule the moment of truth in a final purchase decision be well understand with more complex decision rule and method such as choice base conjoint analysis most recently we ve combine the two stream of research curiously some of the algorithm use by the computer to morph website be reasonably descriptive of how consumer take the future into account in purchase they make today prior research postulate a form of hyperrationality our research suggest that consumer be pretty smart about balance cognitive cost and foresight what be your main interest on the technology side of website morph which algorithm take your fancy and why website morphing use an index solution to learn the good morph for a customer our late effort also identify when to morph a website by embed another dynamic program within the index solution in our research to understand how consumer deal with the future we ve demonstrate that index other than gittin index might be more descriptive of consumer foresight if I think about it as a company you can either win the algorithm competition or the psychology competition or lose do you agree actually the company that will thrive be those that understand the customer cognitive process have the algorithm to match product and marketing to customer cognitive process and have the organization that accept such innovation you need all three be this what marketing will be about in 5 year there be many revolution in market it be an exciting time it s hard to list all of the change but here be a few 1 big datum we know so much more about customer than we ever do before but this knowledge be often hide within the volume of datum one challenge be to develop method that scale well to be data 2 machine learn there be some problem that human solve well than computer and some problem that computer solve well than human morph identify simple decision rule and study consumer foresight be all possible with the advent of good machine learning method but we have only scratch the surface 3 causality marketing have use quite successfully small sample laboratory experiment and assumption laden quantitative model however the advent of big datum and web base data collection have make it possible to do experiment and quasi experiment on a large scale to well establish causality and to well develop theory that be externally valid causality also mean replication there be a strong movement in the journal to require that key finding be replicate 4 the tpm movement theory + practice in marketing conference special issue and organization be now devoted to match managerial need to research with impact in fact a recent survey by the inform society of marketing science suggest that approximately 80 % of the researcher in marketing believe that research should be more focused on application 5 a mature perspective on behavioral science researcher be increasingly less focused on cute finding that apply only in special circumstance they be begin to focus on insight that have a big impact effect size and apply to decision that customer make routinely company that combine algorithm an understanding of customer decision making and the ability to use datum will be the company that succeed originally publish at www sciencerockstar com on october 21 2013 from a quick cheer to a stand ovation clap to show how much you enjoy this story let s fix the future scientific advisor @jadatascience
Arjan Haring 🔮🔨,1,5,https://medium.com/i-love-experiments/using-artificial-intelligence-to-balance-out-customer-value-a251b0ccae6f?source=tag_archive---------8----------------,use artificial intelligence to balance out customer value,december 13 it be that time again the second edition of # projectwaalhalla social science for startup this time with peter van der putten speak as data scientist he be guest researcher at the data mining group algorithm research cluster of the leiden institute of advanced computer science he be also director of decisioning solution worldwide at pegasystem there be accord to peter a lot of potential for new startup in this area be you go to be the next success story I be actually very curious what you as a lead datum scientist think of this whole big datum thingy I be fascinate by learn from datum but have mixed feeling about big datum the concept be be hype a lot at this moment while the algorithm to learn from datum have be study since the 40 in computer science many of the modern big datum technology like hadoop be in fact still limited framework for old fashioned offline batch process datum instead of real time process datum the focus should really not have to be on the datum but on the analysis — how we generate knowledge and learn from datum through data mining — and more importantly how do we operationalize this knowledge how can we use this knowledge because knowledge be not power action be and what be the role of psychology in big datum and of philosophy psychology have begin study intelligence fifty or sixty year before computer science do people animal plant and all intelligent system be basically information processing machinery psychology seek to understand these system and to try explain behavior — if you understand a bit of that system you can use this knowledge for example to teach computer stupid mathematical piece of scrap smart function such as learn and respond to customer behavior what be to say think the other away around that people don t have to think like computer see for example the psychologist daniel kahneman who win the 2002 the sverige riksbank prize in economic science the unofficial nobel prize in economic for his insight that people aren t rational agent that properly weigh all the choice before decide something and philosophy these guy have deal with big datum for more than 3 000 year now just think of the nature vs nurture debate do we acquire intelligence and other property by datum experience or be they innate or the whole philosophy of mind discussion with root in the ancient greek what do we really know and be there be only experience or just reality you have a background in artificial intelligence ai and even study with the famous and wildly attractive bas haring not relate well cousin to be honest if you insist what could ai mean for business and how be it different from big datum as long as ai be not use for old fashioned data manipulation or poor reporting but really as intelligent datum science big datum be one of the tool within learn artificial intelligence that be system that be not smart by the knowledge that be pre insert but which have the capacity to learn and combine what be learn with background knowledge to deduct decision this be what I like to call the field of decisione really intelligent system put that knowledge into action and be part of an ecosystem an environment with other actor system people and the scary outside world sound abstract until the late 90 artificial intelligence be only do in the lab now people interact with ai unconsciously on a daily basis for example if they use google check their facebook page or look at banner on the web take the company where I work next to my academic job when I come in 2002 it be a startup of only 15 man with new software and a launch customer editor s note we know that feeling ; ten year and two acquisition later we have reach more than 1 billion consumer with intelligent datum drive scientifically prove real time recommendation via digital as well as traditional channel like atms shop and callcenter no push product offering anymore but only next good action recommendation that optimize customer value by balance customer experience and predict interest and behavior what opportunity do you see for startup in the artificial intelligence in this area well I see tremendous opportunity not only for 100 % pure ai startup but for all startup if you look at the startup in silicon valley in high tech and biotech artificial intelligence be a major part of the business every startup should consider whether datum be a key asset or a barrier to entry and how ai or datum mining can be use to convert these datum into money where I have to note that customer and citizen rightly so be get more critical after all nsa issue those who can use this technology in a way that it not only benefit company but especially customer will be the most successful in conclusion I be curious about how much you you be look forward to december 13 and what should happen during # projectwaalhalla that would make your wild dream come true very much look forward to it in term of wildest dream I hear a reunion concert of the urban dance squad be not go to happen which I understand but I look forward to exchange view with startup freelancer and multinational on how to create with the help of raw datum diamand and a magical mix of data mining machine learning decisioning and evidence base and real time marketing I will bring some nice metaphorical picture and leave will double integral at home editor s note an uds reunion sound like a plan to we originally publish at www sciencerockstar com on december 6 2013 from a quick cheer to a stand ovation clap to show how much you enjoy this story let s fix the future scientific advisor @jadatascience a blog series about the discipline of business experimentation how to run and learn from experiment in different contexts be a complex matter but lay at the heart of innovation
Shivon Zilis,1.2K,10,https://medium.com/@shivon/the-current-state-of-machine-intelligence-f76c20db2fe1?source=tag_archive---------0----------------,the current state of machine intelligence shivon zilis medium,the 2016 machine intelligence landscape and post can be find here I spend the last three month learn about every artificial intelligence machine learning or datum relate startup I could find — my current list have 2 529 of they to be exact yes I should find well thing to do with my evening and weekend but until then why do this a few year ago investor and startup be chase big datum I help put together a landscape on that industry now we re see a similar explosion of company call themselves artificial intelligence machine learning or somesuch — collectively I call these machine intelligence I ll get into the definition in a second our fund bloomberg beta which be focus on the future of work have be invest in these approach I create this landscape to start to put startup into context I m a thesis orient investor and it s much easy to identify crowded area and see white space once the landscape have some sort of taxonomy what be machine intelligence anyway I mean machine intelligence as a unifying term for what other call machine learning and artificial intelligence some other have use the term before without quite describe it or understand how laden this field have be with debate over description I would have prefer to avoid a different label but when I try either artificial intelligence or machine learning both prove to too narrow when I call it artificial intelligence too many people be distract by whether certain company be true ai and when I call it machine learn many think I wasn t do justice to the more ai esque like the various flavor of deep learning people have immediately grasp machine intelligence so here we be ☺ computer be learn to think read and write they re also pick up human sensory function with the ability to see and hear arguably to touch taste and smell though those have be of a less focus machine intelligence technology cut across a vast array of problem type from classification and clustering to natural language processing and computer vision and method from support vector machine to deep belief network all of these technology be reflect on this landscape what this landscape doesn t include however important be big datum technology some have use this term interchangeably with machine learning and artificial intelligence but I want to focus on the intelligence method rather than datum storage and computation piece of the puzzle for this landscape though of course data technology enable machine intelligence which company be on the landscape I consider thousand of company so while the chart be crowd it s still a small subset of the overall ecosystem admission rate to the chart be fairly in line with those of yale or harvard and perhaps equally arbitrary ☺ I try to pick company that use machine intelligence method as a define part of their technology many of these company clearly belong in multiple area but for the sake of simplicity I try to keep company in their primary area and categorize they by the language they use to describe themselves instead of quibble over whether a company use nlp accurately in its self description if you want to get a sense for innovation at the heart of machine intelligence focus on the core technology layer some of these company have api that power other application some sell their platform directly into enterprise some be at the stage of cryptic demos and some be so stealthy that all we have be a few sentence to describe they the most exciting part for I be see how much be happen in the application space these company separate nicely into those that reinvent the enterprise industry and ourselves if I be look to build a company right now I d use this landscape to help figure out what core and support technology I could package into a novel industry application everyone like solve the sexy problem but there be an incredible amount of unsexy industry use case that have massive market opportunity and powerful enable technology that be beg to be use for creative application e g watson developer cloud alchemyapi reflection on the landscape we ve see a few great article recently outline why machine intelligence be experience a resurgence document the enable factor of this resurgence kevin kelly for example chalk it up to cheap parallel compute large dataset and well algorithm I focus on understand the ecosystem on a company by company level and draw implication from that yes it s true machine intelligence be transform the enterprise industry and human alike on a high level it s easy to understand why machine intelligence be important but it wasn t until I lay out what many of these company be actually do that I start to grok how much it be already transform everything around we as kevin kelly more provocatively put it the business plan of the next 10 000 startup be easy to forecast take x and add ai in many case you don t even need the x — machine intelligence will certainly transform exist industry but will also likely create entirely new one machine intelligence be enable application we already expect like automate assistant siri adorable robot jibo and identify people in image like the highly effective but unfortunately name deepface however it s also do the unexpected protect child from sex trafficking reduce the chemical content in the lettuce we eat help we buy shoe online that fit our foot precisely and destroy 80 s classic video game many company will be acquire I be surprised to find that over 10 % of the eligible non public company on the slide have be acquire it be in stark contrast to big datum landscape we create which have very few acquisition at the time no jaw will drop when I reveal that google be the number one acquirer though there be more than 15 different acquirer just for the company on this chart my guess be that by the end of 2015 almost another 10 % will be acquire for thought on which specific one will get snap up in the next year you ll have to twist my arm big company have a disproportionate advantage especially those that build consumer product the giant in search google baidu social network facebook linkedin pinter content netflix yahoo mobile apple and e commerce amazon be in an incredible position they have massive dataset and constant consumer interaction that enable tight feedback loop for their algorithm and these factor combine to create powerful network effect — and they have the most to gain from the low hang fruit that machine intelligence bear well in class personalization and recommendation algorithm have enable these company success it s both impressive and disconcert that facebook recommend you add the person you have a crush on in college and netflix tee up that perfect guilty pleasure sitcom now they be all compete in a new battlefield the move to mobile win mobile will require lot of machine intelligence state of the art natural language interface like apple s siri visual search like amazon s firefly and dynamic question answer technology that tell you the answer instead of provide a menu of link all of the search company be wrestle with this large enterprise company ibm and microsoft have also make incredible stride in the field though they don t have the same human facing requirement so be focus their attention more on knowledge representation task on large industry dataset like ibm watson s application to assist doctor with diagnosis the talent s in the new ai vy league in the last 20 year most of the good mind in machine intelligence especially the hardcore ai type work in academia they develop new machine intelligence method but there be few real world application that could drive business value now that real world application of more complex machine intelligence method like deep belief net and hierarchical neural network be start to solve real world problem we re see academic talent move to corporate setting facebook recruit nyu professor yann lecun and rob fergus to their ai lab google hire university of toronto s geoffrey hinton baidu woo andrew ng it s important to note that they all still give back significantly to the academic community one of lecun s lab mandate be to work on core research to give back to the community hinton spend half of his time teach ng have make machine intelligence more accessible through coursera but it be clear that a lot of the intellectual horsepower be move away from academia for aspire mind in the space these corporate lab not only offer lucrative salary and access to the godfather of the industry but the most important ingredient datum these lab offer talent access to dataset they could never get otherwise the imagenet dataset be fantastic but can t compare to what facebook google and baidu have in house as a result we ll likely see corporation become the home of many of the most important innovation in machine intelligence and recruit many of the graduate student and postdoc that would have otherwise stay in academia there will be a peace dividend big company have an inherent advantage and it s likely that the one who will win the machine intelligence race will be even more powerful than they be today however the good news for the rest of the world be that the core technology they develop will rapidly spill into other area both via depart talent and publish research similar to the big datum revolution which be spark by the release of google s bigtable and bigquery paper we will see corporation release equally groundbreake new technology into the community those innovation will be adapt to new industry and use case that the google of the world don t have the dna or desire to tackle opportunity for entrepreneur my company do deep learning for x few word will make you more popular in 2015 that be if you can credibly say they deep learning be a particularly popular method in the machine intelligence field that have be get a lot of attention google facebook and baidu have achieve excellent result with the method for vision and language base task and startup like enlitic have show promise result as well yes it will be an overused buzzword with excitement ahead of result and business model but unlike the hundred of company that say they do big datum it s much easy to cut to the chase in term of verify credibility here if you re pay attention the most exciting part about the deep learning method be that when apply with the appropriate level of care and feed it can replace some of the intuition that come from domain expertise with automatically learn feature the hope be that in many case it will allow we to fundamentally rethink what a good in class solution be as an investor who be curious about the quirki application of datum and machine intelligence I can t wait to see what creative problem deep learning practitioner try to solve I completely agree with jeff hawkins when he say a lot of the killer application of these type of technology will sneak up on we I fully intend to keep an open mind acquihire as a business model people say that datum scientist be unicorn in short supply the talent crunch in machine intelligence will make it look like we have a glut of data scientist in the data field many people have industry experience over the past decade most hardcore machine intelligence work have only be in academia we win t be able to grow this talent overnight this shortage of talent be a boon for founder who actually understand machine intelligence a lot of company in the space will get seed funding because there be early sign that the acquihire price for a machine intelligence expert be north of 5x that of a normal technical acquihire take for example deep mind where price per technical head be somewhere between $ 5 10 m if we choose to consider it in the acquihire category I ve have multiple friend ask I only semi jokingly shivon should I just round up all of my smart friend in the ai world and call it a company to be honest I m not sure what to tell they at bloomberg beta we d rather back company build for the long term but that doesn t mean this win t be a lucrative strategy for many enterprise founder a good demo be disproportionately valuable in machine intelligence I remember watch watson play jeopardy when it struggle at the beginning I feel really sad for it when it start trounce its competitor I remember cheer it on as if it be the toronto maple leafs in the stanley cup final disclaimer 1 I be an ibmer at the time so be bias towards my team 2 the maple leafs have not make the final during my lifetime — yet — so that be purely a hypothetical why do these awe inspire demos matter the last wave of technology company to ipo didn t have demos that most of we would watch so why should machine intelligence company the last wave of company be very computer like database company enterprise application and the like sure I d like to see a 10x more performant database but most people wouldn t care machine intelligence win and lose on demos because 1 the technology be very human enough to inspire shock and awe 2 business model tend to take a while to form so they need more funding for long period of time to get they there 3 they be fantastic acquisition bait watson beat the world s good human at trivium even if it think toronto be a us city deepmind blow people away by beat video game vicarious take on captcha there be a few company still in stealth that promise to impress beyond that and I can t wait to see if they get there demo or not I d love to talk to anyone use machine intelligence to change the world there s no industry too unsexy no problem too geeky I d love to be there to help so don t be shy I hope this landscape chart spark a conversation the goal to be make this a live document and I want to know if there be company or category miss I welcome feedback and would like to put together a dynamic visualization where I can add more company and dimension to the data method use datum type end user investment to date location etc so that folk can interact with it to well explore the space question and comment please email I thank you to andrew paprocki aria haghighi beau cronin ben lorica doug fulop david andrzejewski eric berlow eric jonas gary kazantsev gideon mann greg smithies heidi skinner jack clark jon lehr kurt keutzer lauren barless pete skomoroch pete warden roger magoulas sean gourley stephen purpura wes mckinney zach bogue the quid team and the bloomberg beta team for your ever helpful perspective disclaimer bloomberg beta be an investor in adatao alation aviso brightfunnel context relevant mavrx newsle orbital insight pop up archive and two other on the chart that be still undisclosed we re also investor in a few other machine intelligence company that aren t focus on area that be a fit for this landscape so we leave they off for the full resolution version of the landscape please click here from a quick cheer to a stand ovation clap to show how much you enjoy this story partner at bloomberg beta all about machine intelligence for good equal part nerd and athlete straight up canadian stereotype and proud of it
Roland Trimmel,20,6,https://medium.com/@rolandt25/will-all-musicians-become-robots-6221171c5d18?source=tag_archive---------1----------------,will all musician become robot roland trimmel medium,finally we see the rise of the machine and with it develop a certain fear that artificial intelligence ai will render human useless this question be pose at boston s a3e conference last month by a team member at landr their company have receive death threat from people in the mastering industry after have release a diy drag and drop instant online mastering service power by ai algorithms it illustrate the resistance that the world of ai have incite amongst we some fear that robot will take over à la terminator 2 some fear that the virtual and artificial will replace the visceral some cite religious view and other frankly other just seem ignorant that set the tone for our own journey into artificial intelligence and the lesson we have learn from it we have spend more than three year develop algorithm to enable software to read and interpret a composition song like an expert do come from a music and technology background our team be hugely excited have accomplish this make no mistake it s really difficult to make a computer understand music — for we this be an important first step towards a new generation of intelligent music instrument that assist the user in the songwrite process for fast completion of complex task result in no interruption of the creative flow and more creative output when you spend so many year work on a technology product you run the risk of lose sight of the market and this be our first product we have absolutely no idea what to expect to find out we have to bring the product to the attention of the target group and eagerly await their reaction that mean a lot of leg work for we in start discussion on multiple forum and collect user feedback it take time to cut through the noise but create some great thread what be interesting for we to monitor be how the discussion about our product unfold on those forum and how opinion be split between two camp one that embrace what we do and the other that be characterize by anger fear or a complete misunderstanding of what our software do at time we feel like be in the middle of the fight between machine and human we hadn t expect this our aim be to make a cool product that show what the technology be capable of do eventually we spend lot of time clearing misunderstanding explain our product well etc to win over those forum member heart for what we do and occasionally we also have to calm down a heated discussion between member insult each other cause by a fear that our product eliminate the craft in music composition today enter a different reality we have make a lot of progress with our software much of it be down to communicate openly with our community to address any question they may have early and involve they deeply in product development have the tone in discussion about our technology change yes certainly it have but please don t think it s an easy journey it s still hard to convince music producer to rely on the help of a piece of software that in some regard replicate process of the human brain the effort that go into be a pioneer and drive this perceptional battle be drive one close to insanity it s an endless stream of work and it require endurance like during marathon or triathlon here be five thing that we learn from our journey that we d like to share with you so you can judge well before dismiss ai in music let I start with a quick discussion of the first and second digital wave in music the first digital wave bring about digital music technology like synth and daw s and with that everything change sound synthesis and sample make entirely new form of expressiveness possible sequencer in combination with large database of loop clip lay the foundation for electronic dance music which lead to a multifaceted artistic and cultural revolution the second digital wave have be roll along for a few year now and it be wash up intelligent algorithm for processing audio and midi as an example ai s can already help control the finish mastering process of music track as assistant tool or even fully automate in the not too distant future — and we re talk only year from now — we will be use to incredible music make automaton control most complex harmonic figure flawlessly imitate the great artist the output quality by such algorithm be unbelievable computer intelligence can aimlessly merge style of various artist and apply they to yet another piece all that without break a sweat we regard the main application of ai s for music composition and production as helper tool not artist in their own regard and this be not cheat we have be utilize digital production tool for decade it be just a matter of time for more complicated and intelligent code to emerge but rest assure computer will rather not generate music all by themselves the art and craft of composing will prevail there will always be human being behind the actual output control by an ai it will help though to create less complex lean user interface in the tool we use for create music that be simple to operate on to the learning we promise you now definitely not the magic and final decision over creative output will always remain with the human artist a computer be not a human with feeling and emotion what make we get to our knee in awe will keep machine clinically indifferent simple as that and technical approximation as deceptive as they may get be simply not the real thing it already be there be no stop it but then that be the course of a natural evolutionary process which can only push forward a huge one this be a game changer read our statement on main application above it be our ego we cling on to have trot down the same path for decade many believe their laboriously acquire expertise be threaten by robot technology and a new ruthless generation the truth be if we embrace ai s as our help friend and maybe even learn how to think a little more technical who can fathom how ingeniously more colorful the world of music will become in the hand of talented musician of all generation yes because it enable a completely new generation of product and startup like we push for innovation the agreeable side effect it will make people happy musician consumer and businessman alike full circle most importantly though it be not only ai change the music industry social change be equally responsible for it if they don t account for a large part for it anyway here s an excellent article by fast company on this topic and more coverage on a3e in this article by techrepublic it s an interesting time for all of we in music and beyond and there s so much yet to come don t be afraid — human also prevail in terminator there be thing machine will never do they can not possess faith they can not commune with god they can not appreciate beauty they can not create art if they ever learn these thing they win t have to destroy we they ll be we sarah connor image credit daft punk top re compose middle from a quick cheer to a stand ovation clap to show how much you enjoy this story
Espen Waldal,57,6,https://medium.com/bakken-b%C3%A6ck/how-artificial-intelligence-can-improve-online-news-7a24889a6940?source=tag_archive---------2----------------,how artificial intelligence can improve online news,that be say the user experience for online news site today be very much like it be ten and fifteen year ago see the slideshow show the evolution of nyt com you enter a homepage where a carefully select combination of article on sport celebrity reality show dinner recipe and even actual news scream for your attention there s a huge focus on page view and hardly any attention give to personal relevance for the reader smart use of technology could improve the online news experience vastly by just add a bit more structure that be why we create orbit rich structured datum be the foundation for take the online news experience to the next level orbit be a collection of artificial intelligence technology api s use machine learning base content analysis to automatically transform unstructured text into rich structured datum by analyze and organize content in real time and automatically tag and structure large piece of text into cluster of topic orbit create a platform where you can build multiple datum rich application the now 5 month old leak innovation report from the new york times point to several challenge for keep and expand a digital audience to face some of the most critical issue you need to create a well experience for the reader by 1 serve up well recommendation of related content2 provide new way to discover news and add context3 introduce personalization and filtering relevance be essential to create loyal reader and even more so in a time where more and more visit to news site go directly to a specific article mainly due to search and social medium avoid the front page altogether reader arrive through side door like twitter or facebook be less engaged than reader arrive directly which mean it s important to keep these visitor on the site and convert they into loyal reader yet so little be be do to improve the relevance of recommendation and create a connection to the huge amount of valuable content that already exist orbit understand not only the topic a piece contain but also related topic it thereby understand the context of the article and can bring up relate content that the reader wouldn t otherwise have see extend the reader s time spend on the site and increase page view understand context mean that the cluster of topic relate to an article on china sign a historic gas deal with russia include topic such as russia ukraine putin gazprom and energy — thus create recommendation within that cluster and create connection between content rich structured datum open up for new way to navigate and discover news the classic navigation through carefully edit front page have pretty much be the same since the dawn of online news publishing structured datum enable the reader to follow certain topic or story improve search and enable timeline navigation of a news story to help the reader well understand the context of the story and how it have develop at the same time a journalist write a story on the uproar in ukraine have no possible way of know how the story will unfold in the week to come manual tagging of news story lead to inconsistent and incomplete structure due to a subjective understanding of which topic be important and related machine learning base content analysis can identify people organization and place and relate they to each other in real time thereby identify related story as they unfold and cluster they together as the nyt innovation report bring up the true value of structured datum emerge only when the content be structure equally throughout news and content app like circa omni and prismatic and news site like vox have incorporate some of these element and be experiment with how to develop original way to discover news there be many argument against personalization and they be often relate to the dystopian fear of a « fragment » public sphere or the horror of the echo chamber that doesn t mean personalization can t be a good thing ; it merely mean be aware of what a particular type of user want at a particular time we be not talk about a fully customizable news feed base on your subjective interest mean I will not only see article relate to manchester united finance tv show and kim kardashian and be uninformed on all other topic we be merely suggest a smart filtering system and adjustment of what subject you would like to see more and less of on your feed after all we do have different interest for example you may be entirely disintereste in tour de france during its three week medium frenzy in july each year ; unfollow topic or turn the « volume » down today get the news isn t the hard part filter out the excessive info and navigate the overwhelming stream of news in a smart way be where you need great tool a foundation of rich structured datum will not only benefit the reader but make life easy for journalist and editor as well to provide context to a story about syria you could add several component of extra information that would enrich the article a box of background information on the conflict fact about bashar al assad and the different syrian rebel group and so forth with rich structured datum in place you can automatically add relevant fact box and other interactive element to a piece of content base on third party content database such as wikipedia topic can automatically generate their own page with all the related article fact visualization and insight relevant for that specific topic cluster moreover you can use the datum to create new and compelling presentation of your content include visualization and timeline that give the reader a well experience and new insight news content generally have a short lifespan but this doesn t mean that old content can t be valuable in a new context a consistent structuring of archive content will give new life to old content make it easy to reuse and resurrect article that be still relevant and create connection between old and new article what be the trend topic people or organization this week what region get the most medium attention how many of the source be anonymous how many be woman versus man know more about your audience s preference will make it easy to create good content at the right time well organize content create a strong foundation for good insight into how content be consume and why with a well ecosystem for your content include high relevance and more contextual awareness you can present well context base ad to your advertiser and give well insight into who be watch and act on they by use the right technology in smart way journalist and editor can focus on what they be good at create quality news content orbit ai from a quick cheer to a stand ovation clap to show how much you enjoy this story product manager at bakken & bæck the bakken & bæck blog
Joe Johnston,38,4,https://medium.com/universal-mind/how-i-tracked-my-house-movements-using-ibeacons-3e1e9da3f1a9?source=tag_archive---------3----------------,how I track my house movement use ibeacon universal mind medium,recently I ve start experiment more and more with ibeacon be part of the r&d group at universal mind I ve have the opportunity to do a lot of testing and exploring of different product in do so I want to see how someone could utilize ibeacon without build your own app just yet I ll tackle this in a future post the first step be to find ibeacon we could use for our testing our first choice be order the ibeacon from estimote and after wait for they to arrive they never do we order other beacon from various company the first set to arrive be from roximity which come to we as a set of 3 dev ibeacon next I want to see if I could track movement in my own house just as a simple test without create a custom app I look for a few app that could detect the ibeacon and execute an action there be a few app capable of do this but all of they be somewhat limit the only app I find that allow I to control what happen when trigger an ibeacon be an app call launch here formally place although this app wasn t a perfect fit it do allow I to call some action after trigger an ibeacon launch here allow you to use custom url scheme these url scheme allow you to open app and even populate an action one of the more complicated task of set up any ibeacon manually be you need to gather some infomation on the ibeacon itself the 3 key piece of info each ibeacon contain be a uuid major i d and minor i d to get this info you can install an app like locate for ibeacon that detect ibeacon and show this information once you have this info you can set up your ibeacon use launch here its a bit cumbersome to set each one up but you only have to do it once as a side note the launch here app be a bit touchy when set up the ibeacon so be warn you may have to re enter the url scheme info if you fat finger it like I say before the launch here app be control by the user it trigger a lock screen notification when you turn on your phone and be less then 3 meter away from any ibeacon this be a bit interesting but it s the approch that launch here take so they could give the user a bit of control when trigger action ideally this all would happen behind the scene to the user in a custom app the custom url scheme be pretty powerful but you still need to manually trigger they here s my set up I have the tumblr app instal on my phone which have the ablity to use a post url scheme the url scheme look like this tumblr x callback url text title = kitchen once that url scheme be trigger from launch here it open tumblr and pre populate a text post with the word kitchen or with the name of the room I set I manually tap post and its add this allow I to capture each ibeacon location and store the datum the next step be to create a more data friendly format I love use a service call ifttt it s a very power platform that allow you to automatically trigger other service I create a ifttt recipe that auto add a row to a google spreadsheet with the time stamp and text that be enter into a text post to my tumblr account now I have a time stamp dataset track my movement in my house — at least the three room I have set up with that datum you can imagine how you can start to break it apart here s just an example of my current break down base on room as you can see it s possible to track your movement albeit a bit cumbersome take this datum and bubble it up to the user could be very compelling in certain situation I m just use my personal home location here but you can see how this could be very powerful in other setting I be the director of user experience research & development at universal mind — a digital solution agency you can follow I on twitter at @merhl from a quick cheer to a stand ovation clap to show how much you enjoy this story experience & service design director @sparksgrove the experience design division of @northhighland alum of @hugeinc @universalmind @startgarden a collection of article create by universal mind thinker
Nadav Gur,10,9,https://medium.com/the-vanguard/why-natural-search-is-awesome-and-how-we-got-here-fe69b9cdd0db?source=tag_archive---------5----------------,why natural search be awesome and how we get here the vanguard medium,the evolution of desti s search interface this be a story about how one ambitious start up tackle this subject that have riddle people like google apple facebook and other and come up with some pretty clever conclusion if I may say so myself in 2012 2013 we be build desti — a holistic travel search app I e it would search for everything — from hotel through attraction to restaurant use post siri natural language understanding tech and with powerful semantic search capability on the back end that allow desti to reason meaningfully about search result and make highly inform suggestion desti s search be build on a premise that sound very simple but it s actually very hard to pull off we believe that people should be able to ask specifically for what they re interested in and get result that match this sound reasonable right if I m look for a beach resort on the kona coast in hawaii it s pretty obvious what I want and if I also want it to be kid friendly and pet friendly I should just be able to ask for it our goal be to get user inputte relevant specific query because that s what people need that s where desti shine — save you time and effort by deliver exactly what you want now let s assume that desti know which hotel on the kona coast be actually beach resort be kid friendly and pet friendly how can we make express this query easy and intuitive for the user episode I desti be siri s sister or conversational user interface when we start we be very naïve about this we say — first let s just put a search box in there allow the user to type or say whatever they want and let s make sure we understand this then let s leave that box there so they can react to what they see and provide more detail refine or search for something else in that context e g a restaurant near the resort — we call this pivot and let s run a conversation around it kind of like siri what could be more natural to do this we use sri international s vpa platform which be almost literally a post siri natural language interaction platform with which you can have a conversation in context this be more or less what it look like in our beta version search box a conversational ui we launch this monitor use and quickly realize be that early user split into two group discard the 2nd group we re busy people we learn that people don t know how to interact naturally with computer or they have no idea what to ask or expect so they revert to the most primitive query problem be our goal be to answer interesting specific query because we believe that if we give you a great answer that cater to what you want your likelihood of buying be that much high furthermore absolutely no one get the conversational aspect — the fact you can continue refine and pivot through conversation we decide to take away the focus from conversation for the time be episode 2 vegas slot machine or make it dead simple we realize we have to focus on the first query and give people some cue about what s possible and come up with this interface these contextual spinner turn interaction from a totally open end query to something close to multiple choice question in essence these be interchangeable template where you could get idea for what to search for as well as easily input your query what you pick would show up as a textual query in the search bar which we hope people would realize they can edit or add to hope the result — on the one hand progress we see long and more interesting query and more interaction however when talk to user we realize that they be assume that the spinner be a kind of menu system which mean a they can only pick what s in the menu b they have to pick one thing from each menu so while this be well than what most site have for search it be still a far cry from what we want to deliver here s what we learn from this episode 3 fill in the blank — smartly at this stage it be clear that we need well auto suggest and smart auto complete this be similar from a ui perspective to google instant but desti be about semantic search not keyword match in most case google will auto suggest a phrase that match what you ve be type and have be type in by many other people desti should suggest something that semantically match what you enter and make sense give what we know of the destination and about your trip because desti be new and there haven t be a million user search for the same thing before you desti should reason about what you may ask not suggest something someone else ask we realize we have to build a lot of semantically reasonable and statistically relevant auto suggest we still want to keep to the template logic because we believe it help user think about what they be look for and form the query in their mind so we come up with a ui that blend form filling and natural language entry and focus on build smart auto suggest and auto complete this ui be build of a number of rigid field e g location type that adapt to the subject matter so if the type be hotel you re prompt for date and a free text field that allow you to ask for whatever else you want we iterate a lot over the auto complete and auto suggest feature the first thing be to realize they be different with auto complete you have a user who already think of something to type in and you have to guess what that be with auto suggest you really want to inspire the user into add something useful to their query which mean it need to be relevant to whatever you know about the query and user so far but not overwhelming for the user all this require know a lot about specific destination what do people search for in hawaii vs new york and specific type what s relevant for hotel vs museum also on the visual side what the user be put in be often quantitative and easy to set than type — e g a date a price etc so we come up with our first crack at blend text with visual widget the result be a big improvement in the quality and relevance of query over the previous ui but a feeling that this be still too stiff and rigid when people be ask for a type of place — e g a museum a park a hotel — they often can t really answer and it s easy for they to think about a feature of the place instead — e g that they can go hike or bike see art or eat breakfast for linguistic reason it s easy for people to say that they want a romantic hotel than a hotel that s romantic so while this ui be very expressive often it feel unnatural and limit furthermore many user just end up fill the basic field and not add any depth in the open text field despite various visual cue and edit a query for refining or pivot be hard at the same time — the auto suggest auto complete element we ve build at this stage werealmost enough to allow we to just throw out the limit template and move to one search field — but this time a damn clever one episode 4 search go natural to the naked eye this look like we ve go full circle — one text box parse query show as tag what could be simple well not exactly because we still need query to be meaningful one thing that the template give we be build in disambiguation we need a query that have at least a location + a type or something from which we can derive a type and without a template tell we that the hotel be the type and the restaurant be something you want your hotel to have vs maybe the opposite the system need to well understand the grammatical structure or the sentence and cue you into inputte thing the right way when it s suggest and auto completing type a query the query be understand — you can add edit with this new user interface change query refine and pivot be very natural — add tag or take away tag widget be contextually integrate use the auto suggest drop down menu so they be naturally suggest at the right time e g after you say you be look for a hotel we help you choose when how many room etc it s also very easy to suggest thing to search for base on the context for instance if we know your kid be travel with you we d drop in family friendly and you could dismiss it with one click so where be this go so far natural search look and behave well than anything else we ve see in this space from now on most of the focus be on make the guess even smart with more statistic reasoning about what people ask for in different context and more contextual info drive those guess we believe this ui be where vertical search be head consider how nice it would be to input gift for 4 year old boy under $ 30 into target com s search bar or romantic restaurant with great seafood near times square with a table at 8 pm tonight into opentable — and get relevant answer but then again answer specific query be not that easy either but that s the other side of desti to be continue from a quick cheer to a stand ovation clap to show how much you enjoy this story I think then I talk sometime it s the other way around found & run company in ai mobile travel etc ex eir at sri int l ex aerospace nadav gur s tech musing
Pandorabots,14,3,https://medium.com/pandorabots-blog/using-oob-tags-in-aiml-part-i-21214b4d2fcd?source=tag_archive---------6----------------,use oob tag in aiml part I pandorabot blog medium,suppose you be build an intelligent virtual agent or virtual personal assistant vpa that use a pandorabot as the natural language processing engine you might want this vpa to be able to perform task such as send a text message add an event to a calendar or even just initiate a phone call oob tag allow you to do just that oob stand for out of band which be an engineering term use to refer to activity perform on a separate hidden channel for a pandorabot vpa this translate to activity which fall outside of the scope of an ordinary conversation such as place a phone call check dynamic information like the weather or search wikipedia for the answer to some question the task be execute but do not necessarily always produce an effect on the conversation between the pandorabot and the user oob tag be use in aiml template and be write in the follow format < oob > command < oob > the command that be to be execute be specify by a set of tag which occur within the < oob > tag these inner oob tag can be whatever you like and the phone relate action they initiate be define in your application code to place a call you might see something like this < oob><dial > some phone number < dial > < oob > the < dial > tag within the < oob > tag send a message to the phone to dial the number specify when your client indicate they want to dial a number your application will receive a template contain the command specify inside the oob tag within your application this inner command will be interpret and the appropriate action will be execute it be useful to think of the activity initiate by oob tag as fall into one of two category base on whether they return information to the user via the chat interface or not the first category those that do not return information typically involve activity that interrupt the conversation if you ask your vpa to look up restuarant on a map it will open up your map application and perform a search similarly if you ask your bot to make a phone call it will open the dialer application and make a call in both of these example the activity perform interrupt the conversation and display some other screen the second category those that do return information to the user via the chat interface be generally action that be execute in the background of the conversation if you ask your pandorabot to look up the population of the united states on wikipedia it will perform the search and then return the result of the search to the user via the chat window similarly if you ask your pandorabot to send a text message to the friend it will send the text and then return a message to the user via the chat window indicate the success of the action I e your text message be deliver in this second set of example it be useful to distinguish between those activity whose result will be return directly to the user like the wikipedia example and those activity whose successful completion will simply be indicate to the user through the chat interface as with the texte example here be an example of a category that use the phone dialer on android here be an example interaction this category would lead to human dial 1234567 robot call 1234567 here be a slightly more complicated example involve the oob tag which launch a browser and perform a google search human look up pandorabot robot search search please stand by note not show in the previous example be the category random search phrase which deliver a random selection from a short list of possible reply each indicate to the user that the bot correctly interpret their search request for a complete list of oob tag as implement in the callmom virtual personal assistant app for android as well as usage example click here be sure to look out for the upcoming post use oob tag in aiml part ii which will go over a basic example of how to intrepret the oob tag receive from the pandorabot server within the framework of your own vpa application originally publish at blog pandorabot com on october 9 2014 from a quick cheer to a stand ovation clap to show how much you enjoy this story the large most establish chatbot development and host platform www pandorabot com the lead platform for building and deploying chatbot
Denny Vrandečić,4,4,https://medium.com/@vrandezo/ai-is-coming-and-it-will-be-boring-94768de264c6?source=tag_archive---------7----------------,ai be come and it will be bore denny vrandečić medium,I be ask about my opinion on this topic and I think I would have some profound thought on this but I end up ramble and this post doesn t really make any single strong point tl;dr don t worry about ais kill all human it s not likely to happen in an interview with the bbc stephen hawking state that the development of full artificial intelligence could spell the end of the human race whereas this be hard to deny it be rather trivial any sufficiently powerful tool could potentially spell the end of the human race give a person who know how to use that tool in order to achieve such a goal there be far more dangerous development — for example global climate change the arsenal of nuclear weapon or an economic system that continue to sharpen inequality and social tension ai will be a very powerful tool like every powerful tool it will be highly disruptive job and whole industry will be destroy and a few other will be create just as electricity the car penicillin or the internet ai will profoundly change your everyday life the global economy and everything in between if you want to discuss consequence of ai here be a few that be more realistic than human extermination what will happen if ai make many job obsolete how do we ensure that ais make choice compliant with our ethical understanding how to define the idea of privacy in a world where your car be observe you what do it mean to be human if your toaster be more intelligent than you the development of ai will be gradual and so will the change in our life and as ai keep develop thing once consider magical will become bore a watch you could talk to be power by magic in disney s 1991 classic the beauty and the beast and 23 year later you can buy one for less than a hundred dollar a self drive car be the protagonist of the 80 tv show knight rider and thirty year later they be drive on the street of california a system that check if a bird be in a picture be consider a five year research task in september 2014 and less than two month later google announce a system that can provide caption for picture — include bird and these thing will become boring in a few year if not month we will have to remind ourselves how awesome it be to have a computer in our pocket that be more powerful than the one that get apollo to the moon and back that we can make a video of our child play and send it instantaneously to our parent on another continent that we can search for any text in almost any book ever write technology be like that what s exciting today will become boring tomorrow so will ai in the next few year you will have access to system that will gradually become capable to answer more and more of your question that will offer advice and guidance towards help you navigate your life towards the goal you tell it that will be able to sift through text and datum and start to draw novel conclusion they will become increasingly intelligent and there be two major scenario that people be afraid of at this point the skynet scenario be just mythos there be no indication that raw intelligence be sufficient to create intrinsic intention or will the paperclip scenario be more realistic and once we get close to system with such power we will need to put the right safeguard in place the good news be that we will have plenty of ais at our disposal to help we with that the bad news be that discuss such scenario now be premature we simply don t know how these system will look like that s like start a committee a hundred year ago to discuss the danger come from novel weaponry no one in 1914 could have predict nuclear weapon and their risk it be unlikely that the result of such a committee would have provide much relevant ethical guidance for the manhattan project three decade later why should that be any different today in summary there be plenty of consequence of the development of ai that warrant intensive discussion economical consequence ethical decision make by ais etc but it be unlikely that they will bring the end of humanity background image robot trash living room by vincekamp license under cc by nd 3 0 personal permanent url http simia net wiki ai_is_come _ and_it_will_be_bore from a quick cheer to a stand ovation clap to show how much you enjoy this story wikidata founder google ontologist semantic web researcher and author
Thaddeus Howze,15,6,https://medium.com/@ebonstorm/of-comets-and-gods-in-the-making-4f55ecccb9fe?source=tag_archive---------8----------------,of comet and god in the make thaddeus howze medium,asferit have not grow up ; she didn t know where she come from ; could not conceive of childhood no memory of parent no recollection of family on the vast empty world that serve as her lab she build the probe and put a little bit of herself in each one her machine form ancient slow and sputtering come to life wheeze through the long corridor of the silent lab its darkness mask the distant empty space which asferi imagine be once fill with life she look through her thought and realize she have lose any hope of memory that part of she be already circle a distant star aborne with life she look in on those place when she wake to see the result of her work ; on so many world life spawn with the next launch she would lose the memory of those place there be so little of she remain enough for three no four probe then she would cease to remember why she be what she be she would forget how to exist but not yet she complete the next probe wind the engine and orient it along the galactic plane ; her sensor array align the probe with a wander comet ; she plan to deposit herself within the life give molecule within its frozen mass she know little about her past but know that she must not be able to be find this be the only memory that remain ; hide from the darkness as she load the last probe she consider the first probe she ever send millennium ago ; there be monument within the hall of the lab in she hubris then she consider they a successful reincarnation of her people each representation be fill with the temporal signature of that once great race ; a temporal residue of failure it speak of a great race master of time and space ; they flourish in the dark between the star then the darkness come she be overconfident she sleep assure of their success her mission complete in the time between sleep and wake her cycle of regeneration before attempt to seed again the great race be go find they do not heed the warning she send in those early day she give far more of herself then she come to they in vision teach they secret to harness the hidden nature of matter ; reveal to they the nature of energy both planetary and interstellar they would worship she revere she and believe she to be a god in the end it be not enough they be consume their greatness undone she send less and less of herself from then on godhood fail they perhaps obscurity would serve they well she send less each time only tiny package of micromachine capable of change matter capable of modify genome empower the creature spawn of she with ability even great than the first race psychometric representation of they be all that remain echo in the timestream of history in their hubris they rupture time and space and like the world her lab hang above crack the crust of their world and be lose in a temporal vortex of their own making they have such potential squander then she begin send only the memory of what she be embed within complex epigentic echo no long would she shape the universe for they they would have to work for their survival perhaps they would be strong for it she appear to her descendent only in dream ; vision of what they be memory of who she be memory she no long possess her memory be great once and she seed thousand of world with it but like the ephemeral nature of memory so few know what they see many go mad most dreamed of demiurge mad deva whose power ravage world these memory destroy half of they before they could achieve spaceflight and reach for the star themselves religion they spawn consume they now she send only cell and precellular matter the very least of herself the essence of who she be the final matter of her being ; hide in comet cloak in meteor swarm hide on the boot of other starfarer time have teach her patience though she have lose her memory she be confident of this final strategy to hide herself on million of world her final probe ship would leave a legacy on million of world she find the last star she would use and load the final probe ship with the hardy construction she have ever make she deconstruct the worldship ; her lab her home for millenia of millenia break down every part of it reforge it for a final effort the planet below be also consume her last effort would require everything it be a long dead world lose to antiquity when the universe be young of the darkness she could not remember but she know this as long as there be light her people would survive the final instruction to her probeship would have she descend into her planet s unstable star it s final fluctuation reveal what she know be the inevitable outcome ; and she plan to use it to her advantage her final self would not be aware of the result the final cell of her body be distribute within million of piece of her world and her lab each calibrate to arrive at a star somewhere in her galaxy each single cell would find a world ready for life she could no long coerce planet into life she could no long force matter or energy to take the shape she deem she be now only able to influence the tiny aspect asferit would only be able to nudge a planet toward life the darkness would always be ready to claim her people but now they would be scatter ; to world within the galaxy and without she seed the galactic wind and wait for a supernova to blow they where it would her starseed harden against the impend blastwave they would with the tiny bit of her final design travel fast than light toward their final destination as the star which light her world give her people life watch they die and patiently wait until they could be reborn explode asferit now wait in turn in those last second as the wave of radiation and coronal debris sweep over the remnant of her cannibalize world she subsume herself within the starseed and the near immortal be asferit last of her kind be no more and yet now she be pure purpose no ambition no plan no dream of godhood no long a radiant harbinger of doom light the sky of primitive world she would be the essence of life itself ; the darkness be damn of comet and god in the make © thaddeus howze 2013 all right reserve thaddeus howze be a popular and recently award top writer 2016 recipient on the q&a site quora com he be also a moderator and contributor to thescience fiction and fantasy stack exchange with over fourteen hundred article in a four year period thaddeus howze be a california base technologist and author who have work with computer technology since the 1980 s do graphic design computer science programming network administration teach computer science and it leadership his non fiction work have appear in numerous magazine huffington post gizmodo black enterprise the good man project examiner com the enemy panel & frame science x loud journal comicsbeat com and astronaut com he maintain a diverse collection of non fiction at his blog a matter of scale his speculative fiction have appear online at medium scifiideas com and theau courant press journal he have appear in twelve different anthology in the united states the united kingdom and australia a list of his publish work appear on his website hub city blue from a quick cheer to a stand ovation clap to show how much you enjoy this story author | editor | futurist | activist | tech humanist | http bit ly thowzebio |http bit ly thpatreon
Tommy Thompson,17,14,https://medium.com/@t2thompson/ailovespacman-9ffdd21b01ff?source=tag_archive---------9----------------,why ai research love pac man tommy thompson medium,ai and game be a crowdfunded youtube series on the research and application of ai within video game the follow article be a more involved transcription of the topic discuss in the video link to above if you enjoy this work please consider support my future content over on patreon artificial intelligence research have show a small infatuation with the pac man video game series over the past 15 year but why specifically pac man what element of this game have prove interesting to researcher in this time let s discuss why pac man be so important in the world of game ai research for the sake of complete — and in appreciate there be arguably a generation or two not familiar with the game — puck man be an arcade game launch in 1980 by namco in japan and rename pac man upon be license by midway for an american release the name change be drive less by a need for brand awareness but rather because the name can easily be de face to say something else the original game focus on the titular character who must consume as many pill as possible without be catch by one of four antagonist represent by ghost the four ghost inky blinky pinky and clyde all attempt to hunt down the player use slightly different tactic from one another each ghost have their own behaviour ; a bespoke algorithm that dictate how they attack the player player also have the option to consume one of several power pill that appear in each map power pill allow for the player to not just eat pill but the enemy ghost for a short period of time while mechanically simple when compare to modern video game it provide an interesting test bed for ai algorithm learn to play game the game world be relatively simple in nature but complex enough that strategy can be employ for optimal navigation furthermore the varied behaviour of the ghost reinforce the need for strategy ; since their unique albeit predictable behaviour necessitate different tactic if problem solving can be achieve at this level then there be opportunity for it to scale up to more complex game while pac man research begin in earnest in the early 2000 s work by john koza koza 1992 discuss how pac man provide an interesting domain for genetic programming ; a form of evolutionary algorithm that learn to generate basic program the idea behind koza s work and later that of rosca 1996 be to highlight how pac man provide an interesting problem for task prioritisation this be quite relevant give that we be often try to balance the need to consume pill all the while avoid ghost or — when the opportunity present itself — eat they about 10 year later people become more interested in pac man as a control problem this research be often with the intent to explore the application of artificial neural network for the purpose of create a generalise action policy software that would know at any give tick in the game what would be the correct action to take this policy would be build from play the game a number of time and train the system to learn what be effective and what be not typically these neural network be train use an evolutionary algorithm that find optimal network configuration by breed collection of possible solution and use a survival of the fit approach to cull weak candidate kalyanpur and simon 2001 explore how evolutionary learning algorithm could be use to improve strategy for the ghost in time it be evident that the use of crossover and mutation — which be key element of most evolutionary base approach — be effective in improve the overall behaviour however it s important to note that they themselves acknowledge their work use a problem domain similar to pac man and not the actual game gallagher and ryan 2003 use a slightly more accurate representation of the original game while the screenshot be show here the actual implementation only use one ghost rather than the original four in this research the team use an incremental learning algorithm that tailor a series of rule for the player that dictate how pac man be control use a finite state machine fsm this prove highly effective in the simplified version they be play the use of artificial neural network a data structure that mimic the firing of synapsis in the brain — be increasingly popular at the time and once again in most recent research two notable publication on pac man be lucas 2005 which attempt to create a move evaluation function for pac man base on datum scrape from the screen and process as feature e g distance to close ghost while gallagher and ledwich 2007 attempt to learn from raw unprocessed information it s notable here that the work by lucas be in fact do on ms pac man rather than pac man while perhaps not that important to the casual observer this be an important distinction for ai researcher research in the original pac man game catch the interest of the large computational and artificial intelligence community you could argue it be due to the interesting problem that the game present or that a game as notable as pac man be now consider of interest within the ai research community while it be now something that appear commonplace game — more specifically video game — do not receive the same attention within ai research circle as they do today as high quality research in ai application in video game grow it wasn t long before those with a taste for pac man research move on to look at ms pac man give the challenge it present — which we be still conduct research for in 2017 ms pac man be odd in that it be originally an unofficial sequel midway who have release the original pac man in the united states have become frustrated at namco s continued failure to release a sequel while namco do in time release a sequel dub super pac man which in many way be a departure from the original midway decide to take matter into their own hand ms pac man be — for lack of a well term — a mod ; originally conceive by the general computing company base in massachusetts gcc have get themselves into a spot of legal trouble with midway have previously create a mod kit for popular arcade game missile command as a result gcc be essentially ban from make further mod kit without the original game s publisher provide consent despite the recent lawsuit hang over they they decide to show midway their pac man mod dub crazy otto who like it so much they buy it from gcc patch it up to look like a true pac man successor and release it in arcade without namco s consent though this have be dispute note for our young audience mod kit in the 1980 be not simply software we could use to access and modify part of an original game these be actual hardware print circuit board pcb that could either be add next to the exist game in the arcade unit or replace it entirely while nowhere near as common nowadays due to the rise of home console gaming there be many enthusiast who still use and trade pcb fit for arcade game ms pac man look very similar to the original albeit with the somewhat stereotypical bow on ms pac man s hair head and a couple of minor graphical change however the sequel also receive some small change to gameplay that have a significant impact one of the most significant change be that the game now have four different map in addition the placement of fruit be more dynamic and they move around the maze lastly a small change be make to the ghost behaviour such that periodically the ghost will commit a random move otherwise they will continue to exhibit their prescribed behaviour from the original game each of these change have a significant impact on both how human and ai subsequently approach the problem change make to the map do not have a significant impact upon ai approach for many of the approach discuss early it be simply another configuration of the topography use to model the maze or if the agent be use more egocentric model for input I e relative to the pac man then these be not really consider give the input be contextual this be only an issue should the agent s design require some form or pre processing or expert rule that be base explicitly upon the configuration of the map with respect to a human this be also not a huge task the only real issue be that a human would have become accustom to play on a give map ; devise strategy that utilise part of the map to good effect however all they need be practice on the new map in time new strategy can be formulate the small change to ghost behaviour which result in random move occur periodically be highly significant this be due to the fact that the deterministic model that the original game have be completely break previously each ghost have a prescribed behaviour you could — with some computational effort — determine the state and indeed the location of a ghost at frame n of the game where n be a certain number of step ahead of the current state any implementation that be reliant upon this knowledge whether it be use it as part of a heuristic or an expert knowledge base that give explicit instruction base on the assumption of their behaviour be now sub optimal if the ghost can make random decision without any real warning then we no long have the same level of confidence in any of our ghost prediction strategy similarly this have an impact on human player the deterministic behaviour of the ghost in the original pac man while complex can eventually be recognise by a human player this have be recognise by the lead human player who could factor their behaviour at some level into their decision make process however in ms pac man the change to a non deterministic domain have a similar effect to human as it do ai we can no long say with complete confidence what the ghost will do give they can make random move evidence that a particular type of problem or methodology have gain some traction in a research community can be find in competition if a competition exist that be open to the large research community it be in essence a validation that this problem merit consideration in the case of ms pac man there have be two competition the first competition be organise by simon lucas — at the time a professor at the university of essex in the uk — with the first competition hold at the conference on evolutionary computation cec in 2007 it be subsequently hold at a number of conference — notably ieee conference on computational intelligence and game cig — until 2011 http dce essex ac uk staff sml pacman pacmancontest html this competition use a screen capture approach previously mention in lucas 2005 that be reliant on an exist version of the game while the organiser would use microsoft s own version from the revenge of arcade title you could also use the like the webpacman for testing give it be believe to run the same rom code as show in the screenshot the code be actually take information direct from the run game one benefit of this approach be that it deny the ai developer from access the code to potentially cheat you can t access source code and make call to the like of the ghost to determine their current move instead the developer be require to work with the exact same information that a human player would a video of the winner from the ieee cig 2009 competition ice pambush 3 can be see in the video below in 2011 simon lucas in conjunction with philipp rohlfshagen and david roble create the ms pac man vs ghost competition in this iteration the screen scrape approach have be replace with a java implementation of the original game this provide an api to develop your own bot for competition this iteration run at four conference between 2011 and 2012 one of the major change to this competition be that you can now also write ai controller for the ghost competitor submission be then pit against one another the rank submission for both ms pac man and the ghost from the 2012 league be show below during the early competition there be a continue interest in the use of learn algorithm this range from the of an evolutionary algorithm — which we have see in early research — to evolve code that be the most effective at this problem this range from evolve fuzzy system that use a rule drive by fuzzy logic yes that be a real thing show in handa 2008 to the use of influence map in wirth 2008 and a different take that use ant colony optimisation to create competitive player emilio et al 2010 this research also stir interest from researcher in reinforcement learn a different kind of learn algorithm that learn from the positive and negative impact of action note it have be argue that reinforcement learning algorithm be similar to that of how the human brain operate in that feedback be send to the brain upon commit action over time we then associate certain response with good or bad outcome place your hand over a naked flame be quickly associate as bad give that it hurt simon lucas and peter burrow take to the competition framework as mean to assess whether reinforcement learn specifically an approach call temporal difference learning would yield strong return than evolve neural network burrow and lucas 2009 the result appear to favour the use neural net over the reinforcement learning approach despite that one of the major contribution ms pac man have generate be research into monte carlo method an approach where repeat sampling of state and action allow we to ascertain not only the reward that we will typically attain have make an action but also the value of the state more specifically there have be significant exploration of whether monte carlo tree search mct ; an algorithm that assess the potential outcome at a give state by simulate the outcome could prove successful mct have already prove to be effective in game such as go chaslot et al 2008 and klondike solitaire bjarnason et al 2009 naturally — give this be merely an article on the subject and not a literature review — we can not cover this in immense detail however there have be a significant number of paper focusse on this approach for those interested I would advise you read browne et al 2012 which give an extensive overview of the method and it s application one of the reason that this algorithm prove so useful be that it attempt to address the issue of whether your action will prove harmful in the future much of the research discuss in this article be very good at deal with immediate or reflex response however few would determine whether action would hurt you in the long term this be hard to determine for ai without put some processing power behind it and even hard when work in a dynamic video game that require quick response mct have prove useful since it can simulate whether an action take on the current frame will be useful 5 10 100 1000 frame in the future and have lead to significant improvement in ai behaviour while ms pac man help push mct research many resarcher have now move onto the physical travel salesman problem ptsp which provide it s own unique challenge due to the nature of the game environment ms pac man be still to date an interesting research area give the challenge that it present we be still see research conduct within the community as we attempt to overcome the challenge that one small change to the game code present in addition we have move on from simply focusse on represent the player and start to focus on the ghost as well lend to the aforementione pac man vs ghost competition while the game community at large have more or less forget about the series it have have a significant impact on the ai research community while the interest in pac man and ms pac man be begin to dissipate it have encourage research that have provide significant contribution to artificial and computational intelligence in general http www pacman vs ghost net — the homepage of the competition where you can download the software kit and try it out yourself http pacman shaunew com — an unofficial remake that be inspire by the aforementione pac man dossier by jamey pittman bjarnason r fern a & tadepalli p 2009 low bounding klondike solitaire with monte carlo planning in proceeding of the international conference on automate planning and schedule 2009 browne c powley e whitehouse d lucas s m cowl p rohlfshagen p tavener s perez d samothrakis s and colton s 2012 a survey of monte carlo tree search method ieee transaction on computational intelligence and ai in game 2012 page 1 43 burrow p and lucas s m 2009 evolution versus temporal difference learning for learn to play ms pac man proceeding of the 2009 ieee symposium on computational intelligence and game emilio m moise m gustavo r and yago s 2010 pac mant optimization base on ant colony apply to develop an agent for ms pac man proceeding of the 2010 ieee symposium on computational intelligence and game gallagher m and ledwich m 2007 evolve pac man player what can we learn from raw input proceeding of the 2007 ieee symposium on computational intelligence and game gallagher m and ryan a 2003 learning to play pac man an evolutionary rule base approach proceeding of the 2003 congress on evolutionary computation cec chaslot g m b winands m h & van den herik h j 2008 parallel monte carlo tree search in computer and game pp 60 71 springer berlin heidelberg handa h evolutionary fuzzy system for generate well ms pacman player proceeding of the ieee world congress on computational intelligence kalyanpur a and simon m 2001 pacman use genetic algorithm and neural network koza j 1992 genetic programming on the programming of computer by mean of natural selection mit press lucas s m 2005 evolve a neural network location evaluator to play ms pac man proceeding of the 2005 ieee symposium on computational intelligence and game pittman j 2011 the pac man dossier retrieve from http home comcast net ~jpittman2 pacman pacmandossier html rosca j 1996 generality versus size in genetic programming proceeding of the genetic programming conference 1996 gp 96 wirth n 2008 an influence map model for play ms pac man proceeding of the 2008 computational intelligence and game symposium originally publish at aiandgame com on february 10 2014 — update to include more contemporary pac man research reference from a quick cheer to a stand ovation clap to show how much you enjoy this story ai and game researcher senior lecturer writer producer of youtube series @aiandgames indie developer with @tableflipgames
Milo Spencer-Harper,7.8K,6,https://medium.com/technology-invention-and-more/how-to-build-a-simple-neural-network-in-9-lines-of-python-code-cc8f23647ca1?source=tag_archive---------0----------------,how to build a simple neural network in 9 line of python code,as part of my quest to learn about ai I set myself the goal of build a simple neural network in python to ensure I truly understand it I have to build it from scratch without use a neural network library thank to an excellent blog post by andrew trask I achieve my goal here it be in just 9 line of code in this blog post I ll explain how I do it so you can build your own I ll also provide a long but more beautiful version of the source code but first what be a neural network the human brain consist of 100 billion cell call neuron connect together by synapsis if sufficient synaptic input to a neuron fire that neuron will also fire we call this process think we can model this process by create a neural network on a computer it s not necessary to model the biological complexity of the human brain at a molecular level just its high level rule we use a mathematical technique call matrix which be grid of number to make it really simple we will just model a single neuron with three input and one output we re go to train the neuron to solve the problem below the first four example be call a training set can you work out the pattern should the be 0 or 1 you might have notice that the output be always equal to the value of the leftmost input column therefore the answer be the should be 1 training process but how do we teach our neuron to answer the question correctly we will give each input a weight which can be a positive or negative number an input with a large positive weight or a large negative weight will have a strong effect on the neuron s output before we start we set each weight to a random number then we begin the training process eventually the weight of the neuron will reach an optimum for the training set if we allow the neuron to think about a new situation that follow the same pattern it should make a good prediction this process be call back propagation formula for calculate the neuron s output you might be wonder what be the special formula for calculate the neuron s output first we take the weighted sum of the neuron s input which be next we normalise this so the result be between 0 and 1 for this we use a mathematically convenient function call the sigmoid function if plot on a graph the sigmoid function draw an s shape curve so by substitute the first equation into the second the final formula for the output of the neuron be you might have notice that we re not use a minimum firing threshold to keep thing simple formula for adjust the weight during the training cycle diagram 3 we adjust the weight but how much do we adjust the weight by we can use the error weight derivative formula why this formula first we want to make the adjustment proportional to the size of the error secondly we multiply by the input which be either a 0 or a 1 if the input be 0 the weight isn t adjust finally we multiply by the gradient of the sigmoid curve diagram 4 to understand this last one consider that the gradient of the sigmoid curve can be find by take the derivative so by substitute the second equation into the first equation the final formula for adjust the weight be there be alternative formulae which would allow the neuron to learn more quickly but this one have the advantage of be fairly simple construct the python code although we win t use a neural network library we will import four method from a python mathematics library call numpy these be for example we can use the array method to represent the training set show early the t function transpose the matrix from horizontal to vertical so the computer be store the number like this ok I think we re ready for the more beautiful version of the source code once I ve give it to you I ll conclude with some final thought I have add comment to my source code to explain everything line by line note that in each iteration we process the entire training set simultaneously therefore our variable be matrix which be grid of number here be a complete work example write in python also available here https github com miloharper simple neural network final thought try run the neural network use this terminal command python main py you should get a result that look like we do it we build a simple neural network use python first the neural network assign itself random weight then train itself use the training set then it consider a new situation 1 0 0 and predict 0 99993704 the correct answer be 1 so very close traditional computer program normally can t learn what s amazing about neural network be that they can learn adapt and respond to new situation just like the human mind of course that be just 1 neuron perform a very simple task but what if we hook million of these neuron together could we one day create something conscious I ve be inspire by the huge response this article have receive I m consider create an online course click here to tell I what topic to cover i d love to hear your feedback from a quick cheer to a stand ovation clap to show how much you enjoy this story study economic at oxford university founder of www moju io interested in politic and ai technology trend and new invention follow this collection to update the late trend update as a collection editor I don t have any permission to add your article in the wild please submit your article and I will approve also follow this collection please
Arik Sosman,1.5K,7,https://blog.arik.io/facebook-m-the-anti-turing-test-74c5af19987c?source=tag_archive---------1----------------,facebook m — the anti ture test arik s blog,facebook have recently launch a limited beta of its ground breaking ai call m m s capability far exceed those of any compete ai where some ais would be hard press to tell you the weather condition for more than one location god forbid you go on a trip m will tell you the weather forecast for every point on your route at the time you re expect to get there and also provide you with convenient gas station suggestion account for traffic in its estimation and provide you with option for food and entertainment at your destination as many people have point out there have be press release state that m be human aid however the point of this article be not to figure out whether or not there be human behind it but to indisputably prove it when communicate with m it insist it s an ai and that it live right inside messenger however its non instantaneous nature and the sheer unlimited complexity of task it can handle suggest otherwise the opinion be split as to whether or not it s a real ai and there seem to be no way of prove its nature one way or the other the big issue with try to prove whether or not m be an ai be that contrary to other ais that pretend to be human m insist it s an ai thus what we would be test for be human pretend to be an ai which be much hard to test than the other way round because it s much easy for human to pretend to be an ai than for an ai to pretend to be a human in this situation a ture test be futile because m s objective be precisely to not pass a ture test so what we want to prove be not the limitation of the ai but the limitlessness of the allege human behind it what we need therefore be a different test an anti ture test if you will as it happen I do find a way of prove m s nature but good storytelling mandate that first I describe my laborious path to the result and the inconclusive experiment I have to conduct before I finally get a definitive answer when I first get m our conversation start like this I use artificial intelligence but people help train I be m s response to my question regard its nature that can mean many thing because use ai be not the same as be a completely autonomous ai so I keep bug it about its nature some people opine that what m refer to as ai be that there be people type out all the response but the tool that help they do that be base on machine learning however directly ask about that didn t yield any new insight m s assertiveness regard its nature be set in stone nonetheless there be some minor tell that arguably betray the underlie human nature of this chatbot to test its limit I have ask it to perform a set of complicated task for I that no other ai out there could pull off I tell it where I work and then slightly modify my request and indeed it respond the most noteworthy aspect of this reply be that google maps wasn t capitalize suggest that maybe just maybe a human type it out in a hurry and indeed even with some other request its response have prove not to be as impeccable as the one we re use to from siri for instance when I ask it to find some nice wallpaper for I take from the berkeley stadium depict the bay area at night preferably with the bay bridge the transamerica pyramid and the sather tower be in the picture m do manage to find some very nice wallpaper for I but it say that it couldn t find any with the campanile as consolation though it say it would let I know if it find any that fit my criterion more precisely now the first issue with the above response be that the wallpaper it send I do have the transamerica pyramid and m know they do what they didn t have be the sather tower so why be it say it s go to let I know about picture with the transamerica pyramid the second issue be that it s call the transamerica pyramid not the transamerican pyramid and lastly note the two with s and the i l it have make two typo and indeed that be not the only time it do while a lot of human struggle with the distinction between its and it s for an ai that should not have be an issue even so it might have be train wrong so as such these lapse be not sufficiently conclusive even the delay response I mention early could have be deliberate include the fact that there s a type indicator show when m be prepare a response rather than send the whole string instantaneously as a regular ai would the result and indication so far didn t satisfy I so I be still look for a way to prove that there be real human behind m just how could I make they come out make they show themselves as it happen the answer come to I at a time when I wasn t actively look for it the movie in cupertino end rather late and I ask m whether there be any place I could get dinner at afterwards that would still be open at that time there be only two place open but I wasn t sure whether their kitchen would still be open too thus I ask m whether it could call they and figure that out and indeed it say it could so I ask m whether it could call my friend nope whether it could call I nope apparently it could only call business for I but not individual so what do I do I make up a business and ask m to call it so m ask I for the phone number and I simply give it mine about five minute later I receive a call with no caller i d when I pick up I hear some rumble noise in the background say hello and then the other end hang up immediately afterward the follow exchange happen with m unfortunately I didn t have a landline phone number so I be a bit disappointed that not even this experiment could prove m s nature a few day later I have to get some work do during the weekend and while at the office I realize that the company do have one the experiment have to be repeat about three minute later we get a phone call in the conference room when I pick up a distinctively human female voice say hello as it happen I have accidentally set the phone to mute before that so she didn t hear I say the company name still the voice be most definitely human and because the reader shouldn t be take I at face value I make a recording of that whole encounter immediately afterward m send a reply what s more it appear to I that they forget to block the caller i d for that particular call because I get to see the phone number they be call from so there very clearly m be call from +1 650 796 2402 as can be see on the photo the automatic reverse lookup match that number to facebook thus here we be we have definitive proof that m be power by human the next question be be it only human or be there at least some ai drive component behind it as to this problem I ll leave it as a homework assignment for the reader to figure out in the meantime I shall enjoy have my own free personal human assistant from a quick cheer to a stand ovation clap to show how much you enjoy this story software engineer @bitgo my experimental blog
Tony Aubé,4.5K,8,https://medium.com/swlh/no-ui-is-the-new-ui-ab3f7ecec6b3?source=tag_archive---------2----------------,no ui be the new ui the startup medium,on the rise of ui less app and why you shouldcare about they as a designer october 23 2015 • 8 minute read a couple of month ago I share with my friend how I think app like magic and operator be go to be the next big thing if you don t know about these app what make they special be that they don t use a traditional ui as a mean of interaction instead the entire app revolve around a single messaging screen these be call invisible and conversational app and since my initial post a slew of similar app come to market even as of writing this facebook be release m a personal assistant that s integrate with messenger to help you do about anything while these app operate in a slew of different market from check your bank account schedule a meeting make a reservation at the good restaurant to be your travel assistant they all have one thing in common they place message at the center stage matti makkonen be a software engineer who pass away a couple of month ago my guess be that you didn t hear of his death and you most likely don t know who he be however makkonen be probably one of the most important individual in the domain of communication and I mean — on the level of alexander bell — important he be the inventor of sms if you didn t realize how pervasive sms have become today think again sm be the most use application in the world three year ago it have an estimate 4 billion active user that be over four time the number of facebook user at the time messaging and particularly sms have be slowly take over the world it be now fundamental to human communication and it be why message app such as whatsapp and wechat be now worth billion while messaging have become center to our everyday life it s currently only use in the narrow context of personal communication what if we could extend message beyond this what if messaging could transform the way we interact with computer the same way it transform the way we interact with each other in the recent movie ex machina a billionaire create ava a female look robot endow with artificial intelligence to test his invention he bring in a young engineer to see if he could fall in love with she the whole premise of the movie be center around the turing test a test invent by alan turing also feature in the recent movie the imitation game in order to determine if a artificial intelligence be equivalent of that of a human a robot pass the ture test would have huge implication on humanity as it would mean that artificial intelligence have reach human level while we be far from create robot that can look and act like human such as ava we ve get pretty good at simulate human intelligence innarrow contexts and one of those context where ai perform well be you ve guess it message this be thank to deep learn a process where the computer be teach to understand and solve a problem by itself rather than have engineer code the solution deep learning be a complete game changer it allow ai to reach new height previously think to be decade away nowadays computer can hear see read and understand human well than ever before this be open a world of opportunity for ai power app toward which entrepreneur be rush in this gold rush messaging be the low hang fruit this be because out of all the possible form of input digital text be the most direct one text be constant it doesn t carry all the ambiguous information that other form of communication do such as voice or gesture furthermore message make for a well user experience than traditional app because it feel natural and familiar when message become the ui you don t need to deal with a constant stream of new interface all fill with different menus button and label this explain the current rise in popularity of invisible and conversational app but the reason you should care about they go beyond that the rise in popularity of these app recently bring I to a startling observation advance in technology especially in ai be increasingly make traditional ui irrelevant as much as I dislike it I now believe that technology progress will eventually make ui a tool of the past something no long essential for human computer interaction and that be a good thing one could argue that conversational and invisible app aren t devoid of ui after all they still require a screen and a chat interface while it be true that these app do require ui design to some extent I believe these be just the tip of the iceberg beyond they new technology have the potential to disrupt the screen entirely to my point have a look at the follow video the first video showcase project soli a small radar chip create by google to allow fine gesture recognition the second one present emotiv a product that can read your brainwave and understand their meaning through — bear with I — electroencephalography or eeg for short while both technology seem completely magical they be not they be currently functional and have something very special in common they don t require a ui for computer input as a designer this be an unsettling trend to internalize in a world where computer can see listen talk understand and reply to you what be the purpose of a user interface why bother design an app to manage your bank account when you could just talk to it directly beyond human interface interaction we be enter the world of brain computer interaction in this world digital telepathy couple with ai and other mean of input could allow we to communicate directly with computer without the need for a screen in his talk at chi 2014 scott jenson introduce the concept of a technological tiller accord to he a technological tiller be when we stick an old design onto a new technology wrongly think it will work out the term be derive from a boat tiller which be for a long time the main navigation tool know to man hence when the first car be invent rather than have steering wheel as a mean of navigation they have boat tiller the result car be horribly hard to control and prone to crash it be only after the steering wheel be invent and add to the design that car could become widely use as a designer this be a valuable lesson a change in context or technology most often require a different design approach in this example the new technology of the motor engine need the new design of the steering wheel to make the result product the car reach its full potential when a technological tiller be ignore it usually lead to product failure when it be acknowledge and solve it usually lead to a revolution and tremendous success and if one company well understand this principle it be apple with the invention of the iphone and the ipad a technological tiller be nokia stick a physical keyboard on top of a phone good design be to create a touch screen and digital keyboard a technological tiller be microsoft stick windows xp on top of a tablet good design be to develop a new finger friendly os and I believe a technological tiller be stick an ipad screen over every new internet of thing thing what if good design be about avoid the screen altogether learn about technological tiller teach we that stick too much to old perspective and idea be a surefire way to fail the new startup develop invisible and conversational app understand this they understand that the ui be not the product itself but only a scaffolding allow we to access the product and if avoid that scaffolding can lead to a well experience then it definitively should be so do I believe that ai be take over that ui be obsolete and that all visual designer will be out of job soon not really as far as I know ui will still be need for computer output for the foreseeable future people will still use the screen to read watch video visualize datum and so on furthermore as nir mention in his great article on the subject conversational app be currently good at only a specific set of task it be safe to think that this will also be the case for new technology such as emotiv and project solo as game change as these be they will most likely not be good at everything and ui will probably outperform they at specific task what I do believe however be that these new technology be go to fundamentally change how we approach design this be necessary to understand for those plan to have a career in tech in a future where computer can see talk and listen and reply to you what good be go to be your awesome pixel perfect sketch skill let this be a fair warning against complacency as ui designer we have a tendency to presume a ui be the solution to every new design problem if anything the ai revolution will force we to reset our presumption on what it mean to design for interaction it will push we to leave our comfort zone and look at the big picture bring our focus on the design of the experience rather than the actual screen and that be an exciting future for designer 💚 please hit recommend if you enjoy or learn from this text to keep thing concise this text use the term ui as short for graphical user interface more precisely it refer to the web and app visual pattern that have become so pervasive in the recent year this text be originally publish on techcrunch on 11 11 2015 publish in # swlh startup wanderlust and life hack from a quick cheer to a stand ovation clap to show how much you enjoy this story personal thought on the future of design & technology lead design @ osmo medium s large publication for maker subscribe to receive our top story here → https goo gl zhclji
Matt O'Leary,373,12,https://howwegettonext.com/i-let-ibm-s-robot-chef-tell-me-what-to-cook-for-a-week-d881fc884748?source=tag_archive---------3----------------,I let ibm s robot chef tell I what to cook for a week,originally publish at www howwegettonext com if you ve be follow ibm s watson project and like food you may have notice grow excitement among chef gourmand and molecular gastronomist about one aspect of its development the main watson project be an artificial intelligence that engineer have build to answer question in native language — that be question phrase the way people normally talk not in the stilted way a search engine like google understand they and so far it s work watson have be help nurse and doctor diagnose illness and it s also manage a major jeopardy win now chef watson — develop alongside bon appetit magazine and several of the world s fine flavor profiler — have be launch in beta enable you to mash recipe accord to ingredient of your own choosing and receive taste matching advice which reportedly can t fail while some of the world s foremost tech luminary and conspiracy theorist be a bit skeptical about the wiseness of a i if it s go to be use at all allow it to tell you what to make out of a fridge full of unloved leftover seem like an inoffensive enough place to start I decide to put it to the test while employ as a food writer for well over a decade I ve also spend a good part of the last nine year work on and off in kitchen figure out how to use spare ingredient have become quite commonplace in my professional life I ve also develop a healthy disregard for recipe as anything other than source of inspiration or annoyance but for the purpose of this experiment be willing to follow along and try any ingredient at least once so with this in mind I m go to let watson tell I what to eat for a week I ve spend a good amount of time play around with the app which can be find here and I m go to follow its instruction to the letter where possible I have an audience of willing tester for the food and intend to do my good in recreate its recipe on the plate still I m go to try to test it a bit I want to see whether or not it can save I time in the kitchen ; also whether it have any amazing suggestion for dazzle taste match ; if it can help I use thing up in the fridge ; and whether or not it s go to try to get I to buy a load of stuff I don t really need a lot of work have go into the creation of this app — and a lot of expertise but be it useable can human being understand its recipe will we want to eat they let s find out a disclaimer before we start chef watson isn t great at tell you when stuff be actually ready and cook you need to use your common sense take all of its advice as advice and inspiration only it s the flavor that really count monday the tailgating corn salmon sandwich my first impression be that the app be intuitive and pretty simple to use once you ve add an ingredient it suggest a number of flavor match type of dish and mood include some off the wall one like mother s day choose a few of these option and the actual recipe begin to bunch up on the right of the screen I select salmon and corn then opt for the wildly suggestive tailgating corn salmon sandwich the recipe page itself have link to the original bon appetit dish that inspire your a i mélange accompany by a couple of picture there s a battery of disclaimer state that chef watson really only want to suggest idea rather than tell you what to eat — presumably to stop people who want to try cook with fiberglass for example from launch no win no fee case my own salmon tailgating recipe seem pretty straightforward there be a couple of nice touch on the page with regard to usability you can swap out any ingredient that you might not have in stock for other which watson will suggest it seem fond of add celery root to dish for this first attempt I decide to follow watson s advice almost to a t I didn t have any garlic chile sauce but manage to make a presumably functional analog out of some garlic and chili sauce the only other change I make involve add some broad bean because I like broad bean during prep I employ a nearly unconscious bit of initiative namely when I cook the salmon it s entirely likely that watson be as seem to be the case suggest that I use raw salmon but it s monday night and I m not in the mood for anything too mind bend team watson if I ruin your tailgater with my pig head insistence on cooked fish I m sorry although I m not too sorry because you know it be actually a really good dish I be at first unsure — the basil seem like a bit of an afterthought ; I wasn t sure the lime zest be necessary ; and cold salmon salad on a burger bun isn t really an easy sell but damn it I d make that sandwich again it be miss some substance overall it make enough for two small bun so I team it up with a nice bit of korean spiced pickled cucumber on the side which work well my fellow diner deem it fine if a little uninteresting — and yes maybe it could have do with a bit more sharpness and depth and maybe a little more a computer tell I how to make this flavor wackiness but overall well do hint definitely add broad bean they totally work now to mull over what tailgating might mean tuesday spanish blood sausage porridge it be day two of the chef watson guest slot in the kitchen and thing be about to get interesting buoy by yesterday s tailgating salmon sandwich success I decide to give watson something to sink its digital tooth into and supply only one ingredient blood sausage I also specify main as a style really so that he she it know that I wasn t expect dessert if I m be very honest I ve read more appetizing recipe than blood sausage porridge even the inclusion of the word spanish doesn t do anything to fancy it up and a bit concerningly this be a recipe that watson have extrapolate from one for rye porridge with morel replace the rye with rice the mushroom with sausage and the original s chicken liver with a single potato and one tomato still maybe it would be brilliant but unlike yesterday I run into some problem I wasn t sure how many tomato and potato watson expect I to have here — the ingredient list say one of each ; the method suggest many — or also why I have to soak the tomato in boiling water first although it make sense in the original mushroom centric method additionally wastson offer the whimsical instruction to just cook the tomato and potato presumably for as long as I feel like there s a lot of butter involve in this recipe and rather too much liquid recommend eight cup of stock for one and a half of rice I actually get a bit feed up after four and stop add they forty to 50 minute cook time be a bit too long too — again that s be directly extract from the rye recipe but these be mere trifle the dish taste great it s a lovely blend of flavor and texture thank to the blood sausage and the potato the butter work brilliantly and the tomato on top be a nice touch and it prove watson s functionality you can suggest one ingredient that you find in the fridge use your initiative a bit and you ll be leave with something lovely and buttery lovely and buttery well do watson wednesday diner cod pizza when I read this recipe I wonder whether this be go to be it for I and watson diner cod and pizza be three word that don t really belong together and the ingredient list seem more like a supermarket sweep than a recipe now that I ve actually make the meal I don t know what to think about anything you might remember a classic 1978 george a romero direct horror film call dawn of the dead its 2004 remake follow the paradigm shift to run zombie in 28 day later suffer critically my impression of this remake be always that if it d just be call something different — zombie go shop for instance — every single person who see it would have love it as it be viewer think it seem unauthentic and it gather what be essentially some unfair criticism see also the recent robocop remake or as I call it cyberswede vs detroit this meal be my culinary dawn of the dead if only watson have call it something other than pizza it would have be utterly perfect it emphatically isn t a pizza it have as much in common with pizza as cake do but there s something about radish cod ginger olive tomato and green onion on a pizza crust that just work remarkably well to be clear I fully expect to throw this meal away I have the website for curry delivery already open on my phone that s all before I eat two of the pizza they taste like nothing on earth the addition of comté cheese and chive be the sort of genius absurdity that make people into millionaire I be however nervous to give one to my pregnant fiancée ; the ingredient be so weird that I be just sure she d suffer some really strange psychic reaction or that the baby would grow up to be extremely contrary be careful with this recipe preparation as I ve find with watson it doesn t tell you how to assure that your fish be cook ; nor do it tell you how long to pre bake the crust base these kind of thing be really important you need to make sure this dish be cook properly it take long than you might expect I m write this from sweden the home of the ridiculous pizza and yet I have a feeling that if I be to show this recipe to a chef who ordinarily think nothing of pile a kilo of kebab meat and béarnaise sauce on bread and serve it in a cardboard box with a side salad of ferment cabbage he or she would balk and tell I that I ve go too far which would be his or her loss I think I m go to have to take this to dragon s den instead watson I don t know how I m go to cope with normal recipe after our little holiday together you re change the way I think about food thursday fall celery sour cream parsley lemon taco follow yesterday s culinary epiphany I be keen to keep a cool head and a critical eye on chef watson so I decide to road test one theory from an article I find on the internet it mention that some of the most frequently discard item in american fridge be celery sour cream fresh herb and lemon let s not dwell too much on the luxury problem aspect of this I can t imagine that people everywhere in the world be lament the amount of sour cream and flat leaf parsley they toss and focus instead on what watson can do with this admittedly tricky sound shopping list what it do be this immediately add shrimp tortilla and salsa verde the salsa verde it recommend from an un watsone recipe courtesy of bon appetit be fantastic it s nothing like the salsa verde I know and love with its caper and dill pickle and anchovy this iteration require a bit of a simmer be super spicy and delicious I have to cheat and use normal tomato instead of tomatillos but I don t think it make a huge difference the marinade for the shrimp be unusual in that like a lot of what watson recommend it use a ton of butter a hefty wallop of our old friend kosher salt too now I ve work as a chef on and off for several year so be unfaze by the appearance of salt and butter in recipe they re how you make thing taste nice however there s no get away from the fact that I buy a stick of butter at the start of the week and it s already go the assemble taco be good — they be uncontroversial my dining companion deem the salsa a bit too spicy but I like the kick it give the dish and the sour cream calm it down a bit it strike I as a bit of a shame to fire up the barbecue for only about two minute worth of cook time but it s may and the sun be shine so what the heck be this recipe as absurd as yesterday s absolutely not be it as memorable sadly I don t think so would I make it again I m sorry watson but probably not these taco be good but ultimately not worth the prep hassle friday mexican mushroom lasagna before I start I don t want you to get the impression that my love affair which reach the height of its passion on wednesday with watson be over it absolutely isn t I have be consistently impressed with the software s intelligence its ease of use and the audacity of some of its suggestion for flavor match it s incredible it really work it probably win t save you any money ; it win t make you thin ; and it win t teach you how to actually cook — all of that stuff you have to work out for yourself but at this stage it s a distinctly impressive and worthwhile project do give it a go but be prepare to have to coax something workable out of it every once in a while today it take I a long time to find a meat free recipe which didn t when it come down to it contain some sort of meat I select meat as an option for what I didn t want to include and it take I to a recipe for sausage lasagne with one and a half pound of sausage in it I remove the sausage and it replace it with turkey mince maybe someone just need to tell watson that neither sausage nor turkey grow on tree after much tinker and submit and resubmitte the recipe I end up with be for lasagne top with a sort of creamy mash potato sauce it s very easy and it s a profoundly smart use of ingredient the lasagne be not the world s most aesthetically appeal dish and it s not as astonishingly flavor as some of this week s other revelation but I don t think I ll be make my cheese sauce in any other way from this point onwards top mark and in essence this kind of sum up watson for I you need to tinker with it a bit before you can find something usable you may need to make a do I want to put mashed potato on this lasagne leap of faith and you re go to have to actually go with it if you want the app s full benefit you ll consume a lot of dairy product and you might find yourself daydream about nice simple unadorned salad if you decide to go all in with its suggestion but an a i that can tell we how to make a pizza out of cod ginger and radish that you know be go to taste amazing one that will gladly suggest a workable recipe for blood sausage porridge and walk you through it without too much hassle that give you a how crazy option for each ingredient that be only design to make the life of food enthusiast more interesting why on earth not watson and I be go to be good friend from this point forward even if we don t speak every day and I can t wait to introduce it to other now though I m go to only consume smoothie for a week seriously if I even look at butter in the next few day I m probably go to puke this fall medium and how we get to next be explore the future of food and what it mean for we all to get the late and join the conversation you can follow future of food from a quick cheer to a stand ovation clap to show how much you enjoy this story inspire story about the people and place build our future create by steven johnson edit by ian steadman duncan geere anjali ramachandran and elizabeth minkel support by the gates foundation
Tanay Jaipuria,1.1K,5,https://medium.com/@tanayj/self-driving-cars-and-the-trolley-problem-5363b86cb82d?source=tag_archive---------4----------------,self drive car and the trolley problem tanay jaipuria medium,google recently announce that their self drive car have drive more than a million mile accord to morgan stanley self drive car will be commonplace in society by ~2025 this get I think about the ethic and philosophy behind these car which be what the piece be about in 1942 isaac asimov introduce three law of robotic in his short story runaround they be as follow he later add a fourth law the zeroth law 0 a robot may not harm humanity or by inaction allow humanity to come to harm though fictional they provide a good philosophical grounding of how ai can coexist with society if self drive car be to follow they we re in a pretty good spot right let s leave aside the argument that self drive car lead to loss of job of taxi driver and truck driver and so should not exist per the 0th 1st law however there s one problem which the law of robotic don t quite address it s a famous thought experiment in philosophy call the trolley problem and go as follow it s not hard to see how a similar situation would come up in a world with self drive car with the car have to make a similar decision say for example a human drive car run a red light and a self drive car have two option what should the car do from a utilitarian perspective the answer be obvious to turn right or pull the lever lead to the death of only one person as oppose to five incidentally in a survey of professional philosopher on the trolley problem 68 2 % agree say that one should pull the lever so maybe this problem isn t a problem at all and the answer be to simply do the utilitarian thing that great happiness to the great number but can you imagine a world in which your life could be sacrifice at any moment for no wrongdoing to save the life of two other now consider this version of the trolley problem involve a fat man most people that go the utilitarian route in the initial problem say they wouldn t push the fat man but from a utilitarian perspective there be no difference between this and the initial problem — so why do they change their mind and be the right answer to stay the course then kant s categorical imperative go some way to explain it in simple word it say that we shouldn t merely use people as mean to an end and so kill someone for the sole purpose of save other be not okay and would be a no no by kant s categorical imperative another issue with utilitarianism be that it be a bit naive at least how we define it the world be complex and so the answer be rarely as simple as perform the action that save the most people what if go back to the example of the car instead of a family of five inside the car that run the red light be five bank robber speed after rob a bank and sit in the other car be a prominent scientist who have just make a breakthrough in cure cancer would you still want the car to perform the action that simply save the most people so may be we fix that by make the definition of utilitarianism more intricate in that we assign a value to each individual life in that case the right answer could still be to kill the five robber if say our estimate of utility of the scientist s life be more than that of the five robber but can you imagine a world in which say google or apple place a value on each of our life which could be use at any moment of time to turn a car into we to save other would you be okay with that and so there you have it though the answer seem simple it be anything but which be what make the problem so interesting and so hard it will be a question that come up time and time again as self drive car become a reality google apple uber etc will probably have to come up with an answer to pull or not to pull lastly I want to leave you another question that will need to be answer that of ownership say a self drive car which have one passenger in it the owner skid in the rain and be go to crash into a car in front push that car off a cliff it can either take a sharp turn and fall of the cliff or continue go straight lead to the other car fall of the cliff both car have one passenger what should the car do should it favor the person that buy it — its owner thank for read feel free to share this post and leave a note write a response to share your thought I m tanayj on twitter if you want to discuss far from a quick cheer to a stand ovation clap to show how much you enjoy this story product @facebook previously @mckinsey I like tech econ strategy and @manutd view and banter my own
Milo Spencer-Harper,2.2K,3,https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a?source=tag_archive---------5----------------,how to build a multi layer neural network in python,in my last blog post thank to an excellent blog post by andrew trask I learn how to build a neural network for the first time it be super simple 9 line of python code model the behaviour of a single neuron but what if we be face with a more difficult problem can you guess what the should be the trick be to notice that the third column be irrelevant but the first two column exhibit the behaviour of a xor gate if either the first column or the second column be 1 then the output be 1 however if both column be 0 or both column be 1 then the output be 0 so the correct answer be 0 however this would be too much for our single neuron to handle this be consider a nonlinear pattern because there be no direct one to one relationship between the input and the output instead we must create an additional hidden layer consist of four neuron layer 1 this layer enable the neural network to think about combination of input you can see from the diagram that the output of layer 1 feed into layer 2 it be now possible for the neural network to discover correlation between the output of layer 1 and the output in the training set as the neural network learn it will amplify those correlation by adjust the weight in both layer in fact image recognition be very similar there be no direct relationship between pixel and apple but there be a direct relationship between combination of pixel and apple the process of add more layer to a neural network so it can think about combination be call deep learning ok be we ready for the python code first I ll give you the code and then I ll explain far also available here https github com miloharper multi layer neural network this code be an adaptation from my previous neural network so for a more comprehensive explanation it s worth look back at my early blog post what s different this time be that there be multiple layer when the neural network calculate the error in layer 2 it propagate the error backwards to layer 1 adjust the weight as it go this be call back propagation ok let s try run it use the terminal command python main py you should get a result that look like this first the neural network assign herself random weight to her synaptic connection then she train herself use the training set then she consider a new situation 1 1 0 that she hadn t see before and predict 0 0078876 the correct answer be 0 so she be pretty close you might have notice that as my neural network have become smart I ve inadvertently personify she by use she instead of it that s pretty cool but the computer be do lot of matrix multiplication behind the scene which be hard to visualise in my next blog post I ll visually represent our neural network with an animate diagram of her neuron and synaptic connection so we can see her thinking from a quick cheer to a stand ovation clap to show how much you enjoy this story study economic at oxford university founder of www moju io interested in politic and ai technology trend and new invention follow this collection to update the late trend update as a collection editor I don t have any permission to add your article in the wild please submit your article and I will approve also follow this collection please
Ben Brown,1.1K,7,https://blog.howdy.ai/what-will-the-automated-workplace-look-like-495f9d1e87da?source=tag_archive---------6----------------,start automate your business task with slack howdy,if you haven t read about it in the time or hear about it on npr yet you be soon go to be replace by a robot at your job all the job we think be safe because they require experience and nuance can now be do by computer martin ford author of the book the time and npr be report on call it the threat of a jobless future a future where computer write our newspaper article create our legal contract and compose our symphony automate this type of complicated quasi creative task be really impressive it require super computer and use the forefront of artificial intelligence to achieve this shocking result it require ton of datum and lot of programming use advanced system not available to ordinary people but not everything require deep learn some of the thing we do in our every day live especially at our job can be automate though it use to be the domain of the geek scripting and automation be invade all aspect of the workplace worker and organization who can master scripting and automation will gain an edge on those who can t we all have to face the reality that a well build script might be fast and more reliable than we can be at some part of our job those of we who can create and wield this type of tool will be able to do well work fast luckily inside message tool like slack create customize interactive automation tool for business task be possible with a little open source code some cloud tool that be mostly free and a bit of self reflection bot be app that live alongside user in a chatroom user can issue command to bot by send message to they or by use special keyword in the chatroom traditionally bot have be use for thing like server maintenance and running software test but now use the connect device all around we nearly anything can be automate and control by a bot a common task in many technology team be the stand up meet everyone stand up and one at a time tell the team what they ve be work on what they ve get come up next and any problem they be face each person take a few minute to speak in many team this be already take place in a chat room if there be 10 people on a team and each person speak for just 90 second they ll spend 15 minute just bring people up to speed nothing have be discuss no problem have yet be solve what happen if this process be automate use a bot in an environment like slack a stand up be trigger — automatically or by a project manager use a flexible script the bot simultaneously reach out to every member of the team via a private message on slack the bot have an interactive conversation with each team member in parallel and collect everyone s response everyone still spend 90 second talk about their work but now it be the same 90 second the bot now finish collect the checkin response share its report with all the stakeholder just 2 minute into the meeting everyone involve have a single document to look at that contain the up to date status of the project the team gain 13 minute during which they can discuss this information clear blocker and get back to work now this be admittedly an aggressive application of this approach that win t work for everyone — some team may need the sequential listing of update some team may need to actually stand up and use their voice the point I m try to make be that automate thing like this expose way for the work to be improve for time to be save and for the process to evolve what other process could be automate like this what if there be a meeting runner bot that automatically send out an agenda to all attendee before the meeting then collect collate and deliver update to team member it could make meeting short and more productive by reduce the time need to bring everyone up to speed what if there be an hr bot that could collect performance review and feedback what if there be a task management bot that could not only manage the creation of task and list but also create and deliver up to date progress report to the whole team there be a lot to be gain with simple process automation like this so how can you and your organization benefit from this type of automation tool first you ll need to commit to adopt a tool like slack where your team can communicate and use this type of bot then you ll have to customize slack to take advantage of build in and custom integration which take some programming — though not much as there be a ton of open source tool ready to use an organization like my company xoxco can help you do this before you can automate something you have to know the process and be able to write it down in detail you ll have to think about all the special case that occur not only will this allow you to build an automation script it will help you to hone and document the process by which your business be conduct when we do thing we do they one at a time robot can do lot of thing at once — so once you ve get your process document think about how the step might be able to run in parallel for example could the bot talk to multiple people at once instead of do it sequentially since your script can only do what you tell it you ll need to plan for the contingency that might occur while it run what if someone doesn t respond in time what if information be unavailable what if a step in the process fail think through these case and prepare your script to handle they for example we build in a 5 minute timeout for our project manager bot — if a user doesn t respond in 5 minute they get a reminder to checkin in person and their lack of a response be indicate in the report this may sound complicated but when it boil down we re just talk about include an else for every if — a good practice for any software or process to incorporate your bot once deploy can become valuable member of your team their success be dependent on your team s desire to use they and that they provide a well fast more reliable way to achieve organizational goal bot should have a user friendly personality and represent and support company culture bot should talk like real people but not pretend to be real people our rule of thumb try to be as smart as a puppy which will engender an attitude of forgiveness when the bot do something not quite right this type of software automation have be common in certain group for year there may already be a software automation expert in your midst she s probably part of the server administration team or the quality assurance group right now she work on code deployment or write software test go find she and go put she in a room with a project manager and a content strategist and see if they can identify and automate the team s top three time suck activity in a way that be not only useful but fun to use when we start to design software for message the entire application must be boil down to word without color to choose navigation to click and sidebar to fill with widget this can help we not only build well more useful software but put simply require we to run our business in a more organized document and well understand way don t wait for the artificial intelligence explosion to arrive start put these tool to work today update you can now use a fully realize version of the bot discuss in this post — we ve launch it under the name howdy add howdy to your team to run meeting capture information and automate common task for your team read more about our launch here from a quick cheer to a stand ovation clap to show how much you enjoy this story I m a designer and technologist in austin texas I co found xoxco in 2008 the official blog of howdy ai and botkit
Frank Diana,428,11,https://medium.com/@frankdiana/digital-transformation-of-business-and-society-5d9286e39dbf?source=tag_archive---------7----------------,digital transformation of business and society frank diana medium,at a recent kpmg robotic innovation event futurist and friend gerd leonhard deliver a keynote title the digital transformation of business and society challenge and opportunity by 2020 I highly recommend view the video of his presentation as gerd describe he be a futurist focus on foresight and observation — not predict the future we be at a point in history where every company need a gerd leonhard for many of the reason present in the video future thinking be rapidly grow in importance as gerd so rightly point out we be still vastly under estimate the sheer velocity of change with regard to future think gerd use my future scenario slide to describe both the exponential and combinatorial nature of future scenario — not only do we need to think exponentially but we also need to think in a combinatorial manner gerd mention tesla as a company that really know how to do this he then describe our current pivot point of exponential change a point in history where humanity will change more in the next twenty year than in the previous 300 with that as a backdrop he encourage the audience to look five year into the future and spend 3 to 5 % of their time focus on foresight he quote peter drucker in time of change the great danger be to act with yesterday s logic and state that leader must shift from a focus on what be to a focus on what could be gerd add that wait and see mean wait and die love that by the way he urge leader to focus on 2020 and build a plan to participate in that future emphasize the question be no long what if but what when we be enter an era where the impossible be doable and the headline for that era be exponential convergent combinatorial and inter dependent — word that should be a key part of the leadership lexicon go forward here be some snapshot from his presentation gerd then summarize the session as follow the future be exponential combinatorial and interdependent the soon we can adjust our thinking lateral the well we will be at design our future my take gerd hit on a key point leader must think differently there be very little in a leader s collective experience that can guide they through the type of change ahead — it require we all to think differently when look at ai consider try ia first intelligent assistance augmentation my take these consideration allow we to create the future in a way that avoid unintended consequence technology as a supplement not a replacement efficiency and cost reduction base on automation ai ia and robotization be good story but not the final destination we need to go beyond the 7 ation and inevitable abundance to create new value that can not be easily automate my take future thinking be critical for we to be effective here we have to have a sense as to where all of this be head if we be to effectively create new source of value we win t just need well algorithm — we also need strong humarithm I e value ethic standard principle and social contract my take gerd be an evangelist for create our future in a way that avoid hellish outcome — and kudo to he for be that voice the good way to predict the future be to create it alan kay my take our context when we think about the future put it year away and that be just not the case anymore what we think will take ten year be likely to happen in two we can t create the future if we don t focus on it through an exponential lens originally publish at frankdiana wordpress com on september 10 2015 from a quick cheer to a stand ovation clap to show how much you enjoy this story tcs executive focus on the rapid evolution of society and business fascinate by the view of the world in the next decade and beyond https frankdiana net
Rand Hindi,693,12,https://medium.com/snips-ai/how-artificial-intelligence-will-make-technology-disappear-503cd88e1e6a?source=tag_archive---------8----------------,how artificial intelligence will make technology disappear,this be a redacted transcript of a tedx talk I give last april at ecole polytechnique in france the video can be see on youtube here enjoy ; last march I be in costa rica with my girlfriend spend our day between beautiful beach and jungle full of exotic animal there be barely any connectivity and we be immerse in nature in a way that we could never be in a big city it feel great but in the evening when we get back to the hotel and connect to the wifi our phone would immediately start push an entire day s worth of notification constantly interrupt our special time together it interrupt we while watch the sunset while sip a cocktail while have dinner while have an intimate moment it take emotional time away from we and it s not just that our phone vibrate it s also that we keep check they to see if we have receive anything as if we have some sort of compulsive addiction to it those rare message that be highly rewarding like be notify that ashton kutcher just tweet this article make consciously unplug impossible just like pavlov s dog before we we have become condition in this case though it have get so out of control that today 9 out of 10 people experience phantom vibration which be when you think your phone vibrate in your pocket whereas in fact it didn t how do this happen back in 1990 we didn t have any connect device this be the unplugged era there be no push notification no interruption nada thing be analog thing be human around 1995 the internet start take off and our computer become connect with it come email and the infamous you ve get mail notification we start get interrupt by people company and spammer send we electronic message at random moment 10 year later we enter the mobile era this time it be not 1 but 3 device that be connect a computer a phone and a tablet the trouble be that since these device don t know which one you be currently use the default strategy have be to push all notification on all device like when someone call you on your phone and it also ring on your computer and actually keep ring after you ve answer it on one of your device and it s not just notification ; access a service and finding content be equally frustrating on mobile device with those million of app and tiny keyboard if we take notification and the need for explicit interaction as a proxy for technological friction then each connected device add more of it unfortunately this be about to get much bad since the number of connect device be increase exponentially this year in 2015 we be officially enter what be call the internet of thing era that s when your watch fridge car and lamp be connect it be expect that there will be more than 100 billion connect device by 2025 or 14 for every person on this planet just imagine what it will feel like to interact manually and receive notification simultaneously on 14 device that s definitely not the future we be promise there be hope though there be hope that artificial intelligence will fix this not the one elon musk refer to that will enslave we all but rather a human centric domain of a I call context awareness which be about give device the ability to adapt to our current situation it s about figure out which device to push notification on it s about figure out you be late for a meeting and notify people for you it s about figure out you be on a date and deactivate your non urgent notification it s about give you back the freedom to experience the real world again when you look at the trend in the capability of a I what you see it that it take a bit long to start but when it do it grow much fast we already have a i s that can learn to play video game and beat world champion so it s just a matter of time before they reach human level intelligence there be an inflexion point and we just cross it take the connected device curve and subtract the one for a I we see that the overall friction keep increase over the next few year until the point where a I become so capable that this friction flip around and quickly disappear in this era call ubiquitous computing add new connect device do not add friction it actually add value for example our phone and computer will be smart enough to know where to route the notification our car will drive themselves already know the destination our bed will be monitor our sleep and anticipate when we will be wake up so that we have freshly brew coffee ready in the kitchen it will also connect with the accelerometer in our phone and the electricity socket to determine how many people be in the bed and adjust accordingly our alarm clock win t need to be set ; they will be connect to our calendar and bed to determine when we fall asleep and when we need to wake up all of this can also be aggregate offer public transport operator access to predict passenger flow so that there be always enough train run traffic light will adjust base on self drive car plan route power plant will produce just enough electricity saving cost and the environment smart city smart home smart grid they be all just consequence of have ubiquitous computing by the time this happen technology will have become so deeply integrate in our life and ourselves that we simply win t notice it anymore artificial intelligence will have make technology disappear from our consciousness and the world will feel unplugged again I know this sound crazy but there be historical example of other technology that follow a similar pattern for example back in the 1800s electricity be very tangible it be expensive hard to produce would cut all the time and be dangerous you would get electrocute and your house could catch fire back then people actually believe that oil lamp be safe but as electricity mature it become cheap more reliable and safe eventually it be everywhere in our wall lamp car phone and body it become ubiquitous and we stop notice it today the exact same thing be happen with connect device build this ubiquitous computing future rely on give device the ability to sense and react to the current context which be call context awareness a good way to think about it be through the combination of 4 layer the device layer which be about make device talk to each other ; the individual layer which encompass everything relate to a particular person such as his location history calendar email or health record ; the social layer which model the relationship between individual and finally the environmental layer which be everything else such as the weather the building the street tree and car for example to model the social layer we can look at the email that be send and receive by someone which give we an indication of social connection strength between a group of people the graph show above be extract from my professional email account use the mit immersion tool over a period of 6 month the huge green bubble be one of my co founder which send way too many email as be the red bubble the other fairly large one be other people in my team that I work closely with but what s interesting be that we can also see who in my network work together as they will tend to be include together in email thread and thus form cluster in this graph if you add some contextual information such as the activity I be engage in or the type of language be use in the email you can determine the nature of the relationship I have with each person personal professional intimate as well as its degree and if you now take the difference in these pattern over time you can detect major event such as change job close an investment round launch a new product or hire key people of course all this can be do on social graph as well as professional one now that we have a well representation of someone s social connection we can use it to perform well natural language processing nlp of calendar event by disambiguate event like chat with michael which would then assign a high probability to my co founder but a calendar win t help we figure out habit such as go to the gym after work or hang out in a specific neighborhood on friday evening for that we need another source of data geolocation by monitor our location over time and detect the place we have be to we can understand our habit and thus predict what we will be do next in fact know the exact place we be at be essential to predict our intention since most of the thing we do with our device be base on what we be do in the real world unfortunately location be very noisy and we never know exactly where someone be for example below I be have lunch in san francisco and this be what my phone record while I be not move clearly it be impossible to know where I actually be to circumvent this problem we can score each place accord to the current context for example we be more likely to be at a restaurant during lunch time than at a nightclub if we then combine this with a user specific model base on their location history we can achieve very high level of accuracy for example if I have be to a starbuck in the past it will increase the probability that I be there now as well as the probability of any other coffee shop and because we now know that I be in a restaurant my device can surface the app and information that be relevant to this particular place such as review or mobile payment app accept there if I be at the gym it would be my sport app if I be home it would be my leisure and home automation app if we combine this timeline of place with the phone s accelerometer pattern we can then determine the transportation mode that be take between those place with this our connected watch could now tell we to stand up when it detect we be still stop at a rest area when it detect we be drive or tell we where the close bike stand be when cycle these individual transit pattern can then be aggregate over several thousand user to recreate very precise population flow in the city s infrastructure as we have do below for paris not only do it give we an indication of how many people transit in each station it also give we the route they have be take where they change train or if they walk between station combine this with datum from the city — concert office and residential building population demographic — enable you to see how each factor impact public transport and even predict how many people will be boarding train throughout the day it can then be use to notify commuter that they should take a different train if they want to sit on their way home and dynamically adjust the train schedule maximize the efficiency of the network both in term of energy save and comfort and it s not just public transport the same model and datum can be use to predict queue in post office by take into account hyperlocal factor such as when the welfare check be be pay the bank holiday the proximity of other post office and the staff strike this be show below where the blue curve be the real load and the orange one be the predict load this model can be use to notify people of the good time to drop and pickup their parcel which result in well yield management and customer service it can also be use to plan the construction of new post office by size they accordingly and since a post office be just a retail store everything that work here can work for all retailer grocery store supermarket shoe shop etc it could then be plug into our device enable they to optimize our shopping schedule and make sure we never queue again this contextual modeling approach be in fact so powerful that it can even predict the risk of car accident just by look at feature such as the street topologie the proximity of bar that just close the road surface or the weather since these feature be generalizable throughout the city we can make prediction even in place where there be never a car accident for example here we can see that our model correctly detect trafalgar square as be dangerous even though nowhere do we explicitly say so it discover it automatically from the datum itself it be even able to identify the impact of cultural event such as st patrick s day or new year s eve how cool would it be if our self drive car could take this into account if we combine all these different layer — personal social environmental — we can recreate a highly contextualize timeline of what we have be do throughout the day which in turn enable we to predict what our intention be make our device able to figure out our current context and predict our intention be the key to build truly intelligent product with that in mind our team have be prototype a new kind of smartphone interface one that leverage this contextual intelligence to anticipate which service and app be need at any give time link directly to the relevant content inside they it s not yet perfect but it s a first step towards our long term vision — and it certainly save a lot of time swipe and tap one thing in particular that we be really proud of be that we be able to build privacy by design full post come soon it be a tremendous engineering challenge but we be now run all our algorithm directly on the device whether it s the machine learning classifier the signal process the natural language processing or the email mining they be all confine to our smartphone and never upload to our server basically it mean we can now harness the full power of a i without compromise our privacy something that have never be achieve before it s important to understand that this be not just about build some cool tech or the next viral app nor be it about make our future look like a science fiction movie it s actually about make technology disappear into the background so that we can regain the freedom to spend quality time with the people we care about if you enjoy this article it would really help if you hit recommend below and share it on twitter we be @randhindi & @snip from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur & ai researcher work on make technology disappear ceo @ snip ai # ai # privacy and # blockchain follow http instagram com randhindi this publication feature the article write by the snip team fellow and friend snip start as an ai lab in 2013 and now build private by design decentralize open source voice assistant
samim,323,8,https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0?source=tag_archive---------9----------------,obama rnn — machine generate political speech samim medium,political speech be among the most powerful tool leader use to influence entire population throughout history political speech have be use to start war end empire fuel movement & inspire the masse political speech apply many of the trick find in the field of social engineering congruent communication intentional body language neuro linguistic programming humanbuffer overflow and more read more about the art of human hacking here in recent year barack obama have emerge as one of the most memorable and effective political speaker on the world stage message like hope and yes we can have clearly leave a mark on our collective consciousness since 2007 obama s highly skilled speech writer have write over 4 3megabyte or 730895 word of text not count interview and debate all of obama s speech be conveniently readable here with powerful artificial intelligence machine learning librarie become readily available as open source it seem obvious to apply they to speech write a particularly interesting class of algorithm be recurrent neural network rnn recently andrej karpathy a cs phd student at stanford have release char rnn a multi layer recurrent neural network for character level language model the library take an arbitrary text file as input and learn to predict the next character in the sequence as the result be pretty amazing many interesting experiment have spring up range from compose music rap writing cooking recipe and even re write the bible step 1 be to feed the model datum the more the well for this I write a web crawler in python that gather all publicly available obama speech parse out the text and remove any interview debate step 2 be to train the model on the collect text training an rnn take a bit of fiddle as I painfully find out while train a model on 500 mb of classical music midi file mozart rnn be wild luckily the standard setting that andrej suggest be a good starting point for the obama rnn step 3 be to test the model which automatically generate an unlimited amount of new speech in the vein of obama ́s previous speeche the model can be seed with a text from which it will start the sequence e g war on terror and a temperature which make the output more conservative or diverse at cost of more mistake here be a selection of some of my favorite speech the obama rnn generate so far keep in mind this be a just a quick hack project with more time & effort the result can be improve one of the most hilarious pattern to emerge be that the obama rnn really love to politely say good afternoon good day god bless you good bless the united states of america thank you I do a test combine obamas speech with other famous speech from the 20st century include everything from mother theresa malcom x to mussolini and hitler this give we an rather insane amalgam of human thought see through the eye of a machine a story for an other day on this note god bless you good bless the united states of america thank you you can run your own obama rnn by follow these instruction get in touch here https twitter com samim | http samim io from a quick cheer to a stand ovation clap to show how much you enjoy this story designer & code magician work at the intersection of hci machine learning & creativity building tool for enlightenment narrative engineering
Adam Geitgey,10.4K,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=tag_archive---------4----------------,machine learning be fun part 2 adam geitgey medium,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in italiano español français türkçe русский 한국어 português فارسی tiếng việt or 普通话 in part 1 we say that machine learning be use generic algorithm to tell you something interesting about your datum without write any code specific to the problem you be solve if you haven t already read part 1 read it now this time we be go to see one of these generic algorithm do something really cool — create video game level that look like they be make by human we ll build a neural network feed it exist super mario level and watch new one pop out just like part 1 this guide be for anyone who be curious about machine learning but have no idea where to start the goal be be accessible to anyone — which mean that there s a lot of generalization and we skip lot of detail but who care if this get anyone more interested in ml then mission accomplish back in part 1 we create a simple algorithm that estimate the value of a house base on its attribute give datum about a house like this we end up with this simple estimation function in other word we estimate the value of the house by multiply each of its attribute by a weight then we just add those number up to get the house s value instead of use code let s represent that same function as a simple diagram however this algorithm only work for simple problem where the result have a linear relationship with the input what if the truth behind house price isn t so simple for example maybe the neighborhood matter a lot for big house and small house but doesn t matter at all for medium sized house how could we capture that kind of complicated detail in our model to be more clever we could run this algorithm multiple time with different of weight that each capture different edge case now we have four different price estimate let s combine those four price estimate into one final estimate we ll run they through the same algorithm again but use another set of weight our new super answer combine the estimate from our four different attempt to solve the problem because of this it can model more case than we could capture in one simple model let s combine our four attempt to guess into one big diagram this be a neural network each node know how to take in a set of input apply weight to they and calculate an output value by chain together lot of these node we can model complex function there s a lot that I m skip over to keep this brief include feature scaling and the activation function but the most important part be that these basic idea click it s just like lego we can t model much with one single lego block but we can model anything if we have enough basic lego block to stick together the neural network we ve see always return the same answer when you give it the same input it have no memory in programming term it s a stateless algorithm in many case like estimate the price of house that s exactly what you want but the one thing this kind of model can t do be respond to pattern in datum over time imagine I hand you a keyboard and ask you to write a story but before you start my job be to guess the very first letter that you will type what letter should I guess I can use my knowledge of english to increase my odd of guess the right letter for example you will probably type a letter that be common at the beginning of word if I look at story you write in the past I could narrow it down far base on the word you usually use at the beginning of your story once I have all that datum I could use it to build a neural network to model how likely it be that you would start with any give letter our model might look like this but let s make the problem hard let s say I need to guess the next letter you be go to type at any point in your story this be a much more interesting problem let s use the first few word of ernest hemingway s the sun also rise as an example what letter be go to come next you probably guess n — the word be probably go to be box we know this base on the letter we ve already see in the sentence and our knowledge of common word in english also the word middleweight give we an extra clue that we be talk about box in other word it s easy to guess the next letter if we take into account the sequence of letter that come right before it and combine that with our knowledge of the rule of english to solve this problem with a neural network we need to add state to our model each time we ask our neural network for an answer we also save a set of our intermediate calculation and re use they the next time as part of our input that way our model will adjust its prediction base on the input that it have see recently keep track of state in our model make it possible to not just predict the most likely first letter in the story but to predict the most likely next letter give all previous letter this be the basic idea of a recurrent neural network we be update the network each time we use it this allow it to update its prediction base on what it see most recently it can even model pattern over time as long as we give it enough of a memory predict the next letter in a story might seem pretty useless what s the point one cool use might be auto predict for a mobile phone keyboard but what if we take this idea to the extreme what if we ask the model to predict the next most likely character over and over — forever we d be ask it to write a complete story for we we see how we could guess the next letter in hemingway s sentence let s try generate a whole story in the style of hemingway to do this we be go to use the recurrent neural network implementation that andrej karpathy write andrej be a deep learning researcher at stanford and he write an excellent introduction to generate text with rnn you can view all the code for the model on github we ll create our model from the complete text of the sun also rise — 362 239 character use 84 unique letter include punctuation uppercase lowercase etc this datum set be actually really small compare to typical real world application to generate a really good model of hemingway s style it would be much well to have at several time as much sample text but this be good enough to play around with as an example as we just start to train the rnn it s not very good at predict letter here s what it generate after a 100 loop of training you can see that it have figure out that sometimes word have space between they but that s about it after about 1000 iteration thing be look more promising the model have start to identify the pattern in basic sentence structure it s add period at the end of sentence and even quote dialog a few word be recognizable but there s also still a lot of nonsense but after several thousand more training iteration it look pretty good at this point the algorithm have capture the basic pattern of hemingway s short direct dialog a few sentence even sort of make sense compare that with some real text from the book even by only look for pattern one character at a time our algorithm have reproduce plausible looking prose with proper formatting that be kind of amazing we don t have to generate text completely from scratch either we can seed the algorithm by supply the first few letter and just let it find the next few letter for fun let s make a fake book cover for our imaginary book by generate a new author name and a new title use the seed text of er he and the s not bad but the really mind blow part be that this algorithm can figure out pattern in any sequence of datum it can easily generate real looking recipe or fake obama speech but why limit ourselves human language we can apply this same idea to any kind of sequential datum that have a pattern in 2015 nintendo release super mario makertm for the wii u gaming system this game let you draw out your own super mario brother level on the gamepad and then upload they to the internet so you friend can play through they you can include all the classic power up and enemy from the original mario game in your level it s like a virtual lego set for people who grow up play super mario brother can we use the same model that generate fake hemingway text to generate fake super mario brother level first we need a datum set for train our model let s take all the outdoor level from the original super mario brothers game release in 1985 this game have 32 level and about 70 % of they have the same outdoor style so we ll stick to those to get the design for each level I take an original copy of the game and write a program to pull the level design out of the game s memory super mario bros be a 30 year old game and there be lot of resource online that help you figure out how the level be store in the game s memory extract level datum from an old video game be a fun programming exercise that you should try sometime here s the first level from the game which you probably remember if you ever play it if we look closely we can see the level be make of a simple grid of object we could just as easily represent this grid as a sequence of character with one character represent each object we ve replace each object in the level with a letter and so on use a different letter for each different kind of object in the level I end up with text file that look like this look at the text file you can see that mario level don t really have much of a pattern if you read they line by line the pattern in a level really emerge when you think of the level as a series of column so in order for the algorithm to find the pattern in our datum we need to feed the datum in column by column figure out the most effective representation of your input datum call feature selection be one of the key of use machine learning algorithm well to train the model I need to rotate my text file by 90 degree this make sure the character be feed into the model in an order where a pattern would more easily show up just like we see when create the model of hemingway s prose a model improve as we train it after a little training our model be generate junk it sort of have an idea that s and = s should show up a lot but that s about it it hasn t figure out the pattern yet after several thousand iteration it s start to look like something the model have almost figure out that each line should be the same length it have even start to figure out some of the logic of mario the pipe in mario be always two block wide and at least two block high so the p s in the datum should appear in 2x2 cluster that s pretty cool with a lot more training the model get to the point where it generate perfectly valid datum let s sample an entire level s worth of datum from our model and rotate it back horizontal this data look great there be several awesome thing to notice finally let s take this level and recreate it in super mario maker play it yourself if you have super mario maker you can play this level by bookmarke it online or by look it up use level code 4ac9 0000 0157 f3c3 the recurrent neural network algorithm we use to train our model be the same kind of algorithm use by real world company to solve hard problem like speech detection and language translation what make our model a toy instead of cut edge be that our model be generate from very little datum there just aren t enough level in the original super mario brothers game to provide enough datum for a really good model if we could get access to the hundred of thousand of user create super mario maker level that nintendo have we could make an amazing model but we can t — because nintendo win t let we have they big company don t give away their datum for free as machine learning become more important in more industry the difference between a good program and a bad program will be how much datum you have to train your model that s why company like google and facebook need your datum so badly for example google recently open source tensorflow its software toolkit for build large scale machine learning application it be a pretty big deal that google give away such important capable technology for free this be the same stuff that power google translate but without google s massive trove of datum in every language you can t create a competitor to google translate data be what give google its edge think about that the next time you open up your google map location history or facebook location history and notice that it store every place you ve ever be in machine learn there s never a single way to solve a problem you have limitless option when decide how to pre process your datum and which algorithm to use often combine multiple approach will give you well result than any single approach reader have send I link to other interesting approach to generate super mario level if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 3 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Arthur Juliani,9K,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------5----------------,simple reinforcement learning with tensorflow part 0 q learning with table and neural network,for this tutorial in my reinforcement learning series we be go to be explore a family of rl algorithms call q learning algorithms these be a little different than the policy base algorithm that will be look at in the the follow tutorial part 1 3 instead of start with a complex and unwieldy deep neural network we will begin by implement a simple lookup table version of the algorithm and then show how to implement a neural network equivalent use tensorflow give that we be go back to basic it may be good to think of this as part 0 of the series it will hopefully give an intuition into what be really happen in q learning that we can then build on go forward when we eventually combine the policy gradient and q learning approach to build state of the art rl agent if you be more interested in policy network or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient method which attempt to learn function which directly map an observation to an action q learning attempt to learn the value of be in a give state and take a specific action there while both approach ultimately allow we to take intelligent action give a situation the mean of get to that action differ significantly you may have hear about deepq network which can play atari game these be really just large and more complex implementation of the q learning algorithm we be go to discuss here for this tutorial we be go to be attempt to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provide an easy way for people to experiment with their learn agent in an array of provide toy game the frozenlake environment consist of a 4x4 grid of block each one either be the start block the goal block a safe frozen block or a dangerous hole the objective be to have an agent learn to navigate from the start to the goal without move onto a hole at any give time the agent can choose to move either up down left or right the catch be that there be a wind which occasionally blow the agent onto a space they didn t choose as such perfect performance every time be impossible but learn to avoid the hole and reach the goal be certainly still doable the reward at every step be 0 except for enter the goal which provide a reward of 1 thus we will need an algorithm that learn long term expect reward this be exactly what q learning be design to provide in it s simple implementation q learning be a table of value for every state row and action column possible in the environment within each cell of the table we learn a value for how good it be to take a give action within a give state in the case of the frozenlake environment we have 16 possible state one for each block and 4 possible action the four direction of movement give we a 16x4 table of q value we start by initialize the table to be uniform all zero and then as we observe the reward we obtain for various action we update the table accordingly we make update to our q table use something call the bellman equation which state that the expect long term reward for a give action be equal to the immediate reward from the current action combine with the expect reward from the good future action take at the follow state in this way we reuse our own q table when estimate how to update our table for future action in equation form the rule look like this this say that the q value for a give state s and action a should represent the current reward r plus the maximum discount γ future reward expect accord to our own table for the next state s we would end up in the discount variable allow we to decide how important the possible future reward be compare to the present reward by update in this way the table slowly begin to obtain accurate measure of the expect future reward for a give action in a give state below be a python walkthrough of the q table algorithm implement in the frozenlake environment thank to praneet d for find the optimal hyperparameter for this approach now you may be think table be great but they don t really scale do they while it be easy to have a 16x4 table for a simple grid world the number of possible state in any modern game or real world environment be nearly infinitely large for most interesting problem table simply don t work we instead need some way to take a description of our state and produce q value for action without a table that be where neural network come in by act as a function approximator we can take any number of possible state that can be represent as a vector and learn to map they to q value in the case of the frozenlake example we will be use a one layer network which take the state encode in a one hot vector 1x16 and produce a vector of 4 q value one for each action such a simple network act kind of like a glorify table with the network weight serve as the old cell the key difference be that we can easily expand the tensorflow network with add layer activation function and different input type whereas all that be impossible with a regular table the method of update be a little different as well instead of directly update our table with a network we will be use backpropagation and a loss function our loss function will be sum of square loss where the difference between the current predict q value and the target value be compute and the gradient pass through the network in this case our q target for the choose action be the equivalent to the q value compute in equation 1 above below be the tensorflow walkthrough of implement our simple q network while the network learn to solve the frozenlake problem it turn out it doesn t do so quite as efficiently as the q table while neural network allow for great flexibility they do so at the cost of stability when it come to q learning there be a number of possible extension to our simple q network which allow for great performance and more robust learn two trick in particular be refer to as experience replay and freeze target network those improvement and other tweak be the key to get atari play deep q network and we will be explore those addition in the future for more info on the theory behind q learning see this great post by tambet matiisen I hope this tutorial have be helpful for those curious about how to implement simple q learning algorithms if this post have be valuable to you please consider donate to help support future tutorial article and implementation any contribution be greatly appreciate if you d like to follow my work on deep learning ai and cognitive science follow I on medium @arthur juliani or on twitter @awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @unity3d & cognitive neuroscience phd student explore frontier technology through the lens of artificial intelligence datum science and the shape of thing to come
Adam Geitgey,6.8K,11,https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=tag_archive---------6----------------,machine learning be fun part 6 how to do speech recognition with deep learning,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 普通话 한국어 tiếng việt or русский speech recognition be invade our life it s build into our phone our game console and our smart watch it s even automate our home for just $ 50 you can get an amazon echo dot — a magic box that allow you to order pizza get a weather report or even buy trash bag — just by speak out loud the echo dot have be so popular this holiday season that amazon can t seem to keep they in stock but speech recognition have be around for decade so why be it just now hit the mainstream the reason be that deep learning finally make speech recognition accurate enough to be useful outside of carefully control environment andrew ng have long predict that as speech recognition go from 95 % accurate to 99 % accurate it will become a primary way that we interact with computer the idea be that this 4 % accuracy gap be the difference between annoyingly unreliable and incredibly useful thank to deep learning we re finally crest that peak let s learn how to do speech recognition with deep learning if you know how neural machine translation work you might guess that we could simply feed sound recording into a neural network and train it to produce text that s the holy grail of speech recognition with deep learning but we aren t quite there yet at least at the time that I write this — I bet that we will be in a couple of year the big problem be that speech vary in speed one person might say hello very quickly and another person might say heeeelllllllllllllooooo very slowly produce a much long sound file with much more datum both both sound file should be recognize as exactly the same text — hello automatically align audio file of various length to a fix length piece of text turn out to be pretty hard to work around this we have to use some special trick and extra precessing in addition to a deep neural network let s see how it work the first step in speech recognition be obvious — we need to feed sound wave into a computer in part 3 we learn how to take an image and treat it as an array of number so that we can feed directly into a neural network for image recognition but sound be transmit as wave how do we turn sound wave into number let s use this sound clip of I say hello sound wave be one dimensional at every moment in time they have a single value base on the height of the wave let s zoom in on one tiny part of the sound wave and take a look to turn this sound wave into number we just record of the height of the wave at equally space point this be call sample we be take a reading thousand of time a second and record a number represent the height of the sound wave at that point in time that s basically all an uncompressed wav audio file be cd quality audio be sample at 44 1khz 44 100 reading per second but for speech recognition a sampling rate of 16khz 16 000 sample per second be enough to cover the frequency range of human speech let sample our hello sound wave 16 000 time per second here s the first 100 sample you might be think that sampling be only create a rough approximation of the original sound wave because it s only take occasional reading there s gap in between our reading so we must be lose datum right but thank to the nyquist theorem we know that we can use math to perfectly reconstruct the original sound wave from the space out sample — as long as we sample at least twice as fast as the high frequency we want to record I mention this only because nearly everyone get this wrong and assume that use high sampling rate always lead to well audio quality it doesn t < end rant > we now have an array of number with each number represent the sound wave s amplitude at 1 16 000th of a second interval we could feed these number right into a neural network but try to recognize speech pattern by process these sample directly be difficult instead we can make the problem easy by do some pre processing on the audio datum let s start by group our sample audio into 20 millisecond long chunk here s our first 20 millisecond of audio I e our first 320 sample plot those number as a simple line graph give we a rough approximation of the original sound wave for that 20 millisecond period of time this recording be only 1 50th of a second long but even this short recording be a complex mish mash of different frequency of sound there s some low sound some mid range sound and even some high pitch sound sprinkle in but take all together these different frequency mix together to make up the complex sound of human speech to make this datum easy for a neural network to process we be go to break apart this complex sound wave into it s component part we ll break out the low pitch part the next low pitch part and so on then by add up how much energy be in each of those frequency band from low to high we create a fingerprint of sort for this audio snippet imagine you have a recording of someone play a c major chord on a piano that sound be the combination of three musical note — c e and g — all mixed together into one complex sound we want to break apart that complex sound into the individual note to discover that they be c e and g this be the exact same idea we do this use a mathematic operation call a fouri transform it break apart the complex sound wave into the simple sound wave that make it up once we have those individual sound wave we add up how much energy be contain in each one the end result be a score of how important each frequency range be from low pitch I e bass note to high pitch each number below represent how much energy be in each 50hz band of our 20 millisecond audio clip but this be a lot easy to see when you draw this as a chart if we repeat this process on every 20 millisecond chunk of audio we end up with a spectrogram each column from leave to right be one 20ms chunk a spectrogram be cool because you can actually see musical note and other pitch pattern in audio datum a neural network can find pattern in this kind of datum more easily than raw sound wave so this be the datum representation we ll actually feed into our neural network now that we have our audio in a format that s easy to process we will feed it into a deep neural network the input to the neural network will be 20 millisecond audio chunk for each little audio slice it will try to figure out the letter that correspond the sound currently be speak we ll use a recurrent neural network — that be a neural network that have a memory that influence future prediction that s because each letter it predict should affect the likelihood of the next letter it will predict too for example if we have say hel so far it s very likely we will say lo next to finish out the word hello it s much less likely that we will say something unpronounceable next like xyz so have that memory of previous prediction help the neural network make more accurate prediction go forward after we run our entire audio clip through the neural network one chunk at a time we ll end up with a mapping of each audio chunk to the letter most likely speak during that chunk here s what that mapping look like for I say hello our neural net be predict that one likely thing I say be hhhee_ll_lllooo but it also think that it be possible that I say hhhuu_ll_lllooo or even aaauu_ll_lllooo we have some step we follow to clean up this output first we ll replace any repeat character a single character then we ll remove any blank that leave we with three possible transcription — hello hullo and aullo if you say they out loud all of these sound similar to hello because it s predict one character at a time the neural network will come up with these very sound out transcription for example if you say he would not go it might give one possible transcription as he wud net go the trick be to combine these pronunciation base prediction with likelihood score base on large database of write text book news article etc you throw out transcription that seem the least likely to be real and keep the transcription that seem the most realistic of our possible transcription hello hullo and aullo obviously hello will appear more frequently in a database of text not to mention in our original audio base training datum and thus be probably correct so we ll pick hello as our final transcription instead of the other do you might be think but what if someone say hullo it s a valid word maybe hello be the wrong transcription of course it be possible that someone actually say hullo instead of hello but a speech recognition system like this train on american english will basically never produce hullo as the transcription it s just such an unlikely thing for a user to say compare to hello that it will always think you be say hello no matter how much you emphasize the u sound try it out if your phone be set to american english try to get your phone s digital assistant to recognize the world hullo you can t it refuse it will always understand it as hello not recognize hullo be a reasonable behavior but sometimes you ll find annoying case where your phone just refuse to understand something valid you be say that s why these speech recognition model be always be retrain with more datum to fix these edge case one of the cool thing about machine learning be how simple it sometimes seem you get a bunch of datum feed it into a machine learn algorithm and then magically you have a world class ai system run on your game laptop s video card right that sort of true in some case but not for speech recognize speech be a hard problem you have to overcome almost limitless challenge bad quality microphone background noise reverb and echo accent variation and on and on all of these issue need to be present in your training datum to make sure the neural network can deal with they here s another example do you know that when you speak in a loud room you unconsciously raise the pitch of your voice to be able to talk over the noise human have no problem understand you either way but neural network need to be train to handle this special case so you need training datum with people yell over noise to build a voice recognition system that perform on the level of siri google now or alexa you will need a lot of training datum — far more datum than you can likely get without hire hundred of people to record it for you and since user have low tolerance for poor quality voice recognition system you can t skimp on this no one want a voice recognition system that work 80 % of the time for a company like google or amazon hundred of thousand of hour of spoken audio record in real life situation be gold that s the single big thing that separate their world class speech recognition system from your hobby system the whole point of put google now and siri on every cell phone for free or sell $ 50 alexa unit that have no subscription fee be to get you to use they as much as possible every single thing you say into one of these system be record forever and use as training datum for future version of speech recognition algorithm that s the whole game don t believe I if you have an android phone with google now click here to listen to actual recording of yourself say every dumb thing you ve ever say into it so if you be look for a start up idea I wouldn t recommend try to build your own speech recognition system to compete with google instead figure out a way to get people to give you recording of themselves talk for hour the datum can be your product instead if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 7 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Adam Geitgey,5.8K,16,https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa?source=tag_archive---------7----------------,machine learning be fun part 5 language translation with deep learning and the magic of sequence,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 普通话 русский 한국어 tiếng việt or italiano we all know and love google translate the website that can instantly translate between 100 different human language as if by magic it be even available on our phone and smartwatche the technology behind google translate be call machine translation it have change the world by allow people to communicate when it wouldn t otherwise be possible but we all know that high school student have be use google translate to umm assist with their spanish homework for 15 year isn t this old news it turn out that over the past two year deep learning have totally rewrite our approach to machine translation deep learning researcher who know almost nothing about language translation be throw together relatively simple machine learning solution that be beat the good expert build language translation system in the world the technology behind this breakthrough be call sequence to sequence learn it s very powerful technique that be use to solve many kind problem after we see how it be use for translation we ll also learn how the exact same algorithm can be use to write ai chat bot and describe picture let s go so how do we program a computer to translate human language the simple approach be to replace every word in a sentence with the translate word in the target language here s a simple example of translate from spanish to english word by word this be easy to implement because all you need be a dictionary to look up each word s translation but the result be bad because it ignore grammar and context so the next thing you might do be start add language specific rule to improve the result for example you might translate common two word phrase as a single group and you might swap the order noun and adjective since they usually appear in reverse order in spanish from how they appear in english that work if we just keep add more rule until we can handle every part of grammar our program should be able to translate any sentence right this be how the early machine translation system work linguist come up with complicated rule and program they in one by one some of the smart linguist in the world labor for year during the cold war to create translation system as a way to interpret russian communication more easily unfortunately this only work for simple plainly structured document like weather report it didn t work reliably for real world document the problem be that human language doesn t follow a fix set of rule human language be full of special case regional variation and just flat out rule break the way we speak english more influence by who invade who hundred of year ago than it be by someone sit down and define grammar rule after the failure of rule base system new translation approach be develop use model base on probability and statistic instead of grammar rule build a statistic base translation system require lot of training datum where the exact same text be translate into at least two language this double translate text be call parallel corpora in the same way that the rosetta stone be use by scientist in the 1800 to figure out egyptian hieroglyph from greek computer can use parallel corpus to guess how to convert text from one language to another luckily there s lot of double translate text already sit around in strange place for example the european parliament translate their proceeding into 21 language so researcher often use that datum to help build translation system the fundamental difference with statistical translation system be that they don t try to generate one exact translation instead they generate thousand of possible translation and then they rank those translation by likely each be to be correct they estimate how correct something be by how similar it be to the training datum here s how it work first we break up our sentence into simple chunk that can each be easily translate next we will translate each of these chunk by find all the way human have translate those same chunk of word in our training datum it s important to note that we be not just look up these chunk in a simple translation dictionary instead we be see how actual people translate these same chunk of word in real world sentence this help we capture all of the different way they can be use in different context some of these possible translation be use more frequently than other base on how frequently each translation appear in our training datum we can give it a score for example it s much more common for someone to say quiero to mean I want than to mean I try so we can use how frequently quiero be translate to I want in our training datum to give that translation more weight than a less frequent translation next we will use every possible combination of these chunk to generate a bunch of possible sentence just from the chunk translation we list in step 2 we can already generate nearly 2 500 different variation of our sentence by combine the chunk in different way here be some example but in a real world system there will be even more possible chunk combination because we ll also try different ordering of word and different way of chunk the sentence now need to scan through all of these generate sentence to find the one that be that sound the most human to do this we compare each generate sentence to million of real sentence from book and news story write in english the more english text we can get our hand on the well take this possible translation it s likely that no one have ever write a sentence like this in english so it would not be very similar to any sentence in our datum set we ll give this possible translation a low probability score but look at this possible translation this sentence will be similar to something in our training set so it will get a high probability score after try all possible sentence we ll pick the sentence that have the most likely chunk translation while also be the most similar overall to real english sentence our final translation would be I want to go to the pretty beach not bad statistical machine translation system perform much well than rule base system if you give they enough training datum franz josef och improve on these idea and use they to build google translate in the early 2000 machine translation be finally available to the world in the early day it be surprising to everyone that the dumb approach to translate base on probability work well than rule base system design by linguist this lead to a somewhat mean say among researcher in the 80 statistical machine translation system work well but they be complicated to build and maintain every new pair of language you want to translate require expert to tweak and tune a new multi step translation pipeline because it be so much work to build these different pipeline trade off have to be make if you be ask google to translate georgian to telegu it have to internally translate it into english as an intermediate step because there s not enough georgain to telegu translation happen to justify invest heavily in that language pair and it might do that translation use a less advanced translation pipeline than if you have ask it for the more common choice of french to english wouldn t it be cool if we could have the computer do all that annoying development work for we the holy grail of machine translation be a black box system that learn how to translate by itself — just by look at training datum with statistical machine translation human be still need to build and tweak the multi step statistical model in 2014 kyunghyun cho s team make a breakthrough they find a way to apply deep learning to build this black box system their deep learning model take in a parallel corpus and and use it to learn how to translate between those two language without any human intervention two big idea make this possible — recurrent neural network and encoding by combine these two idea in a clever way we can build a self learn translation system we ve already talk about recurrent neural network in part 2 but let s quickly review a regular non recurrent neural network be a generic machine learning algorithm that take in a list of number and calculate a result base on previous training neural network can be use as a black box to solve lot of problem for example we can use a neural network to calculate the approximate value of a house base on attribute of that house but like most machine learning algorithm neural network be stateless you pass in a list of number and the neural network calculate a result if you pass in those same number again it will always calculate the same result it have no memory of past calculation in other word 2 + 2 always equal 4 a recurrent neural network or rnn for short be a slightly tweak version of a neural network where the previous state of the neural network be one of the input to the next calculation this mean that previous calculation change the result of future calculation why in the world would we want to do this shouldn t 2 + 2 always equal 4 no matter what we last calculate this trick allow neural network to learn pattern in a sequence of datum for example you can use it to predict the next most likely word in a sentence base on the first few word rnn be useful any time you want to learn pattern in datum because human language be just one big complicated pattern rnn be increasingly use in many area of natural language processing if you want to learn more about rnn you can read part 2 where we use one to generate a fake ernest hemingway book and then use another one to generate fake super mario brothers level the other idea we need to review be encoding we talk about encoding in part 4 as part of face recognition to explain encoding let s take a slight detour into how we can tell two different people apart with a computer when you be try to tell two face apart with a computer you collect different measurement from each face and use those measurement to compare face for example we might measure the size of each ear or the spacing between the eye and compare those measurement from two picture to see if they be the same person you re probably already familiar with this idea from watch any primetime detective show like csi the idea of turn a face into a list of measurement be an example of an encoding we be take raw datum a picture of a face and turn it into a list of measurement that represent it the encoding but like we see in part 4 we don t have to come up with a specific list of facial feature to measure ourselves instead we can use a neural network to generate measurement from a face the computer can do a well job than we in figure out which measurement be well able to differentiate two similar people this be our encoding it let we represent something very complicated a picture of a face with something simple 128 number now compare two different face be much easy because we only have to compare these 128 number for each face instead of compare full image guess what we can do the same thing with sentence we can come up with an encoding that represent every possible different sentence as a series of unique number to generate this encoding we ll feed the sentence into the rnn one word at time the final result after the last word be process will be the value that represent the entire sentence great so now we have a way to represent an entire sentence as a set of unique number we don t know what each number in the encoding mean but it doesn t really matter as long as each sentence be uniquely identify by it s own set of number we don t need to know exactly how those number be generate ok so we know how to use an rnn to encode a sentence into a set of unique number how do that help we here s where thing get really cool what if we take two rnn and hook they up end to end the first rnn could generate the encoding that represent a sentence then the second rnn could take that encoding and just do the same logic in reverse to decode the original sentence again of course be able to encode and then decode the original sentence again isn t very useful but what if and here s the big idea we could train the second rnn to decode the sentence into spanish instead of english we could use our parallel corpora training datum to train it to do that and just like that we have a generic way of convert a sequence of english word into an equivalent sequence of spanish word this be a powerful idea note that we gloss over some thing that be require to make this work with real world datum for example there s additional work you have to do to deal with different length of input and output sentence see bucketing and padding there s also issue with translate rare word correctly if you want to build your own language translation system there s a work demo include with tensorflow that will translate between english and french however this be not for the faint of heart or for those with limited budget this technology be still new and very resource intensive even if you have a fast computer with a high end video card it might take about a month of continuous processing time to train your own language translation system also sequence to sequence language translation technique be improve so rapidly that it s hard to keep up many recent improvement like add an attention mechanism or track context be significantly improve result but these development be so new that there aren t even wikipedia page for they yet if you want to do anything serious with sequence to sequence learn you ll need to keep with new development as they occur so what else can we do with sequence to sequence model about a year ago researcher at google show that you can use sequence to sequence model to build ai bot the idea be so simple that it s amazing it work at all first they capture chat log between google employee and google s tech support team then they train a sequence to sequence model where the employee s question be the input sentence and the tech support team s response be the translation of that sentence when a user interact with the bot they would translate each of the user s message with this system to get the bot s response the end result be a semi intelligent bot that could sometimes answer real tech support question here s part of a sample conversation between a user and the bot from their paper they also try build a chat bot base on million of movie subtitle the idea be to use conversation between movie character as a way to train a bot to talk like a human the input sentence be a line of dialog say by one character and the translation be what the next character say in response this produce really interesting result not only do the bot converse like a human but it display a small bit of intelligence this be only the beginning of the possibility we aren t limit to convert one sentence into another sentence it s also possible to make an image to sequence model that can turn an image into text a different team at google do this by replace the first rnn with a convolutional neural network like we learn about in part 3 this allow the input to be a picture instead of a sentence the rest work basically the same way and just like that we can turn picture into word as long as we have lot and lot of training datum andrej karpathy expand on these idea to build a system capable of describe image in great detail by process multiple region of an image separately this make it possible to build image search engine that be capable of find image that match oddly specific search query there s even researcher work on the reverse problem generate an entire picture base on just a text description just from these example you can start to imagine the possibility so far there have be sequence to sequence application in everything from speech recognition to computer vision I bet there will be a lot more over the next year if you want to learn more in depth about sequence to sequence model and translation here s some recommend resource if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 6 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Chris Dixon,5.3K,12,https://medium.com/@cdixon/eleven-reasons-to-be-excited-about-the-future-of-technology-ef5f9b939cb2?source=tag_archive---------8----------------,eleven reason to be excited about the future of technology,in the year 1820 a person could expect to live less than 35 year 94 % of the global population live in extreme poverty and less that 20 % of the population be literate today human life expectancy be over 70 year less that 10 % of the global population live in extreme poverty and over 80 % of people be literate these improvement be due mainly to advance in technology begin in the industrial age and continue today in the information age there be many exciting new technology that will continue to transform the world and improve human welfare here be eleven of they self drive car exist today that be safe than human drive car in most driving condition over the next 3 5 year they ll get even safe and will begin to go mainstream the world health organization estimate that 1 25 million people die from car relate injury per year half of the death be pedestrian bicyclist and motorcyclist hit by car car be the lead cause of death for people age 15 29 year old just as car reshape the world in the 20th century so will self drive car in the 21st century in most city between 20 30 % of usable space be take up by parking space and most car be park about 95 % of the time self drive car will be in almost continuous use most likely hail from a smartphone app thereby dramatically reduce the need for parking car will communicate with one another to avoid accident and traffic jam and rider will be able to spend commute time on other activity like work education and socialize attempt to fight climate change by reduce the demand for energy haven t work fortunately scientist engineer and entrepreneur have be work hard on the supply side to make clean energy convenient and cost effective due to steady technological and manufacturing advance the price of solar cell have drop 99 5 % since 1977 solar will soon be more cost efficient than fossil fuel the cost of wind energy have also drop to an all time low and in the last decade represent about a third of newly instal we energy capacity forward thinking organization be take advantage of this for example in india there be an initiative to convert airport to self sustain clean energy tesla be make high performance affordable electric car and instal electric charge station worldwide there be hopeful sign that clean energy could soon be reach a tipping point for example in japan there be now more electric charge station than gas station and germany produce so much renewable energy it sometimes produce even more than it can use computer processor only recently become fast enough to power comfortable and convincing virtual and augmented reality experience company like facebook google apple and microsoft be invest billion of dollar to make vr and ar more immersive comfortable and affordable people sometimes think vr and ar will be use only for gaming but over time they will be use for all sort of activity for example we ll use they to manipulate 3 d object to meet with friend and colleague from around the world and even for medical application like treat phobia or help rehabilitate paralysis victim vr and ar have be dream about by science fiction fan for decade in the next few year they ll finally become a mainstream reality gps start out as a military technology but be now use to hail taxi get mapping direction and hunt pokémon likewise drone start out as a military technology but be increasingly be use for a wide range of consumer and commercial application for example drone be be use to inspect critical infrastructure like bridge and power line to survey area strike by natural disaster and many other creative use like fight animal poach amazon and google be build drone to deliver household item the startup zipline use drone to deliver medical supply to remote village that can t be access by road there be also a new wave of startup work on fly car include two fund by the cofounder of google larry page fly car use the same advanced technology use in drone but be large enough to carry people due to advance in material battery and software fly car will be significantly more affordable and convenient than today s plane and helicopter artificial intelligence have make rapid advance in the last decade due to new algorithm and massive increase in data collection and computing power ai can be apply to almost any field for example in photography an ai technique call artistic style transfer transform photograph into the style of a give painter google build an ai system that control its datacenter power system save hundred of million of dollar in energy cost the broad promise of ai be to liberate people from repetitive mental task the same way the industrial revolution liberate people from repetitive physical task some people worry that ai will destroy job history have show that while new technology do indeed eliminate job it also create new and well job to replace they for example with advent of the personal computer the number of typographer job drop but the increase in graphic designer job more than make up for it it be much easy to imagine job that will go away than new job that will be create today million of people work as app developer ride share driver drone operator and social medium marketer — job that didn t exist and would have be difficult to even imagine ten year ago by 2020 80 % of adult on earth will have an internet connect smartphone an iphone 6 have about 2 billion transistor roughly 625 time more transistor than a 1995 intel pentium computer today s smartphone be what use to be consider supercomputer internet connect smartphone give ordinary people ability that just a short time ago be only available to an elite few protocol be the plumbing of the internet most of the protocol we use today be develop decade ago by academia and government since then protocol development mostly stop as energy shift to develop proprietary system like social network and message app cryptocurrency and blockchain technology be change this by provide a new business model for internet protocol this year alone hundred of million of dollar be raise for a broad range of innovative blockchain base protocol protocol base on blockchain also have capability that previous protocol didn t for example ethereum be a new blockchain base protocol that can be use to create smart contract and trust database that be immune to corruption and censorship while college tuition skyrocket anyone with a smartphone can study almost any topic online access educational content that be mostly free and increasingly high quality encyclopedia britannica use to cost $ 1 400 now anyone with a smartphone can instantly access wikipedia you use to have to go to school or buy programming book to learn computer programming now you can learn from a community of over 40 million programmer at stack overflow youtube have million of hour of free tutorial and lecture many of which be produce by top professor and university the quality of online education be get well all the time for the last 15 year mit have be record lecture and compile material that cover over 2000 course as perhaps the great research university in the world mit have always be ahead of the trend over the next decade expect many other school to follow mit s lead earth be run out of farmable land and fresh water this be partly because our food production system be incredibly inefficient it take an astounding 1799 gallon of water to produce 1 pound of beef fortunately a variety of new technology be be develop to improve our food system for example entrepreneur be develop new food product that be tasty and nutritious substitute for traditional food but far more environmentally friendly the startup impossible food invent meat product that look and taste like the real thing but be actually make of plant their burger use 95 % less land 74 % less water and produce 87 % less greenhouse gas emission than traditional burger other startup be create plant base replacement for milk egg and other common food soylent be a healthy inexpensive meal replacement that use advanced engineer ingredient that be much friendly to the environment than traditional ingredient some of these product be develop use genetic modification a powerful scientific technique that have be widely mischaracterize as dangerous accord to a study by the pew organization 88 % of scientist think genetically modify food be safe another exciting development in food production be automate indoor farming due to advance in solar energy sensor lighting robotic and artificial intelligence indoor farm have become viable alternative to traditional outdoor farm compare to traditional farm automate indoor farm use roughly 10 time less water and land crop be harvest many more time per year there be no dependency on weather and no need to use pesticide until recently computer have only be at the periphery of medicine use primarily for research and record keeping today the combination of computer science and medicine be lead to a variety of breakthrough for example just fifteen year ago it cost $ 3b to sequence a human genome today the cost be about a thousand dollar and continue to drop genetic sequencing will soon be a routine part of medicine genetic sequencing generate massive amount of datum that can be analyze use powerful datum analysis software one application be analyze blood sample for early detection of cancer further genetic analysis can help determine the good course of treatment another application of computer to medicine be in prosthetic limb here a young girl be use prosthetic hand she control use her upper arm muscle soon we ll have the technology to control prothetic limb with just our thought use brain to machine interface computer be also become increasingly effective at diagnose disease an artificial intelligence system recently diagnose a rare disease that human doctor fail to diagnose by find hidden pattern in 20 million cancer record since the beginning of the space age in the 1950 the vast majority of space funding have come from government but that funding have be in decline for example nasa s budget drop from about 4 5 % of the federal budget in the 1960 to about 0 5 % of the federal budget today the good news be that private space company have start fill the void these company provide a wide range of product and service include rocket launch scientific research communication and imaging satellite and emerge speculative business model like asteroid mine the most famous private space company be elon musk s spacex which successfully send rocket into space that can return home to be reuse perhaps the most intriguing private space company be planetary resource which be try to pioneer a new industry mining mineral from asteroid if successful asteroid mining could lead to a new gold rush in outer space like previous gold rush this could lead to speculative excess but also dramatically increase funding for new technology and infrastructure these be just a few of the amazing technology we ll see develop in the come decade 2016 be just the beginning of a new age of wonder as futurist kevin kelly say from a quick cheer to a stand ovation clap to show how much you enjoy this story www cdixon org about
Tal Perry,2.6K,17,https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02?source=tag_archive---------9----------------,deep learn the stock market tal perry medium,update 25 1 17 — take I a while but here be an ipython notebook with a rough implementation in the past few month I ve be fascinate with deep learning especially its application to language and text I ve spend the bulk of my career in financial technology mostly in algorithmic trading and alternative datum service you can see where this be go I write this to get my idea straight in my head while I ve become a deep learning enthusiast I don t have too many opportunity to brain dump an idea in most of its messy glory I think that a decent indication of a clear thought be the ability to articulate it to people not from the field I hope that I ve succeed in do that and that my articulation be also a pleasurable read why nlp be relevant to stock prediction in many nlp problem we end up take a sequence and encode it into a single fix size representation then decode that representation into another sequence for example we might tag entity in the text translate from english to french or convert audio frequency to text there be a torrent of work come out in these area and a lot of the result be achieve state of the art performance in my mind the big difference between the nlp and financial analysis be that language have some guarantee of structure it s just that the rule of the structure be vague market on the other hand don t come with a promise of a learnable structure that such a structure exist be the assumption that this project would prove or disprove rather it might prove or disprove if I can find that structure assume the structure be there the idea of summarize the current state of the market in the same way we encode the semantic of a paragraph seem plausible to I if that doesn t make sense yet keep read it will you shall know a word by the company it keep firth j r 1957 11 there be ton of literature on word embedding richard socher s lecture be a great place to start in short we can make a geometry of all the word in our language and that geometry capture the meaning of word and relationship between they you may have see the example of king man + woman = queen or something of the sort embedding be cool because they let we represent information in a condensed way the old way of represent word be hold a vector a big list of number that be as long as the number of word we know and set a 1 in a particular place if that be the current word we be look at that be not an efficient approach nor do it capture any meaning with embedding we can represent all of the word in a fix number of dimension 300 seem to be plenty 50 work great and then leverage their high dimensional geometry to understand they the picture below show an example an embedding be train on more or less the entire internet after a few day of intensive calculation each word be embed in some high dimensional space this space have a geometry concept like distance and so we can ask which word be close together the author inventor of that method make an example here be the word that be close to frog but we can embed more than just word we can do say stock market embedding market2vec the first word embed algorithm I hear about be word2vec I want to get the same effect for the market though I ll be use a different algorithm my input datum be a csv the first column be the date and there be 4 * 1000 column correspond to the high low open closing price of 1000 stock that be my input vector be 4000 dimensional which be too big so the first thing I m go to do be stuff it into a low dimensional space say 300 because I like the movie take something in 4000 dimension and stuff it into a 300 dimensional space my sound hard but its actually easy we just need to multiply matrix a matrix be a big excel spreadsheet that have number in every cell and no format problem imagine an excel table with 4000 column and 300 row and when we basically bang it against the vector a new vector come out that be only of size 300 I wish that s how they would have explain it in college the fanciness start here as we re go to set the number in our matrix at random and part of the deep learning be to update those number so that our excel spreadsheet change eventually this matrix spreadsheet I ll stick with matrix from now on will have number in it that bang our original 4000 dimensional vector into a concise 300 dimensional summary of itself we re go to get a little fancier here and apply what they call an activation function we re go to take a function and apply it to each number in the vector individually so that they all end up between 0 and 1 or 0 and infinity it depend why it make our vector more special and make our learning process able to understand more complicated thing how so what what I m expect to find be that that new embedding of the market price the vector into a small space capture all the essential information for the task at hand without waste time on the other stuff so I d expect they d capture correlation between other stock perhaps notice when a certain sector be decline or when the market be very hot I don t know what trait it will find but I assume they ll be useful now what let put aside our market vector for a moment and talk about language model andrej karpathy write the epic post the unreasonable effectiveness of recurrent neural network if I d summarize in the most liberal fashion the post boil down to and then as a punchline he generate a bunch of text that look like shakespeare and then he do it again with the linux source code and then again with a textbook on algebraic geometry so I ll get back to the mechanic of that magic box in a second but let I remind you that we want to predict the future market base on the past just like he predict the next word base on the previous one where karpathy use character we re go to use our market vector and feed they into the magic black box we haven t decide what we want it to predict yet but that be okay we win t be feed its output back into it either go deeply I want to point out that this be where we start to get into the deep part of deep learning so far we just have a single layer of learn that excel spreadsheet that condense the market now we re go to add a few more layer and stack they to make a deep something that s the deep in deep learning so karpathy show we some sample output from the linux source code this be stuff his black box write notice that it know how to open and close parenthesis and respect indentation convention ; the content of the function be properly indent and the multi line printk statement have an inner indentation that mean that this magic box understand long range dependency when it s indent within the print statement it know it s in a print statement and also remember that it s in a function or at least another indented scope that s nuts it s easy to gloss over that but an algorithm that have the ability to capture and remember long term dependency be super useful because we want to find long term dependency in the market inside the magical black box what s inside this magical black box it be a type of recurrent neural network rnn call an lstm an rnn be a deep learning algorithm that operate on sequence like sequence of character at every step it take a representation of the next character like the embedding we talk about before and operate on the representation with a matrix like we see before the thing be the rnn have some form of internal memory so it remember what it see previously it use that memory to decide how exactly it should operate on the next input use that memory the rnn can remember that it be inside of an intended scope and that be how we get properly nest output text a fancy version of an rnn be call a long short term memory lstm lstm have cleverly design memory that allow it to so an lstm can see a { and say to itself oh yeah that s important I should remember that and when it do it essentially remember an indication that it be in a nested scope once it see the corresponding } it can decide to forget the original opening brace and thus forget that it be in a nested scope we can have the lstm learn more abstract concept by stack a few of they on top of each other that would make we deep again now each output of the previous lstm become the input of the next lstm and each one go on to learn high abstraction of the datum come in in the example above and this be just illustrative speculation the first layer of lstms might learn that character separate by a space be word the next layer might learn word type like static void action_new_function the next layer might learn the concept of a function and its argument and so on it s hard to tell exactly what each layer be do though karpathy s blog have a really nice example of how he do visualize exactly that connect market2vec and lstms the studious reader will notice that karpathy use character as his input not embedding technically a one hot encoding of character but lar eidne actually use word embedding when he write auto generating clickbait with recurrent neural network the figure above show the network he use ignore the softmax part we ll get to it later for the moment check out how on the bottom he put in a sequence of word vector at the bottom and each one remember a word vector be a representation of a word in the form of a bunch of number like we see in the beginning of this post lar input a sequence of word vector and each one of they we re go to do the same thing with one difference instead of word vector we ll input marketvector those market vector we describe before to recap the marketvector should contain a summary of what s happen in the market at a give point in time by put a sequence of they through lstms I hope to capture the long term dynamic that have be happen in the market by stack together a few layer of lstms I hope to capture high level abstraction of the market s behavior what come out thus far we haven t talk at all about how the algorithm actually learn anything we just talk about all the clever transformation we ll do on the datum we ll defer that conversation to a few paragraph down but please keep this part in mind as it be the se up for the punch line that make everything else worthwhile in karpathy s example the output of the lstms be a vector that represent the next character in some abstract representation in eidne example the output of the lstms be a vector that represent what the next word will be in some abstract space the next step in both case be to change that abstract representation into a probability vector that be a list that say how likely each character or word respectively be likely to appear next that s the job of the softmax function once we have a list of likelihood we select the character or word that be the most likely to appear next in our case of predict the market we need to ask ourselves what exactly we want to market to predict some of the option that I think about be 1 and 2 be regression problem where we have to predict an actual number instead of the likelihood of a specific event like the letter n appear or the market go up those be fine but not what I want to do 3 and 4 be fairly similar they both ask to predict an event in technical jargon — a class label an event could be the letter n appear next or it could be move up 5 % while not go down more than 3 % in the last 10 minute the trade off between 3 and 4 be that 3 be much more common and thus easy to learn about while 4 be more valuable as not only be it an indicator of profit but also have some constraint on risk 5 be the one we ll continue with for this article because it s similar to 3 and 4 but have mechanic that be easy to follow the vix be sometimes call the fear index and it represent how volatile the stock in the s&p500 be it be derive by observe the imply volatility for specific option on each of the stock in the index sidenote — why predict the vix what make the vix an interesting target be that back to our lstm output and the softmax how do we use the formulation we see before to predict change in the vix a few minute in the future for each point in our dataset we ll look what happen to the vix 5 minute later if it go up by more than 1 % without go down more than 0 5 % during that time we ll output a 1 otherwise a 0 then we ll get a sequence that look like we want to take the vector that our lstms output and squish it so that it give we the probability of the next item in our sequence be a 1 the squishing happen in the softmax part of the diagram above technically since we only have 1 class now we use a sigmoid so before we get into how this thing learn let s recap what we ve do so far how do this thing learn now the fun part everything we do until now be call the forward pass we d do all of those step while we train the algorithm and also when we use it in production here we ll talk about the backward pass the part we do only while in training that make our algorithm learn so during train not only do we prepare year worth of historical datum we also prepare a sequence of prediction target that list of 0 and 1 that show if the vix move the way we want it to or not after each observation in our datum to learn we ll feed the market datum to our network and compare its output to what we calculate compare in our case will be simple subtraction that be we ll say that our model s error be or in english the square root of the square of the difference between what actually happen and what we predict here s the beauty that s a differential function that be we can tell by how much the error would have change if our prediction would have change a little our prediction be the outcome of a differentiable function the softmax the input to the softmax the lstms be all mathematical function that be differentiable now all of these function be full of parameter those big excel spreadsheet I talk about age ago so at this stage what we do be take the derivative of the error with respect to every one of the million of parameter in all of those excel spreadsheet we have in our model when we do that we can see how the error will change when we change each parameter so we ll change each parameter in a way that will reduce the error this procedure propagate all the way to the beginning of the model it tweak the way we embed the input into marketvector so that our marketvector represent the most significant information for our task it tweak when and what each lstm choose to remember so that their output be the most relevant to our task it tweak the abstraction our lstms learn so that they learn the most important abstraction for our task which in my opinion be amazing because we have all of this complexity and abstraction that we never have to specify anywhere it s all infer mathamagically from the specification of what we consider to be an error what s next now that I ve lay this out in writing and it still make sense to I I want so if you ve come this far please point out my error and share your input other thought here be some mostly more advanced thought about this project what other thing I might try and why it make sense to I that this may actually work liquidity and efficient use of capital generally the more liquid a particular market be the more efficient that be I think this be due to a chicken and egg cycle whereas a market become more liquid it be able to absorb more capital move in and out without that capital hurt itself as a market become more liquid and more capital can be use in it you ll find more sophisticated player move in this be because it be expensive to be sophisticated so you need to make return on a large chunk of capital in order to justify your operational cost a quick corollary be that in less liquid market the competition isn t quite as sophisticated and so the opportunity a system like this can bring may not have be trade away the point being be I to try and trade this I would try and trade it on less liquid segment of the market that be maybe the tase 100 instead of the s&p 500 this stuff be new the knowledge of these algorithm the framework to execute they and the computing power to train they be all new at least in the sense that they be available to the average joe such as myself I d assume that top player have figure this stuff out year ago and have have the capacity to execute for as long but as I mention in the above paragraph they be likely execute in liquid market that can support their size the next tier of market participant I assume have a slow velocity of technological assimilation and in that sense there be or soon will be a race to execute on this in as yet untapped market multiple time frame while I mention a single stream of input in the above I imagine that a more efficient way to train would be to train market vector at least on multiple time frame and feed they in at the inference stage that be my low time frame would be sample every 30 second and I d expect the network to learn dependency that stretch hour at most I don t know if they be relevant or not but I think there be pattern on multiple time frame and if the cost of computation can be bring low enough then it be worthwhile to incorporate they into the model I m still wrestle with how good to represent these on the computational graph and perhaps it be not mandatory to start with marketvector when use word vector in nlp we usually start with a pretraine model and continue adjust the embedding during training of our model in my case there be no pretraine market vector available nor be tehre a clear algorithm for train they my original consideration be to use an auto encoder like in this paper but end to end training be cool a more serious consideration be the success of sequence to sequence model in translation and speech recognition where a sequence be eventually encode as a single vector and then decode into a different representation like from speech to text or from english to french in that view the entire architecture I describe be essentially the encoder and I haven t really lay out a decoder but I want to achieve something specific with the first layer the one that take as input the 4000 dimensional vector and output a 300 dimensional one I want it to find correlation or relation between various stock and compose feature about they the alternative be to run each input through an lstm perhaps concatenate all of the output vector and consider that output of the encoder stage I think this will be inefficient as the interaction and correlation between instrument and their feature will be lose and thre will be 10x more computation require on the other hand such an architecture could naively be parallel across multiple gpu and host which be an advantage cnns recently there have be a spur of paper on character level machine translation this paper catch my eye as they manage to capture long range dependency with a convolutional layer rather than an rnn I haven t give it more than a brief read but I think that a modification where I d treat each stock as a channel and convolve over channel first like in rgb image would be another way to capture the market dynamic in the same way that they essentially encode semantic meaning from character from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of https lighttag io platform to annotate text for nlp google developer expert in ml I do deep learning on text for a living and for fun
Gil Fewster,3.3K,5,https://medium.freecodecamp.org/the-mind-blowing-ai-announcement-from-google-that-you-probably-missed-2ffd31334805?source=tag_archive---------0----------------,the mind blow ai announcement from google that you probably miss,disclaimer I m not an expert in neural network or machine learning since originally write this article many people with far more expertise in these field than myself have indicate that while impressive what google have achieve be evolutionary not revolutionary in the very least it s fair to say that I m guilty of anthropomorphise in part of the text I ve leave the article s content unchanged because I think it s interesting to compare the gut reaction I have with the subsequent comment of expert in the field I strongly encourage reader to browse the comment after read the article for some perspective more sober and inform than my own in the closing week of 2016 google publish an article that quietly sail under most people s radar which be a shame because it may just be the most astonishing article about machine learning that I read last year don t feel bad if you miss it not only be the article compete with the pre christmas rush that most of we be navigate — it be also tuck away on google s research blog beneath the geektastic headline zero shot translation with google s multilingual neural machine translation system this doesn t exactly scream must read do it especially when you ve get project to wind up gift to buy and family feud to be resolve — all while the advent calendar relentlessly count down the day until christmas like some kind of chocolate fill yuletide doomsday clock luckily I m here to bring you up to speed here s the deal up until september of last year google translate use phrase base translation it basically do the same thing you and I do when we look up key word and phrase in our lonely planet language guide it s effective enough and blisteringly fast compare to awkwardly thumb your way through a bunch of page look for the french equivalent of please bring I all of your cheese and don t stop until I fall over but it lack nuance phrase base translation be a blunt instrument it do the job well enough to get by but map roughly equivalent word and phrase without an understanding of linguistic structure can only produce crude result this approach be also limit by the extent of an available vocabulary phrase base translation have no capacity to make educated guess at word it doesn t recognize and can t learn from new input all that change in september when google give their translation tool a new engine the google neural machine translation system gnmt this new engine come fully load with all the hot 2016 buzzword like neural network and machine learn the short version be that google translate get smart it develop the ability to learn from the people who use it it learn how to make educated guess about the content tone and meaning of phrase base on the context of other word and phrase around they and — here s the bit that should make your brain explode — it get creative google translate invent its own language to help it translate more effectively what s more nobody tell it to it didn t develop a language or interlingua as google call it because it be code to it develop a new language because the software determine over time that this be the most efficient way to solve the problem of translation stop and think about that for a moment let it sink in a neural computing system design to translate content from one human language into another develop its own internal language to make the task more efficient without be tell to do so in a matter of week I ve add a correction retraction of this paragraph in the note to understand what s go on we need to understand what zero shot translation capability be here s google s mike schuster nikhil thorat and melvin johnson from the original blog post here you can see an advantage of google s new neural machine over the old phrase base approach the gmnt be able to learn how to translate between two language without be explicitly teach this wouldn t be possible in a phrase base model where translation be dependent upon an explicit dictionary to map word and phrase between each pair of language be translate and this lead the google engineer onto that truly astonishing discovery of creation so there you have it in the last week of 2016 as journos around the world start pen their be this the bad year in living memory thinkpiece google engineer be quietly document a genuinely astonishing breakthrough in software engineering and linguistic I just think maybe you d want to know ok to really understand what s go on we probably need multiple computer science and linguistic degree I m just barely scrape the surface here if you ve get time to get a few degree or if you ve already get they please drop I a line and explain it all I to slowly update 1 in my excitement it s fair to say that I ve exaggerate the idea of this as an intelligent system — at least so far as we would think about human intelligence and decision making make sure you read chris mcdonald s comment after the article for a more sober perspective update 2 nafrondel s excellent detailed reply be also a must read for an expert explanation of how neural network function from a quick cheer to a stand ovation clap to show how much you enjoy this story a tinkerer our community publish story worth read on development design and datum science
David Venturi,10.6K,20,https://medium.freecodecamp.org/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0?source=tag_archive---------1----------------,every single machine learning course on the internet rank by your review,a year and a half ago I drop out of one of the good computer science program in canada I start create my own data science master s program use online resource I realize that I could learn everything I need through edx coursera and udacity instead and I could learn it fast more efficiently and for a fraction of the cost I m almost finish now I ve take many data science relate course and audit portion of many more I know the option out there and what skill be need for learner prepare for a data analyst or data scientist role so I start create a review drive guide that recommend the good course for each subject within data science for the first guide in the series I recommend a few code class for the beginner datum scientist then it be statistic and probability class then introduction to data science also data visualization for this guide I spend a dozen hour try to identify every online machine learning course offer as of may 2017 extract key bit of information from their syllabus and review and compile their rating my end goal be to identify the three good course available and present they to you below for this task I turn to none other than the open source class central community and its database of thousand of course rating and review since 2011 class central founder dhawal shah have keep a close eye on online course than arguably anyone else in the world dhawal personally help I assemble this list of resource each course must fit three criterion we believe we cover every notable course that fit the above criterion since there be seemingly hundred of course on udemy we choose to consider the most reviewed and high rate one only there s always a chance that we miss something though so please let we know in the comment section if we leave a good course out we compile average rating and number of review from class central and other review site to calculate a weighted average rating for each course we read text review and use this feedback to supplement the numerical rating we make subjective syllabus judgment call base on three factor a popular definition originate from arthur samuel in 1959 machine learning be a subfield of computer science that give computer the ability to learn without be explicitly program in practice this mean develop computer program that can make prediction base on datum just as human can learn from experience so can computer where datum = experience a machine learn workflow be the process require for carry out a machine learning project though individual project can differ most workflow share several common task problem evaluation datum exploration datum preprocesse model training testing deployment etc below you ll find helpful visualization of these core step the ideal course introduce the entire process and provide interactive example assignment and or quiz where student can perform each task themselves first off let s define deep learning here be a succinct description as would be expect portion of some of the machine learn course contain deep learning content I choose not to include deep learning only course however if you be interested in deep learning specifically we ve get you cover with the follow article my top three recommendation from that list would be several course list below ask student to have prior programming calculus linear algebra and statistic experience these prerequisite be understandable give that machine learning be an advanced discipline miss a few subject good news some of this experience can be acquire through our recommendation in the first two article program statistic of this data science career guide several top rank course below also provide gentle calculus and linear algebra refresher and highlight the aspect most relevant to machine learning for those less familiar stanford university s machine learning on coursera be the clear current winner in term of rating review and syllabus fit teach by the famous andrew ng google brain founder and former chief scientist at baidu this be the class that spark the founding of coursera it have a 4 7 star weight average rating over 422 review release in 2011 it cover all aspect of the machine learn workflow though it have a small scope than the original stanford class upon which it be base it still manage to cover a large number of technique and algorithm the estimate timeline be eleven week with two week dedicate to neural network and deep learning free and pay option be available ng be a dynamic yet gentle instructor with a palpable experience he inspire confidence especially when share practical implementation tip and warning about common pitfall a linear algebra refresher be provide and ng highlight the aspect of calculus most relevant to machine learning evaluation be automatic and be do via multiple choice quiz that follow each lesson and programming assignment the assignment there be eight of they can be complete in matlab or octave which be an open source version of matlab ng explain his language choice though python and r be likely more compelling choice in 2017 with the increase popularity of those language reviewer note that that shouldn t stop you from take the course a few prominent reviewer note the follow columbia university s machine learning be a relatively new offering that be part of their artificial intelligence micromaster on edx though it be new and doesn t have a large number of review the one that it do have be exceptionally strong professor john paisley be note as brilliant clear and clever it have a 4 8 star weight average rating over 10 review the course also cover all aspect of the machine learn workflow and more algorithm than the above stanford offer columbia s be a more advanced introduction with reviewer note that student should be comfortable with the recommend prerequisite calculus linear algebra statistic probability and code quiz 11 programming assignment 4 and a final exam be the mode of evaluation student can use either python octave or matlab to complete the assignment the course s total estimate timeline be eight to ten hour per week over twelve week it be free with a verify certificate available for purchase below be a few of the aforementioned sparkling review machine learn a ztm on udemy be an impressively detailed offering that provide instruction in both python and r which be rare and can t be say for any of the other top course it have a 4 5 star weight average rating over 8 119 review which make it the most review course of the one consider it cover the entire machine learn workflow and an almost ridiculous in a good way number of algorithm through 40 5 hour of on demand video the course take a more apply approach and be light math wise than the above two course each section start with an intuition video from eremenko that summarize the underlie theory of the concept be teach de ponteve then walk through implementation with separate video for both python and r as a bonus the course include python and r code template for student to download and use on their own project there be quiz and homework challenge though these aren t the strong point of the course eremenko and the superdatascience team be revere for their ability to make the complex simple also the prerequisite list be just some high school mathematic so this course might be a well option for those daunt by the stanford and columbia offering a few prominent reviewer note the follow our # 1 pick have a weight average rating of 4 7 out of 5 star over 422 review let s look at the other alternative sort by descend rating a reminder that deep learning only course be not include in this guide — you can find those here the analytic edge massachusetts institute of technology edx more focused on analytic in general though it do cover several machine learning topic use r strong narrative that leverage familiar real world example challenge ten to fifteen hour per week over twelve week free with a verify certificate available for purchase it have a 4 9 star weight average rating over 214 review python for datum science and machine learning bootcamp jose portilla udemy have large chunk of machine learning content but cover the whole data science process more of a very detailed intro to python amazing course though not ideal for the scope of this guide 21 5 hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 6 star weight average rating over 3316 review data science and machine learning bootcamp with r jose portilla udemy the comment for portilla s above course apply here as well except for r 17 5 hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 6 star weight average rating over 1317 review machine learning series lazy programmer inc udemy teach by a data scientist big data engineer full stack software engineer with an impressive resume lazy programmer currently have a series of 16 machine learning focus course on udemy in total the course have 5000 + rating and almost all of they have 4 6 star a useful course ordering be provide in each individual course s description use python cost varie depend on udemy discount which be frequent machine learn georgia tech udacity a compilation of what be three separate course supervise unsupervised and reinforcement learning part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms bite sized video as be udacity s style friendly professor estimate timeline of four month free it have a 4 56 star weight average rating over 9 review implement predictive analytic with spark in azure hdinsight microsoft edx introduce the core concept of machine learning and a variety of algorithms leverage several big data friendly tool include apache spark scala and hadoop use both python and r four hour per week over six week free with a verify certificate available for purchase it have a 4 5 star weight average rating over 6 review data science and machine learning with python — hand on frank kane udemy use python kane have nine year of experience at amazon and imdb nine hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 5 star weight average rating over 4139 review scala and spark for big datum and machine learning jose portilla udemy big datum focus specifically on implementation in scala and spark ten hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 5 star weight average rating over 607 review machine learning engineer nanodegree udacity udacity s flagship machine learning program which feature a good in class project review system and career support the program be a compilation of several individual udacity course which be free co create by kaggle estimate timeline of six month currently cost $ 199 usd per month with a 50 % tuition refund available for those who graduate within 12 month it have a 4 5 star weight average rating over 2 review learn from datum introductory machine learning california institute of technology edx enrollment be currently close on edx but be also available via caltech s independent platform see below it have a 4 49 star weight average rating over 42 review learn from datum introductory machine learn yaser abu mostafa california institute of technology a real caltech course not a water down version review note it be excellent for understand machine learning theory the professor yaser abu mostafa be popular among student and also write the textbook upon which this course be base video be tape lecture with lecture slide picture in picture upload to youtube homework assignment be pdf file the course experience for online student isn t as polished as the top three recommendation it have a 4 43 star weight average rating over 7 review mining massive dataset stanford university machine learn with a focus on big datum introduce modern distribute file system and mapreduce ten hour per week over seven week free it have a 4 4 star weight average rating over 30 review aws machine learn a complete guide with python chandra lingam udemy a unique focus on cloud base machine learning and specifically amazon web service use python nine hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 4 star weight average rating over 62 review introduction to machine learning & face detection in python holczer balazs udemy use python eight hour of on demand video cost varie depend on udemy discount which be frequent it have a 4 4 star weight average rating over 162 review statlearne statistical learning stanford university base on the excellent textbook an introduction to statistical learning with application in r and teach by the professor who write it reviewer note that the mooc isn t as good as the book cite thin exercise and mediocre video five hour per week over nine week free it have a 4 35 star weight average rating over 84 review machine learning specialization university of washington coursera great course but last two class include the capstone project be cancel reviewer note that this series be more digestable read easy for those without strong technical background than other top machine learn course e g stanford s or caltech s be aware that the series be incomplete with recommend system deep learning and a summary miss free and pay option available it have a 4 31 star weight average rating over 80 review from 0 to 1 machine learn nlp & python cut to the chase loony corn udemy a down to earth shy but confident take on machine learning technique teach by four person team with decade of industry experience together use python cost varie depend on udemy discount which be frequent it have a 4 2 star weight average rating over 494 review principle of machine learn microsoft edx use r python and microsoft azure machine learn part of the microsoft professional program certificate in data science three to four hour per week over six week free with a verify certificate available for purchase it have a 4 09 star weight average rating over 11 review big data statistical inference and machine learning queensland university of technology futurelearn a nice brief exploratory machine learning course with a focus on big datum cover a few tool like r h2o flow and weka only three week in duration at a recommend two hour per week but one reviewer note that six hour per week would be more appropriate free and pay option available it have a 4 star weight average rating over 4 review genomic datum science and cluster bioinformatics v university of california san diego coursera for those interested in the intersection of computer science and biology and how it represent an important frontier in modern science focus on clustering and dimensionality reduction part of ucsd s bioinformatics specialization free and pay option available it have a 4 star weight average rating over 3 review intro to machine learn udacity prioritize topic breadth and practical tool in python over depth and theory the instructor sebastian thrun and katie malone make this class so fun consist of bite sized video and quiz follow by a mini project for each lesson currently part of udacity s data analyst nanodegree estimate timeline of ten week free it have a 3 95 star weight average rating over 19 review machine learning for data analysis wesleyan university coursera a brief intro machine learning and a few select algorithms cover decision tree random forest lasso regression and k mean cluster part of wesleyan s datum analysis and interpretation specialization estimate timeline of four week free and pay option available it have a 3 6 star weight average rating over 5 review program with python for data science microsoft edx produce by microsoft in partnership with code dojo use python eight hour per week over six week free and pay option available it have a 3 46 star weight average rating over 37 review machine learning for trade georgia tech udacity focus on apply probabilistic machine learning approach to trading decision use python part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree om estimate timeline of four month free it have a 3 29 star weight average rating over 14 review practical machine learning johns hopkins university coursera a brief practical introduction to a number of machine learning algorithm several one two star review express a variety of concern part of jhu s data science specialization four to nine hour per week over four week free and pay option available it have a 3 11 star weight average rating over 37 review machine learning for datum science and analytics columbia university edx introduce a wide range of machine learn topic some passionate negative review with concern include content choice a lack of programming assignment and uninspire presentation seven to ten hour per week over five week free with a verify certificate available for purchase it have a 2 74 star weight average rating over 36 review recommender system specialization university of minnesota coursera strong focus one specific type of machine learning — recommender system a four course specialization plus a capstone project which be a case study teach use lenskit an open source toolkit for recommender system free and pay option available it have a 2 star weight average rating over 2 review machine learning with big datum university of california san diego coursera terrible review that highlight poor instruction and evaluation some note it take they mere hour to complete the whole course part of ucsd s big datum specialization free and pay option available it have a 1 86 star weight average rating over 14 review practical predictive analytic model and method university of washington coursera a brief intro to core machine learning concept one reviewer note that there be a lack of quiz and that the assignment be not challenge part of uw s data science at scale specialization six to eight hour per week over four week free and pay option available it have a 1 75 star weight average rating over 4 review the follow course have one or no review as of may 2017 machine learn for musician and artist goldsmith university of london kadenze unique student learn algorithms software tool and machine learn good practice to make sense of human gesture musical audio and other real time datum seven session in length audit free and premium $ 10 usd per month option available it have one 5 star review apply machine learning in python university of michigan coursera teach use python and the scikit learn toolkit part of the apply data science with python specialization schedule to start may 29th free and pay option available apply machine learn microsoft edx teach use various tool include python r and microsoft azure machine learning note microsoft produce the course include hand on lab to reinforce the lecture content three to four hour per week over six week free with a verify certificate available for purchase machine learn with python big datum university teach use python target towards beginner estimate completion time of four hour big datum university be affiliate with ibm free machine learning with apache systemml big data university teach use apache systemml which be a declarative style language design for large scale machine learning estimate completion time of eight hour big datum university be affiliate with ibm free machine learning for data science university of california san diego edx doesn t launch until january 2018 programming example and assignment be in python use jupyter notebook eight hour per week over ten week free with a verify certificate available for purchase introduction to analytic model georgia tech edx the course advertise r as its primary programming tool five to ten hour per week over ten week free with a verify certificate available for purchase predictive analytic gain insight from big datum queensland university of technology futurelearn brief overview of a few algorithm use hewlett packard enterprise s vertica analytic platform as an apply tool start date to be announce two hour per week over four week free with a certificate of achievement available for purchase introducción al machine learning universita telefónica miríada x teach in spanish an introduction to machine learning that cover supervised and unsupervised learn a total of twenty estimate hour over four week machine learning path step dataquest teach in python use dataquest s interactive in browser platform multiple guide project and a plus project where you build your own machine learning system use your own datum subscription require the follow six course be offer by datacamp datacamp s hybrid teaching style leverage video and text base instruction with lot of example through an in browser code editor a subscription be require for full access to each course introduction to machine learn datacamp cover classification regression and clustering algorithm use r fifteen video and 81 exercise with an estimate timeline of six hour supervised learning with scikit learn datacamp use python and scikit learn cover classification and regression algorithms seventeen video and 54 exercise with an estimate timeline of four hour unsupervised learning in r datacamp provide a basic introduction to clustering and dimensionality reduction in r sixteen video and 49 exercise with an estimate timeline of four hour machine learn toolbox datacamp teach the big idea in machine learning use r 24 video and 88 exercise with an estimate timeline of four hour machine learn with the expert school budget datacamp a case study from a machine learning competition on drivendata involve build a model to automatically classify item in a school s budget datacamp s supervised learning with scikit learn be a prerequisite fifteen video and 51 exercise with an estimate timeline of four hour unsupervised learning in python datacamp cover a variety of unsupervised learning algorithm use python scikit learn and scipy the course end with student build a recommend system to recommend popular musical artist thirteen video and 52 exercise with an estimate timeline of four hour machine learning tom mitchell carnegie mellon university carnegie mellon s graduate introductory machine learning course a prerequisite to their second graduate level course statistical machine learning tape university lecture with practice problem homework assignment and a midterm all with solution post online a 2011 version of the course also exist cmu be one of the good graduate school for study machine learning and have a whole department dedicate to ml free statistical machine learn larry wasserman carnegie mellon university likely the most advanced course in this guide a follow up to carnegie mellon s machine learning course tape university lecture with practice problem homework assignment and a midterm all with solution post online free undergraduate machine learn nando de freitas university of british columbia an undergraduate machine learning course lecture be film and put on youtube with the slide post on the course website the course assignment be post as well no solution though de freita be now a full time professor at the university of oxford and receive praise for his teaching ability in various forum graduate version available see below machine learn nando de freitas university of british columbia a graduate machine learning course the comment in de freitas undergraduate course above apply here as well this be the fifth of a six piece series that cover the good online course for launch yourself into the data science field we cover programming in the first article statistic and probability in the second article intro to data science in the third article and datum visualization in the fourth the final piece will be a summary of those article plus the good online course for other key topic such as datum wrangle database and even software engineering if you re look for a complete list of data science online course you can find they on class central s data science and big data subject page if you enjoy read this check out some of class central s other piece if you have suggestion for course I miss let I know in the response if you find this helpful click the 💚 so more people will see it here on medium this be a condensed version of my original article publish on class central where I ve include detailed course syllabus from a quick cheer to a stand ovation clap to show how much you enjoy this story curriculum lead project @ datacamp I create my own data science master s program our community publish story worth read on development design and datum science
Vishal Maini,32K,10,https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12?source=tag_archive---------2----------------,a beginner s guide to ai ml 🤖 👶 machine learn for human medium,part 1 why machine learning matter the big picture of artificial intelligence and machine learning — past present and future part 2 1 supervised learning learn with an answer key introduce linear regression loss function overfitte and gradient descent part 2 2 supervise learn ii two method of classification logistic regression and svms part 2 3 supervise learn iii non parametric learner k near neighbor decision tree random forest introduce cross validation hyperparameter tuning and ensemble model part 3 unsupervised learning cluster k mean hierarchical dimensionality reduction principal component analysis pca singular value decomposition svd part 4 neural network & deep learning why where and how deep learning work draw inspiration from the brain convolutional neural network cnns recurrent neural network rnn real world application part 5 reinforcement learning exploration and exploitation markov decision process q learning policy learning and deep reinforcement learn the value learn problem appendix the good machine learning resource a curate list of resource for create your machine learning curriculum this guide be intend to be accessible to anyone basic concept in probability statistic program linear algebra and calculus will be discuss but it isn t necessary to have prior knowledge of they to gain value from this series artificial intelligence will shape our future more powerfully than any other innovation this century anyone who do not understand it will soon find themselves feel leave behind wake up in a world full of technology that feel more and more like magic the rate of acceleration be already astounding after a couple of ai winter and period of false hope over the past four decade rapid advance in datum storage and computer processing power have dramatically change the game in recent year in 2015 google train a conversational agent ai that could not only convincingly interact with human as a tech support helpdesk but also discuss morality express opinion and answer general fact base question the same year deepmind develop an agent that surpass human level performance at 49 atari game receive only the pixel and game score as input soon after in 2016 deepmind obsolete their own achievement by release a new state of the art gameplay method call a3c meanwhile alphago defeat one of the good human player at go — an extraordinary achievement in a game dominate by human for two decade after machine first conquer chess many master could not fathom how it would be possible for a machine to grasp the full nuance and complexity of this ancient chinese war strategy game with its 10170 possible board position there be only 1080atoms in the universe in march 2017 openai create agent that invent their own language to cooperate and more effectively achieve their goal soon after facebook reportedly successfully train agent to negotiate and even lie just a few day ago as of this writing on august 11 2017 openai reach yet another incredible milestone by defeat the world s top professional in 1v1 match of the online multiplayer game dota 2 much of our day to day technology be power by artificial intelligence point your camera at the menu during your next trip to taiwan and the restaurant s selection will magically appear in english via the google translate app today ai be use to design evidence base treatment plan for cancer patient instantly analyze result from medical test to escalate to the appropriate specialist immediately and conduct scientific research for drug discovery in everyday life it s increasingly commonplace to discover machine in role traditionally occupy by human really don t be surprised if a little housekeeping delivery bot show up instead of a human next time you call the hotel desk to send up some toothpaste in this series we ll explore the core machine learning concept behind these technology by the end you should be able to describe how they work at a conceptual level and be equip with the tool to start build similar application yourself artificial intelligence be the study of agent that perceive the world around they form plan and make decision to achieve their goal its foundation include mathematics logic philosophy probability linguistic neuroscience and decision theory many field fall under the umbrella of ai such as computer vision robotic machine learning and natural language processing machine learning be a subfield of artificial intelligence its goal be to enable computer to learn on their own a machine s learn algorithm enable it to identify pattern in observed datum build model that explain the world and predict thing without have explicit pre program rule and model the technology discuss above be example of artificial narrow intelligence ani which can effectively perform a narrowly define task meanwhile we re continue to make foundational advance towards human level artificial general intelligence agi also know as strong ai the definition of an agi be an artificial intelligence that can successfully perform any intellectual task that a human being can include learn planning and decision making under uncertainty communicate in natural language make joke manipulate people trade stock or reprogramme itself and this last one be a big deal once we create an ai that can improve itself it will unlock a cycle of recursive self improvement that could lead to an intelligence explosion over some unknown time period range from many decade to a single day you may have hear this point refer to as the singularity the term be borrow from the gravitational singularity that occur at the center of a black hole an infinitely dense one dimensional point where the law of physics as we understand they start to break down a recent report by the future of humanity institute survey a panel of ai researcher on timeline for agi and find that researcher believe there be a 50 % chance of ai outperform human in all task in 45 year grace et al 2017 we ve personally speak with a number of sane and reasonable ai practitioner who predict much long timeline the upper limit be never and other whose timeline be alarmingly short — as little as a few year the advent of great than human level artificial superintelligence asi could be one of the good or bad thing to happen to our specie it carry with it the immense challenge of specify what ais will want in a way that be friendly to human while it s impossible to say what the future hold one thing be certain 2017 be a good time to start understand how machine think to go beyond the abstraction of a philosopher in an armchair and intelligently shape our roadmap and policy with respect to ai we must engage with the detail of how machine see the world — what they want their potential bias and failure mode their temperamental quirk — just as we study psychology and neuroscience to understand how human learn decide act and feel machine learning be at the core of our journey towards artificial general intelligence and in the meantime it will change every industry and have a massive impact on our day to day live that s why we believe it s worth understand machine learning at least at a conceptual level — and we design this series to be the good place to start you don t necessarily need to read the series cover to cover to get value out of it here be three suggestion on how to approach it depend on your interest and how much time you have vishal most recently lead growth at upstart a lending platform that utilize machine learning to price credit automate the borrowing process and acquire user he spend his time think about startup apply cognitive science moral philosophy and the ethic of artificial intelligence samer be a master s student in computer science and engineering at ucsd and co founder of conigo lab prior to grad school he found tablescribe a business intelligence tool for smb and spend two year advise fortune 100 company at mckinsey samer previously study computer science and ethic politic and economic at yale most of this series be write during a 10 day trip to the united kingdom in a frantic blur of train plane cafe pub and wherever else we could find a dry place to sit our aim be to solidify our own understanding of artificial intelligence machine learning and how the method therein fit together — and hopefully create something worth share in the process and now without further ado let s dive into machine learning with part 2 1 supervised learning more from machine learning for human 🤖 👶 a special thank to jonathan eng edoardo conti grant schneider sunny kumar stephanie he tarun wadhwa and sachin maini series editor for their significant contribution and feedback from a quick cheer to a stand ovation clap to show how much you enjoy this story research comms @deepmindai previously @upstart @yale @trueventurestec demystify artificial intelligence & machine learning discussion on safe and intentional application of ai for positive social impact
Tim Anglade,7K,23,https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3?source=tag_archive---------3----------------,how hbo s silicon valley build not hotdog with mobile tensorflow keras & react native,the hbo show silicon valley release a real ai app that identify hotdog — and not hotdog — like the one show on season 4 s 4th episode the app be now available on android as well as io to achieve this we design a bespoke neural architecture that run directly on your phone and train it with tensorflow keras & nvidia gpus while the use case be farcical the app be an approachable example of both deep learning and edge computing all ai work be power 100 % by the user s device and image be process without ever leave their phone this provide user with a snappy experience no round trip to the cloud offline availability and well privacy this also allow we to run the app at a cost of $ 0 even under the load of a million user provide significant saving compare to traditional cloud base ai approach the app be develop in house by the show by a single developer run on a single laptop & attach gpu use hand curate datum in that respect it may provide a sense of what can be achieve today with a limited amount of time & resource by non technical company individual developer and hobbyist alike in that spirit this article attempt to give a detailed overview of step involve to help other build their own app if you haven t see the show or try the app you should the app let you snap a picture and then tell you whether it think that image be of a hotdog or not it s a straightforward use case that pay homage to recent ai research and application in particular imagenet while we ve probably dedicate more engineering resource to recognize hotdog than anyone else the app still fail in horrible and or subtle way conversely it s also sometimes able to recognize hotdog in complex situation accord to engadget it s incredible I ve have more success identify food with the app in 20 minute than I have have tag and identify song with shazam in the past two year have you ever find yourself read hacker news thinking they raise a 10 m series a for that I could build it in one weekend this app probably feel a lot like that and the initial prototype be indeed build in a single weekend use google cloud platform s vision api and react native but the final app we end up release on the app store require month of additional part time work to deliver meaningful improvement that would be difficult for an outsider to appreciate we spend week optimize overall accuracy training time inference time iterate on our setup & tooling so we could have a fast development iteration and spend a whole weekend optimize the user experience around ios & android permission don t even get I start on that one all too often technical blog post or academic paper skip over this part prefer to present the final choose solution in the interest of help other learn from our mistake & choice we will present an abridge view of the approach that didn t work for we before we describe the final architecture we end up shipping in the next section we choose react native to build the prototype as it would give we an easy sandbox to experiment with and would help we quickly support many device the experience end up be a good one and we keep react native for the remainder of the project it didn t always make thing easy and the design for the app be purposefully limited but in the end react native get the job do the other main component we use for the prototype — google cloud s vision api be quickly abandon there be 3 main factor for these reason we start experiment with what s trendily call edge computing which for our purpose mean that after train our neural network on our laptop we would export it and embe it directly into our mobile app so that the neural network execution phase or inference would run directly inside the user s phone through a chance encounter with pete warden of the tensorflow team we have become aware of its ability to run tensorflow directly embed on an ios device and start explore that path after react native tensorflow become the second fix part of our stack it only take a day of work to integrate tensorflow s objective c++ camera example in our react native shell it take slightly long to use their transfer learning script which help you retrain the inception architecture to deal with a more specific image problem inception be the name of a family of neural architecture build by google to deal with image recognition problem inception be available pre train which mean the training phase have be complete and the weight be set most often for image recognition network they have be train on imagenet a dataset contain over 20 000 different type of object hotdog be one of they however much like google cloud s vision api imagenet training reward breadth as much as depth here and out of the box accuracy on a single one of the 20 000 + category can be lack as such retraining also call transfer learning aim to take a full train neural net and retrain it to perform well on the specific problem you d like to handle this usually involve some degree of forget either by excise entire layer from the stack or by slowly erase the network s ability to distinguish a type of object e g chair in favor of well accuracy at recognize the one you care about I e hotdog while the network inception in this case may have be train on the 14 m image contain in imagenet we be able to retrain it on a just a few thousand hotdog image to get drastically enhance hotdog recognition the big advantage of transfer learning be you will get well result much fast and with less datum than if you train from scratch a full training might take month on multiple gpu and require million of image while retraining can conceivably be do in hour on a laptop with a couple thousand image one of the big challenge we encounter be understand exactly what should count as a hotdog and what should not define what a hotdog be end up be surprisingly difficult do cut up sausage count and if so which kind and subject to cultural interpretation similarly the open world nature of our problem mean we have to deal with an almost infinite number of inputs while certain computer vision problem have relatively limit input say x ray of bolt with or without a mechanical default we have to prepare the app to be feed selfie nature shot and any number of food suffice to say this approach be promise and do lead to some improved result however it have to be abandon for a couple of reason first the nature of our problem mean a strong imbalance in training datum there be many more example of thing that be not hotdog than thing that be hotdog in practice this mean that if you train your algorithm on 3 hotdog image and 97 non hotdog image and it recognize 0 % of the former but 100 % of the latter it will still score 97 % accuracy by default this be not straightforward to solve out of the box use tensorflow s retrain tool and basically necessitate set up a deep learning model from scratch import weight and train in a more control manner at this point we decide to bite the bullet and get something start with keras a deep learning library that provide nice easy to use abstraction on top of tensorflow include pretty awesome training tool and a class_weight option which be ideal to deal with this sort of dataset imbalance we be deal with we use that opportunity to try other popular neural architecture like vgg but one problem remain none of they could comfortably fit on an iphone they consume too much memory which lead to app crash and would sometime take up to 10 second to compute which be not ideal from a ux standpoint many thing be attempt to mitigate that but in the end it these architecture be just too big to run efficiently on mobile to give you a context out of time this be roughly the mid way point of the project by that time the ui be 90%+ do and very little of it be go to change but in hindsight the neural net be at well 20 % do we have a good sense of challenge & a good dataset but 0 line of the final neural architecture have be write none of our neural code could reliably run on mobile and even our accuracy be go to improve drastically in the week to come the problem directly ahead of we be simple if inception and vgg be too big be there a simple pre train neural network we could retrain at the suggestion of the always excellent jeremy p howard where have that guy be all our life we explore xception enet and squeezenet we quickly settle on squeezenet due to its explicit positioning as a solution for embed deep learning and the availability of a pre train keras model on github yay open source so how big of a difference do this make an architecture like vgg use about 138 million parameter essentially the number of number necessary to model the neuron and value between they inception be already a massive improvement require only 23 million parameter squeezenet in comparison only require 1 25 million this have two advantage there be tradeoff of course during this phase we start experiment with tune the neural network architecture in particular we start use batch normalization and try different activation function after add batch normalization and elu to squeezenet we be able to train neural network that achieve 90%+ accuracy when train from scratch however they be relatively brittle mean the same network would overfit in some case or underfit in other when confront to real life testing even add more example to the dataset and play with data augmentation fail to deliver a network that meet expectation so while this phase be promise and for the first time give we a function app that could work entirely on an iphone in less than a second we eventually move to our 4th & final architecture our final architecture be spur in large part by the publication on april 17 of google s mobilenet paper promise a new neural architecture with inception like accuracy on simple problem like ours with only 4 m or so parameter this mean it sit in an interesting sweet spot between a squeezenet that have maybe be overly simplistic for our purpose and the possibly overwrought elephant try to squeeze in a tutu of use inception or vgg on mobile the paper introduce some capacity to tune the size & complexity of network specifically to trade memory cpu consumption against accuracy which be very much top of mind for we at the time with less than a month to go before the app have to launch we endeavor to reproduce the paper s result this be entirely anticlimactic as within a day of the paper be publish a keras implementation be already offer publicly on github by refik can malli a student at istanbul technical university whose work we have already benefit from when we take inspiration from his excellent keras squeezenet implementation the depth & openness of the deep learning community and the presence of talente mind like r c be what make deep learning viable for application today — but they also make work in this field more thrilling than any tech trend we ve be involve with our final architecture end up make significant departure from the mobilenet architecture or from convention in particular so how do this stack work exactly deep learning often get a bad rap for be a black box and while it s true many component of it can be mysterious the network we use often leak information about how some of their magic work we can look at the layer of this stack and how they activate on specific input image give we a sense of each layer s ability to recognize sausage bun or other particularly salient hotdog feature data quality be of the utmost importance a neural network can only be as good as the datum that train it and improve training set quality be probably one of the top 3 thing we spend time on during this project the key thing we do to improve this be the final composition of our dataset be 150k image of which only 3k be hotdog there be only so many hotdog you can look at but there be many not hotdog to look at the 49 1 imbalance be deal with by say a keras class weight of 49 1 in favor of hotdog of the remain 147k image most be of food with just 3k photo of non food item to help the network generalize a bit more and not get trick into see a hotdog if present with an image of a human in a red outfit our data augmentation rule be as follow these number be derive intuitively base on experiment and our understanding of the real life usage of our app as oppose to careful experimentation the final key to our data pipeline be use patrick rodriguez s multiprocess image datum generator for keras while keras do have a build in multi thread and multiprocess implementation we find patrick s library to be consistently fast in our experiment for reason we do not have time to investigate this library cut our training time to a third of what it use to be the network be train use a 2015 macbook pro and attach external gpu egpu specifically an nvidia gtx 980 ti we d probably buy a 1080 ti if we be start today we be able to train the network on batch of 128 image at a time the network be train for a total of 240 epoch mean we run all 150k image through the network 240 time this take about 80 hour we train the network in 3 phase while learn rate be identify by run the linear experiment recommend by the clr paper they seem to intuitively make sense in that the max for each phase be within a factor of 2 of the previous minimum which be align with the industry standard recommendation of halve your learning rate if your accuracy plateaus during training in the interest of time we perform some training run on a paperspace p5000 instance run ubuntu in those case we be able to double the batch size and find that optimal learning rate for each phase be roughly double as well even have design a relatively compact neural architecture and have train it to handle situation it may find in a mobile context we have a lot of work leave to make it run properly try to run a top of the line neural net architecture out of the box can quickly burn hundred megabyte of ram which few mobile device can spare today beyond network optimization it turn out the way you handle image or even load tensorflow itself can have a huge impact on how quickly your network run how little ram it use and how crash free the experience will be for your user this be maybe the most mysterious part of this project relatively little information can be find about it possibly due to the dearth of production deep learning application run on mobile device as of today however we must commend the tensorflow team and particularly pete warden andrew harp and chad whipkey for the exist documentation and their kindness in answer our inquiry instead of use tensorflow on io we look at use apple s build in deep learning librarie instead bnn mpscnn and later on coreml we would have design the network in keras train it with tensorflow export all the weight value re implement the network with bnns or mpscnn or import it via coreml and load the parameter into that new implementation however the big obstacle be that these new apple library be only available on io 10 + and we want to support old version of io as io 10 + adoption and these framework continue to improve there may not be a case for use tensorflow on device in the near future if you think inject javascript into your app on the fly be cool try inject neural net into your app the last production trick we use be to leverage codepush and apple s relatively permissive term of service to live inject new version of our neural network after submission to the app store while this be mostly do to help we quickly deliver accuracy improvement to our user after release you could conceivably use this approach to drastically expand or alter the feature set of your app without go through an app store review again there be a lot of thing that didn t work or we didn t have time to do and these be the idea we d investigate in the future finally we d be remiss not to mention the obvious and important influence of user experience developer experience and build in bias in develop an ai app each probably deserve their own post or their own book but here be the very concrete impact of these 3 thing in our experience ux user experience be arguably more critical at every stage of the development of an ai app than for a traditional application there be no deep learning algorithm that will give you perfect result right now but there be many situation where the right mix of deep learning + ux will lead to result that be indistinguishable from perfect proper ux expectation be irreplaceable when it come to set developer on the right path to design their neural network set the proper expectation for user when they use the app and gracefully handle the inevitable ai failure building ai app without a ux first mindset be like train a neural net without stochastic gradient descent you will end up stuck in the local minima of the uncanny valley on your way to build the perfect ai use case dx developer experience be extremely important as well because deep learning training time be the new horsing around while wait for your program to compile we suggest you heavily favor dx first hence keras as it s always possible to optimize runtime for later run manual gpu parallelization multi process data augmentation tensorflow pipeline even re implement for caffe2 pytorch even project with relatively obtuse apis & documentation like tensorflow greatly improve dx by provide a highly test highly use well maintain environment for training & run neural network for the same reason it s hard to beat both the cost as well as the flexibility of have your own local gpu for development be able to look at edit image locally edit code with your prefer tool without delay greatly improve the development quality & speed of building ai project most ai app will hit more critical cultural bias than ours but as an example even our straightforward use case catch we flat foot with build in bias in our initial dataset that make the app unable to recognize french style hotdog asian hotdog and more oddity we do not have immediate personal experience with it s critical to remember that ai do not make well decision than human — they be infect by the same human bias we fall prey to via the training set human provide thank to mike judge alec berg clay tarver todd silverstein jonathan dotan lisa schomas amy solomon dorothy street & rich toyon and all the writer of the show — the app would simply not exist without they meaghan dana david jay and everyone at hbo scale venture partner & gitlab rachel thomas and jeremy howard & fast ai for all that they have teach I and for kindly review a draft of this post check out their free online deep learning course it s awesome jp simard for his help on io and finally the tensorflow team & r machinelearning for their help & inspiration and thank to everyone who use & share the app it make stare at picture of hotdog for month on end totally worth it 😅 from a quick cheer to a stand ovation clap to show how much you enjoy this story a i startup & hbo s silicon valley get in touch timanglade@gmail com
Sophia Ciocca,53K,9,https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe?source=tag_archive---------4----------------,how do spotify know you so well member feature story medium,member feature story a software engineer explain the science behind personalize music recommendation photo by studioeast getty image photo by studioeast getty image this monday — just like every monday before it — over 100 million spotify user find a fresh new playlist wait for they call discover weekly it s a custom mixtape of 30 song they ve never listen to before but will probably love and it s pretty much magic I m a huge fan of spotify and particularly discover weekly why it make I feel see it know my musical taste well than any person in my entire life ever have and I m consistently delight by how satisfyingly just right it be every week with track I probably would never have find myself or know I would like for those of you who live under a soundproof rock let I introduce you to my virtual good friend as it turn out I m not alone in my obsession with discover weekly the user base go crazy for it which have drive spotify to rethink its focus and invest more resource into algorithm base playlist ever since discover weekly debut in 2015 I ve be die to know how it work what s more I m a spotify fangirl so I sometimes like to pretend that I work there and research their product after three week of mad googling I feel like I ve finally get a glimpse behind the curtain so how do spotify do such an amazing job of choose those 30 song for each person each week let s zoom out for a second to look at how other music service have tackle music recommendation and how spotify s do it well back in the 2000 songza kick off the online music curation scene use manual curation to create playlist for user this mean that a team of music expert or other human curator would put together playlist that they just think sound good and then user would listen to those playlist later beat music would employ this same strategy manual curation work alright but it be base on that specific curator s choice and therefore couldn t take into account each listener s individual music taste like songza pandora be also one of the original player in digital music curation it employ a slightly more advanced approach instead manually tag attribute of song this mean a group of people listen to music choose a bunch of descriptive word for each track and tag the track accordingly then pandora s code could simply filter for certain tag to make playlist of similar sound music around that same time a music intelligence agency from the mit medium lab call the echo nest be bear which take a radical cutting edge approach to personalize music the echo nest use algorithm to analyze the audio and textual content of music allow it to perform music identification personalize recommendation playlist creation and analysis finally take another approach be last fm which still exist today and use a process call collaborative filtering to identify music its user might like but more on that in a moment so if that s how other music curation service have handle recommendation how do spotify s magic engine run how do it seem to nail individual user taste so much more accurately than any of the other service spotify doesn t actually use a single revolutionary recommendation model instead they mix together some of the good strategy use by other service to create their own uniquely powerful discovery engine to create discover weekly there be three main type of recommendation model that spotify employ let s dive into how each of these recommendation model work first some background when people hear the word collaborative filtering they generally think of netflix as it be one of the first company to use this method to power a recommendation model take user star base movie rating to inform its understanding of which movie to recommend to other similar user after netflix be successful the use of collaborative filtering spread quickly and be now often the starting point for anyone try to make a recommendation model unlike netflix spotify doesn t have a star base system with which user rate their music instead spotify s data be implicit feedback — specifically the stream count of the track and additional streaming datum such as whether a user save the track to their own playlist or visit the artist s page after listen to a song but what be collaborative filtering truly and how do it work here s a high level rundown explain in a quick conversation what s go on here each of these individual have track preference the one on the left like track p q r and s while the one on the right like track q r s and t collaborative filtering then use that datum to say hmmm you both like three of the same track — q r and s — so you be probably similar user therefore you re each likely to enjoy other track that the other person have listen to that you haven t hear yet therefore it suggest that the one on the right check out track p — the only track not mention but that his similar counterpart enjoy — and the one on the left check out track t for the same reasoning simple right but how do spotify actually use that concept in practice to calculate million of user suggest track base on million of other user preference with matrix math do with python library in actuality this matrix you see here be gigantic each row represent one of spotify s 140 million user — if you use spotify you yourself be a row in this matrix — and each column represent one of the 30 million song in spotify s database then the python library run this long complicated matrix factorization formula when it finish we end up with two type of vector represent here by x and y x be a user vector represent one single user s taste and y be a song vector represent one single song s profile now we have 140 million user vector and 30 million song vector the actual content of these vector be just a bunch of number that be essentially meaningless on their own but be hugely useful when compare to find out which user musical taste be most similar to mine collaborative filtering compare my vector with all of the other user vector ultimately spit out which user be the close match the same go for the y vector song you can compare a single song s vector with all the other and find out which song be most similar to the one in question collaborative filtering do a pretty good job but spotify know they could do even well by add another engine enter nlp the second type of recommendation model that spotify employ be natural language processing nlp model the source datum for these model as the name suggest be regular ol word track metadata news article blog and other text around the internet natural language processing which be the ability of a computer to understand human speech as it be speak be a vast field unto itself often harness through sentiment analysis api the exact mechanism behind nlp be beyond the scope of this article but here s what happen on a very high level spotify crawl the web constantly look for blog post and other write text about music to figure out what people be say about specific artist and song — which adjective and what particular language be frequently use in reference to those artist and song and which other artist and song be also be discuss alongside they while I don t know the specific of how spotify choose to then process this scrape datum I can offer some insight base on how the echo nest use to work with they they would bucket spotify s datum up into what they call cultural vector or top term each artist and song have thousand of top term that change on the daily each term have an associated weight which correlate to its relative importance — roughly the probability that someone will describe the music or artist with that term then much like in collaborative filter the nlp model use these term and weight to create a vector representation of the song that can be use to determine if two piece of music be similar cool right first a question you might be think first of all add a third model far improve the accuracy of the music recommendation service but this model also serve a secondary purpose unlike the first two type raw audio model take new song into account take for example a song your singer songwriter friend have put up on spotify maybe it only have 50 listen so there be few other listener to collaboratively filter it against it also isn t mention anywhere on the internet yet so nlp model win t pick it up luckily raw audio model don t discriminate between new track and popular track so with their help your friend s song could end up in a discover weekly playlist alongside popular song but how can we analyze raw audio datum which seem so abstract with convolutional neural network convolutional neural network be the same technology use in facial recognition software in spotify s case they ve be modify for use on audio datum instead of pixel here s an example of a neural network architecture this particular neural network have four convolutional layer see as the thick bar on the left and three dense layer see as the more narrow bar on the right the input be time frequency representation of audio frame which be then concatenate or link together to form the spectrogram the audio frame go through these convolutional layer and after pass through the last one you can see a global temporal pooling layer which pool across the entire time axi effectively compute statistic of the learn feature across the time of the song after process the neural network spit out an understanding of the song include characteristic like estimate time signature key mode tempo and loudness below be a plot of datum for a 30 second snippet of around the world by daft punk ultimately this reading of the song s key characteristic allow spotify to understand fundamental similarity between song and therefore which user might enjoy they base on their own listening history that cover the basic of the three major type of recommendation model feed spotify s recommendation pipeline and ultimately power the discover weekly playlist of course these recommendation model be all connect to spotify s large ecosystem which include giant amount of datum storage and use lot of hadoop cluster to scale recommendation and make these engine work on enormous matrix endless online music article and huge number of audio file I hope this be informative and pique your curiosity like it do mine for now I ll be work my way through my own discover weekly find my new favorite music while appreciate all the machine learning that s go on behind the scene 🎶 thank also to ladycollective for read this article and suggest edit software engineer writer and generally creative human interested in art feminism mindfulness and authenticity http sophiaciocca com welcome to a place where word matter on medium smart voice and original idea take center stage — with no ad in sight watch follow all the topic you care about and we ll deliver the good story for you to your homepage and inbox explore get unlimited access to the good story on medium — and support writer while you re at it just $ 5 month upgrade
François Chollet,35K,18,https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec?source=tag_archive---------5----------------,the impossibility of intelligence explosion françois chollet medium,in 1965 I j good describe for the first time the notion of intelligence explosion as it relate to artificial intelligence ai decade later the concept of an intelligence explosion — lead to the sudden rise of superintelligence and the accidental end of the human race — have take hold in the ai community famous business leader be cast it as a major risk great than nuclear war or climate change average graduate student in machine learning be endorse it in a 2015 email survey target ai researcher 29 % of respondent answer that intelligence explosion be likely or highly likely a further 21 % consider it a serious possibility the basic premise be that in the near future a first seed ai will be create with general problem solve ability slightly surpass that of human this seed ai would start design well ais initiate a recursive self improvement loop that would immediately leave human intelligence in the dust overtake it by order of magnitude in a short time proponent of this theory also regard intelligence as a kind of superpower confer its holder with almost supernatural capability to shape their environment — as see in the science fiction movie transcendence 2014 for instance superintelligence would thus imply near omnipotence and would pose an existential threat to humanity this science fiction narrative contribute to the dangerously misleading public debate that be ongoing about the risk of ai and the need for ai regulation in this post I argue that intelligence explosion be impossible — that the notion of intelligence explosion come from a profound misunderstanding of both the nature of intelligence and the behavior of recursively self augment system I attempt to base my point on concrete observation about intelligent system and recursive system the reasoning behind intelligence explosion like many of the early theory about ai that arise in the 1960 and 1970 be sophistic it consider intelligence in a completely abstract way disconnected from its context and ignore available evidence about both intelligent system and recursively self improve system it doesn t have to be that way we be after all on a planet that be literally pack with intelligent system include we and self improve system so we can simply observe they and learn from they to answer the question at hand instead of come up with evidence free circular reasoning to talk about intelligence and its possible self improve property we should first introduce necessary background and context what be we talk about when we talk about intelligence precisely define intelligence be in itself a challenge the intelligence explosion narrative equate intelligence with the general problem solve ability display by individual intelligent agent — by current human brain or future electronic brain this be not quite the full picture so let s use this definition as a starting point and expand on it the first issue I see with the intelligence explosion theory be a failure to recognize that intelligence be necessarily part of a broad system — a vision of intelligence as a brain in jar that can be make arbitrarily intelligent independently of its situation a brain be just a piece of biological tissue there be nothing intrinsically intelligent about it beyond your brain your body and sense — your sensorimotor affordance — be a fundamental part of your mind your environment be a fundamental part of your mind human culture be a fundamental part of your mind these be after all where all of your thought come from you can not dissociate intelligence from the context in which it express itself in particular there be no such thing as general intelligence on an abstract level we know this for a fact via the no free lunch theorem — state that no problem solve algorithm can outperform random chance across all possible problem if intelligence be a problem solve algorithm then it can only be understand with respect to a specific problem in a more concrete way we can observe this empirically in that all intelligent system we know be highly specialized the intelligence of the ais we build today be hyper specialize in extremely narrow task — like play go or classify image into 10 000 know category the intelligence of an octopus be specialize in the problem of be an octopus the intelligence of a human be specialize in the problem of be human what would happen if we be to put a freshly create human brain in the body of an octopus and let in live at the bottom of the ocean would it even learn to use its eight legged body would it survive past a few day we can not perform this experiment but we do know that cognitive development in human and animal be drive by hardcoded innate dynamic human baby be bear with an advanced set of reflex behavior and innate learning template that drive their early sensorimotor development and that be fundamentally intertwine with the structure of the human sensorimotor space the brain have hardcode conception of have a body with hand that can grab a mouth that can suck eye mount on a move head that can be use to visually follow object the vestibulo ocular reflex and these preconception be require for human intelligence to start take control of the human body it have even be convincingly argue for instance by chomsky that very high level human cognitive feature such as our ability to develop language be innate similarly one can imagine that the octopus have its own set of hardcode cognitive primitive require in order to learn how to use an octopus body and survive in its octopus environment the brain of a human be hyper specialize in the human condition — an innate specialization extend possibly as far as social behavior language and common sense — and the brain of an octopus would likewise be hyper specialize in octopus behavior a human baby brain properly graft in an octopus body would most likely fail to adequately take control of its unique sensorimotor space and would quickly die off not so smart now mr superior brain what would happen if we be to put a human — brain and body — into an environment that do not feature human culture as we know it would mowgli the man cub raise by a pack of wolf grow up to outsmart his canine sibling to be smart like we and if we swap baby mowgli with baby einstein would he eventually educate himself into develop grand theory of the universe empirical evidence be relatively scarce but from what we know child that grow up outside of the nurture environment of human culture don t develop any human intelligence feral child raise in the wild from their early year become effectively animal and can no long acquire human behavior or language when return to civilization saturday mthiyane raise by monkey in south africa and find at five keep behave like a monkey into adulthood — jump and walk on all four incapable of language and refuse to eat cook food feral child who have human contact for at least some of their most formative year tend to have slightly well luck with reeducation although they rarely graduate to fully function human if intelligence be fundamentally link to specific sensorimotor modalitie a specific environment a specific upbringing and a specific problem to solve then you can not hope to arbitrarily increase the intelligence of an agent merely by tune its brain — no more than you can increase the throughput of a factory line by speed up the conveyor belt intelligence expansion can only come from a co evolution of the mind its sensorimotor modality and its environment if the gear of your brain be the define factor of your problem solve ability then those rare human with iqs far outside the normal range of human intelligence would live live far outside the scope of normal life would solve problem previously think unsolvable and would take over the world — just as some people fear smart than human ai will do in practice genius with exceptional cognitive ability usually live overwhelmingly banal life and very few of they accomplish anything of note in terman s landmark genetic study of genius he note that most of his exceptionally gifted subject would pursue occupation as humble as those of policeman seaman typist and filing clerk there be currently about seven million people with iqs high than 150 — well cognitive ability than 99 9 % of humanity — and mostly these be not the people you read about in the news of the people who have actually attempt to take over the world hardly any seem to have have an exceptional intelligence ; anecdotally hitler be a high school dropout who fail to get into the vienna academy of art — twice people who do end up make breakthrough on hard problem do so through a combination of circumstance character education intelligence and they make their breakthrough through incremental improvement over the work of their predecessor success — express intelligence — be sufficient ability meet a great problem at the right time most of these remarkable problem solver be not even that clever — their skill seem to be specialize in a give field and they typically do not display great than average ability outside of their own domain some people achieve more because they be well team player or have more grit and work ethic or great imagination some just happen to have live in the right context to have the right conversation at the right time intelligence be fundamentally situational intelligence be not a superpower ; exceptional intelligence do not on its own confer you with proportionally exceptional power over your circumstance however it be a well document fact that raw cognitive ability — as measure by iq which may be debatable — correlate with social attainment for slice of the spectrum that be close to the mean this be first evidence in terman s study and later confirm by other — for instance an extensive 2006 metastudy by strenze find a visible if somewhat weak correlation between iq and socioeconomic success so a person with an iq of 130 be statistically far more likely to succeed in navigate the problem of life than a person with an iq of 70 — although this be never guarantee at the individual level — but here s the thing this correlation break down after a certain point there be no evidence that a person with an iq of 170 be in any way more likely to achieve a great impact in their field than a person with an iq of 130 in fact many of the most impactful scientist tend to have have iqs in the 120 or 130s — feynman report 126 james watson co discoverer of dna 124 — which be exactly the same range as legion of mediocre scientist at the same time of the roughly 50 000 human alive today who have astounding iqs of 170 or high how many will solve any problem a tenth as significant as professor watson why would the real world utility of raw cognitive ability stall past a certain threshold this point to a very intuitive fact that high attainment require sufficient cognitive ability but that the current bottleneck to problem solve to express intelligence be not latent cognitive ability itself the bottleneck be our circumstance our environment which determine how our intelligence manifest itself put a hard limit on what we can do with our brain — on how intelligent we can grow up to be on how effectively we can leverage the intelligence that we develop on what problem we can solve all evidence point to the fact that our current environment much like past environment over the previous 200 000 year of human history and prehistory do not allow high intelligence individual to fully develop and utilize their cognitive potential a high potential human 10 000 year ago would have be raise in a low complexity environment likely speak a single language with few than 5 000 word would never have be teach to read or write would have be expose to a limited amount of knowledge and to few cognitive challenge the situation be a bit well for most contemporary human but there be no indication that our environmental opportunity currently outpace our cognitive potential a smart human raise in the jungle be but a hairless ape similarly an ai with a superhuman brain drop into a human body in our modern world would likely not develop great capability than a smart contemporary human if it could then exceptionally high iq human would already be display proportionally exceptional level of personal attainment ; they would achieve exceptional level of control over their environment and solve major outstanding problem — which they don t in practice it s not just that our body sense and environment determine how much intelligence our brain can develop — crucially our biological brain be just a small part of our whole intelligence cognitive prosthetic surround we plug into our brain and extend its problem solve capability your smartphone your laptop google search the cognitive tool your be gift in school book other people mathematical notation program the most fundamental of all cognitive prosthetic be of course language itself — essentially an operate system for cognition without which we couldn t think very far these thing be not merely knowledge to be feed to the brain and use by it they be literally external cognitive process non biological way to run thread of thought and problem solve algorithm — across time space and importantly across individuality these cognitive prosthetic not our brain be where most of our cognitive ability reside we be our tool an individual human be pretty much useless on its own — again human be just bipedal ape it s a collective accumulation of knowledge and external system over thousand of year — what we call civilization — that have elevate we above our animal nature when a scientist make a breakthrough the thought process they be run in their brain be just a small part of the equation — the researcher offload large extent of the problem solve process to computer to other researcher to paper note to mathematical notation etc and they be only able to succeed because they be stand on the shoulder of giant — their own work be but one last subroutine in a problem solve process that span decade and thousand of individual their own individual cognitive work may not be much more significant to the whole process than the work of a single transistor on a chip an overwhelming amount of evidence point to this simple fact a single human brain on its own be not capable of design a great intelligence than itself this be a purely empirical statement out of billion of human brain that have come and go none have do so clearly the intelligence of a single human over a single lifetime can not design intelligence or else over billion of trial it would have already occur however these billion of brain accumulate knowledge and develop external intelligent process over thousand of year implement a system — civilization — which may eventually lead to artificial brain with great intelligence than that of a single human it be civilization as a whole that will create superhuman ai not you nor I nor any individual a process involve countless human over timescale we can barely comprehend a process involve far more externalized intelligence — book computer mathematics science the internet — than biological intelligence on an individual level we be but vector of civilization build upon previous work and pass on our finding we be the momentary transistor on which the problem solve algorithm of civilization run will the superhuman ais of the future develop collectively over century have the capability to develop ai great than themselves no no more than any of we can answer yes would fly in the face of everything we know — again remember that no human nor any intelligent entity that we know of have ever design anything smart than itself what we do be gradually collectively build external problem solve system that be great than ourselves however future ais much like human and the other intelligent system we ve produce so far will contribute to our civilization and our civilization in turn will use they to keep expand the capability of the ais it produce ai in this sense be no different than computer or book or language itself it s a technology that empower our civilization the advent of superhuman ai will thus be no more of a singularity than the advent of computer or book or language civilization will develop ai and just march on civilization will eventually transcend what we be now much like it have transcend what we be 10 000 year ago it s a gradual process not a sudden shift the basic premise of intelligence explosion — that a seed ai will arise with great than human problem solve ability lead to a sudden recursive runaway intelligence improvement loop — be false our problem solve ability in particular our ability to design ai be already constantly improve because these ability do not reside primarily in our biological brain but in our external collective tool the recursive loop have be in action for a long time and the rise of well brain will not qualitatively affect it — no more than any previous intelligence enhance technology our brain themselves be never a significant bottleneck in the ai design process in this case you may ask isn t civilization itself the runaway self improve brain be our civilizational intelligence explode no crucially the civilization level intelligence improve loop have only result in measurably linear progress in our problem solve ability over time not an explosion but why wouldn t recursively improve x mathematically result in x grow exponentially no — in short because no complex real world system can be model as ` x t + 1 = x t * a a > 1 ` no system exist in a vacuum and especially not intelligence nor human civilization we don t have to speculate about whether an explosion would happen the moment an intelligent system start optimize its own intelligence as it happen most system be recursively self improve we re surround with they so we know exactly how such system behave — in a variety of contexts and over a variety of timescale you be yourself a recursively self improve system educate yourself make you smart in turn allow you to educate yourself more efficiently likewise human civilization be recursively self improve over a much long timescale mechatronic be recursively self improve — well manufacturing robot can manufacture well manufacturing robot military empire be recursively self expand — the large your empire the great your military mean to expand it far personal investing be recursively self improve — the more money you have the more money you can make example abound consider for instance software writing software obviously empower software write first we program compiler that could perform automate programming then we use compiler to develop new language implement more powerful programming paradigm we use these language to develop advanced developer tool — debugger ide linter bug predictor in the future software will even write itself and what be the end result of this recursively self improve process can you do 2x more with your the software on your computer than you could last year will you be able to do 2x more next year arguably the usefulness of software have be improve at a measurably linear pace while we have invest exponential effort into produce it the number of software developer have be boom exponentially for decade and the number of transistor on which we be run our software have be explode as well follow moore s law yet our computer be only incrementally more useful to we than they be in 2012 or 2002 or 1992 but why primarily because the usefulness of software be fundamentally limit by the context of its application — much like intelligence be both define and limit by the context in which it express itself software be just one cog in a big process — our economy our life — just like your brain be just one cog in a big process — human culture this context put a hard limit on the maximum potential usefulness of software much like our environment put a hard limit on how intelligent any individual can be — even if gift with a superhuman brain beyond contextual hard limit even if one part of a system have the ability to recursively self improve other part of the system will inevitably start act as bottleneck antagonistic process will arise in response to recursive self improvement and squash it — in software this would be resource consumption feature creep ux issue when it come to personal invest your own rate of spending be one such antagonistic process — the more money you have the more money you spend when it come to intelligence inter system communication arise as a brake on any improvement of underlie module — a brain with smart part will have more trouble coordinate they ; a society with smart individual will need to invest far more in networking and communication etc it be perhaps not a coincidence that very high iq people be more likely to suffer from certain mental illness it be also perhaps not random happenstance that military empire of the past have end up collapse after surpass a certain size exponential progress meet exponential friction one specific example that be worth pay attention to be that of scientific progress because it be conceptually very close to intelligence itself — science as a problem solve system be very close to be a runaway superhuman ai science be of course a recursively self improve system because scientific progress result in the development of tool that empower science — whether lab hardware e g quantum physics lead to laser which enable a wealth of new quantum physics experiment conceptual tool e g a new theorem a new theory cognitive tool e g mathematical notation software tool communication protocol that enable scientist to well collaborate e g the internet yet modern scientific progress be measurably linear I write about this phenomenon at length in a 2012 essay title the singularity be not come we didn t make great progress in physics over the 1950 2000 period than we do over 1900 1950 — we do arguably about as well mathematic be not advance significantly fast today than it do in 1920 medical science have be make linear progress on essentially all of its metric for decade and this be despite we invest exponential effort into science — the headcount of researcher double roughly once every 15 to 20 year and these researcher be use exponentially fast computer to improve their productivity how come what bottleneck and adversarial counter reaction be slow down recursive self improvement in science so many I can t even count they here be a few importantly every single one of they would also apply to recursively self improve ais in practice system bottleneck diminish return and adversarial reaction end up squash recursive self improvement in all of the recursive process that surround we self improvement do indeed lead to progress but that progress tend to be linear or at good sigmoidal your first seed dollar invest will not typically lead to a wealth explosion ; instead a balance between investment return and grow spending will usually lead to a roughly linear growth of your saving over time and that s for a system that be order of magnitude simple than a self improve mind likewise the first superhuman ai will just be another step on a visibly linear ladder of progress that we start climb long ago the expansion of intelligence can only come from a co evolution of brain biological or digital sensorimotor affordance environment and culture — not from merely tune the gear of some brain in a jar in isolation such a co evolution have already be happen for eon and will continue as intelligence move to an increasingly digital substrate no intelligence explosion will occur as this process advance at a roughly linear pace @fchollet november 2017 marketing footnote my book deep learning with python have just be release if you have python skill and you want to understand what deep learning can and can not do and how to use it to solve difficult real world problem this book be write for you from a quick cheer to a stand ovation clap to show how much you enjoy this story
Max Pechyonkin,23K,8,https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b?source=tag_archive---------8----------------,understand hinton s capsule network part I intuition,part I intuition you be read it now part ii how capsule workpart iii dynamic routing between capsulespart iv capsnet architecture quick announcement about our new publication ai3 we be get the good writer together to talk about the theory practice and business of ai and machine learning follow it to stay up to date on the late trend last week geoffrey hinton and his team publish two paper that introduce a completely new type of neural network base on so call capsule in addition to that the team publish an algorithm call dynamic routing between capsule that allow to train such a network for everyone in the deep learning community this be huge news and for several reason first of all hinton be one of the founder of deep learning and an inventor of numerous model and algorithm that be widely use today secondly these paper introduce something completely new and this be very exciting because it will most likely stimulate additional wave of research and very cool application in this post I will explain why this new architecture be so important as well as intuition behind it in the follow post I will dive into technical detail however before talk about capsule we need to have a look at cnn which be the workhorse of today s deep learning cnns convolutional neural network be awesome they be one of the reason deep learning be so popular today they can do amazing thing that people use to think computer would not be capable of do for a long long time nonetheless they have their limit and they have fundamental drawback let we consider a very simple and non technical example imagine a face what be the component we have the face oval two eye a nose and a mouth for a cnn a mere presence of these object can be a very strong indicator to consider that there be a face in the image orientational and relative spatial relationship between these component be not very important to a cnn how do cnn work the main component of a cnn be a convolutional layer its job be to detect important feature in the image pixel layer that be deeply close to the input will learn to detect simple feature such as edge and color gradient whereas high layer will combine simple feature into more complex feature finally dense layer at the top of the network will combine very high level feature and produce classification prediction an important thing to understand be that high level feature combine low level feature as a weighted sum activation of a precede layer be multiply by the follow layer neuron s weight and add before be pass to activation nonlinearity nowhere in this setup there be pose translational and rotational relationship between simple feature that make up a high level feature cnn approach to solve this issue be to use max pooling or successive convolutional layer that reduce spacial size of the datum flow through the network and therefore increase the field of view of high layer s neuron thus allow they to detect high order feature in a large region of the input image max pooling be a crutch that make convolutional network work surprisingly well achieve superhuman performance in many area but do not be fool by its performance while cnn work well than any model before they max pooling nonetheless be lose valuable information hinton himself state that the fact that max pooling be work so well be a big mistake and a disaster of course you can do away with max pooling and still get good result with traditional cnn but they still do not solve the key problem in the example above a mere presence of 2 eye a mouth and a nose in a picture do not mean there be a face we also need to know how these object be orient relative to each other computer graphic deal with construct a visual image from some internal hierarchical representation of geometric datum note that the structure of this representation need to take into account relative position of object that internal representation be store in computer s memory as array of geometrical object and matrix that represent relative position and orientation of these object then special software take that representation and convert it into an image on the screen this be call render inspire by this idea hinton argue that brain in fact do the opposite of render he call it inverse graphic from visual information receive by eye they deconstruct a hierarchical representation of the world around we and try to match it with already learn pattern and relationship store in the brain this be how recognition happen and the key idea be that representation of object in the brain do not depend on view angle so at this point the question be how do we model these hierarchical relationship inside of a neural network the answer come from computer graphic in 3d graphic relationship between 3d object can be represent by a so call pose which be in essence translation plus rotation hinton argue that in order to correctly do classification and object recognition it be important to preserve hierarchical pose relationship between object part this be the key intuition that will allow you to understand why capsule theory be so important it incorporate relative relationship between object and it be represent numerically as a 4d pose matrix when these relationship be build into internal representation of datum it become very easy for a model to understand that the thing that it see be just another view of something that it have see before consider the image below you can easily recognize that this be the statue of liberty even though all the image show it from different angle this be because internal representation of the statue of liberty in your brain do not depend on the view angle you have probably never see these exact picture of it but you still immediately know what it be for a cnn this task be really hard because it do not have this build in understanding of 3d space but for a capsnet it be much easy because these relationship be explicitly model the paper that use this approach be able to cut error rate by 45 % as compare to the previous state of the art which be a huge improvement another benefit of the capsule approach be that it be capable of learn to achieve state of the art performance by only use a fraction of the datum that a cnn would use hinton mention this in his famous talk about what be wrong with cnn in this sense the capsule theory be much close to what the human brain do in practice in order to learn to tell digit apart the human brain need to see only a couple of dozen of example hundred at most cnn on the other hand need ten of thousand of example to achieve very good performance which seem like a brute force approach that be clearly inferior to what we do with our brain the idea be really simple there be no way no one have come up with it before and the truth be hinton have be think about this for decade the reason why there be no publication be simply because there be no technical way to make it work before one of the reason be that computer be just not powerful enough in the pre gpu base era before around 2012 another reason be that there be no algorithm that allow to implement and successfully learn a capsule network in the same fashion the idea of artificial neuron be around since 1940 s but it be not until mid 1980 s when backpropagation algorithm show up and allow to successfully train deep network in the same fashion the idea of capsule itself be not that new and hinton have mention it before but there be no algorithm up until now to make it work this algorithm be call dynamic routing between capsule this algorithm allow capsule to communicate with each other and create representation similar to scene graph in computer graphic capsule introduce a new building block that can be use in deep learning to well model hierarchical relationship inside of internal knowledge representation of a neural network intuition behind they be very simple and elegant hinton and his team propose a way to train such a network make up of capsule and successfully train it on a simple data set achieve state of the art performance this be very encouraging nonetheless there be challenge current implementation be much slow than other modern deep learning model time will show if capsule network can be train quickly and efficiently in addition we need to see if they work well on more difficult data set and in different domain in any case the capsule network be a very interesting and already work model which will definitely get more develop over time and contribute to further expansion of deep learning application domain this conclude part one of the series on capsule network in the part ii more technical part I will walk you through the capsnet s internal working step by step you can follow I on twitter let s also connect on linkedin from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learn the ai revolution be here navigate the ever change industry with our thoughtfully write article whether your a researcher engineer or entrepreneur
Slav Ivanov,3.9K,17,https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=tag_archive---------9----------------,the $ 1700 great deep learning box assembly setup and benchmark,update april 2018 use cuda 9 cudnn 7 and tensorflow 1 5 after year of use a thin client in the form of increasingly thin macbook I have get use to it so when I get into deep learning dl I go straight for the brand new at the time amazon p2 cloud server no upfront cost the ability to train many model simultaneously and the general coolness of have a machine learning model out there slowly teach itself however as time pass the aws bill steadily grow large even as I switch to 10x cheap spot instance also I didn t find myself train more than one model at a time instead I d go to lunch workout etc while the model be train and come back later with a clear head to check on it but eventually the model complexity grow and take long to train I d often forget what I do differently on the model that have just complete its 2 day training nudge by the great experience of the other folk on the fast ai forum I decide to settle down and to get a dedicated dl box at home the most important reason be save time while prototyping model — if they train fast the feedback time would be short thus it would be easy for my brain to connect the dot between the assumption I have for the model and its result then I want to save money — I be use amazon web service aws which offer p2 instance with nvidia k80 gpus lately the aws bill be around $ 60 70 month with a tendency to get large also it be expensive to store large dataset like imagenet and lastly I haven t have a desktop for over 10 year and want to see what have change in the meantime spoiler alert mostly nothing what follow be my choice inner monologue and gotcha from choose the component to benchmarke a sensible budget for I would be about 2 year worth of my current compute spending at $ 70 month for aw this put it at around $ 1700 for the whole thing you can check out all the component use the pc part picker site be also really helpful in detect if some of the component don t play well together the gpu be the most crucial component in the box it will train these deep network fast shorten the feedback cycle disclosure the follow be affiliate link to help I pay for well more gpu the choice be between a few of nvidia s card gtx 1070 gtx 1070 ti gtx 1080 gtx 1080 ti and finally the titan x the price might fluctuate especially because some gpu be great for cryptocurrency mining wink 1070 wink on performance side gtx 1080 ti and titan x be similar roughly speak the gtx 1080 be about 25 % fast than gtx 1070 and gtx 1080 ti be about 30 % fast than gtx 1080 the new gtx 1070 ti be very close in performance to gtx 1080 tim dettmer have a great article on pick a gpu for deep learning which he regularly update as new card come on the market here be the thing to consider when pick a gpu consider all of this I pick the gtx 1080 ti mainly for the training speed boost I plan to add a second 1080 ti soonish even though the gpu be the mvp in deep learning the cpu still matter for example data preparation be usually do on the cpu the number of core and thread per core be important if we want to parallelize all that data prep to stay on budget I pick a mid range cpu the intel i5 7500 it s relatively cheap but good enough to not slow thing down edit as a few people have point out probably the big gotcha that be unique to dl multi gpu be to pay attention to the pcie lane support by the cpu motherboard by andrej karpathy we want to have each gpu have 16 pcie lane so it eat datum as fast as possible 16 gb s for pcie 3 0 this mean that for two card we need 32 pcie lane however the cpu I have pick have only 16 lane so 2 gpu would run in 2x8 mode instead of 2x16 this might be a bottleneck lead to less than ideal utilization of the graphic card thus a cpu with 40 line be recommend edit 2 however tim dettmer point out that have 8 lane per card should only decrease performance by 0 10 % for two gpu so currently my recommendation be go with 16 pcie lane per video card unless it get too expensive for you otherwise 8 lane should do as well a good solution with to have for a double gpu machine would be an intel xeon processor like the e5 1620 v4 40 pcie lane or if you want to splurge go for a high end processor like the desktop i7 6850k memory ram it s nice to have a lot of memory if we be to be work with rather big dataset I get 2 stick of 16 gb for a total of 32 gb of ram and plan to buy another 32 gb later follow jeremy howard s advice I get a fast ssd disk to keep my os and current datum on and then a slow spin hdd for those huge dataset like imagenet ssd I remember when I get my first macbook air year ago how blow away be I by the ssd speed to my delight a new generation of ssd call nvme have make its way to market in the meantime a 480 gb mydigitalssd nvme drive be a great deal this baby copy file at gigabyte per second hdd 2 tb seagate while ssds have be get fast hdd have be get cheap to somebody who have use macbook with 128 gb disk for the last 7 year have this much space feel almost obscene the one thing that I keep in mind when pick a motherboard be the ability to support two gtx 1080 ti both in the number of pci express lane the minimum be 2x8 and the physical size of 2 card also make sure it s compatible with the choose cpu an asus tuf z270 do it for I msi — x99a sli plus should work great if you get an intel xeon cpu rule of thumb power supply should provide enough juice for the cpu and the gpu plus 100 watt extra the intel i5 7500 processor use 65w and the gpus 1080 ti need 250w each so I get a deepcool 750w gold psu currently unavailable evga 750 gq be similar the gold here refer to the power efficiency I e how much of the power consume be waste as heat the case should be the same form factor as the motherboard also have enough led to embarrass a burner be a bonus a friend recommend the thermaltake n23 case which I promptly get no led sadly here be how much I spend on all the component your cost may vary $ 700 gtx 1080 ti + $ 190 cpu + $ 230 ram + $ 230 ssd + $ 66 hdd + $ 130 motherboard + $ 75 psu + $ 50 case = = = = = = = = = = = = $ 1671 total add tax and fee this nicely match my preset budget of $ 1700 if you don t have much experience with hardware and fear you might break something a professional assembly might be the good option however this be a great learning opportunity that I couldn t pass even though I ve have my share of hardware relate horror story the first and important step be to read the installation manual that come with each component especially important for I as I ve do this before once or twice and I have just the right amount of inexperience to mess thing up this be do before instal the motherboard in the case next to the processor there be a lever that need to be pull up the processor be then place on the base double check the orientation finally the lever come down to fix the cpu in place but I have a quite the difficulty do this once the cpu be in position the lever wouldn t go down I actually have a more hardware capable friend of mine video walk I through the process turn out the amount of force require to get the lever lock down be more than what I be comfortable with next be fix the fan on top of the cpu the fan leg must be fully secure to the motherboard consider where the fan cable will go before instal the processor I have come with thermal paste if yours doesn t make sure to put some paste between the cpu and the cool unit also replace the paste if you take off the fan I put the power supply unit psu in before the motherboard to get the power cable snugly place in case back side pretty straight forward — carefully place it and screw it in a magnetic screwdriver be really helpful then connect the power cable and the case button and led just slide it in the m2 slot and screw it in piece of cake the memory prove quite hard to install require too much effort to properly lock in a few time I almost give up thinking I must be do it wrong eventually one of the stick click in and the other one promptly follow at this point I turn the computer on to make sure it work to my relief it start right away finally the gpu slide in effortlessly 14 pin of power later and it be run nb do not plug your monitor in the external card right away most probably it need driver to function see below finally it s complete now that we have the hardware in place only the soft part remain out with the screwdriver in with the keyboard note on dual booting if you plan to install window because you know for benchmark totally not for game it would be wise to do window first and linux second I didn t and have to reinstall ubuntu because window mess up the boot partition livewire have a detailed article on dual boot most dl framework be design to work on linux first and eventually support other operating system so I go for ubuntu my default linux distribution an old 2 gb usb drive be lay around and work great for the installation unetbootin osx or rufus window can prepare the linux thumb drive the default option work fine during the ubuntu install at the time of write ubuntu 17 04 be just release so I opt for the previous version 16 04 whose quirk be much well document online ubuntu server or desktop the server and desktop edition of ubuntu be almost identical with the notable exception of the visual interface call x not be instal with server I instal the desktop and disabled autostarte x so that the computer would boot it in terminal mode if need one could launch the visual desktop later by type startx let s get our install up to date from jeremy howard s excellent install gpu script to deep learn on our machine we need a stack of technology to use our gpu download cuda from nvidia or just run the code below update to specify version 9 of cuda thank to @zhanwenchen for the tip if you need to add later version of cuda click here after cuda have be instal the follow code will add the cuda installation to the path variable now we can verify that cuda have be instal successfully by run this should have instal the display driver as well for I nvidia smi show err as the device name so I instal the late nvidia driver as of may 2018 to fix it remove cuda nvidia driver if at any point the driver or cuda seem break as they do for I — multiple time it might be well to start over by run since version 1 5 tensorflow support cudnn 7 so we install that to download cudnn one need to register for a free developer account after download install with the follow anaconda be a great package manager for python I ve move to python 3 6 so will be use the anaconda 3 version the popular dl framework by google installation validate tensorfow install to make sure we have our stack run smoothly I like to run the tensorflow mnist example we should see the loss decrease during training keras be a great high level neural network framework an absolute pleasure to work with installation can t be easy too pytorch be a newcomer in the world of dl framework but its api be model on the successful torch which be write in lua pytorch feel new and exciting mostly great although some thing be still to be implement we install it by run jupyter be a web base ide for python which be ideal for data sciency task it s instal with anaconda so we just configure and test it now if we open http localhost 8888 we should see a jupyter screen run jupyter on boot rather than run the notebook every time the computer be restart we can set it to autostart on boot we will use crontab to do this which we can edit by run crontab e then add the following after the last line in the crontab file I use my old trusty macbook air for development so I d like to be able to log into the dl box both from my home network also when on the run ssh key it s way more secure to use a ssh key to login instead of a password digital ocean have a great guide on how to setup this ssh tunnel if you want to access your jupyter notebook from another computer the recommend way be to use ssh tunneling instead of open the notebook to the world and protect with a password let s see how we can do this 2 then to connect over ssh tunnel run the follow script on the client to test this open a browser and try http localhost 8888 from the remote machine your jupyter notebook should appear setup out of network access finally to access the dl box from the outside world we need 3 thing set up out of network access depend on the router network setup so I m not go into detail now that we have everything run smoothly let s put it to the test we ll be compare the newly build box to an aws p2 xlarge instance which be what I ve use so far for dl the test be computer vision relate mean convolutional network with a fully connect model throw in we time training model on aws p2 instance gpu k80 aw p2 virtual cpu the gtx 1080 ti and intel i5 7500 cpu andre hernandez point out that my comparison do not use tensorflow that be optimize for these cpu which would have help the they perform well check his insightful comment for more detail the hello world of computer vision the mnist database consist of 70 000 handwritten digit we run the keras example on mnist which use multilayer perceptron mlp the mlp mean that we be use only fully connect layer not convolution the model be train for 20 epoch on this dataset which achieve over 98 % accuracy out of the box we see that the gtx 1080 ti be 2 4 time fast than the k80 on aws p2 in train the model this be rather surprising as these 2 card should have about the same performance I believe this be because of the virtualization or underclocking of the k80 on aws the cpus perform 9 time slow than the gpu as we will see later it s a really good result for the processor this be due to the small model which fail to fully utilize the parallel processing power of the gpu interestingly the desktop intel i5 7500 achieve 2 3x speedup over the virtual cpu on amazon a vgg net will be finetune for the kaggle dog vs cat competition in this competition we need to tell apart picture of dog and cat run the model on cpus for the same number of batch wasn t feasible therefore we finetune for 390 batch 1 epoch on the gpu and 10 batch on the cpus the code use be on github the 1080 ti be 5 5 time fast that the aws gpu k80 the difference in the cpus performance be about the same as the previous experiment i5 be 2 6x fast however it s absolutely impractical to use cpu for this task as the cpus be take ~200x more time on this large model that include 16 convolutional layer and a couple semi wide 4096 fully connect layer on top a gan generative adversarial network be a way to train a model to generate image gan achieve this by pit two network against each other a generator which learn to create well and well image and a discriminator that try to tell which image be real and which be dream up by the generator the wasserstein gan be an improvement over the original gan we will use a pytorch implementation that be very similar to the one by the wgan author the model be train for 50 step and the loss be all over the place which be often the case with gan cpus aren t consider the gtx 1080 ti finish 5 5x fast than the aws p2 k80 which be in line with the previous result the final benchmark be on the original style transfer paper gatys et al implement on tensorflow code available style transfer be a technique that combine the style of one image a painting for example and the content of another image check out my previous post for more detail on how style transfer work the gtx 1080 ti outperform the aws k80 by a factor of 4 3 this time the cpu be 30 50 time slow than graphic card the slowdown be less than on the vgg finetune task but more than on the mnist perceptron experiment the model use mostly the early layer of the vgg network and I suspect this be too shallow to fully utilize the gpus the dl box be in the next room and a large model be train on it be it a wise investment time will tell but it be beautiful to watch the glow led in the dark and to hear its quiet hum as model be try to squeeze out that extra accuracy percentage point from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur hacker machine learn deep learning and other type of learn
Geoff Nesnow,14.9K,19,https://medium.com/@DonotInnovate/73-mind-blowing-implications-of-a-driverless-future-58d23d1f338d?source=tag_archive---------1----------------,73 mind blow implication of a driverless future,I originally write and publish a version of this article in september 2016 since then quite a bit have happen far cement my view that these change be come and that the implication will be even more substantial I decide it be time to update this article with some additional idea and a few change as I write this uber just announce that it just order 24 000 self drive volvos tesla just release an electric long haul tractor trailer with extraordinary technical spec range performance and self drive capability up just preordere 125 and tesla just announce what will probably be the quick production car ever make — perhaps the fast it will go zero to sixty in about the time it take you to read zero to sixty and of course it will be able to drive itself the future be quickly become now google just order thousand of chrysler for its self drive fleet that be already on the road in az in september of 2016 uber have just roll out its first self drive taxi in pittsburgh tesla and mercede be roll out limited self drive capability and city around the world be negotiate with company who want to bring self drive car and truck to their city since then all of the major car company have announce significant step towards mostly or entirely electric vehicle more investment have be make in autonomous vehicle driverless truck now seem to be lead rather than follow in term of the first large scale implementation and there ve be a few more incident I e accident I believe that the timeframe for significant adoption of this technology have shrink in the past year as technology have get well fast and as the trucking industry have increase its level of interest and investment I believe that my daughter who be now just over 1 year old will never have to learn to drive or own a car the impact of driverless vehicle will be profound and impact almost every part of our life below be my update thought about what a driverless future will be like some of these update be from feedback to my original article thank to those who contribute some be base on technology advance in the past year and other be just my own speculation what could happen when car and truck drive themselves 1 people win t own their own car transport will be deliver as a service from company who own fleet of self drive vehicle there be so many technical economic safety advantage to the transportation as a service that this change may come much fast than most people expect own a vehicle as an individual will become a novelty for collector and maybe competitive racer 2 software technology company will own more of the world s economy as company like uber google and amazon turn transportation into a pay as you go service software will indeed eat this world over time they ll own so much datum about people pattern route and obstacle that new entrant will have huge barrier to enter the market 3 without government intervention or some sort of organize movement there will be a tremendous transfer of wealth to a very small number of people who own the software battery power manufacturing vehicle servicing and charge power generation maintenance infrastructure there will be massive consolidation of company serve these market as scale and efficiency will become even more valuable car perhaps they ll be rename with some sort of clever acronym will become like the router that run the internet — most consumer win t know or care who make they or who own they 4 vehicle design will change radically — vehicle win t need to withstand crash in the same way all vehicle will be electric self drive + software + service provider = all electric they may look different come in very different shape and size maybe attach to each other in some situation there will likely be many significant innovation in material use for vehicle construction — for example tire and brake will be re optimize with very different assumption especially around variability of load and much more control environment the body will likely be primarily make of composite like carbon fiber and fiberglas and 3d print electric vehicle with no driver control will require 1 10th or few the number of part perhaps even 1 100th and thus will be quick to produce and require much less labor there may even be design with almost no move part other than wheel and motor obviously 5 vehicle will mostly swap battery rather than serve as the host of battery charge battery will be charge in distribute and highly optimize center — likely own by the same company as the vehicle or another national vendor there may be some entrepreneurial opportunity and a marketplace for battery charging and swap but this industry will likely be consolidate quickly the battery will be exchange without human intervention — likely in a carwash like drive thru 6 vehicle be electric will be able to provide portable power for a variety of purpose which will also be sell as a service — construction job site why use generator disaster power failure event etc they may even temporarily or permanently replace power distribution network I e power line for remote location — imagine a distribute power generation network with autonomous vehicle provide last mile service to some location 7 driver s license will slowly go away as will the department of motor vehicle in most state other form of i d may emerge as people no long carry driver s license this will probably correspond with the inevitable digitization of all personal identification — via print retina scan or other biometric scanning 8 there win t be any parking lot or parking space on road or in building garage will be repurpose — maybe as mini loading dock for people and delivery aesthetic of home and commercial building will change as parking lot and space go away there will be a multi year boom in landscaping and basement and garage conversion as these space become available 9 traffic policing will become redundant police transport will also likely change quite a bit unmanned police vehicle may become more common and police officer may use commercial transportation to move around routinely this may dramatically change the nature of police with newfound resource from the lack of traffic policing and dramatically less time spend move around 10 there will be no more local mechanic car dealer consumer car wash auto part store or gas station town that have be build around major thoroughfare will change or fade 11 the auto insurance industry as we know it will go away as will the significant investing power of the major player of this industry most car company will go out of business as will most of their enormous supplier network there will be many few net vehicle on the road maybe 1 10th perhaps even less that be also more durable made of few part and much more commoditized 12 traffic light and sign will become obsolete vehicle may not even have headlight as infrared and radar take the place of the human light spectrum the relationship between pedestrian and bicycle and car and truck will likely change dramatically some will come in the form of cultural and behavioral change as people travel in group more regularly and walk or cycling become practical in place where it isn t today 13 multi modal transportation will become a more integrated and normal part of our way of move around in other word we ll often take one type of vehicle to another especially when travel long distance with coordination and integration the elimination of parking and more deterministic pattern it will become ever more efficient to combine mode of transport 14 the power grid will change power station via alternative power source will become more competitive and local consumer and small business with solar panel small scale tidal or wave power generator windmill and other local power generation will be able to sell kilowatthour to the company who own the vehicle this will change net metering rule and possibly upset the overall power delivery model it might even be the beginning of truly distribute power creation and transport there will likely be a significant boom in innovation in power production and delivery model over time ownership of these service will probably be consolidate across a very small number of company 15 traditional petroleum product and other fossil fuel will become much less valuable as electric car replace fuel power vehicle and as alternative energy source become more viable with portability of power transmission and conversion eat ton of power there be many geopolitical implication to this possible shift as implication of climate change become ever clear and present these trend will likely accelerate petroleum will continue to be valuable for make plastic and other derive material but will not be burn for energy at any scale many company oil rich country and investor have already begin accommodate for these change 16 entertainment funding will change as the auto industry s ad spending go away think about how many ad you see or hear about car car financing car insurance car accessory and car dealer there be likely to be many other structural and cultural change that come from the dramatic change to the transportation industry we ll stop say shift into high gear and other drive relate colloquialism as the reference will be lose on future generation 17 the recent corporate tax rate reduction in the act to provide for reconciliation pursuant to titles ii and v of the concurrent resolution on the budget for fiscal year 2018 will accelerate investment in automation include self drive vehicle and other form of transportation automation flush with new cash and incentive to invest capital soon many business will invest in technology and solution that reduce their labor cost 18 the car financing industry will go away as will the newly huge derivative market for package sub prime auto loan which will likely itself cause a version of the 2008 2009 financial crisis as it blow up 19 increase in unemployment increase student loan vehicle and other debt default could quickly spiral into a full depression the world that emerge on the other side will likely have even more dramatic income and wealth stratification as entry level job relate to transportation and the entire supply chain of the exist transportation system go away the convergence of this with hyper automation in production and service delivery ai robotic low cost computing business consolidation etc may permanently change how society be organize and how people spend their time 20 there will be many new innovation in luggage and bag as people no long keep stuff in car and loading and unload package from vehicle become much more automate the traditional trunk size and shape will change trailer or other similar detachable device will become much more commonplace to add storage space to vehicle many additional on demand service will become available as transportation for good and service become more ubiquitous and cheap imagine be able to design 3d print and put on an outfit as you travel to a party or the office if you re still go to an office 21 consumer will have more money as transportation a major cost especially for low income people and family get much cheap and ubiquitous — though this may be offset by dramatic reduction in employment as technology change many time fast than people s ability to adapt to new type of work 22 demand for taxi and truck driver will go down eventually to zero someone bear today might not understand what a truck driver be or even understand why someone would do that job — much like people bear in the last 30 year don t understand how someone could be employ as a switchboard operator 23 the politic will get ugly as lobbyist for the auto and oil industry unsuccessfully try to stop the driverless car they ll get even ugly as the federal government deal with assume huge pension obligation and other legacy cost associate with the auto industry my guess be that these pension obgligation win t ultimately be honor and certain community will be devastate the same may be true of pollution clean up effort around the factory and chemical plant that be once major component of the vehicle supply chain 24 the new player in vehicle design and manufacturing will be a mix of company like uber google and amazon and company you don t yet know there will probably be 2 or 3 major player who control > 80 % of the customer face transportation market there may become api like access to these network for small player — much like app marketplace for iphone and android however the majority of the revenue will flow to a few large player as it do today to apple and google for smartphone 25 supply chain will be disrupt as shipping change algorithm will allow truck to be fuller excess latent capacity will be price cheap new middleman and warehousing model will emerge as shipping get cheap fast and generally easy retail storefront will continue to lose footing in the marketplace 26 the role of mall and other shopping area will continue to shift — to be replace by place people go for service not product there will be virtually no face to face purchase of physical good 27 amazon and or a few other large player will put fedex up and usp out of business as their transportation network become order of magnitude more cost efficient than exist model — largely from a lack of legacy cost like pension high union labor cost and regulation especially usp that win t keep up with the pace of technology change 3d printing will also contribute to this as many day to day product be print at home rather than purchase 28 the same vehicle will often transport people and good as algorithms optimize all route and off peak utilization will allow for other very inexpensive delivery option in other word package will be increasingly deliver at night add autonomous drone aircraft to this mix and there ll be very little reason to believe that traditional carrier fedex usps up etc will survive at all 29 road will be much empty and small over time as self drive car need much less space between they a major cause of traffic today people will share vehicle more than today carpoole traffic flow will be well regulate and algorithmic timing I e leave at 10 versus 9 30 will optimize infrastructure utilization road will also likely be smooth and turn optimally bank for passenger comfort high speed underground and above ground tunnel maybe integrate hyperloop technology or this novel magnetic track solution will become the high speed network for long haul travel 30 short hop domestic air travel may be largely displace by multi modal travel in autonomous vehicle this may be counter by the advent of low cost more automate air travel this too may become part of integrate multi modal transportation 31 road will wear out much more slowly with few vehicle mile light vehicle with less safety requirement new road material will be develop that drain well last long and be more environmentally friendly these material might even be power generating solar or reclamation from vehicle kinetic energy at the extreme they may even be replace by radically different design — tunnel magnetic track other hyper optimize material 32 premium vehicle service will have more compartmentalize privacy more comfort good business feature quiet wifi bluetooth for each passenger etc massage service and bed for sleep they may also allow for meaningful in transit real and virtual meeting this will also likely include aromatherapy many version of in vehicle entertainment system and even virtual passenger to keep you company 33 exhilaration and emotion will almost entirely leave transportation people win t brag about how nice fast comfortable their car be speed will be measure by time between end point not acceleration handling or top speed 34 city will become much more dense as few road and vehicle will be need and transport will be cheap and more available the walkable city will continue to be more desirable as walk and bike become easy and more commonplace when cost and timeframe of transit change so will the dynamic of who live and work where 35 people will know when they leave when they ll get where they re go there will be few excuse for be late we will be able to leave later and cram more into a day we ll also be able to well track kid spouse employee and so forth we ll be able to know exactly when someone will arrive and when someone need to leave to be somewhere at a particular time 36 there will be no more dui oui offense restaurant and bar will sell more alcohol people will consume more as they no long need to consider how to get home and will be able to consume inside vehicle 37 we ll have less privacy as interior camera and usage log will track when and where we go and have go exterior camera will also probably record surrounding include people this may have a positive impact on crime but will open up many complex privacy issue and likely many lawsuit some people may find clever way to game the system — with physical and digital disguise and spoof 38 many lawyer will lose source of revenue — traffic offense crash litigation will reduce dramatically litigation will more likely be big company versus big company or individual against big company not individual against each other these will settle more quickly with less variability lobbyist will probably succeed in change the rule of litigation to favor the big company far reduce the legal revenue relate to transportation force arbitration and other similar clause will become an explicit component of our contractual relationship with transportation provider 39 some country will nationalize part of their self drive transportation network which will result in low cost few disruption and less innovation 40 city town and police force will lose revenue from traffic ticket toll likely replace if not eliminate and fuel tax revenue drop precipitously these will probably be replace by new taxis probably on vehicle mile these may become a major political hot button issue differentiate party as there will probably be a range of regressive versus progressive tax model most likely this will be a highly regressive tax in the us as fuel taxis be today 41 some employer and or government program will begin partially or entirely subsidize transportation for employee and or people who need the help the tax treatment of this perk will also be very political 42 ambulance and other emergency vehicle will likely be use less and change in nature more people will take regular autonomous vehicle instead of ambulance ambulance will transport people fast same may be true of military vehicle 43 there will be significant innovation in first response capability as dependency on people become reduce over time and as distribute staging of capacity become more common 44 airport will allow vehicle right into the terminal maybe even onto the tarmac as increase control and security become possible terminal design may change dramatically as transportation to and from become normalize and integrate the entire nature of air travel may change as integrate multi modal transport get more sophisticated hyper loop high speed rail automate aircraft and other form of rapid travel will gain as traditional hub and speak air travel on relatively large plane lose ground 45 innovative app like marketplace will open up for in transit purchase range from concierge service to food to exercise to merchandise to education to entertainment purchase vr will likely play a large role in this with integrate system vr via headset or screen or hologram will become standard fare for trip more than a few minute in duration 46 transportation will become more tightly integrate and package into many service — dinner include the ride hotel include local transport etc this may even extend to apartment short term rental like airbnb and other service provider 47 local transport of nearly everything will become ubiquitous and cheap — food everything in your local store drone will likely be integrate into vehicle design to deal with last few foot on pickup and delivery this will accelerate the demise of traditional retail store and their local economic impact 48 biking and walking will become easier safe and more common as road get safe and less congested new pathway reclaim from road parking lot roadside parking come online and with cheap reliable transport available as a backup 49 more people will participate in vehicle race car off road motorcycle to replace their emotional connection to drive virtual racing experience may also grow in popularity as few people have the real experience of drive 50 many many few people will be injure or kill on road though we ll expect zero and be disproportionately upset when accident do happen hack and non malicious technical issue will replace traffic as the main cause of delay over time resilience will increase in the system 51 hacking of vehicle will be a serious issue new software and communication company and technology will emerge to address these issue we ll see the first vehicle hacking and its consequence highly distribute computing perhaps use some form of blockchain will likely become part of the solution as a counterbalance to systemic catastrophe — such as many vehicle be affect simultaneously there will probably be a debate about whether and how law enforcement can control observe and restrict transportation 52 many road and bridge will be privatize as a small number of company control most transport and make deal with municipality over time government may entirely stop fund road bridge and tunnel there will be a significant legislative push to privatize more and more of the transportation network much like internet traffic there will likely become tier of prioritization and some notion of in network versus out of network travel and toll for interconnection regulator will have a tough time keep up with these change most of this will be transparent to end user but will probably create enormous barrier to entry for transportation start up and ultimately reduce option for consumer 53 innovator will come along with many awesome use for driveway and garage that no long contain car 54 there will be a new network of clean safe pay to use restroom and other service food drink etc that become part of the value add of compete service provider 55 mobility for senior and people with disability will be greatly improve over time 56 parent will have more option to move around their kid on their own premium secure end to end child s transport service will likely emerge this may change many family relationship and increase the accessibility of service to parent and child it may also far stratify the experience of family with high income and those with low income 57 person to person movement of good will become cheap and open up new market — think about borrow a tool or buy something on craigslist latent capacity will make transport good very inexpensive this may also open up new opportunity for p2p service at a small scale — like prepare food or clean clothe 58 people will be able to eat drink in transit like on a train or plane consume more information read podcast video etc this will open up time for other activity and perhaps increase productivity 59 some people may have their own pod to get into which will then be pick up by an autonomous vehicle move between vehicle automatically for logistic efficiency these may come in variety of luxury and quality — the louis vuitton pod may replace the louis vuitton trunk as the mark of luxury travel 60 there will be no more getaway vehicle or police vehicle chase 61 vehicle will likely be fill to the brim with advertising of all sort much of which you could probably act on in route though there will probably be way to pay more to have an ad free experience this will include highly personalize en route advertising that be particularly relevant to who you be where you re go 62 these innovation will make it to the develop world where congestion today be often remarkably bad and hugely costly pollution level will come down dramatically even more people will move to the city productivity level will go up fortune will be make as these change happen some country and city will be transform for the well some other will likely experience hyper privatization consolidation and monopoly like control this may play out much like the roll out of cell service in these country — fast consolidated and inexpensive 63 payment option will be greatly expand with package deal like cell phone pre pay model pay as you go model be offer digital currency transact automatically via phone device will probably quickly replace traditional cash or credit card payment 64 there will likely be some very clever innovation for movement of pet equipment luggage and other non people item autonomous vehicle in the medium future 10 20 year may have radically different design that support carry significantly more payload 65 some creative marketer will offer to partially or fully subsidize ride where customer deliver value — by take survey by participate in virtual focus group by promote their brand via social medium etc 66 sensor of all sort will be embed in vehicle that will have secondary use — like improve weather forecasting crime detection and prevention find fugitive infrastructure condition such as pothole this datum will be monetize likely by the company who own the transportation service 67 company like google and facebook will add to their database everything about customer movement and location unlike gps chip that only tell they where someone be at the moment and where they ve be autonomous vehicle system will know where you re go in real time and with whom 68 autonomous vehicle will create some new job and opportunity for entrepreneur however these will be off set many time by extraordinary job loss by nearly everyone in the transportation value chain today in the autonomous future a large number of job will go away this include driver which be in many state today the most common job mechanic gas station employee most of the people who make car and car part or support those who do due to huge consolidation of maker and supply chain and manufacture automation the marketing supply chain for vehicle many people who work on and build road bridge employee of vehicle insurance and financing company and their partner supplier toll booth operator most of whom have already be displace many employee of restaurant that support traveler truck stop retail worker and all the people whose business support these different type of company and worker 69 there will be some hardcore hold out who really like driving but over time they ll become a less statistically relevant voting group as young people who ve never drive will outnumber they at first this may be a 50 state regulate system — where drive yourself may actually become illegal in some state in the next 10 year while other state may continue to allow it for a long time some state will try unsuccessfully to block autonomous vehicle 70 there will be lot of discussion about new type of economic system — from universal basic income to new variation of socialism to a more regulated capitalist system — that will result from the enormous impact of autonomous vehicle 71 in the path to a truly driverless future there will be a number of key tipping point at the moment freight delivery may push autonomous vehicle use soon than people transport large trucking company may have the financial mean and legislative influence to make rapid dramatic change they be also well position to support hybrid approach where only part of their fleet or part of the route be automate 72 autonomous vehicle will radically change the power center of the world they will be the beginning of the end of burn hydrocarbon the powerful interest who control these industry today will fight viciously to stop this there may even be war to slow down this process as oil price start to plummet and demand dry up 73 autonomous vehicle will continue to play a large role in all aspect of war — from surveillance to troop robot movement to logistic support to actual engagement drone will be complement by additional on the ground in space in the water and under the water autonomous vehicle note my original article be inspire by a presentation by ryan chin ceo of optimus ridespeak at an mit event about autonomous vehicle he really get I think about how profound these advance could be to our life I m sure some of my thought above come from he from a quick cheer to a stand ovation clap to show how much you enjoy this story co founder @mycityatpeace | faculty @hult_biz | producer @couragetolisten | naturally curious dot connector | more at www geoffnesnow com
Blaise Aguera y Arcas,8.7K,15,https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477?source=tag_archive---------2----------------,do algorithm reveal sexual orientation or just expose our stereotype,by blaise agüera y arcas alexander todorov and margaret mitchell a study claim that artificial intelligence can infer sexual orientation from facial image cause a medium uproar in the fall of 2017 the economist feature this work on the cover of their september 9th magazine ; on the other hand two major lgbtq organization the human right campaign and glaad immediately label it junk science michal kosinski who co author the study with fellow researcher yilun wang initially express surprise call the critique knee jerk reaction however he then proceed to make even bolder claim that such ai algorithm will soon be able to measure the intelligence political orientation and criminal inclination of people from their facial image alone kosinski s controversial claim be nothing new last year two computer scientist from china post a non peer review paper online in which they argue that their ai algorithm correctly categorize criminal with nearly 90 % accuracy from a government i d photo alone technology startup have also begin to crop up claim that they can profile people s character from their facial image these development have prompt the three of we to collaborate early in the year on a medium essay physiognomy s new clothe to confront claim that ai face recognition reveal deep character trait we describe how the junk science of physiognomy have root go back into antiquity with practitioner in every era resurrect belief base on prejudice use the new methodology of the age in the 19th century this include anthropology and psychology ; in the 20th genetic and statistical analysis ; and in the 21st artificial intelligence in late 2016 the paper motivate our physiognomy essay seem well outside the mainstream in tech and academia but as in other area of discourse what recently feel like a fringe position must now be address head on kosinski be a faculty member of stanford s graduate school of business and this new study have be accept for publication in the respected journal of personality and social psychology much of the ensue scrutiny have focus on ethic implicitly assume that the science be valid we will focus on the science the author train and test their sexual orientation detector use 35 326 image from public profile on a us date website composite image of the lesbian gay and straight man and woman in the sample reveal a great deal about the information available to the algorithm clearly there be difference between these four composite face wang and kosinski assert that the key difference be in physiognomy mean that a sexual orientation tend to go along with a characteristic facial structure however we can immediately see that some of these difference be more superficial for example the average straight woman appear to wear eyeshadow while the average lesbian do not glass be clearly visible on the gay man and to a less extent on the lesbian while they seem absent in the heterosexual composite might it be the case that the algorithm s ability to detect orientation have little to do with facial structure but be due rather to pattern in groom presentation and lifestyle we conduct a survey of 8 000 american use amazon s mechanical turk crowdsourcing platform to see if we could independently confirm these pattern ask 77 yes no question such as do you wear eyeshadow do you wear glass and do you have a beard as well as question about gender and sexual orientation the result show that lesbian indeed use eyeshadow much less than straight woman do gay man and woman do both wear glass more and young opposite sex attract man be considerably more likely to have prominent facial hair than their gay or same sex attract peer break down the answer by the age of the respondent can provide a rich and clear view of the datum than any single statistic in the follow figure we show the proportion of woman who answer yes to do you ever use makeup top and do you wear eyeshadow bottom average over 6 year age interval the blue curve represent strictly opposite sex attract woman a nearly identical set to those who answer yes to be you heterosexual or straight ; the cyan curve represent woman who answer yes to either or both of be you sexually attract to woman and be you romantically attract to woman ; and the red curve represent woman who answer yes to be you homosexual gay or lesbian 1 the shaded region around each curve show 68 % confidence interval 2 the pattern reveal here be intuitive ; it win t be break news to most that straight woman tend to wear more makeup and eyeshadow than same sex attract and even more so lesbian identify woman on the other hand these curve also show we how often these stereotype be violate that same sex attract man of most age wear glass significantly more than exclusively opposite sex attract man do might be a bit less obvious but this trend be equally clear 3 a proponent of physiognomy might be tempt to guess that this be somehow relate to difference in visual acuity between these population of man however ask the question do you like how you look in glass reveal that this be likely more of a stylistic choice same sex attract woman also report wear glass more as well as like how they look in glass more across a range of age one can also see how opposite sex attract woman under the age of 40 wear contact lense significantly more than same sex attract woman despite report that they have a vision defect at roughly the same rate far illustrate how the difference be drive by an aesthetic preference 4 similar analysis show that young same sex attract man be much less likely to have hairy face than opposite sex attract man serious facial hair in our plot be define as answer yes to have a goatee beard or moustache but no to stubble overall opposite sex attract man in our sample be 35 % more likely to have serious facial hair than same sex attract man and for man under the age of 31 who be overrepresente on date website this rise to 75 % wang and kosinski speculate in their paper that the faintness of the beard and moustache in their gay male composite might be connect with prenatal underexposure to androgen male hormone result in a feminize effect hence sparser facial hair the fact that we see a cohort of same sex attract man in their 40 who have just as much facial hair as opposite sex attract man suggest a different story in which fashion trend and cultural norm play the dominant role in choice about facial hair among man not differ exposure to hormone early in development the author of the paper additionally note that the heterosexual male composite appear to have dark skin than the other three composite our survey confirm that opposite sex attract man consistently self report have a tan face yes to be your face tan slightly more often than same sex attract man once again wang and kosinski reach for a hormonal explanation writing while the brightness of the facial image might be drive by many factor previous research find that testosterone stimulate melanocyte structure and function lead to a dark skin however a simple answer be suggest by the response to the question do you work outdoors overall opposite sex attract man be 29 % more likely to work outdoors and among man under 31 this rise to 39 % previous research have find that increase exposure to sunlight lead to dark skin 5 none of these result prove that there be no physiological basis for sexual orientation ; in fact ample evidence show we that orientation run much deep than a choice or a lifestyle in a critique aim in part at fraudulent conversion therapy program united states surgeon general david satcher write in a 2001 report sexual orientation be usually determine by adolescence if not early and there be no valid scientific evidence that sexual orientation can be change it follow that if we dig deeply enough into human physiology and neuroscience we will eventually find reliable correlate and maybe even the origin of sexual orientation in our survey we also find some evidence of outwardly visible correlate of orientation that be not cultural perhaps most strikingly very tall woman be overrepresente among lesbian identify respondent 6 however while this be interesting it s very far from a good predictor of woman s sexual orientation makeup and eyeshadow do much well the way wang and kosinski measure the efficacy of their ai gaydar be equivalent to choose a straight and a gay or lesbian face image both from datum hold out during the training process and ask how often the algorithm correctly guess which be which 50 % performance would be no well than random chance for woman guess that the taller of the two be the lesbian achieve only 51 % accuracy — barely above random chance this be because despite the statistically meaningful overrepresentation of tall woman among the lesbian population the great majority of lesbian be not unusually tall by contrast the performance measure in the paper 81 % for gay man and 71 % for lesbian woman seem impressive 7 consider however that we can achieve comparable result with trivial model base only on a handful of yes no survey question about presentation for example for pair of woman one of whom be lesbian the follow not exactly superhuman algorithm be on average 63 % accurate if neither or both woman wear eyeshadow flip a coin ; otherwise guess that the one who wear eyeshadow be straight and the other lesbian add six more yes no question about presentation do you ever use makeup do you have long hair do you have short hair do you ever use colored lipstick do you like how you look in glass and do you work outdoors as additional signal raise the performance to 70 % 8 give how many more detail about presentation be available in a face image 71 % performance no long seem so impressive several study include a recent one in the journal of sex research have show that human judge gaydar be no more reliable than a coin flip when the judgement be base on picture take under well control condition head pose lighting glass makeup etc it s well than chance if these variable be not control for because a person s presentation — especially if that person be out — involve social signal we signal our orientation and many other kind of status presumably in order to attract the kind of attention we want and to fit in with people like we 9 wang and kosinski argue against this interpretation on the ground that their algorithm work on facebook selfie of openly gay man as well as date website selfie the issue however be not whether the image come from a date website or facebook but whether they be self post or take under standardized condition most people present themselves in way that have be calibrate over many year of medium consumption observe other look in the mirror and gauge social reaction in one of the early gaydar study use social medium participant could categorize gay man with about 58 % accuracy ; but when the researcher use facebook image of gay and heterosexual man post by their friend still far from a perfect control the accuracy drop to 52 % if subtle bias in image quality expression and grooming can be pick up on by human these bias can also be detect by an ai algorithm while wang and kosinski acknowledge grooming and style they believe that the chief difference between their composite image relate to face shape argue that gay man s face be more feminine narrow jaw long nose large forehead while lesbian face be more masculine large jaw shorter nose small forehead as with less facial hair on gay man and dark skin on straight man they suggest that the mechanism be gender atypical hormonal exposure during development this echo a widely discredit 19th century model of homosexuality sexual inversion more likely heterosexual man tend to take selfie from slightly below which will have the apparent effect of enlarge the chin shorten the nose shrink the forehead and attenuate the smile see our selfie below this view emphasize dominance — or perhaps more benignly an expectation that the viewer will be short on the other hand as a wedding photographer note in her blog when you shoot from above your eye look big which be generally attractive — especially for woman this may be a heteronormative assessment when a face be photograph from below the nostril be prominent while high shooting angle de emphasize and eventually conceal they altogether look again at the composite image we can see that the heterosexual male face have more pronounced dark spot corresponding to the nostril than the gay male while the opposite be true for the female face this be consistent with a pattern of heterosexual man on average shooting from below heterosexual woman from above as the wedding photographer suggest and gay man and lesbian woman from directly in front a similar pattern be evident in the eyebrow shoot from above make they look more v shape but their apparent shape become flatter and eventually caret shape ^ as the camera be lower shoot from below also make the outer corner of the eye appear low in short the change in the average position of facial landmark be consistent with what we would expect to see from differ selfie angle the ambiguity between shoot angle and the real physical size of facial feature be hard to fully disentangle from a two dimensional image both for a human viewer and for an algorithm although the author be use face recognition technology design to try to cancel out all effect of head pose lighting grooming and other variable not intrinsic to the face we can confirm that this doesn t work perfectly ; that s why multiple distinct image of a person help when group photo by subject in google photo and why a person may initially appear in more than one group tom white a researcher at victoria university in new zealand have experiment with the same facial recognition engine kosinski and wang use vgg face and have find that its output vary systematically base on variable like smile and head pose when he train a classifier base on vgg face s output to distinguish a happy expression from a neutral one it get the answer right 92 % of the time — which be significant give that the heterosexual female composite have a much more pronounced smile change in head pose might be even more reliably detectable ; for 576 test image a classifier be able to pick out the one face to the right with 100 % accuracy in summary we have show how the obvious difference between lesbian or gay and straight face in selfie relate to groom presentation and lifestyle — that be difference in culture not in facial structure these difference include we ve demonstrate that just a handful of yes no question about these variable can do nearly as good a job at guess orientation as supposedly sophisticated facial recognition ai far the current generation of facial recognition remain sensitive to head pose and facial expression therefore — at least at this point — it s hard to credit the notion that this ai be in some way superhuman at out we base on subtle but unalterable detail of our facial structure this doesn t negate the privacy concern the author and various commentator have raise but it emphasize that such concern relate less to ai per se than to mass surveillance which be troubling regardless of the technology use even when as in the day of the stasi in east germany these be nothing but paper file and audiotape like computer or the internal combustion engine ai be a general purpose technology that can be use to automate a great many task include one that should not be undertake in the first place we be hopeful about the confluence of new powerful ai technology with social science but not because we believe in revive the 19th century research program of infer people s inner character from their outer appearance rather we believe ai be an essential tool for understand pattern in human culture and behavior it can expose stereotype inherent in everyday language it can reveal uncomfortable truth as in google s work with the geena davis institute where our face gender classifier establish that man be see and hear nearly twice as often as woman in hollywood movie yet female lead film outperform other at the box office make social progress and hold ourselves to account be more difficult without such hard evidence even when it only confirm our suspicion two of us margaret mitchell and blaise agüera y arca be research scientist specialize in machine learning and ai at google ; agüera y arcas lead a team that include deep learning apply to face recognition and power face group in google photo alex todorov be a professor in the psychology department at princeton where he direct the social perception lab he be the author of face value the irresistible influence of first impression 1 this wording be base on several large national survey which we be able to use to sanity check our number about 6 % of respondent identify as homosexual gay or lesbian and 85 % as heterosexual about 4 % of all gender be exclusively same sex attract of the man 10 % be either sexually or romantically same sex attract and of the woman 20 % just under 1 % of respondent be trans and about 2 % identify with both or neither of the pronoun she and he these number be broadly consistent with other survey especially when consider as a function of age the mechanical turk population skew somewhat young than the overall population of the us and consistent with other study our datum show that young people be far more likely to identify non heteronormatively 2 these be wide for same sex attract and lesbian woman because they be minority population result in a large sampling error the same hold for old people in our sample 3 for the remainder of the plot we stick to opposite sex attract and same sex attract as the count be high and the error bar therefore small ; these category be also somewhat less culturally freight since they rely on question about attraction rather than identity as with eyeshadow and makeup the effect be similar and often even large when compare heterosexual identifying with lesbian or gay identify people 4 although we didn t test this explicitly slightly different rate of laser correction surgery seem a likely cause of the small but grow disparity between opposite sex attract and same sex attract woman who answer yes to the vision defect question as they age 5 this finding may prompt the further question why do more opposite sex attract man work outdoors this be not address by any of our survey question but hopefully the other evidence present here will discourage an essentialist assumption such as straight man be just more outdoorsy without the evidence of a control study that can support the leap from correlation to cause such explanation be a form of logical fallacy sometimes call a just so story an unverifiable narrative explanation for a cultural practice 6 of the 253 lesbian identify woman in the sample 5 or 2 % be over six foot and 25 or 10 % be over 5 9 out of 3 333 heterosexual woman woman who answer yes to be you heterosexual or straight only 16 or 0 5 % be over six foot and 152 or 5 % be over 5 9 7 they note that these figure rise to 91 % for man and 83 % for woman if 5 image be consider 8 these result be base on the simple possible machine learning technique a linear classifier the classifier be train on a randomly choose 70 % of the datum with the remain 30 % of the datum hold out for test over 500 repetition of this procedure the error be 69 53 % ± 2 98 % with the same number of repetition and holdout base the decision on height alone give an error of 51 08 % ± 3 27 % and base it on eyeshadow alone yield 62 96 % ± 2 39 % 9 a longstanding body of work e g goffman s the presentation of self in everyday life 1959 and jones and pittman s toward a general theory of strategic self presentation 1982 delf more deeply into why we present ourselves the way we do both for instrumental reason status power attraction and because our presentation inform and be inform by how we conceive of our social self from a quick cheer to a stand ovation clap to show how much you enjoy this story blaise aguera y arcas lead google s ai group in seattle he found seadragon and be one of the creator of photosynth at microsoft
François Chollet,16.8K,17,https://medium.com/@francois.chollet/what-worries-me-about-ai-ed9df072b704?source=tag_archive---------6----------------,what worry I about ai françois chollet medium,disclaimer these be my own personal view I do not speak for my employer if you quote this article please have the honesty to present these idea as what they be personal speculative opinion to be judge on their own merit if you be around in the 1980 and 1990 you may remember the now extinct phenomenon of computerphobia I have personally witness it a few time as late as the early 2000 — as personal computer be introduce into our life in our workplace and home quite a few people would react with anxiety fear or even aggressivity while some of we be fascinate by computer and awestruck by the potential they could glimpse in they most people didn t understand they they feel alien abstruse and in many way threaten people fear get replace by technology most of we react to technological shift with unease at good panic at bad maybe that be true of any change at all but remarkably most of what we worry about end up never happen fast forward a few year and the computer hater have learn to live with they and to use they for their own benefit computer do not replace we and trigger mass unemployment — and nowadays we couldn t imagine life without our laptop tablet and smartphone threaten change have become comfortable status quo but at the same time as our fear fail to materialize computer and the internet have enable threat that almost no one be warn we about in the 1980 and 1990 ubiquitous mass surveillance hacker go after our infrastructure or our personal data psychological alienation on social medium the loss of our patience and our ability to focus the political or religious radicalization of easily influence mind online hostile foreign power hijack social network to disrupt western democracy if most of our fear turn out to be irrational inversely most of the truly worrying development that have happen in the past as a result of technological change stem from thing that most people didn t worry about until it be already there a hundred year ago we couldn t really forecast that the transportation and manufacturing technology we be develop would enable a new form of industrial warfare that would wipe out ten of million in two world war we didn t recognize early on that the invention of the radio would enable a new form of mass propaganda that would facilitate the rise of fascism in italy and germany the progress of theoretical physics in the 1920s and 1930 wasn t accompany by anxious press article about how these development would soon enable thermonuclear weapon that would place the world forever under the threat of imminent annihilation and today even as alarm have be sound for decade about the most dire problem of our time climate a large fraction 44 % of the american public still choose to ignore it as a civilization we seem to be really bad at correctly identify future threat and rightfully worry about they just as we seem to be extremely prone to panic due to irrational fear today like many time in the past we be face with a new wave of radical change cognitive automation which could be broadly sum up under the keyword ai and like many time in the past we be worried that this new set of technology will harm we — that ai will lead to mass unemployment or that ai will gain an agency of its own become superhuman and choose to destroy we but what if we re worry about the wrong thing like we have almost every single time before what if the real danger of ai be far remote from the superintelligence and singularity narrative that many be panic about today in this post I d like to raise awareness about what really worry I when it come to ai the highly effective highly scalable manipulation of human behavior that ai enable and its malicious use by corporation and government of course this be not the only tangible risk that arise from the development of cognitive technology — there be many other in particular issue relate to the harmful bias of machine learning model other people be raise awareness of these problem far well than I could I choose to write about mass population manipulation specifically because I see this risk as press and direly under appreciate this risk be already a reality today and a number of long term technological trend be go to considerably amplify it over the next few decade as our life become increasingly digitize social medium company get ever great visibility into our life and mind at the same time they gain increase access to behavioral control vector — in particular via algorithmic newsfeed which control our information consumption this cast human behavior as an optimization problem as an ai problem it become possible for social medium company to iteratively tune their control vector in order to achieve specific behavior just like a game ai would iterative refine its play strategy in order to beat a level drive by score feedback the only bottleneck to this process be the intelligence of the algorithm in the loop — and as it happen the large social network company be currently invest billion in fundamental ai research let I explain in detail in the past 20 year our private and public life have move online we spend an ever great fraction of each day stare at screen our world be move to a state where most of what we do consist of digital information consumption modification or creation a side effect of this long term trend be that corporation and government be now collect staggering amount of datum about we in particular through social network service who we communicate with what we say what content we ve be consume — image movie music news what mood we be in at specific time ultimately almost everything we perceive and everything we do will end up record on some remote server this datum in theory allow the entity that collect it to build extremely accurate psychological profile of both individual and group your opinion and behavior can be cross correlate with that of thousand of similar people achieve an uncanny understanding of what make you tick — probably more predictive than what yourself could achieve through mere introspection for instance facebook like enable algorithm to well assess your personality that your own friend could this data make it possible to predict a few day in advance when you will start a new relationship and with whom and when you will end your current one or who be at risk of suicide or which side you will ultimately vote for in an election even while you re still feel undecided and it s not just individual level profiling power — large group can be even more predictable as aggregate data point erase randomness and individual outlier passive data collection be not where it end increasingly social network service be in control of what information we consume what see in our newsfeed have become algorithmically curate opaque social medium algorithm get to decide to an ever increase extent which political article we read which movie trailer we see who we keep in touch with whose feedback we receive on the opinion we express integrate over many year of exposure the algorithmic curation of the information we consume give the algorithm in charge considerable power over our life — over who we be who we become if facebook get to decide over the span of many year which news you will see real or fake whose political status update you ll see and who will see yours then facebook be in effect in control of your worldview and your political belief facebook s business lie in influence people that s what the service it sell to its customer — advertiser include political advertiser as such facebook have build a fine tune algorithmic engine that do just that this engine isn t merely capable of influence your view of a brand or your next smart speaker purchase it can influence your mood tune the content it feed you in order to make you angry or happy at will it may even be able to swing election in short social network company can simultaneously measure everything about we and control the information we consume and that s an accelerate trend when you have access to both perception and action you re look at an ai problem you can start establish an optimization loop for human behavior in which you observe the current state of your target and keep tune what information you feed they until you start observe the opinion and behavior you want to see a large subset of the field of ai — in particular reinforcement learning — be about develop algorithm to solve such optimization problem as efficiently as possible to close the loop and achieve full control of the target at hand — in this case we by move our life to the digital realm we become vulnerable to that which rule it — ai algorithm this be make all the easy by the fact that the human mind be highly vulnerable to simple pattern of social manipulation consider for instance the follow vector of attack from an information security perspective you would call these vulnerability know exploit that can be use to take over a system in the case of the human mind these vulnerability never get patch they be just the way we work they re in our dna the human mind be a static vulnerable system that will come increasingly under attack from ever smart ai algorithm that will simultaneously have a complete view of everything we do and believe and complete control of the information we consume remarkably mass population manipulation — in particular political control — arise from place ai algorithm in charge of our information diet do not necessarily require very advanced ai you don t need self aware superintelligent ai for this to be a dire threat — current technology may well suffice social network company have be work on it for a few year with significant result and while they may only be try to maximize engagement and to influence your purchase decision rather than to manipulate your view of the world the tool they ve develop be already be hijack by hostile state actor for political purpose — as see in the 2016 brexit referendum or the 2016 we presidential election this be already our reality but if mass population manipulation be already possible today — in theory — why hasn t the world be upend yet in short I think it s because we re really bad at ai but that may be about to change until 2015 all ad target algorithm across the industry be run on mere logistic regression in fact that s still true to a large extent today — only the big player have switch to more advanced model logistic regression an algorithm that predate the computing era be one of the most basic technique you could use for personalization it be the reason why so many of the ad you see online be desperately irrelevant likewise the social medium bot use by hostile state actor to sway public opinion have little to no ai in they they re all extremely primitive for now machine learning and ai have be make fast progress in recent year and that progress be only begin to get deploy in target algorithm and social medium bot deep learning have only start to make its way into newsfeed and ad network in 2016 who know what will be next it be quite striking that facebook have be invest enormous amount in ai research and development with the explicit goal of become a leader in the field when your product be a social newsfeed what use be you go to make of natural language processing and reinforcement learning we re look at a company that build fine grain psychological profile of almost two billion human that serve as a primary news source for many of they that run large scale behavior manipulation experiment and that aim at develop the good ai technology the world have ever see personally it scare I and consider that facebook may not even be the most worrying threat here ponder for instance china s use of information control to enable unprecedented form of totalitarianism such as its social credit system many people like to pretend that large corporation be the all powerful ruler of the modern world but what power they hold be dwarf by that of government if give algorithmic control over our mind government may well turn into far bad actor than corporation now what can we do about it how can we defend ourselves as technologist what can we do to avert the risk of mass manipulation via our social newsfeed importantly the existence of this threat doesn t mean that all algorithmic curation be bad or that all targeted advertising be bad far from it both of these can serve a valuable purpose with the rise of the internet and ai place algorithm in charge of our information diet isn t just an inevitable trend — it s a desirable one as our life become increasingly digital and connected and as our world become increasingly information intensive we will need ai to serve as our interface to the world in the long run education and self development will be some of the most impactful application of ai — and this will happen through dynamic that almost entirely mirror that of a nefarious ai enable newsfeed try to manipulate you algorithmic information management have tremendous potential to help we to empower individual to realize more of their potential and to help society well manage itself the issue be not ai itself the issue be control instead of let newsfeed algorithms manipulate the user to achieve opaque goal such as sway their political opinion or maximally waste their time we should put the user in charge of the goal that the algorithm optimize for we be talk after all about your news your worldview your friend your life — the impact that technology have on you should naturally be place under your own control information management algorithm should not be a mysterious force inflict on we to serve end that run opposite to our own interest ; instead they should be a tool in our hand a tool that we can use for our own purpose say for education and personal instead of entertainment here s an idea — any algorithmic newsfeed with significant adoption should we should build ai to serve human not to manipulate they for profit or political gain what if newsfeed algorithms didn t operate like casino operator or propagandist what if instead they be close to a mentor or a good librarian someone who use their keen understanding of your psychology — and that of million of other similar people — to recommend to you that next book that will most resonate with your objective and make you grow a sort of navigation tool for your life — an ai capable of guide you through the optimal path in experience space to get where you want to go can you imagine look at your own life through the lens of a system that have see million of life unfold or write a book together with a system that have read every book or conduct research in collaboration with a system that see the full scope of current human knowledge in product where you be fully in control of the ai that interact with you a more sophisticated algorithm instead of be a threat would be a net positive letting you achieve your own goal more efficiently in summary our future be one where ai will be our interface to the world — a world make of digital information this can equally lead to empower individual to gain great control over their life or to a total loss of agency unfortunately social medium be currently engage on the wrong road but it s still early enough that we can reverse course as an industry we need to develop product category and market where the incentive be align with place the user in charge of the algorithm that affect they instead of use ai to exploit the user s mind for profit or political gain we need to strive towards product that be the anti facebook in the far future such product will likely take the form of ai assistant digital mentor program to help you that put you in control of the objective they pursue in their interaction with you and in the present search engine could be see as an early more primitive example of an ai drive information interface that serve user instead of seek to hijack their mental space search be a tool that you deliberately use to reach specific goal rather than a passive always on feed that elect what to show you you tell it what to it should do for you and instead of seek to maximally waste your time a search engine attempt to minimize the time it take to go from question to answer from problem to solution you may be think since a search engine be still an ai layer between we and the information we consume could it bias its result to attempt to manipulate we yes that risk be latent in every information management algorithm but in stark contrast with social network market incentive in this case be actually align with user need push search engine to be as relevant and objective as possible if they fail to be maximally useful there s essentially no friction for user to move to a compete product and importantly a search engine would have a considerably small psychological attack surface than a social newsfeed the threat we ve profile in this post require most of the following to be present in a product most ai drive information management product don t meet these requirement social network on the other hand be a frightening combination of risk factor as technologist we should gravitate towards product that do not feature these characteristic and push back against product that combine they all if only because of their potential for dangerous misuse build search engine and digital assistant not social newsfeed make your recommendation engine transparent configurable and constructive rather than slot like machine that maximize engagement and waste hour of human time invest your ui ux and ai expertise into build great configuration panel for your algorithm to enable your user to use your product on their own term and importantly we should educate user about these issue so that they reject manipulative product generate enough market pressure to align the incentive of the technology industry with that of consumer conclusion the fork in the road ahead one path lead to a place that really scare I the other lead to a more humane future there s still time to take the well one if you work on these technology keep this in mind you may not have evil intention you may simply not care you may simply value your rsus more than our share future but whether or not you care because you have a hand in shape the infrastructure of the digital world your choice affect we all and you may eventually be hold responsible for they from a quick cheer to a stand ovation clap to show how much you enjoy this story
Simon Greenman,10.2K,16,https://towardsdatascience.com/who-is-going-to-make-money-in-ai-part-i-77a2f30b8cef?source=tag_archive---------7----------------,who be go to make money in ai part I towards data science,we be in the midst of a gold rush in ai but who will reap the economic benefit the mass of startup who be all gold pan the corporate who have massive gold mining operation the technology giant who be supply the pick and shovel and which nation have the rich seam of gold we be currently experience another gold rush in ai billion be be invest in ai startup across every imaginable industry and business function google amazon microsoft and ibm be in a heavyweight fight invest over $ 20 billion in ai in 2016 corporate be scramble to ensure they realise the productivity benefit of ai ahead of their competitor while look over their shoulder at the startup china be put its considerable weight behind ai and the european union be talk about a $ 22 billion ai investment as it fear lose ground to china and the us ai be everywhere from the 3 5 billion daily search on google to the new apple iphone x that use facial recognition to amazon alexa that cutely answer our question media headline tout the story of how ai be help doctor diagnose disease bank well assess customer loan risk farmer predict crop yield marketer target and retain customer and manufacturer improve quality control and there be think tank dedicate to study the physical cyber and political risk of ai ai and machine learning will become ubiquitous and weave into the fabric of society but as with any gold rush the question be who will find gold will it just be the brave the few and the large or can the snappy upstart grab their nugget will those provide the pick and shovel make most of the money and who will hit pay dirt as I start think about who be go to make money in ai I end up with seven question who will make money across the 1 chip maker 2 platform and infrastructure provider 3 enable model and algorithm provider 4 enterprise solution provider 5 industry vertical solution provider 6 corporate user of ai and 7 nation while there be many way to skin the cat of the ai landscape hopefully below provide a useful explanatory framework — a value chain of sort the company note be representative of large player in each category but in no way be this list intend to be comprehensive or predictive even though the price of computational power have fall exponentially demand be rise even fast ai and machine learning with its massive dataset and its trillion of vector and matrix calculation have a ferocious and insatiable appetite bring on the chip nvidia s stock be up 1500 % in the past two year benefit from the fact that their graphical processing unit gpu chip that be historically use to render beautiful high speed flow game graphic be perfect for machine learning google recently launch its second generation of tensor processing unit tpus and microsoft be build its own brainwave ai machine learning chip at the same time startup such as graphcore who have raise over $ 110 m be look to enter the market incumbent chip provider such as ibm intel qualcomm and amd be not stand still even facebook be rumour to be build a team to design its own ai chip and the chinese be emerge as serious chip player with cambricon technology announce the first cloud ai chip this past week what be clear be that the cost of design and manufacturing chip then sustain a position as a global chip leader be very high it require extremely deep pocket and a world class team of silicon and software engineer this mean that there will be very few new winner just like the gold rush day those that provide the cheap and most widely use pick and shovel will make a lot of money the ai race be now also take place in the cloud amazon realise early that startup would much rather rent computer and software than buy it and so it launch amazon web service aws in 2006 today ai be demand so much compute power that company be increasingly turn to the cloud to rent hardware through infrastructure as a service iaas and platform as a service paas offering the fight be on among the tech giant microsoft be offer their hybrid public and private azure cloud service that allegedly have over one million computer and in the past few week they announce that their brainwave hardware solutionsdramatically accelerate machine learning with their own bing search engine performance improve by a factor of ten google be rush to play catchup with its own googlecloud offering and we be see the chinese alibaba start to take global share amazon — microsoft — google and ibm be go to continue to duke this one out and watch out for the massively scale cloud player from china the big pick and shovel guy will win again today google be the world s large ai company attract the good ai mind spend small country size gdp budget on r&d and sit on the good dataset gleam from the billion of user of their service ai be power google s search autonomous vehicle speech recognition intelligent reasoning massive search and even its own work on drug discovery and disease diangosis and the incredible ai machine learning software and algorithm that be power all of google s ai activity — tensorflow — be now be give away for free yes for free tensorflow be now an open source software project available to the world and why be they do this as jeff dean head of google brain recently say there be 20 million organisation in the world that could benefit from machine learning today if million of company use this good in class free ai software then they be likely to need lot of computing power and who be well serve to offer that well google cloud be of course optimise for tensorflow and related ai service and once you become reliant on their software and their cloud you become a very sticky customer for many year to come no wonder it be a brutal race for global ai algorithm dominance with amazon — microsoft — ibm also offer their own cheap or free ai software service we be also see a fight for not only machine learning algorithm but cognitive algorithm that offer service for conversational agent and bot speech natural language processing nlp and semantic vision and enhance core algorithm one startup in this increasingly contest space be clarifai who provide advanced image recognition system for business to detect near duplicate and visual search it have raise nearly $ 40 m over the past three year the market for vision relate algorithm and service be estimate to be a cumulative $ 8 billion in revenue between 2016 and 2025 the giant be not stand still ibm for example be offer its watson cognitive product and service they have twenty or so apis for chatbots vision speech language knowledge management and empathy that can be simply be plug into corporate software to create ai enable application cognitive api be everywhere kdnugget list here over 50 of the top cognitive service from the giant and startup these service be be put into the cloud as ai as a service aiaas to make they more accessible just recently microsoft s ceo satya nadella claim that a million developer be use their ai apis service and tool for building ai power app and nearly 300 000 developer be use their tool for chatbot I wouldn t want to be a startup compete with these goliath the winner in this space be likely to favour the heavyweight again they can hire the good research and engineering talent spend the most money and have access to the large dataset to flourish startup be go to have to be really well fund support by lead researcher with a whole battery of ip patent and publish paper deep domain expertise and have access to quality dataset and they should have excellent navigational skill to sail ahead of the giant or sail different race there will many startup casualty but those that can scale will find themselves as global enterprise or quickly acquire by the heavyweight and even if a startup have not find a path to commercialisation then they could become acquihire company buy for their talent if they be work on enable ai algorithm with a strong research orient team we see this in 2014 when deepmind a two year old london base company that develop unique reinforcement machine learning algorithm be acquire by google for $ 400 m enterprise software have be dominate by giant such as salesforce ibm oracle and sap they all recognise that ai be a tool that need to be integrate into their enterprise offering but many startup be rush to become the next generation of enterprise service fill in gap where the incumbent don t currently tread or even attempt to disrupt they we analyse over two hundred use case in the enterprise space range from customer management to market to cybersecurity to intelligence to hr to the hot area of cognitive robotic process automation rpa the enterprise field be much more open than previous space with a veritable medley of startup provide point solution for these use case today there be over 200 ai powered company just in the recruitment space many of they ai startups cybersecurity leader darktrace and rpa leader uipathhave war chest in the $ 100 million the incumbent also want to make sure their ecosystem stay on the forefront and be invest in startup that enhance their offering salesforce have invest in digital genius a customer management solution and similarly unbable that offer enterprise translation service incumbent also often have more press problem sap for example be rush to play catchup in offer a cloud solution let alone catchup in ai we be also see tool provider try to simplify the task require to create deploy and manage ai service in the enterprise machine learning training for example be a messy business where 80 % of time can be spend on datum wrangling and an inordinate amount of time be spend on testing and tuning of what be call hyperparameter petuum a tool provider base in pittsburgh in the us have raise over $ 100 m to help accelerate and optimise the deployment of machine learning model many of these enterprise startup provider can have a healthy future if they quickly demonstrate that they be solve and scale solution to meet real world enterprise need but as always happen in software gold rush there will be a handful of winner in each category and for those ai enterprise category winner they be likely to be snap up along with the good in class tool provider by the giant if they look too threatening ai be drive a race for the good vertical industry solution there be a wealth of new ai power startup provide solution to corporate use case in the healthcare financial service agriculture automative legal and industrial sector and many startup be take the ambitious path to disrupt the incumbent corporate player by offer a service directly to the same customer it be clear that many startup be provide valuable point solution and can succeed if they have access to 1 large and proprietary data training set 2 domain knowledge that give they deep insight into the opportunity within a sector 3 a deep pool of talent around apply ai and 4 deep pocket of capital to fund rapid growth those startup that be do well generally speak the corporate commercial language of customer business efficiency and roi in the form of well develop go to market plan for example zestfinance have raise nearly $ 300 m to help improve credit decision making that will provide fair and transparent credit to everyone they claim they have the world s good data scientist but they would wouldn t they for those startup that be look to disrupt exist corporate player they need really deep pocket for example affirm that offer loan to consumer at the point of sale have raise over $ 700 m these company quickly need to create a defensible moat to ensure they remain competitive this can come from datum network effect where more datum beget well ai base service and product that get more revenue and customer that get more datum and so the flywheel effect continue and while corporate might look to new vendor in their industry for ai solution that could enhance their top and bottom line they be not go to sit back and let upstart muscle in on their customer and they be not go to sit still and let their corporate competitor gain the first advantage through ai there be currently a massive race for corporate innovation large company have their own venture group invest in startup run accelerator and build their own startup to ensure that they be leader in ai drive innovation large corporate be in a strong position against the startup and small company due to their data asset datum be the fuel for ai and machine learning who be well place to take advantage of ai than the insurance company that have ream of historic datum on underwriting claim the financial service company that know everything about consumer financial product buy behaviour or the search company that see more user search for information than any other corporate large and small be well positioned to extract value from ai in fact gartner research predict ai derive business value be project to reach up to $ 3 9 trillion by 2022 there be hundred if not thousand of valuable use case that ai can address across organisation corporate can improve their customer experience save cost low price drive revenue and sell well product and service power by ai ai will help the big get big often at the expense of small company but they will need to demonstrate strong visionary leadership an ability to execute and a tolerance for not always get technology enable project right on the first try country be also also in a battle for ai supremacy china have not be shy about its call to arm around ai it be invest massively in grow technical talent and develop startup its more lax regulatory environment especially in datum privacy help china lead in ai sector such as security and facial recognition just recently there be an example of chinese police pick out one most wanted face in a crowd of 50 000 at a music concert and sensetime group ltd that analyse face and image on a massive scale report it raise $ 600 m become the most valuable global ai startup the chinese point out that their mobile market be 3x the size of the we and there be 50x more mobile payment take place — this be a massive datum advantage the european focus on datum privacy regulation could put they at a disadvantage in certain area of ai even if the union be talk about a $ 22b investment in ai the uk germany france and japan have all make recent announcement about their nation state ai strategy for example president macron say the french government will spend $ 1 85 billion over the next five year to support the ai ecosystem include the creation of large public dataset company such as google s deepmind and samsung have commit to open new paris labs and fujitsu be expand its paris research centre the british just announce a $ 1 4 billion push into ai include funding of 1000 ai phds but while nation be invest in ai talent and the ecosystem the question be who will really capture the value will france and the uk simply be subsidise phds who will be hire by google and while payroll and income taxis will be healthy on those six figure machine learning salarie the bulk of the economic value create could be with this american company its shareholder and the smile american treasury ai will increase productivity and wealth in company and country but how will that wealth be distribute when the headline suggest that 30 to 40 % of our job will be take by the machine economist can point to lesson from hundred of year of increase technology automation will there be net job creation or net job loss the public debate often cite geoffrey hinton the godfather of machine learning who suggest radiologist will lose their job by the dozen as machine diagnose disease from medical image but then we can look to the chinese who be use ai to assist radiologist in manage the overwhelming demand to review 1 4 billion ct scan annually for lung cancer the result be not job loss but an expand market with more efficient and accurate diagnosis however there be likely to be a period of upheaval when much of the value will go to those few company and country that control ai technology and datum and low skilled country whose wealth depend on job that be target of ai automation will likely suffer ai will favour the large and the technologically skilled in examine the landscape of ai it have become clear that we be now enter a truly golden era for ai and there be few key theme appear as to where the economic value will migrate in short it look like the ai gold rush will favour the company and country with control and scale over the good ai tool and technology the datum the well technical worker the most customer and the strong access to capital those with scale will capture the lion s share of the economic value from ai in some way plus ça change plus c est la même choose but there will also be large golden nugget that will be find by a few choice brave startup but like any gold rush many startup will hit pay dirt and many individual and society will likely feel like they have not see the benefit of the gold rush this be the first part in a series of article I intend to write on the topic of the economic of ai I welcome your feedback write by simon greenman I be a lover of technology and how it can be apply in the business world I run my own advisory firm good practice ai help executive of enterprise and startup accelerate the adoption of roi base ai application please get in touch to discuss this if you enjoy this piece I d love it if you hit the clap button 👏 so other might stumble upon it and please post your comment or you can email I directly or find I on linkedin or twitter or follow I at simon greenman from a quick cheer to a stand ovation clap to show how much you enjoy this story ai guy mapquest guy grow innovate and transform company with tech start up investor mentor and geek sharing concept idea and code
Aman Agarwal,7K,24,https://medium.freecodecamp.org/explained-simply-how-an-ai-program-mastered-the-ancient-game-of-go-62b8940a9080?source=tag_archive---------8----------------,explain simply how an ai program master the ancient game of go,this be about alphago google deepmind s go play ai that shake the technology world in 2016 by defeat one of the good player in the world lee sedol go be an ancient board game which have so many possible move at each step that future position be hard to predict — and therefore it require strong intuition and abstract thinking to play because of this reason it be believe that only human could be good at play go most researcher think that it would still take decade to build an ai which could think like that in fact I m release this essay today because this week march 8 15 mark the two year anniversary of the alphago vs sedol match but alphago didn t stop there 8 month later it play 60 professional game on a go website under disguise as a player name master and win every single game against dozen of world champion of course without rest between game naturally this be a huge achievement in the field of ai and spark worldwide discussion about whether we should be excited or worry about artificial intelligence today we be go to take the original research paper publish by deepmind in the nature journal and break it down paragraph by paragraph use simple english after this essay you ll know very clearly what alphago be and how it work I also hope that after read this you will not believe all the news headline make by journalist to scare you about ai and instead feel excited about it worry about the grow achievement of ai be like worry about the grow ability of microsoft powerpoint yes it will get well with time with new feature be add to it but it can t just uncontrollably grow into some kind of hollywood monster you don t need to know how to play go to understand this paper in fact I myself have only read the first 3 4 line in wikipedia s opening paragraph about it instead surprisingly I use some example from basic chess to explain the algorithm you just have to know what a 2 player board game be in which each player take turn and there be one winner at the end beyond that you don t need to know any physics or advanced math or anything this will make it more approachable for people who only just now start learn about machine learning or neural network and especially for those who don t use english as their first language which can make it very difficult to read such paper if you have no prior knowledge of ai and neural network you can read the deep learning section of one of my previous essay here after read that you ll be able to get through this essay if you want to get a shallow understanding of reinforcement learn too optional reading you can find it here here s the original paper if you want to try read it as for I hi I m aman an ai and autonomous robot engineer I hope that my work will save you a lot of time and effort if you be to study this on your own do you speak japanese ryohji ikebe have kindly write a brief memo about this essay in japanese in a series of tweet as you know the goal of this research be to train an ai program to play go at the level of world class professional human player to understand this challenge let I first talk about something similar do for chess in the early 1990s ibm come out with the deep blue computer which defeat the great champion gary kasparov in chess he s also a very cool guy make sure to read more about he later how do deep blue play well it use a very brute force method at each step of the game it take a look at all the possible legal move that could be play and go ahead to explore each and every move to see what would happen and it would keep explore move after move for a while form a kind of huge decision tree of thousand of move and then it would come back along that tree observe which move seem most likely to bring a good result but what do we mean by good result well deep blue have many carefully design chess strategy build into it by expert chess player to help it make well decision — for example how to decide whether to protect the king or get advantage somewhere else they make a specific evaluation algorithm for this purpose to compare how advantageous or disadvantageous different board position be ibm hard code expert chess strategy into this evaluation function and finally it choose a carefully calculate move on the next turn it basically go through the whole thing again as you can see this mean deep blue think about million of theoretical position before play each move this be not so impressive in term of the ai software of deep blue but rather in the hardware — ibm claim it to be one of the most powerful computer available in the market at that time it could look at 200 million board position per second now we come to go just believe I that this game be much more open end and if you try the deep blue strategy on go you wouldn t be able to play well there would be so many position to look at at each step that it would simply be impractical for a computer to go through that hell for example at the opening move in chess there be 20 possible move in go the first player have 361 possible move and this scope of choice stay wide throughout the game this be what they mean by enormous search space moreover in go it s not so easy to judge how advantageous or disadvantageous a particular board position be at any specific point in the game — you kinda have to play the whole game for a while before you can determine who be win but let s say you magically have a way to do both of these and that s where deep learning come in so in this research deepmind use neural network to do both of these task if you haven t read about they yet here s the link again they train a policy neural network to decide which be the most sensible move in a particular board position so it s like follow an intuitive strategy to pick move from any position and they train a value neural network to estimate how advantageous a particular board arrangement be for the player or in other word how likely you be to win the game from this position they train these neural network first with human game example your good old ordinary supervised learning after this the ai be able to mimic human playing to a certain degree so it act like a weak human player and then to train the network even far they make the ai play against itself million of time this be the reinforcement learn part with this the ai get well because it have more practice with these two network alone deepmind s ai be able to play well against state of the art go play program that other researcher have build before these other program have use an already popular pre exist game play algorithm call the monte carlo tree search mct more about this later but guess what we still haven t talk about the real deal deepmind s ai isn t just about the policy and value network it doesn t use these two network as a replacement of the monte carlo tree search instead it use the neural network to make the mct algorithm work well and it get so much well that it reach superhuman level this improved variation of mct be alphago the ai that beat lee sedol and go down in ai history as one of the great breakthrough ever so essentially alphago be simply an improve implementation of a very ordinary computer science algorithm do you understand now why ai in its current form be absolutely nothing to be scared of wow we ve spend a lot of time on the abstract alone alright — to understand the paper from this point on first we ll talk about a gaming strategy call the monte carlo tree search algorithm for now I ll just explain this algorithm at enough depth to make sense of this essay but if you want to learn about it in depth some smart people have also make excellent video and blog post on this 1 a short video series from udacity2 jeff bradberry s explanation of mcts3 an mct tutorial by fullstack academy the follow section be long but easy to understand I ll try my good and very important so stay with I the rest of the essay will go much quick let s talk about the first paragraph of the essay above remember what I say about deep blue make a huge tree of million of board position and move at each step of the game you have to do simulation and look at and compare each and every possible move as I say before that be a simple approach and very straightforward approach — if the average software engineer have to design a game playing ai and have all the strong computer of the world he or she would probably design a similar solution but let s think about how do human themselves play chess let s say you re at a particular board position in the middle of the game by game rule you can do a dozen different thing — move this pawn here move the queen two square here or three square there and so on but do you really make a list of all the possible move you can make with all your piece and then select one move from this long list no — you intuitively narrow down to a few key move let s say you come up with 3 sensible move that you think make sense and then you wonder what will happen in the game if you choose one of these 3 move you might spend 15 20 second consider each of these 3 move and their future — and note that during these 15 second you don t have to carefully plan out the future of each move ; you can just roll out a few mental move guide by your intuition without too much careful thought well a good player would think far and more deeply than an average player this be because you have limit time and you can t accurately predict what your opponent will do at each step in that lovely future you re cook up in your brain so you ll just have to let your gut feeling guide you I ll refer to this part of the thinking process as rollout so take note of it so after roll out your few sensible move you finally say screw it and just play the move you find well then the opponent make a move it might be a move you have already well anticipate which mean you be now pretty confident about what you need to do next you don t have to spend too much time on the rollout again or it could be that your opponent hit you with a pretty cool move that you have not expect so you have to be even more careful with your next move this be how the game carry on and as it get close and close to the finishing point it would get easy for you to predict the outcome of your move — so your rollout don t take as much time the purpose of this long story be to describe what the mct algorithm do on a superficial level — it mimic the above thinking process by build a search tree of move and position every time again for more detail you should check out the link I mention early the innovation here be that instead of go through all the possible move at each position which deep blue do it instead intelligently select a small set of sensible move and explore those instead to explore they it roll out the future of each of these move and compare they base on their imagine outcome seriously — this be all I think you need to understand this essay now — come back to the screenshot from the paper go be a perfect information game please read the definition in the link don t worry it s not scary and theoretically for such game no matter which particular position you be at in the game even if you have just play 1 2 move it be possible that you can correctly guess who will win or lose assume that both player play perfectly from that point on I have no idea who come up with this theory but it be a fundamental assumption in this research project and it work so that mean give a state of the game s there be a function v * s which can predict the outcome let s say probability of you win this game from 0 to 1 they call it the optimal value function because some board position be more likely to result in you win than other board position they can be consider more valuable than the other let I say it again value = probability between 0 and 1 of you win the game but wait — say there be a girl name foma sit next to you while you play chess and she keep tell you at each step if you re win or lose you re win you re lose nope still lose I think it wouldn t help you much in choose which move you need to make she would also be quite annoying what would instead help you be if you draw the whole tree of all the possible move you can make and the state that those move would lead to — and then foma would tell you for the entire tree which state be win state and which state be lose state then you can choose move which will keep lead you to win state all of a sudden foma be your partner in crime not an annoying friend here foma behave as your optimal value function v * s early it be believe that it s not possible to have an accurate value function like foma for the game of go because the game have so much uncertainty but — even if you have the wonderful foma this wonderland strategy of draw out all the possible position for foma to evaluate will not work very well in the real world in a game like chess or go as we say before if you try to imagine even 7 8 move into the future there can be so many possible position that you don t have enough time to check all of they with foma so foma be not enough you need to narrow down the list of move to a few sensible move that you can roll out into the future how will your program do that enter lusha lusha be a skilled chess player and enthusiast who have spend decade watch grand master play chess against each other she can look at your board position look quickly at all the available move you can make and tell you how likely it would be that a chess expert would make any of those move if they be sit at your table so if you have 50 possible move at a point lusha will tell you the probability that each move would be pick by an expert of course a few sensible move will have a much high probability and other pointless move will have very little probability she be your policy function p a s for a give state s she can give you probability for all the possible move that an expert would make wow — you can take lusha s help to guide you in how to select a few sensible move and foma will tell you the likelihood of win from each of those move you can choose the move that both foma and lusha approve or if you want to be extra careful you can roll out the move select by lusha have foma evaluate they pick a few of they to roll out far into the future and keep let foma and lusha help you predict very far into the game s future — much quick and more efficient than to go through all the move at each step into the future this be what they mean by reduce the search space use a value function foma to predict outcome and use a policy function lusha to give you grand master probability to help narrow down the move you roll out these be call monte carlo rollout then while you backtrack from future to present you can take average value of all the different move you roll out and pick the most suitable action so far this have only work on a weak amateur level in go because the policy function and value function that they use to guide these rollout weren t that great phew the first line be self explanatory in mct you can start with an unskilled foma and unskille lusha the more you play the well they get at predict solid outcome and move narrow the search to a beam of high probability action be just a sophisticated way of say lusha help you narrow down the move you need to roll out by assign they probability that an expert would play they prior work have use this technique to achieve strong amateur level ai player even with simple or shallow as they call it policy function yeah convolutional neural network be great for image processing and since a neural network take a particular input and give an output it be essentially a function right so you can use a neural network to become a complex function so you can just pass in an image of the board position and let the neural network figure out by itself what s go on this mean it s possible to create neural network which will behave like very accurate policy and value function the rest be pretty self explanatory here we discuss how foma and lusha be train to train the policy network predict for a give position which move expert would pick you simply use example of human game and use they as datum for good old supervised learning and you want to train another slightly different version of this policy network to use for rollout ; this one will be small and fast let s just say that since lusha be so experienced she take some time to process each position she s good to start the narrow down process with but if you try to make she repeat the process she ll still take a little too much time so you train a * fast policy network * for the rollout process I ll call it lusha s young brother jerry I know I know enough with these name after that once you ve train both of the slow and fast policy network enough use human player datum you can try let lusha play against herself on a go board for a few day and get more practice this be the reinforcement learning part — make a well version of the policy network then you train foma for value prediction determine the probability of you win you let the ai practice through play itself again and again in a simulated environment observe the end result each time and learn from its mistake to get well and well I win t go into detail of how these network be train you can read more technical detail in the later section of the paper method which I haven t cover here in fact the real purpose of this particular paper be not to show how they use reinforcement learning on these neural network one of deepmind s previous paper in which they teach ai to play atari game have already discuss some reinforcement learning technique in depth and I ve already write an explanation of that paper here for this paper as I lightly mention in the abstract and also underline in the screenshot above the big innovation be the fact that they use rl with neural network for improve an already popular game play algorithm mct rl be a cool tool in a toolbox that they use to fine tune the policy and value function neural network after the regular supervised training this research paper be about prove how versatile and excellent this tool it be not about teach you how to use it in television lingo the atari paper be a rl infomercial and this alphago paper be a commercial a quick note before you move on would you like to help I write more such essay explain cool research paper if you re serious I d be glad to work with you please leave a comment and I ll get in touch with you so the first step be in train our policy nn lusha to predict which move be likely to be play by an expert this nn s goal be to allow the ai to play similar to an expert human this be a convolutional neural network as I mention before it s a special kind of nn that be very useful in image processing that take in a simplified image of a board arrangement rectifi nonlinearitie be layer that can be add to the network s architecture they give it the ability to learn more complex thing if you ve ever train nns before you might have use the relu layer that s what these be the training datum here be in the form of random pair of board position and the label be the action choose by human when they be in those position just regular supervised learning here they use stochastic gradient ascent well this be an algorithm for backpropagation here you re try to maximise a reward function and the reward function be just the probability of the action predict by a human expert ; you want to increase this probability but hey — you don t really need to think too much about this normally you train the network so that it minimise a loss function which be essentially the error difference between predict outcome and actual label that be call gradient descent in the actual implementation of this research paper they have indeed use the regular gradient descent you can easily find a loss function that behave opposite to the reward function such that minimise this loss will maximise the reward the policy network have 13 layer and be call sl policy network sl = supervise learn the datum come from a I ll just say it s a popular website on which million of people play go how good do this sl policy network perform it be more accurate than what other researcher have do early the rest of the paragraph be quite self explanatory as for the rollout policy you do remember from a few paragraph ago how lusha the sl policy network be slow so it can t integrate well with the mct algorithm and we train another fast version of lusha call jerry who be her young brother well this refer to jerry right here as you can see jerry be just half as accurate as lusha but it s thousand of time fast it will really help get through roll out simulation of the future fast when we apply the mct for this next section you don t * have * to know about reinforcement learning already but then you ll have to assume that whatever I say work if you really want to dig into detail and make sure of everything you might want to read a little about rl first once you have the sl network train in a supervised manner use human player move with the human move datum as I say before you have to let her practice by itself and get well that s what we re do here so you just take the sl policy network save it in a file and make another copy of it then you use reinforcement learning to fine tune it here you make the network play against itself and learn from the outcome but there s a problem in this training style if you only forever practice against one opponent and that opponent be also only practice with you exclusively there s not much of new learning you can do you ll just be train to practice how to beat that one player this be you guess it overfitte your technique play well against one opponent but don t generalize well to other opponent so how do you fix this well every time you fine tune a neural network it become a slightly different kind of player so you can save this version of the neural network in a list of player who all behave slightly differently right great — now while train the neural network you can randomly make it play against many different old and new version of the opponent choose from that list they be version of the same player but they all play slightly differently and the more you train the more player you get to train even more with bingo in this training the only thing guide the training process be the ultimate goal I e win or lose you don t need to specially train the network to do thing like capture more area on the board etc you just give it all the possible legal move it can choose from and say you have to win and this be why rl be so versatile ; it can be use to train policy or value network for any game not just go here they test how accurate this rl policy network be just by itself without any mct algorithm as you would remember this network can directly take a board position and decide how an expert would play it — so you can use it to single handedly play game well the result be that the rl fine tune network win against the sl network that be only train on human move it also win against other strong go playing program must note here that even before train this rl policy network the sl policy network be already well than the state of the art — and now it have far improve and we haven t even come to the other part of the process like the value network do you know that baby penguin can sneeze louder than a dog can bark actually that s not true but I think you d like a little joke here to distract from the scary looking equation above come to the essay again we re do training lusha here now back to foma — remember the optimal value function v * s > that only tell you how likely you be to win in your current board position if both player play perfectly from that point on so obviously to train an nn to become our value function we would need a perfect player which we don t have so we just use our strong player which happen to be our rl policy network it take the current state board state s and output the probability that you will win the game you play a game and get to know the outcome win or loss each of the game state act as a data sample and the outcome of that game act as the label so by play a 50 move game you have 50 datum sample for value prediction lol no this approach be naive you can t use all 50 move from the game and add they to the dataset the training datum set have to be choose carefully to avoid overfitte each move in the game be very similar to the next one because you only move once and that give you a new position right if you take the state at all 50 of those move and add they to the training datum with the same label you basically have lot of kinda duplicate datum and that cause overfitte to prevent this you choose only very distinct look game state so for example instead of all 50 move of a game you only choose 5 of they and add they to the training set deepmind take 30 million position from 30 million different game to reduce any chance of there be duplicate datum and it work now something conceptual here there be two way to evaluate the value of a board position one option be a magical optimal value function like the one you train above the other option be to simply roll out into the future use your current policy lusha and look at the final outcome in this roll out obviously the real game would rarely go by your plan but deepmind compare how both of these option do you can also do a mixture of both these option we will learn about this mix parameter a little bit later so make a mental note of this concept well your single neural network try to approximate the optimal value function be even well than do thousand of mental simulation use a rollout policy foma really kick ass here when they replace the fast rollout policy with the twice as accurate but slow rl policy lusha and do thousand of simulation with that it do well than foma but only slightly well and too slowly so foma be the winner of this competition she have prove that she can t be replace now that we have train the policy and value function we can combine they with mct and give birth to our former world champion destroyer of grand master the breakthrough of a generation weigh two hundred and sixty eight pound one and only alphaaaaa go in this section ideally you should have a slightly deep understanding of the inner working of the mct algorithm but what you have learn so far should be enough to give you a good feel for what s go on here the only thing you should note be how we re use the policy probability and value estimation we combine they during roll out to narrow down the number of move we want to roll out at each step q s a represent the value function and u s a be a store probability for that position I ll explain remember that the policy network use supervise learn to predict expert move and it doesn t just give you most likely move but rather give you probability for each possible move that tell how likely it be to be an expert move this probability can be store for each of those action here they call it prior probability and they obviously use it while select which action to explore so basically to decide whether or not to explore a particular move you consider two thing first by play this move how likely be you to win yes we already have our value network to answer this first question and the second question be how likely be it that an expert would choose this move if a move be super unlikely to be choose by an expert why even waste time consider it this we get from the policy network then let s talk about the mix parameter see come back to it as discuss early to evaluate position you have two option one simply use the value network you have be use to evaluate state all along and two you can try to quickly play a rollout game with your current strategy assume the other player will play similarly and see if you win or lose we see how the value function be well than do rollout in general here they combine both you try give each prediction 50 50 importance or 40 60 or 0 100 and so on if you attach a % of x to the first you ll have to attach 100 x to the second that s what this mix parameter mean you ll see these hit and trial result later in the paper after each roll out you update your search tree with whatever information you gain during the simulation so that your next simulation be more intelligent and at the end of all simulation you just pick the good move interesting insight here remember how the rl fine tune policy nn be well than just the sl human train policy nn but when you put they within the mct algorithm of alphago use the human train nn prove to be a well choice than the fine tune nn but in the case of the value function which you would remember use a strong player to approximate a perfect player training foma use the rl policy work well than train she with the sl policy do all this evaluation take a lot of computing power we really have to bring out the big gun to be able to run these damn program self explanatory lol our program literally blow the pant off of every other program that come before we this go back to that mix parameter again while evaluate position give equal importance to both the value function and the rollout perform well than just use one of they the rest be self explanatory and reveal an interesting insight self explanatory self explanatory but read that red underlined sentence again I hope you can see clearly now that this line right here be pretty much the summary of what this whole research project be all about conclude paragraph let we brag a little more here because we deserve it oh and if you re a scientist or tech company and need some help in explain your science to non technical people for marketing pr or training etc I can help you drop I a message on twitter @mngrwl from a quick cheer to a stand ovation clap to show how much you enjoy this story engineer teacher learner of foreign language lover of history cinema and art our community publish story worth read on development design and datum science
Lance Ulanoff,15.1K,5,https://medium.com/@LanceUlanoff/did-google-duplex-just-pass-the-turing-test-ffcfe6868b02?source=tag_archive---------9----------------,do google duplex just pass the ture test lance ulanoff medium,I think it be the first um that be the moment when I realize I be hear something extraordinary a computer carry out a completely natural and very human sound conversation with a real person and it wasn t just a random talk this conversation have a purpose a destination to make an appointment at a hair salon the entity make the call and appointment be google assistant run duplex google s still experimental ai voice system and the venue be google I o google s yearly developer conference which this year focus heavily on the late development in ai machine and deep learning google ceo sundar pichai explain that what we be hear be a real phone call make to a hair salon that didn t know it be part of an experiment or that they be talk to a computer he launch duplex by ask google assistant to book a haircut appointment for tuesday morning the ai do the rest duplex make the call and when someone at the salon pick up the voice ai start the conversation with hi I m call to book a woman s hair cut appointment for a client um I m look for something on may third when the attendant ask duplex to give she one second duplex respond with mmm hmm the conversation continue as the salon representative present various date and time and the ai ask about other option eventually the ai and the salon worker agree on an appointment date and time what I hear be so convincing I have trouble discern who be the salon worker and who what be the duplex ai it be stunning and somewhat disconcert I liken it to the feeling you d get if a store mannequin suddenly smile at you it be easily the most remarkable human computer conversation I d ever hear and the close thing I ve see a voice ai pass the turing test which be the ai threshold suggest by computer scientist alan ture in the 1950 ture posit that by 2000 computer would be able to fool human into thinking they be converse with other human at least 30 % of the time he be right in 2014 a chatbot name eugene goostman successfully impersonate a wise ass 14 year old programmer during lengthy text base chat with unsuspecting human ture however hadn t necessarily consider voice base system and for obvious reason talk computer be somewhat less adept at fool human spend a few minute converse with your voice assistant of choice and you ll soon discover their limitation their speech can be stilte pronunciation off and response time can be slow especially if they re try to access a cloud base server and forget about conversation most can handle two consecutive query at most and they virtually all require a trigger phrase like alexa or hey siri google be work on remove unnecessary okay google in short back and forth convos with the digital assistant google assistant run duplex didn t exhibit any of those short coming it sound like a young female assistant carefully schedule her boss s haircut in addition to the natural cadence google add speech disfluencie the verbal tick um uhs and mm hmms and latency or pause that naturally occur when people be speak the result be a perfectly human voice produce entirely by a computer the second call demonstration where a male voice duplex try to make restaurant reservation be even more remarkable the human call participant didn t entirely understand duplex s verbal request and then tell duplex that for the number of people it want to bring to the restaurant they didn t need a reservation duplex handle all this without miss a beat the amazing thing be that the assistant can actually understand the nuance of conversation say pichai during the keynote that ability come by way of neural network technology and intensive machine learning for as accomplished as duplex be in make hair appointment and restaurant reservation it might stumble in deep or more abstract conversation in a blog post on duplex development google engineer explain that they constrain duplex s training to closed domain or well define topic like dinner reservation and hair appointment this give they the ability to perform intense exploration of the topic and focus training duplex be guide during training within the domain by experienced operator who could keep track of mistake and work with engineer to improve response in short this mean that while duplex have your hair and dine out option cover it could stumble in movie reservation and negotiation with your cable provider even so duplex fool two human I hear no hesitation or confusion in the hair salon call there be no indication that the salon worker think something be amiss she want to help this young woman make an appointment what will she think when she learn she be dupe by duplex obviously duplex s conversation be also short each last less than a minute put they well short of the ture test benchmark I would ve enjoy hear the conversation devolve as they extend a few minute or more I m sure duplex will soon tackle more domain and long conversation and it will someday pass the turing test it s only a matter of time before duplex be handle other mundane or difficult call for we like call our parent with our own voice see wavenet technology eventually we ll have our duplex voice call each other handle pleasantry and make plan which google assistant can then drop in our google calendar but that s the future for now duplex s performance stand as a powerful proof of concept for our long imagine future of conversational ai s capable of help entertaining and engage with we it s the first major step on the path to the ai depict in the movie she where joaquin phoenix star as a man who fall in love with his chatty voice assistant play by the disembodied voice of scarlett johansson so no duplex didn t pass the ture test but I do wonder what alan ture would think of it from a quick cheer to a stand ovation clap to show how much you enjoy this story tech expert journalist social medium commentator amateur cartoonist and robotic fan
Gant Laborde,1.3K,7,https://medium.freecodecamp.org/machine-learning-how-to-go-from-zero-to-hero-40e26f8aa6da?source=---------0----------------,machine learn how to go from zero to hero freecodecamp,if your understanding of a i and machine learning be a big question mark then this be the blog post for you here I gradually increase your awesomenessicitytm by glue inspirational video together with friendly text sit down and relax these video take time and if they don t inspire you to continue to the next section fair enough however if you find yourself at the bottom of this article you ve earn your well rounded knowledge and passion for this new world where you go from there be up to you a I be always cool from move a paddle in pong to light you up with combo in street fighter a I have always revolve around a programmer s functional guess at how something should behave fun but programmer aren t always gift in program a i as we often see just google epic game fail to see glitch in a i physics and sometimes even experience human player regardless a I have a new talent you can teach a computer to play video game understand language and even how to identify people or thing this tip of the iceberg new skill come from an old concept that only recently get the processing power to exist outside of theory I m talk about machine learn you don t need to come up with advanced algorithm anymore you just have to teach a computer to come up with its own advanced algorithm so how do something like that even work an algorithm isn t really write as much as it be sort of breed I m not use breeding as an analogy watch this short video which give excellent commentary and animation to the high level concept of create the a i wow right that s a crazy process now how be it that we can t even understand the algorithm when it s do one great visual be when the a I be write to beat mario game as a human we all understand how to play a side scroller but identify the predictive strategy of the result a I be insane impressed there s something amazing about this idea right the only problem be we don t know machine learning and we don t know how to hook it up to video game fortunately for you elon musk already provide a non profit company to do the latter yes in a dozen line of code you can hook up any a I you want to countless game task I have two good answer on why you should care firstly machine learning ml be make computer do thing that we ve never make computer do before if you want to do something new not just new to you but to the world you can do it with ml secondly if you don t influence the world the world will influence you right now significant company be invest in ml and we re already see it change the world think leader be warn that we can t let this new age of algorithm exist outside of the public eye imagine if a few corporate monolith control the internet if we don t take up arm the science win t be our I think christian heilmann say it well in his talk on ml the concept be useful and cool we understand it at a high level but what the heck be actually happen how do this work if you want to jump straight in I suggest you skip this section and move on to the next how do I get start section if you re motivated to be a doer in ml you win t need these video if you re still try to grasp how this could even be a thing the follow video be perfect for walk you through the logic use the classic ml problem of handwriting pretty cool huh that video show that each layer get simple rather than more complicated like the function be chew datum into small piece that end in an abstract concept you can get your hand dirty in interact with this process on this site by adam harley it s cool watch datum go through a train model but you can even watch your neural network get train one of the classic real world example of machine learning in action be the iris data set from 1936 in a presentation I attend by javafxpert s overview on machine learning I learn how you can use his tool to visualize the adjustment and back propagation of weight to neuron on a neural network you get to watch it train the neural model even if you re not a java buff the presentation jim give on all thing machine learning be a pretty cool 1 5 + hour introduction into ml concept which include more info on many of the example above these concept be exciting be you ready to be the einstein of this new era breakthrough be happen every day so get start now there be ton of resource available I ll be recommend two approach in this approach you ll understand machine learn down to the algorithm and the math I know this way sound tough but how cool would it be to really get into the detail and code this stuff from scratch if you want to be a force in ml and hold your own in deep conversation then this be the route for you I recommend that you try out brilliant org s app always great for any science lover and take the artificial neural network course this course have no time limit and help you learn ml while kill time in line on your phone this one cost money after level 1 combine the above with simultaneous enrollment in andrew ng s stanford course on machine learning in 11 week this be the course that jim weaver recommend in his video above I ve also have this course independently suggest to I by jen looper everyone provide a caveat that this course be tough for some of you that s a show stopper but for other that s why you re go to put yourself through it and collect a certificate say you do this course be 100 % free you only have to pay for a certificate if you want one with those two course you ll have a lot of work to do everyone should be impressed if you make it through because that s not simple but more so if you do make it through you ll have a deep understanding of the implementation of machine learning that will catapult you into successfully apply it in new and world change way if you re not interested in write the algorithm but you want to use they to create the next breathtaking website app you should jump into tensorflow and the crash course tensorflow be the de facto open source software library for machine learn it can be use in countless way and even with javascript here s a crash course plenty more information on available course and ranking can be find here if take a course be not your style you re still in luck you don t have to learn the nitty gritty of ml in order to use it today you can efficiently utilize ml as a service in many way with tech giant who have train model ready I would still caution you that there s no guarantee that your data be safe or even yours but the offering of service for ml be quite attractive use an ml service might be the good solution for you if you re excited and able to upload your datum to amazon microsoft google I like to think of these service as a gateway drug to advanced ml either way it s good to get start now I have to say thank you to all the aforementione people and video they be my inspiration to get start and though I m still a newb in the ml world I m happy to light the path for other as we embrace this awe inspire age we find ourselves in it s imperative to reach out and connect with people if you take up learn this craft without friendly face answer and sound board anything can be hard just be able to ask and get a response be a game changer add I and add the people mention above friendly people with friendly advice help see I hope this article have inspire you and those around you to learn ml from a quick cheer to a stand ovation clap to show how much you enjoy this story software consultant adjunct professor publish author award win speaker mentor organizer and immature nerd d — lately full of react native tech our community publish story worth read on development design and datum science
Emmanuel Ameisen,935,11,https://blog.insightdatascience.com/reinforcement-learning-from-scratch-819b65f074d8?source=---------1----------------,reinforcement learning from scratch insight datum,want to learn about apply artificial intelligence from lead practitioner in silicon valley new york or toronto learn more about the insight artificial intelligence fellow program be you a company work in ai and would like to get involve in the insight ai fellow program feel free to get in touch recently I give a talk at the o reilly ai conference in beijing about some of the interesting lesson we ve learn in the world of nlp while there I be lucky enough to attend a tutorial on deep reinforcement learning deep rl from scratch by unity technology I think that the session lead by arthur juliani be extremely informative and want to share some big takeaway below in our conversation with company we ve see a rise of interesting deep rl application tool and result in parallel the inner working and application of deep rl such as alphago picture above can often seem esoteric and hard to understand in this post I will give an overview of core aspect of the field that can be understand by anyone many of the visual be from the slide of the talk and some be new the explanation and opinion be mine if anything be unclear reach out to I here deep rl be a field that have see vast amount of research interest include learn to play atari game beat pro player at dota 2 and defeat go champion contrary to many classical deep learning problem that often focus on perception do this image contain a stop sign deep rl add the dimension of action that influence the environment what be the goal and how do I get there in dialog system for example classical deep learning aim to learn the right response for a give query on the other hand deep reinforcement learning focus on the right sequence of sentence that will lead to a positive outcome for example a happy customer this make deep rl particularly attractive for task that require planning and adaptation such as manufacturing or self drive however industry application have trail behind the rapidly advance result come out of the research community a major reason be that deep rl often require an agent to experiment million of time before learn anything useful the good way to do this rapidly be by use a simulation environment this tutorial will be use unity to create environment to train agent in for this workshop lead by arthur juliani and leon chen their goal be to get every participant to successfully train multiple deep rl algorithm in 4 hour a tall order below be a comprehensive overview of many of the main algorithm that power deep rl today for a more complete set of tutorial arthur juliani write an 8 part series start here deep rl can be use to best the top human player at go but to understand how that s do you first need to understand a few simple concept start with much easy problem 1 it all start with slot machine let s imagine you be face with 4 chest that you can pick from at each turn each of they have a different average payout and your goal be to maximize the total payout you receive after a fix number of turn this be a classic problem call multi armed bandit and be where we will start the crux of the problem be to balance exploration which help we learn about which state be good and exploitation where we now use what we know to pick the good slot machine here we will utilize a value function that map our action to an estimate reward call the q function first we ll initialize all q value at equal value then we ll update the q value of each action pick each chest base on how good the payout be after choose this action this allow we to learn a good value function we will approximate our q function use a neural network start with a very shallow one that learn a probability distribution by use a softmax over the 4 potential chest while the value function tell we how good we estimate each action to be the policy be the function that determine which action we end up take intuitively we might want to use a policy that pick the action with the high q value this perform poorly in practice as our q estimate will be very wrong at the start before we gather enough experience through trial and error this be why we need to add a mechanism to our policy to encourage exploration one way to do that be to use epsilon greedy which consist of take a random action with probability epsilon we start with epsilon be close to 1 always choose random action and low epsilon as we go along and learn more about which chest be good eventually we learn which chest be good in practice we might want to take a more subtle approach than either take the action we think be the good or a random action a popular method be boltzmann exploration which adjust probability base on our current estimate of how good each chest be add in a randomness factor 2 add different state the previous example be a world in which we be always in the same state wait to pick from the same 4 chest in front of we most real word problem consist of many different state that be what we will add to our environment next now the background behind chest alternate between 3 color at each turn change the average value of the chest this mean we need to learn a q function that depend not only on the action the chest we pick but the state what the color of the background be this version of the problem be call contextual multi armed bandit surprisingly we can use the same approach as before the only thing we need to add be an extra dense layer to our neural network that will take in as input a vector represent the current state of the world 3 learn about the consequence of our action there be another key factor that make our current problem simple than most in most environment such as in the maze depict above the action that we take have an impact on the state of the world if we move up on this grid we might receive a reward or we might receive nothing but the next turn we will be in a different state this be where we finally introduce a need for plan first we will define our q function as the immediate reward in our current state plus the discount reward we be expect by take all of our future action this solution work if our q estimate of state be accurate so how can we learn a good estimate we will use a method call temporal difference td learn to learn a good q function the idea be to only look at a limited number of step in the future td 1 for example only use the next 2 state to evaluate the reward surprisingly we can use td 0 which look at the current state and our estimate of the reward the next turn and get great result the structure of the network be the same but we need to go through one forward step before receive the error we then use this error to back propagate gradient like in traditional deep learning and update our value estimate 3 + introduce monte carlo another method to estimate the eventual success of our action be monte carlo estimate this consist of play out the entire episode with our current policy until we reach an end success by reach a green block or failure by reach a red block in the image above and use that result to update our value estimate for each traverse state this allow we to propagate value efficiently in one batch at the end of an episode instead of every time we make a move the cost be that we be introduce noise to our estimate since we attribute very distant reward to they 4 the world be rarely discrete the previous method be use neural network to approximate our value estimate by mapping from a discrete number of state and action to a value in the maze for example there be 49 state square and 4 action move in each adjacent direction in this environment we be try to learn how to balance a ball on a 2 dimensional paddle by decide at each time step whether we want to tilt the paddle leave or right here the state space become continuous the angle of the paddle and the position of the ball the good news be we can still use neural network to approximate this function a note about off policy vs on policy learn the method we use previously be off policy method mean we can generate datum with any strategy use epsilon greedy for example and learn from it on policy method can only learn from action that be take follow our policy remember a policy be the method we use to determine which action to take this constrain our learning process as we have to have an exploration strategy that be build in to the policy itself but allow we to tie result directly to our reasoning and enable we to learn more efficiently the approach we will use here be call policy gradient and be an on policy method previously we be first learn a value function q for each action in each state and then build a policy on top in vanilla policy gradient we still use monte carlo estimate but we learn our policy directly through a loss function that increase the probability of choose reward action since we be learn on policy we can not use method such as epsilon greedy which include random choice to get our agent to explore the environment the way that we encourage exploration be by use a method call entropy regularization which push our probability estimate to be wide and thus will encourage we to make risky choice to explore the space 4 + leverage deep learning for representation in practice many state of the art rl method require learn both a policy and value estimate the way we do this with deep learning be by have both be two separate output of the same backbone neural network which will make it easy for our neural network to learn good representation one method to do this be advantage actor critic a2c we learn our policy directly with policy gradient define above and learn a value function use something call advantage instead of update our value function base on reward we update it base on our advantage which measure how much well or bad an action be than our previous value function estimate it to be this help make learn more stable compare to simple q learning and vanilla policy gradient 5 learn directly from the screen there be an additional advantage to use deep learning for these method which be that deep neural network excel at perceptive task when a human play a game the information receive be not a list of state but an image usually of a screen or a board or the surround environment image base learning combine a convolutional neural network cnn with rl in this environment we pass in a raw image instead of feature and add a 2 layer cnn to our architecture without change anything else we can even inspect activation to see what the network pick up on to determine value and policy in the example below we can see that the network use the current score and distant obstacle to estimate the value of the current state while focus on nearby obstacle for determine action neat as a side note while toy around with the provide implementation I ve find that visual learning be very sensitive to hyperparameter change the discount rate slightly for example completely prevent the neural network from learn even on a toy application this be a widely know problem but it be interesting to see it first hand 6 nuanced action so far we ve play with environment with continuous and discrete state space however every environment we study have a discrete action space we could move in one of four direction or tilt the paddle to the left or right ideally for application such as self drive car we would like to learn continuous action such as turn the steering wheel between 0 and 360 degree in this environment call 3d ball world we can choose to tilt the paddle to any value on each of its axis this give we more control as to how we perform action but make the action space much large we can approach this by approximate our potential choice with gaussian distribution we learn a probability distribution over potential action by learn the mean and variance of a gaussian distribution and our policy we sample from that distribution simple in theory 7 next step for the brave there be a few concept that separate the algorithm describe above from state of the art approach it s interesting to see that conceptually the good robotic and game playing algorithm be not that far away from the one we just explore that s it for this overview I hope this have be informative and fun if you be look to dive deeply into the theory of rl give arthur s post a read or diving deeply by follow david silver s ucl course if you be look to learn more about the project we do at insight or how we work with company please check we out below or reach out to I here want to learn about apply artificial intelligence from lead practitioner in silicon valley new york or toronto learn more about the insight artificial intelligence fellow program be you a company work in ai and would like to get involve in the insight ai fellow program feel free to get in touch from a quick cheer to a stand ovation clap to show how much you enjoy this story ai lead at insight ai @emmanuelameisen insight fellow program your bridge to a career in datum
Irhum Shafkat,2K,15,https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1?source=---------2----------------,intuitively understand convolution for deep learning,the advent of powerful and versatile deep learning framework in recent year have make it possible to implement convolution layer into a deep learning model an extremely simple task often achievable in a single line of code however understand convolution especially for the first time can often feel a bit unnerving with term like kernels filter channel and so on all stack onto each other yet convolution as a concept be fascinatingly powerful and highly extensible and in this post we ll break down the mechanic of the convolution operation step by step relate it to the standard fully connect network and explore just how they build up a strong visual hierarchy make they powerful feature extractor for image the 2d convolution be a fairly simple operation at heart you start with a kernel which be simply a small matrix of weight this kernel slide over the 2d input datum perform an elementwise multiplication with the part of the input it be currently on and then sum up the result into a single output pixel the kernel repeat this process for every location it slide over convert a 2d matrix of feature into yet another 2d matrix of feature the output feature be essentially the weighted sum with the weight be the value of the kernel itself of the input feature locate roughly in the same location of the output pixel on the input layer whether or not an input feature fall within this roughly same location get determine directly by whether it s in the area of the kernel that produce the output or not this mean the size of the kernel directly determine how many or few input feature get combine in the production of a new output feature this be all in pretty stark contrast to a fully connect layer in the above example we have 5×5=25 input feature and 3×3=9 output feature if this be a standard fully connect layer you d have a weight matrix of 25×9 = 225 parameter with every output feature be the weighted sum of every single input feature convolution allow we to do this transformation with only 9 parameter with each output feature instead of look at every input feature only get to look at input feature come from roughly the same location do take note of this as it ll be critical to our later discussion before we move on it s definitely worth look into two technique that be commonplace in convolution layer padding and stride padding do something pretty clever to solve this pad the edge with extra fake pixel usually of value 0 hence the oft use term zero padding this way the kernel when slide can allow the original edge pixel to be at its center while extend into the fake pixel beyond the edge produce an output the same size as the input the idea of the stride be to skip some of the slide location of the kernel a stride of 1 mean to pick slide a pixel apart so basically every single slide act as a standard convolution a stride of 2 mean pick slide 2 pixel apart skip every other slide in the process downsize by roughly a factor of 2 a stride of 3 mean skip every 2 slide downsize roughly by factor 3 and so on more modern network such as the resnet architecture entirely forgo pooling layer in their internal layer in favor of strided convolution when need to reduce their output size of course the diagram above only deal with the case where the image have a single input channel in practicality most input image have 3 channel and that number only increase the deep you go into a network it s pretty easy to think of channel in general as be a view of the image as a whole emphasise some aspect de emphasise other so this be where a key distinction between term come in handy whereas in the 1 channel case where the term filter and kernel be interchangeable in the general case they re actually pretty different each filter actually happen to be a collection of kernel with there be one kernel for every single input channel to the layer and each kernel be unique each filter in a convolution layer produce one and only one output channel and they do it like so each of the kernel of the filter slide over their respective input channel produce a process version of each some kernel may have strong weight than other to give more emphasis to certain input channel than other eg a filter may have a red kernel channel with strong weight than other and hence respond more to difference in the red channel feature than the other each of the per channel process version be then sum together to form one channel the kernel of a filter each produce one version of each channel and the filter as a whole produce one overall output channel finally then there s the bias term the way the bias term work here be that each output filter have one bias term the bias get add to the output channel so far to produce the final output channel and with the single filter case down the case for any number of filter be identical each filter process the input with its own different set of kernel and a scalar bias with the process describe above produce a single output channel they be then concatenate together to produce the overall output with the number of output channel be the number of filter a nonlinearity be then usually apply before pass this as input to another convolution layer which then repeat this process even with the mechanic of the convolution layer down it can still be hard to relate it back to a standard feed forward network and it still doesn t explain why convolution scale to and work so much well for image datum suppose we have a 4×4 input and we want to transform it into a 2×2 grid if we be use a feedforward network we d reshape the 4×4 input into a vector of length 16 and pass it through a densely connect layer with 16 input and 4 output one could visualize the weight matrix w for a layer and although the convolution kernel operation may seem a bit strange at first it be still a linear transformation with an equivalent transformation matrix if we be to use a kernel k of size 3 on the reshaped 4×4 input to get a 2×2 output the equivalent transformation matrix would be note while the above matrix be an equivalent transformation matrix the actual operation be usually implement as a very different matrix multiplication 2 the convolution then as a whole be still a linear transformation but at the same time it s also a dramatically different kind of transformation for a matrix with 64 element there s just 9 parameter which themselves be reuse several time each output node only get to see a select number of input the one inside the kernel there be no interaction with any of the other input as the weight to they be set to 0 it s useful to see the convolution operation as a hard prior on the weight matrix in this context by prior I mean predefine network parameter for example when you use a pretraine model for image classification you use the pretraine network parameter as your prior as a feature extractor to your final densely connect layer in that sense there s a direct intuition between why both be so efficient compare to their alternative transfer learning be efficient by order of magnitude compare to random initialization because you only really need to optimize the parameter of the final fully connect layer which mean you can have fantastic performance with only a few dozen image per class here you don t need to optimize all 64 parameter because we set most of they to zero and they ll stay that way and the rest we convert to share parameter result in only 9 actual parameter to optimize this efficiency matter because when you move from the 784 input of mnist to real world 224×224×3 image that s over 150 000 input a dense layer attempt to halve the input to 75 000 input would still require over 10 billion parameter for comparison the entirety of resnet 50 have some 25 million parameter so fix some parameter to 0 and tie parameter increase efficiency but unlike the transfer learning case where we know the prior be good because it work on a large general set of image how do we know this be any good the answer lie in the feature combination the prior lead the parameter to learn early on in this article we discuss that so with backpropagation come in all the way from the classification node of the network the kernel have the interesting task of learn weight to produce feature only from a set of local input additionally because the kernel itself be apply across the entire image the feature the kernel learn must be general enough to come from any part of the image if this be any other kind of data eg categorical datum of app install this would ve be a disaster for just because your number of app install and app type column be next to each other doesn t mean they have any local share feature common with app install date and time use sure the four may have an underlie high level feature eg which app people want most that can be find but that give we no reason to believe the parameter for the first two be exactly the same as the parameter for the latter two the four could ve be in any consistent order and still be valid pixel however always appear in a consistent order and nearby pixel influence a pixel e g if all nearby pixel be red it s pretty likely the pixel be also red if there be deviation that s an interesting anomaly that could be convert into a feature and all this can be detect from compare a pixel with its neighbor with other pixel in its locality and this idea be really what a lot of early computer vision feature extraction method be base around for instance for edge detection one can use a sobel edge detection filter a kernel with fix parameter operate just like the standard one channel convolution for a non edge contain grid eg the background sky most of the pixel be the same value so the overall output of the kernel at that point be 0 for a grid with an vertical edge there be a difference between the pixel to the left and right of the edge and the kernel compute that difference to be non zero activating and reveal the edge the kernel only work only a 3×3 grid at a time detect anomaly on a local scale yet when apply across the entire image be enough to detect a certain feature on a global scale anywhere in the image so the key difference we make with deep learning be ask this question can useful kernel be learn for early layer operate on raw pixel we could reasonably expect feature detector of fairly low level feature like edge line etc there s an entire branch of deep learning research focus on make neural network model interpretable one of the most powerful tool to come out of that be feature visualization use optimization 3 the idea at core be simple optimize a image usually initialize with random noise to activate a filter as strongly as possible this do make intuitive sense if the optimize image be completely fill with edge that s strong evidence that s what the filter itself be look for and be activate by use this we can peek into the learn filter and the result be stunning one important thing to notice here be that convolve image be still image the output of a small grid of pixel from the top left of an image will still be on the top leave so you can run another convolution layer on top of another such as the two on the left to extract deep feature which we visualize yet however deep our feature detector get without any further change they ll still be operate on very small patch of the image no matter how deep your detector be you can t detect face from a 3×3 grid and this be where the idea of the receptive field come in a essential design choice of any cnn architecture be that the input size grow small and small from the start to the end of the network while the number of channel grow deep this as mention early be often do through stride or pool layer locality determine what input from the previous layer the output get to see the receptive field determine what area of the original input to the entire network the output get to see the idea of a strided convolution be that we only process slide a fix distance apart and skip the one in the middle from a different point of view we only keep output a fix distance apart and remove the rest 1 we then apply a nonlinearity to the output and per usual then stack another new convolution layer on top and this be where thing get interesting even if be we to apply a kernel of the same size 3×3 have the same local area to the output of the strided convolution the kernel would have a large effective receptive field this be because the output of the strided layer still do represent the same image it be not so much cropping as it be resize only thing be that each single pixel in the output be a representative of a large area of whose other pixel be discard from the same rough location from the original input so when the next layer s kernel operate on the output it s operate on pixel collect from a large area note if you re familiar with dilated convolution note that the above be not a dilated convolution both be method of increase the receptive field but dilated convolution be a single layer while this take place on a regular convolution follow a strided convolution with a nonlinearity inbetween this expansion of the receptive field allow the convolution layer to combine the low level feature line edge into high level feature curve texture as we see in the mixed3a layer follow by a pooling stride layer the network continue to create detector for even high level feature part pattern as we see for mixed4a the repeat reduction in image size across the network result in by the 5th block on convolution input size of just 7×7 compare to input of 224×224 at this point each single pixel represent a grid of 32×32 pixel which be huge compare to early layer where an activation mean detect an edge here an activation on the tiny 7×7 grid be one for a very high level feature such as for bird the network as a whole progress from a small number of filter 64 in case of googlenet detect low level feature to a very large number of filter 1024 in the final convolution each look for an extremely specific high level feature follow by a final pooling layer which collapse each 7×7 grid into a single pixel each channel be a feature detector with a receptive field equivalent to the entire image compare to what a standard feedforward network would have do the output here be really nothing short of awe inspire a standard feedforward network would have produce abstract feature vector from combination of every single pixel in the image require intractable amount of datum to train the cnn with the prior impose on it start by learn very low level feature detector and as across the layer as its receptive field be expand learn to combine those low level feature into progressively high level feature ; not an abstract combination of every single pixel but rather a strong visual hierarchy of concept by detect low level feature and use they to detect high level feature as it progress up its visual hierarchy it be eventually able to detect entire visual concept such as face bird tree etc and that s what make they such powerful yet efficient with image datum with the visual hierarchy cnn build it be pretty reasonable to assume that their vision system be similar to human and they re really great with real world image but they also fail in way that strongly suggest their vision system aren t entirely human like the most major problem adversarial example 4 example which have be specifically modify to fool the model adversarial example would be a non issue if the only tamper one that cause the model to fail be one that even human would notice the problem be the model be susceptible to attack by sample which have only be tamper with ever so slightly and would clearly not fool any human this open the door for model to silently fail which can be pretty dangerous for a wide range of application from self drive car to healthcare robustness against adversarial attack be currently a highly active area of research the subject of many paper and even competition and solution will certainly improve cnn architecture to become safe and more reliable cnn be the model that allow computer vision to scale from simple application to power sophisticated product and service range from face detection in your photo gallery to make well medical diagnosis they might be the key method in computer vision go forward or some other new breakthrough might just be around the corner regardless one thing be for sure they re nothing short of amazing at the heart of many present day innovative application and be most certainly worth deeply understanding hope you enjoy this article if you d like to stay connected you ll find I on twitter here if you have a question comment be welcome — I find they to be useful to my own learning process as well from a quick cheer to a stand ovation clap to show how much you enjoy this story curious programmer tinker around in python and deep learning sharing concept idea and code
Abhishek Parbhakar,937,6,https://towardsdatascience.com/must-know-information-theory-concepts-in-deep-learning-ai-e54a5da9769d?source=---------3----------------,must know information theory concept in deep learning ai,information theory be an important field that have make significant contribution to deep learning and ai and yet be unknown to many information theory can be see as a sophisticated amalgamation of basic building block of deep learning calculus probability and statistic some example of concept in ai that come from information theory or related field in the early 20th century scientist and engineer be struggle with the question how to quantify the information be there a analytical way or a mathematical measure that can tell we about the information content for example consider below two sentence it be not difficult to tell that the second sentence give we more information since it also tell that bruno be big and brown in addition to be a dog how can we quantify the difference between two sentence can we have a mathematical measure that tell we how much more information second sentence have as compare to the first scientist be struggle with these question semantic domain and form of datum only add to the complexity of the problem then mathematician and engineer claude shannon come up with the idea of entropy that change our world forever and mark the beginning of digital information age shannon propose that the semantic aspect of datum be irrelevant and nature and meaning of data doesn t matter when it come to information content instead he quantify information in term of probability distribution and uncertainty shannon also introduce the term bit that he humbly credit to his colleague john tukey this revolutionary idea not only lay the foundation of information theory but also open new avenue for progress in field like artificial intelligence below we discuss four popular widely use and must know information theoretic concept in deep learning and datum science also call information entropy or shannon entropy entropy give a measure of uncertainty in an experiment let s consider two experiment if we compare the two experiment in exp 2 it be easy to predict the outcome as compare to exp 1 so we can say that exp 1 be inherently more uncertain unpredictable than exp 2 this uncertainty in the experiment be measure use entropy therefore if there be more inherent uncertainty in the experiment then it have high entropy or less the experiment be predictable more be the entropy the probability distribution of experiment be use to calculate the entropy a deterministic experiment which be completely predictable say toss a coin with p h = 1 have entropy zero an experiment which be completely random say roll fair dice be least predictable have maximum uncertainty and have the high entropy among such experiment another way to look at entropy be the average information gain when we observe outcome of an random experiment the information gain for a outcome of an experiment be define as a function of probability of occurrence of that outcome more the rarer be the outcome more be the information gain from observe it for example in an deterministic experiment we always know the outcome so no new information gain be here from observe the outcome and hence entropy be zero for a discrete random variable x with possible outcome state x_1 x_n the entropy in unit of bit be define as where p x_i be the probability of i^th outcome of x cross entropy be use to compare two probability distribution it tell we how similar two distribution be cross entropy between two probability distribution p and q define over same set of outcome be give by mutual information be a measure of mutual dependency between two probability distribution or random variable it tell we how much information about one variable be carry by the another variable mutual information capture dependency between random variable and be more generalized than vanilla correlation coefficient which capture only the linear relationship mutual information of two discrete random variable x and y be define as where p x y be the joint probability distribution of x and y and p x and p y be the marginal probability distribution of x and y respectively also call relative entropy kl divergence be another measure to find similarity between two probability distribution it measure how much one distribution diverge from the other suppose we have some datum and true distribution underlie it be p but we don t know this p so we choose a new distribution q to approximate this datum since q be just an approximation it win t be able to approximate the datum as good as p and some information loss will occur this information loss be give by kl divergence kl divergence between p and q tell we how much information we lose when we try to approximate datum give by p with q kl divergence of a probability distribution q from another probability distribution p be define as kl divergence be commonly use in unsupervised machine learning technique variational autoencoder information theory be originally formulate by mathematician and electrical engineer claude shannon in his seminal paper a mathematical theory of communication in 1948 note term experiment random variable & ai machine learn deep learning data science have be use loosely above but have technically different meaning in case you like the article do follow I abhishek parbhakar for more article relate to ai philosophy and economic from a quick cheer to a stand ovation clap to show how much you enjoy this story finding equilibria among ai philosophy and economic sharing concept idea and code
Aman Dalmia,2.3K,17,https://blog.usejournal.com/what-i-learned-from-interviewing-at-multiple-ai-companies-and-start-ups-a9620415e4cc?source=---------4----------------,what I learn from interview at multiple ai company and start up,over the past 8 month I ve be interview at various company like google s deepmind wadhwani institute of ai microsoft ola fractal analytic and a few other primarily for the role — data scientist software engineer & research engineer in the process not only do I get an opportunity to interact with many great mind but also have a peek at myself along with a sense of what people really look for when interview someone I believe that if I d have this knowledge before I could have avoid many mistake and have prepare in a much well manner which be what the motivation behind this post be to be able to help someone bag their dream place of work this post arise from a discussion with one of my junior on the lack of really fulfil job opportunity offer through campus placement for people work in ai also when I be prepare I notice people use a lot of resource but as per my experience over the past month I realise that one can do away with a few minimal one for most role in ai all of which I m go to mention at the end of the post I begin with how to get notice a k a the interview then I provide a list of company and start up to apply which be follow by how to ace that interview base on whatever experience I ve have I add a section on what we should strive to work for I conclude with minimal resource you need for preparation note for people who be sit for campus placement there be two thing I d like to add firstly most of what I m go to say except for the last one maybe be not go to be relevant to you for placement but and this be my second point as I mention before opportunity on campus be mostly in software engineering role have no intersection with ai so this post be specifically mean for people who want to work on solve interesting problem use ai also I want to add that I haven t clear all of these interview but I guess that s the essence of failure — it s the great teacher the thing that I mention here may not all be useful but these be thing that I do and there s no way for I to know what might have end up make my case strong to be honest this step be the most important one what make off campus placement so tough and exhausting be get the recruiter to actually go through your profile among the plethora of application that they get have a contact inside the organisation place a referral for you would make it quite easy but in general this part can be sub divide into three key step a do the regulatory preparation and do that well so with regulatory preparation I mean — a linkedin profile a github profile a portfolio website and a well polished cv firstly your cv should be really neat and concise follow this guide by udacity for clean up your cv — resume revamp it have everything that I intend to say and I ve be use it as a reference guide myself as for the cv template some of the in build format on overleaf be quite nice I personally use deedy resume here s a preview as it can be see a lot of content can be fit into one page however if you really do need more than that then the format link above would not work directly instead you can find a modified multi page format of the same here the next most important thing to mention be your github profile a lot of people underestimate the potential of this just because unlike linkedin it doesn t have a who view your profile option people do go through your github because that s the only way they have to validate what you have mention in your cv give that there s a lot of noise today with people associate all kind of buzzword with their profile especially for data science open source have a big role to play too with majority of the tool implementation of various algorithms list of learn resource all be open source I discuss the benefit of get involve in open source and how one can start from scratch in an early post here the bare minimum for now should be • create a github account if you don t already have one • create a repository for each of the project that you have do • add documentation with clear instruction on how to run the code• add documentation for each file mention the role of each function the meaning of each parameter proper format e g pep8 for python along with a script to automate the previous step optional move on the third step be what most people lack which be have a portfolio website demonstrate their experience and personal project make a portfolio indicate that you be really serious about get into the field and add a lot of point to the authenticity factor also you generally have space constraint on your cv and tend to miss out on a lot of detail you can use your portfolio to really delve deep into the detail if you want to and it s highly recommend to include some sort of visualisation or demonstration of the project idea it s really easy to create one too as there be a lot of free platform with drag and drop feature make the process really painless I personally use weebly which be a widely use tool it s well to have a reference to begin with there be a lot of awesome one out there but I refer to deshraj yadav s personal website to begin with make mine finally a lot of recruiter and start up have nowadays start use linkedin as their go to platform for hire a lot of good job get post there apart from recruiter the people work at influential position be quite active there as well so if you can grab their attention you have a good chance of get in too apart from that maintain a clean profile be necessary for people to have the will to connect with you an important part of linkedin be their search tool and for you to show up you must have the relevant keyword intersperse over your profile it take I a lot of iteration and re evaluation to finally have a decent one also you should definitely ask people with or under whom you ve work with to endorse you for your skill and add a recommendation talk about their experience of work with you all of this increase your chance of actually get notice I ll again point towards udacity s guide for linkedin and github profile all this might seem like a lot but remember that you don t need to do it in a single day or even a week or a month it s a process it never end set up everything at first would definitely take some effort but once it s there and you keep update it regularly as event around you keep happen you ll not only find it to be quite easy but also you ll be able to talk about yourself anywhere anytime without have to explicitly prepare for it because you become so aware about yourself b stay authentic I ve see a lot of people do this mistake of present themselves as per different job profile accord to I it s always well to first decide what actually interest you what would you be happy do and then search for relevant opportunity ; not the other way round the fact that the demand for ai talent surpass the supply for the same give you this opportunity spending time on your regulatory preparation mention above would give you an all around perspective on yourself and help make this decision easy also you win t need to prepare answer to various kind of question that you get ask during an interview most of they would come out naturally as you d be talk about something you really care about c network once you re do with a figure out b networking be what will actually help you get there if you don t talk to people you miss out on hear about many opportunity that you might have a good shot at it s important to keep connect with new people each day if not physically then on linkedin so that upon compound it after many day you have a large and strong network networking be not message people to place a referral for you when I be start off I do this mistake way too often until I stumble upon this excellent article by mark meloon where he talk about the importance of build a real connection with people by offer our help first another important step in networking be to get your content out for example if you re good at something blog about it and share that blog on facebook and linkedin not only do this help other it help you as well once you have a good enough network your visibility increase multi fold you never know how one person from your network like or comment on your post may help you reach out to a much broad audience include people who might be look for someone of your expertise I m present this list in alphabetical order to avoid the misinterpretation of any specific preference however I do place a * on the one that I d personally recommend this recommendation be base on either of the following mission statement people personal interaction or scope of learn more than 1 * be purely base on the 2nd and 3rd factor your interview begin the moment you have enter the room and a lot of thing can happen between that moment and the time when you re ask to introduce yourself — your body language and the fact that you re smile while greet they play a big role especially when you re interview for a start up as culture fit be something that they extremely care about you need to understand that as much as the interviewer be a stranger to you you re a stranger to he she too so they re probably just as nervous as you be it s important to view the interview as more of a conversation between yourself and the interviewer both of you be look for a mutual fit — you be look for an awesome place to work at and the interviewer be look for an awesome person like you to work with so make sure that you re feel good about yourself and that you take the charge of make the initial moment of your conversation pleasant for they and the easy way I know how to make that happen be to smile there be mostly two type of interview — one where the interviewer have come with come prepare set of question and be go to just ask you just that irrespective of your profile and the second where the interview be base on your cv I ll start with the second one this kind of interview generally begin with a can you tell I a bit about yourself at this point 2 thing be a big no — talk about your gpa in college and talk about your project in detail an ideal statement should be about a minute or two long should give a good idea on what have you be do till now and it s not restrict to academic you can talk about your hobby like read book play sport meditation etc — basically anything that contribute to define you the interviewer will then take something that you talk about here as a cue for his next question and then the technical part of the interview begin the motive of this kind of interview be to really check whether whatever you have write on your cv be true or not there would be a lot of question on what could be do differently or if x be use instead of y what would have happen at this point it s important to know the kind of trade off that be usually make during implementation for e g if the interviewer say that use a more complex model would have give well result then you might say that you actually have less datum to work with and that would have lead to overfitte in one of the interview I be give a case study to work on and it involve designing algorithm for a real world use case I ve notice that once I ve be give the green flag to talk about a project the interviewer really like it when I talk about it in the follow flow problem > 1 or 2 previous approach > our approach > result > intuition the other kind of interview be really just to test your basic knowledge don t expect those question to be too hard but they would definitely scratch every bit of the basic that you should be have mainly base around linear algebra probability statistic optimisation machine learning and or deep learn the resource mention in the minimal resource you need for preparation section should suffice but make sure that you don t miss out one bit among they the catch here be the amount of time you take to answer those question since these cover the basic they expect that you should be answer they almost instantly so do your preparation accordingly throughout the process it s important to be confident and honest about what you know and what you don t know if there s a question that you re certain you have no idea about say it upfront rather than make aah um sound if some concept be really important but you be struggle with answer it the interviewer would generally depend on how you do in the initial part be happy to give you a hint or guide you towards the right solution it s a big plus if you manage to pick their hint and arrive at the correct solution try to not get nervous and the good way to avoid that is by again smile now we come to the conclusion of the interview where the interviewer would ask you if you have any question for they it s really easy to think that your interview be do and just say that you have nothing to ask I know many people who get reject just because of fail at this last question as I mention before it s not only you who be be interview you be also look for a mutual fit with the company itself so it s quite obvious that if you really want to join a place you must have many question regard the work culture there or what kind of role be they see you in it can be as simple as be curious about the person interview you there s always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you re truly interested in be a part of their team a final question that I ve start ask all my interviewer be for a feedback on what they might want I to improve on this have help I tremendously and I still remember every feedback that I ve get which I ve incorporate into my daily life that s it base on my experience if you re just honest about yourself be competent truly care about the company you re interview for and have the right mindset you should have tick all the right box and should be get a congratulatory mail soon 😄 we live in an era full of opportunity and that apply to anything that you love you just need to strive to become the good at it and you will find a way to monetise it as gary vaynerchuk just follow he already say this be a great time to be work in ai and if you re truly passionate about it you have so much that you can do with ai you can empower so many people that have always be under represent we keep nagging about the problem surround we but there s be never such a time where common people like we can actually do something about those problem rather than just complain jeffrey hammerbacher founder cloudera have famously say we can do so much with ai than we can ever imagine there be many extremely challenging problem out there which require incredibly smart people like you to put your head down on and solve you can make many life well time to let go of what be cool or what would look good think and choose wisely any data science interview comprise of question mostly of a subset of the follow four category computer science math statistic and machine learning if you re not familiar with the math behind deep learning then you should consider go over my last post for resource to understand they however if you be comfortable I ve find that the chapter 2 3 and 4 of the deep learning book be enough to prepare revise for theoretical question during such interview I ve be prepare summary for a few chapter which you can refer to where I ve try to even explain a few concept that I find challenge to understand at first in case you be not willing to go through the entire chapter and if you ve already do a course on probability you should be comfortable answer a few numerical as well for stat cover these topic should be enough now the range of question here can vary depend on the type of position you be apply for if it s a more traditional machine learning base interview where they want to check your basic knowledge in ml you can complete any one of the follow course machine learning by andrew ng — cs 229 machine learning course by caltech professor yas abu mostafa important topic be supervise learn classification regression svm decision tree random forest logistic regression multi layer perceptron parameter estimation bayes decision rule unsupervise learn k mean cluster gaussian mixture model dimensionality reduction pca now if you re apply for a more advanced position there s a high chance that you might be question on deep learning in that case you should be very comfortable with convolutional neural network cnn and or depend upon what you ve work on recurrent neural network rnn and their variant and by be comfortable you must know what be the fundamental idea behind deep learning how cnns rnns actually work what kind of architecture have be propose and what have be the motivation behind those architectural change now there s no shortcut for this either you understand they or you put enough time to understand they for cnn the recommend resource be stanford s cs 231n and cs 224n for rnn I find this neural network class by hugo larochelle to be really enlighten too refer this for a quick refresher too udacity come to the aid here too by now you should have figure out that udacity be a really important place for an ml practitioner there be not a lot of place work on reinforcement learning rl in india and I too be not experience in rl as of now so that s one thing to add to this post sometime in the future get place off campus be a long journey of self realisation I realise that this have be another long post and I m again extremely grateful to you for value my thought I hope that this post find a way of be useful to you and that it help you in some way to prepare for your next datum science interview well if it do I request you to really think about what I talk about in what we should strive to work for I m very thankful to my friend from iit guwahati for their helpful feedback especially ameya godbole kothapalli vignesh and prabal jain a majority of what I mention here like view an interview as a conversation and seek feedback from our interviewer arise from multiple discussion with prabal who have be advise I constantly on how I can improve my interview skill this story be publish in noteworthy where thousand come every day to learn about the people & idea shape the product we love follow our publication to see more product & design story feature by the journal team from a quick cheer to a stand ovation clap to show how much you enjoy this story ai fanatic • math lover • dreamer the official journal blog
Gaurav Kaila,2.1K,10,https://medium.com/nanonets/how-we-flew-a-drone-to-monitor-construction-projects-in-africa-using-deep-learning-b792f5c9c471?source=---------5----------------,how to easily automate drone base monitoring use deep learning,this article be a comprehensive overview of use deep learning base object detection method for aerial imagery via drone do you know drone and it s associate function be set to be a $ 50 billion industry by 2023 currently drone be use in domain such as agriculture construction public safety and security to name a few and be rapidly be adopt by other with deep learning base computer vision now power these drone industry expert be now predict unprecedented use in previously unimaginable or infeasible application we explore some of these application along with challenge in automation of drone base monitoring through deep learning finally a case study be present for automate remote inspection of construction project in africa use nanonet machine learning framework man have always be feed fascinated with the view of the world from the top — building watch tower high fortwall capture the high mountain peak to capture a glimpse and share it with the world people go to great length to defy gravity enlist the help of ladder tall building kite balloon plane and rocket today access to drone that can fly as high as 2kms be possible even for the general public these drone have high resolution camera attach to they that be capable of acquire quality image which can be use for various kind of analysis with easy access to drone we re see a lot of interest and activity by photographer & hobbyist who be use it to make creative project such as capture inequality in south africa or breathtaking view of new york which might make woody allen proud we explore some here energy inspection of solar farm routine inspection and maintenance be a herculean task for solar farm the traditional manual inspection method can only support the inspection frequency of once in three month because of the hostile environment solar panel may have defect ; break solar panel unit reduce the power output efficiency agriculture early plant disease detection researcher at imperial college london be mount multi spectral camera on drone that will use special filter to capture reflect light from select region of the electromagnetic spectrum stress plant typically display a spectral signature that distinguish they from healthy plant public safety shark detection analysis of overhead view of a large mass of land water can yield a vast amount of information in term of security and public safety one such example be spot shark in the water off the coast of australia australia base westpac group have develop a deep learning base object detection system to detect shark in the water there be various other application to aerial image such as civil engineering routine bridge inspection power line surveillance and traffic surveying oil and gas on & offshore inspection of oil and gas platform drill rig public safety motor vehicle accident nuclear accident structural fire ship collision plane and train crash & security traffic surveillance border surveillance coastal surveillance control hostile demonstration and rioting to comprehensively capture terrain & landscape the process of acquire aerial image can be summarise in two step after image stitch the generate map can be use for various kind of analysis for the application mention above high resolution aerial imagery be increasingly available at the global scale and contain an abundance of information about feature of interest that could be correlate with maintenance land development disease control defect localisation surveillance etc unfortunately such datum be highly unstructured and thus challenge to extract meaningful insight from at scale even with intensive manual analysis for eg classification of urban land use be typically base on survey perform by train professional as such this task be labor intensive infrequent slow and costly as a result such datum be mostly available in develop country and big city that have the resource and the vision necessary to collect and curate it another motivation for automate the analysis of aerial imagery stem from the urgency of predict change in the region of interest for eg crowd counting and crowd behaviour be frequently do during large public gathering such as concert football match protest etc traditionally a human be behind the analysis of image be stream from a cctv camera directly to the command centre as you may imagine there be several problem with this approach such as human latency or error in detect an event and lack of sufficient view via standard static cctv camera below be some of the commonly occur challenge when use aerial imagery there be several challenge to overcome when automate the analysis of drone imagery follow list a few of they with a prospective solution pragmatic master a south african robotic as a service collaborate with nanonet for automation of remotely monitor progress of a housing construction project in africa we aim to detect the follow infrastructure to capture the construction progress of a house in it s various stage a foundation start wallplate in progress roof partially complete apron finish touch and geyser ready to move in pragmatic master choose nanonet as it s deep learning provider because of it s easy to use web platform and plug&play apis the end to end process of use the nanonet api be as simple as four step 2 labelling of image labelling image be probably the hard and the most time consume step in any supervised machine learning pipeline but at nanonet we have this cover for you we have in house expert that have multiple year of work with aerial image they will annotate your image with high precision and accuracy to aid well model training for the pragmatic master use case we be label the follow object and their total count in all the image 3 model training at nanonet we employ the principle of transfer learning while train on your image this involve re train a pre train model that have already be pre train with a large number of aerial image this help the model identify micro pattern such as edge line and contour easily on your image and focus on the more specific macro pattern such as house tree human car etc transfer learning also give a boost in term of training time as the model do not need to be train for a large number of iteration to give a good performance our proprietary deep learning software smartly select the good model along with optimise the hyper parameter for your use case this involve search through multiple model and through a hyperspace of parameter use advanced search algorithm the hard object to detect be the small one due to their low resolution our model training strategy be optimise to detect very small object such as geyser and apron which have an area of a few pixel follow be the mean average precision per class that we get roof 95 1%geyser 88%wallplate 92%apron 81 % note add more image can lead to an increase in the mean average precision our api also support detect multiple object in the same image such as roof and apron in one image 4 test & integrate once the model be train you can either integrate nanonet s api directly into your system or we also provide a docker image with the train model and inference code that you can use docker image can easily scale and provide a fault tolerant inference system customer trust be our top priority we be commit towards provide you ownership and control over your content at all time we provide two plan for use our service for both the plan we use highly sophisticated datum privacy and security protocol in collaboration with amazon web service which be our cloud partner your dataset be anonymised and go through minimal human intervention during the pre processing and training process all our human labeller have sign a non disclosure agreement nda to protect your datum from go into wrong hand as we believe in the philosophy of your datum be yours you can request we to delete your datum from our server at any stage nanonet be a web service that make it easy to use deep learning you can build a model with your own datum to achieve high accuracy & use our api to integrate the same in your application pragmatic master be a south african robotic as a service company that provide camera mount drone to acquire image of construction farming and mining site these image be analyse to track progress identify challenge eliminate inefficiency and provide an overall aerial view of the site from a quick cheer to a stand ovation clap to show how much you enjoy this story machine learning engineer nanonet machine learn api
James Loy,8.5K,6,https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6?source=---------6----------------,how to build your own neural network from scratch in python,motivation as part of my personal journey to gain a well understanding of deep learning I ve decide to build a neural network from scratch without a deep learning library like tensorflow I believe that understand the inner working of a neural network be important to any aspire data scientist this article contain what I ve learn and hopefully it ll be useful for you as well most introductory text to neural network bring up brain analogy when describe they without delve into brain analogy I find it easy to simply describe neural network as a mathematical function that map a give input to a desire output neural network consist of the follow component the diagram below show the architecture of a 2 layer neural network note that the input layer be typically exclude when count the number of layer in a neural network create a neural network class in python be easy train the neural network the output ŷ of a simple 2 layer neural network be you might notice that in the equation above the weight w and the biases b be the only variable that affect the output ŷ naturally the right value for the weight and bias determine the strength of the prediction the process of fine tune the weight and bias from the input datum be know as train the neural network each iteration of the training process consist of the follow step the sequential graph below illustrate the process as we ve see in the sequential graph above feedforward be just simple calculus and for a basic 2 layer neural network the output of the neural network be let s add a feedforward function in our python code to do exactly that note that for simplicity we have assume the bias to be 0 however we still need a way to evaluate the goodness of our prediction I e how far off be our prediction the loss function allow we to do exactly that there be many available loss function and the nature of our problem should dictate our choice of loss function in this tutorial we ll use a simple sum of sqaure error as our loss function that be the sum of square error be simply the sum of the difference between each predict value and the actual value the difference be square so that we measure the absolute value of the difference our goal in training be to find the good set of weight and bias that minimize the loss function now that we ve measure the error of our prediction loss we need to find a way to propagate the error back and to update our weight and bias in order to know the appropriate amount to adjust the weight and bias by we need to know the derivative of the loss function with respect to the weight and bias recall from calculus that the derivative of a function be simply the slope of the function if we have the derivative we can simply update the weight and bias by increase reduce with it refer to the diagram above this be know as gradient descent however we can t directly calculate the derivative of the loss function with respect to the weight and bias because the equation of the loss function do not contain the weight and bias therefore we need the chain rule to help we calculate it phew that be ugly but it allow we to get what we need — the derivative slope of the loss function with respect to the weight so that we can adjust the weight accordingly now that we have that let s add the backpropagation function into our python code for a deep understanding of the application of calculus and the chain rule in backpropagation I strongly recommend this tutorial by 3blue1brown now that we have our complete python code for do feedforward and backpropagation let s apply our neural network on an example and see how well it do our neural network should learn the ideal set of weight to represent this function note that it isn t exactly trivial for we to work out the weight just by inspection alone let s train the neural network for 1500 iteration and see what happen look at the loss per iteration graph below we can clearly see the loss monotonically decrease towards a minimum this be consistent with the gradient descent algorithm that we ve discuss early let s look at the final prediction output from the neural network after 1500 iteration we do it our feedforward and backpropagation algorithm train the neural network successfully and the prediction converge on the true value note that there s a slight difference between the prediction and the actual value this be desirable as it prevent overfitte and allow the neural network to generalize well to unseen datum fortunately for we our journey isn t over there s still much to learn about neural network and deep learning for example I ll be write more on these topic soon so do follow I on medium and keep and eye out for they I ve certainly learn a lot write my own neural network from scratch although deep learning librarie such as tensorflow and keras make it easy to build deep net without fully understand the inner working of a neural network I find that it s beneficial for aspire data scientist to gain a deep understanding of neural network this exercise have be a great investment of my time and I hope that it ll be useful for you as well from a quick cheer to a stand ovation clap to show how much you enjoy this story graduate student in machine learning @ georgia tech | linkedin https www linkedin com in jamesloy1 sharing concept idea and code
Chintan Trivedi,1.2K,8,https://towardsdatascience.com/using-deep-q-learning-in-fifa-18-to-perfect-the-art-of-free-kicks-f2e4e979ee66?source=---------7----------------,use deep q learning in fifa 18 to perfect the art of free kick,a code tutorial in tensorflow that use reinforcement learning to take free kick in my previous article I present an ai bot train to play the game of fifa use supervised learning technique with this approach the bot quickly learn the basic of the game like pass and shoot however the training datum require to improve it far quickly become cumbersome to gather and provide little to no improvement make this approach very time consume for this sake I decide to switch to reinforcement learning as suggest by almost everyone who comment on that article in this article I ll provide a short description of what reinforcement learning be and how I apply it to this game a big challenge in implement this be that we do not have access to the game s code so we can only make use of what we see on the game screen due to this reason I be unable to train the ai on the full game but could find a work around to implement it for skill game in practice mode for this tutorial I will be try to teach the bot to take 30 yard free kick but you can modify it to play other skill game as well let s start with understand the reinforcement learning technique and how we can formulate our free kick problem to fit this technique contrary to supervised learning we do not need to manually label the training datum in reinforcement learning instead we interact with our environment and observe the outcome of our interaction we repeat this process multiple time gain example of positive and negative experience which act as our training datum thus we learn by experimentation and not imitation let s say our environment be in a particular state s and upon take an action a it change to state s for this particular action the immediate reward you observe in the environment be r any set of action that follow this action will have their own immediate reward until you stop interact due to a positive or a negative experience these be call future reward thus for the current state s we will try to estimate out of all action possible which action will fetch we the maximum immediate + future reward denote by q s a call the q function this give we q s a = r + γ * q s a which denote the expect final reward by take action a in state s here γ be a discount factor to account for uncertainty in predict the future thus we want to trust the present a bit more than the future deep q learning be a special type of reinforcement learning technique where the q function be learn by a deep neural network give the environment s state as an image input to this network it try to predict the expect final reward for all possible action like a regression problem the action with the maximum predict q value be choose as our action to be take in the environment hence the name deep q learning note if we have a performance meter in kick off mode of fifa like there be in the practice mode we might have be able to formulate this problem for play the entire game and not restrict ourselves to just take free kick that or we need access to game s internal code which we don t have anyways let s make the most of what we do have while the bot have not master all different kind of free kick it have learn some situation very well it almost always hit the target in absence of wall of player but struggle in its presence also when it hasn t encounter a situation frequently in training like not face the goal it behave bonker however with every training epoch this behavior be notice to decrease on an average as show in the figure above the average goal scoring rate grow from 30 % to 50 % on an average after training for 1000 epoch this mean the current bot score about half of the free kick it attempt for reference a human would average around 75 80 % do consider that fifa tend to behave non deterministically which make learn very difficult more result in video format can be find on my youtube channel with the video embed below please subscribe to my channel if you wish to keep track of all my project we shall implement this in python use tool like tensorflow kera for deep learning and pytesseract for ocr the git link be provide below with the requirement setup instruction in the repository description I would recommend below gist of code only for the purpose of understand this tutorial since some line have be remove for brevity please use the full code from git while run it let s go over the 4 main part of the code we do not have any readymade api available that give we access to the code so let s make our own api instead we ll use game s screenshot to observe the state simulate key press to take action in the game environment and optical character recognition to read our reward in the game we have three main method in our fifa class observe act _ get_reward and an additional method is_over to check if the free kick have be take or not throughout the training process we want to store all our experience and observe reward we will use this as the training datum for our q learning model so for every action we take we store the experience < s a r s > along with a game_over flag the target label that our model will try to learn be the final reward for each action which be a real number for our regression problem now that we can interact with the game and store our interaction in memory let s start train our q learning model for this we will attain a balance between exploration take a random action in the game and exploitation take action predict by our model this way we can perform trial and error to obtain different experience in the game the parameter epsilon be use for this purpose which be an exponentially decrease factor that balance exploration and exploitation in the beginning when we know nothing we want to do more exploration but as number of epoch increase and we learn more we want to do more exploitation and less exploration hence the decaying value of the epsilon parameter for this tutorial I have only train the model for 1000 epoch due to time and performance constraint but in the future I would like to push it to at least 5000 epoch at the heart of the q learning process be a 2 layered dense fully connect network with relu activation it take the 128 dimensional feature map as input state and output 4 q value for each possible action the action with the maximum predict q value be the desire action to be take as per the network s policy for the give state this be the starting point of execution of this code but you ll have to make sure the game fifa 18 be run in windowed mode on a second display and you load up the free kick practice mode under skill game shooting menu make sure the game control be in sync with the key you have hard code in the fifa py script overall I think the result be quite satisfactory even though it fail to reach human level of performance switching from supervised to reinforcement technique for learning help ease the pain of collect training datum give enough time to explore it perform very well in problem like learn how to play simple game however reinforcement setting seem to fail when it encounter unfamiliar situation which make I believe formulate it as a regression problem can not extrapolate information as well as formulate it as a classification problem in supervised setting perhaps a combination of the two could address the weakness of both these approach maybe that s where we ll see the good result in building ai for game something for I to try in the future I would like to acknowledge this tutorial of deep q learning and this git repository of game with python for provide majority of the code with the exception of the fifa custom api most of the code s backbone have come from these source thank to these guy thank you for read if you like this tutorial please follow I on medium github or subscribe to my youtube channel from a quick cheer to a stand ovation clap to show how much you enjoy this story data scientist ai enthusiast blogger youtuber chelsea fc fanatic also look to build my virtual clone before I die share concept idea and code
Abhishek Parbhakar,1.7K,3,https://towardsdatascience.com/why-data-scientists-love-gaussian-6e7a7b726859?source=---------8----------------,why datum scientist love gaussian towards data science,for deep learning & machine learning engineer out of all the probabilistic model in the world gaussian distribution model simply stand out even if you have never work on an ai project there be a significant chance that you have come across the gaussian model gaussian distribution model often identify with its iconic bell shape curve also refer as normal distribution be so popular mainly because of three reason incredible number of process in nature and social science naturally follow the gaussian distribution even when they don t the gaussian give the good model approximation for these process some example include central limit theorem state that when we add large number of independent random variable irrespective of the original distribution of these variable their normalize sum tend towards a gaussian distribution for example the distribution of total distance cover in an random walk tend towards a gaussian probability distribution the theorem s implication include that large number of scientific and statistical method that have be develop specifically for gaussian model can also be apply to wide range of problem that may involve any other type of distribution the theorem can also be see as a explanation why many natural phenomenon follow gaussian distribution unlike many other distribution that change their nature on transformation a gaussian tend to remain a gaussian for every gaussian model approximation there may exist a complex multi parameter distribution that give well approximation but still gaussian be preferred because it make the math a lot simple gaussian distribution be name after great mathematician and physicist carl friedrich gaus from a quick cheer to a stand ovation clap to show how much you enjoy this story finding equilibria among ai philosophy and economic sharing concept idea and code
Leon Zhou,184,6,https://towardsdatascience.com/the-best-words-cf6fc2333c31?source=---------9----------------,the good word towards data science,utter in the heat of a campaign rally in south carolina on december 30 2015 this statement be just another of a grow collection of trumpism by our now president donald j trump these statement both make donald more beloved by his supporter as their relatable president while also a cause of ridicule by seemingly everyone else regardless of one s personal view of the man it can not be deny donald have a way of speak that be well so uniquely he — his smattering of superlative and apparent disregard for the constraint of traditional sentence structure be just a few of the thing that make his speech instantly recognizable from that of his predecessor or peer it be this unique style that interest I and I set out to try and capture it use machine learning — to generate text that look and sound like something donald trump might say to learn president trump s style I first have to gather sufficient example of it I focus my effort on two primary source the obvious first place to look for word by donald trump be his twitter feed the current president be unique in his use of the platform as a direct and unfiltered connection to the american people furthermore as a figure of interest his word have naturally be collect and organize for posterity save I the hassle of use the ever change and restrictive twitter api all in all there be a little under 31 000 tweet available for my use in addition to his online persona however I also want to gain a glimpse into his more formal role as president for this I turn to the white house briefing statement archive with the help of some python tool I be able to quickly amass a table of about 420 transcript of speech and other remark by the president these transcript cover a variety of event such as meeting with foreign dignitary round table with congressional member and award presentation unlike with the tweet where every word be write or dictate by trump himself these transcript involve other politician and inquisitive reporter separate donald s word from those of other seem to be a daunting task enter regular expression — a boring name for a powerful and decidedly not bore tool regular expression allow you to specify a pattern to search for ; this pattern can contain any number of very specific constraint wildcard or other restriction to return exactly what you want and no more with some trial and error I be able to generate a complex regular expression to only return word the president speak leave and discard any other word or annotation typically one of the first step in work with text be to normalize it the extent and complexity of this normalization vary accord to one s need range from simply remove punctuation or capital letter to reduce all variant of a word to a base root an example of this workflow can be see here for I however the specific idiosyncrasy and pattern that would be lose in normalization be exactly what I need to preserve so in hope of make my generate text just that much more believable and authentic I elect to bypass most of the standard normalization workflow before diving into a deep learning model I be curious to explore another frequently use text generation method the markov chain markov chain have be the go to for joke text generation for a long time — a quick search will reveal one for star trek past president the simpson and many other the quick and dirty of the markov chain be that it only care about the current word in determine what should come next this algorithm look at every single time a specific word appear and every word that come immediately after it the next word be select randomly with a probability proportional to its frequency let I illustrate with a quick example donald trump say the word taxis if in real life 70 % of the time after he say taxis he follow up with the word bigly the markov chain will choose the next word to be bigly 70 % of the time but sometimes he doesn t say bigly sometimes he end the sentence or move on to a different word the chain will most likely choose bigly but there s a chance it ll go for any of the other available option thus introduce some variety in our generate text and repeat ad nauseam or until the end of the sentence this be great for quick and dirty application but it s easy to see where it can go wrong as the markov chain only ever care about the current word it can easily be sidetrack a sentence that start off talk about the domestic economy could just as easily end talk about the apprentice with my limited text datum set most of my markov chain output be nonsensical but occasionally there be some flash of brilliance and hilarity for passably real text however I need something more sophisticated recurrent neural network rnn have establish themselves as the architecture of choice for many text or sequence base application the detailed inner working of rnn be outside the scope of this post but a strong relatively beginner friendly introduction may be find here the distinguish feature of these neural unit be that they have an internal memory of sort word choice and grammar depend heavily on surround context so this memory be extremely useful in create a coherent thought by keep track of tense subject and object and so on the downside of these type of network be that they be extraordinarily computationally expensive — on my piddly laptop run the entirety of my text through the model once would take over an hour and consider I d need to do so about 200 time this be no good this be where cloud computing come in a number of establish tech company offer cloud service the large being amazon google and microsoft on a heavy gpu computing instance that one hour plus per cycle time become ninety second an over 40x reduction in time can you tell if this follow statement be real or not this be text generate off of trump s endorsement of the republican gubernatorial candidate but it might pass as something that trump tweet in the run up to the 2016 general election the more complex neural network I implement with hide fully connect layer before and after the recurrent layer be capable of generate internally consistent text give any seed of 40 character or less less complex network stumble a little on consistency but still capture the tonal feel of president trump s speech while not quite produce text at a level capable of fool you or I consistently this attempt open my eye to the power of rnn in short order these network learn spell some aspect of grammar and in some instance how to use hashtag and hyperlink — imagine what a well design network with more text to learn from and time to learn might produce if you re interested in look at the code behind these model you can find the repository here and don t hesitate to reach out with any question or feedback you may have from a quick cheer to a stand ovation clap to show how much you enjoy this story I be a data scientist with a background in chemical engineering and biotech I be also homeless and live in my car but that s another thing entirely hire I share concept idea and code
Dr. GP Pulipaka,2,6,https://medium.com/@gp_pulipaka/3-ways-to-apply-latent-semantic-analysis-on-large-corpus-text-on-macos-terminal-jupyterlab-colab-7b4dc3e1622?source=---------3----------------,3 way to apply latent semantic analysis on large corpus text on macos terminal jupyterlab and,latent semantic analysis work on large scale dataset to generate representation to discover the insight through natural language processing there be different approach to perform the latent semantic analysis at multiple level such as document level phrase level and sentence level primarily semantic analysis can be summarize into lexical semantic and the study of combine individual word into paragraph or sentence the lexical semantic classifie and decompose the lexical item apply lexical semantic structure have different context to identify the difference and similarity between the word a generic term in a paragraph or a sentence be hypernym and hyponymy provide the meaning of the relationship between instance of the hyponyms homonyms contain similar syntax or similar spelling with similar structuring with different meaning homonym be not relate to each other book be an example for homonym it can mean for someone to read something or an act of make a reservation with similar spelling form and syntax however the definition be different polysemy be another phenomenon of the word where a single word could be associate with multiple relate sense and distinct meaning the word polysemy be a greek word which mean many sign python provide nltk library to perform tokenization of the word by chop the word in large chunk into phrase or meaningful string processing word through tokenization produce token word lemmatization convert word from the current inflect form into the base form latent semantic analysis apply latent semantic analysis on large dataset of text and document represent the contextual meaning through mathematical and statistical computation method on large corpus of text many time latent semantic analysis overtake human score and subject matter test conduct by human the accuracy of latent semantic analysis be high as it read through machine readable document and text at a web scale latent semantic analysis be a technique that apply singular value decomposition and principal component analysis pca the document can be represent with z x y matrix a the row of the matrix represent the document in the collection the matrix a can represent numerous hundred thousand of row and column on a typical large corpus text document apply singular value decomposition develop a set of operation dub matrix decomposition natural language processing in python with nltk library apply a low rank approximation to the term document matrix later the low rank approximation aid in indexing and retrieve the document know as latent semantic indexing by cluster the number of word in the document brief overview of linear algebra the a with z x y matrix contain the real value entry with non negative value for the term document matrix determine the rank of the matrix come with the number of linearly independent column or row in the the matrix the rank of a ≤ { z y } a square c x c represent as diagonal matrix where off diagonal entry be zero examine the matrix if all the c diagonal matrix be one the identity matrix of the dimension c represent by ic for the square z x z matrix a with a vector k which contain not all zero for λ the matrix decomposition apply on the square matrix factor into the product of matrix from eigenvector this allow to reduce the dimensionality of the word from multi dimension to two dimension to view on the plot the dimensionality reduction technique with principal component analysis and singular value decomposition hold critical relevance in natural language process the zipfian nature of the frequency of the word in a document make it difficult to determine the similarity of the word in a static stage hence eigen decomposition be a by product of singular value decomposition as the input of the document be highly asymmetrical the latent semantic analysis be a particular technique in semantic space to parse through the document and identify the word with polysemy with nlkt library the resource such as punkt and wordnet have to be download from nltk deep learning at scale with google colab notebook train machine learning or deep learning model on cpu could take hour and could be pretty expensive in term of the programming language efficiency with time and energy of the computer resource google build colab notebooks environment for research and development purpose it run entirely on the cloud without require any additional hardware or software setup for each machine it s entirely equivalent of a jupyter notebook that aid the datum scientist to share the colab notebook by store on google drive just like any other google sheet or document in a collaborative environment there be no additional cost associate with enable gpu at runtime for acceleration on the runtime there be some challenge of upload the datum into colab unlike jupyter notebook that can access the datum directly from the local directory of the machine in colab there be multiple option to upload the file from the local file system or a drive can be mount to load the datum through drive fuse wrapper once this step be complete it show the follow log without error the next step would be generate the authentication token to authenticate the google credential for the drive and colab if it show successful retrieval of access token then colab be all set at this stage the drive be not mount yet it will show false when access the content of the text file once the drive be mount colab have access to the dataset from google drive once the file be accessible the python can be execute similar to execute in jupyter environment colab notebook also display the result similar to what we see on jupyter notebook pycharm ide the program can be run compile on pycharm ide environment and run on pycharm or can be execute from osx terminal result from osx terminal jupyter notebook on standalone machine jupyter notebook give a similar output run the latent semantic analysis on the local machine reference gorrell g 2006 generalize hebbian algorithm for incremental singular value decomposition in natural language processing retrieve from https www aclweb org anthology e06 1013 hardeniya n 2016 natural language processing python and nltk birmingham england packt publishing landauer t k foltz p w laham d & university of colorado at boulder 1998 an introduction to latent semantic analysis retrieve from http lsa colorado edu papers dp1 lsaintro pdf stackoverflow 2018 mount google drive on google colab retrieve from https stackoverflow com question 50168315 mount google drive on google colab stanford university 2009 matrix decomposition and latent semantic indexing retrieve from https nlp stanford edu ir book html htmledition matrix decomposition and latent semantic indexing 1 html from a quick cheer to a stand ovation clap to show how much you enjoy this story ganapathi pulipaka | founder and ceo @deepsingularity | bestselle author | big datum | iot | startup | sap | machinelearning | deeplearne | datascience
Erick Muzart Fonseca dos Santos,16,2,https://medium.com/deeplearningbrasilia/o-grupo-de-estudo-em-deep-learning-de-bras%C3%ADlia-est%C3%A1-planejando-o-pr%C3%B3ximo-ciclo-de-encontros-do-4861851ec0ff?source=---------5----------------,o grupo de estudo em deep learning de brasília está planejando o próximo ciclo de encontros do,o grupo de estudo em deep learning de brasília está planejando o próximo ciclo de encontros do grupo que deve iniciar se a partir do meio de junho de 2018 ainda há tempo para manifestar suas preferências para participar do grupo para tal favor preencher o seguinte questionário para que possamos agregar as preferências de nossa comunidade e selecionar as opções que melhor atenderem a todo https goo gl form h4k77sd1dxw6diit1 agradecemos se puder divulgar o grupo junto a sua rede de contato com interesse nos temas de aprendizado automático e deep learning para que possamos iniciar o próximo ciclo já com o máximo de interessados desde o primeiro dia seguem abaixo alguns do resultado iniciais do grupo quanto aos resultados inciais do questionário segue uma síntese das primeira 50 resposta dentre os tópicos de mais interesse destacam se 1o deep learn 87 5 % 2o machine learn 78 6 % 3o aplicações de deep learn em projeto 69 6 % 4o processamento de linguagem natural 51 8 % preferência por curso 1o machine learning da fast ai 67 9 % 2o deep learning parte 2 da fast ai 46 4 % 3o deep learning parte 1 da fast ai 44 6 % atenciosamente organização do grupo de estudo em deep learning de brasília from a quick cheer to a stand ovation clap to show how much you enjoy this story publicações do membros do grupo de estudo em deep learning de brasília
Chris Kalahiki,30,15,https://towardsdatascience.com/beethoven-picasso-and-artificial-intelligence-caf644fc72f9?source=---------6----------------,beethoven picasso and artificial intelligence towards data science,when people think of the great artist who ve ever live they probably think of name like beethoven or picasso no one would ever think of a computer as a great artist but what if one day that be indeed the case could computer learn to create incredible drawing like the mona lisa perhaps one day a robot will be capable of compose the next great symphony some expert believe this to be the case in fact some of the great mind in artificial intelligence be diligently work to develop program that can create drawing and music independently from human the use of artificial intelligence in the field of art have even be pick up by tech giant the like of google the project that be include in this paper could have drastic implication in our everyday life they may also change the way we view art they also showcase the incredible advancement that have be make in the field of artificial intelligence image recognition be not as far as the research go nor be the ability to generate music in the styling of the great artist of our past although these topic will be touch upon we will focus on several more advanced achievement such as text description be turn into image and generate art and music that be totally original each of these project bring something new and innovative to the table and show we exactly how the art space be a great place to far explore application of artificial intelligence we will be discuss problem that have be face in these project and how they have be overcome the future of ai look bright let s look at what the future may hold in do this we may be able to well understand the impact that artificial intelligence can have in an area that be drive by human creativity machine must be educate they learn from instruction how do we lead machine away from emulate what already exist and have they create new technique no creative artist will create art today that try to emulate the baroque or impressionist style or any other traditional style unless try to do so ironically 4 this problem isn t limit to painting either music can be very structured in some respect but be also a form of art that require vast creativity so how do we go about solve such a problem the first concept we will discuss be something call gan generative adversarial network gan although quite complex be become an outdated model if artificial intelligence in the art space be to advance researcher and developer will have to work to find well method to allow machine to generate art and music two of these such method be present in the form of sketch rnn and can creative adversarial network each of these method have their advantage over gan first let s explore what exactly a gan be below be a small excerpt explain how a gan work generative adversarial network gan have two sub network a generator and a discriminator the discriminator have access to a set of image training image the discriminator try to discriminate between real image from the training set and fake image generate by the generator the generator try to generate image similar to the training set without see the image 4 the more image the generator create the close they get to the image from the training set the idea be that after a certain number of image be generate the gan will create image that be very similar to what we consider art this be a very impressive accomplishment to say the least but what if we take it a step far many issue associate with the gan be simply limitation on what it can do the gan be powerful but can t do quite as much as we would like for example the generator in the model describe above will continue to create image close and close to the image give to the discriminator that it isn t produce original art could a gan be train to draw alongside a user it s not likely the model wouldn t be able to turn a text base description of an image into an actual picture either as impressive as the gan may be we would all agree that it can be improve each of the shortcoming mention have actually be address and to an extent solve let s look at how this be do sketch rnn be a recurrent neural network model develop by google the goal of sketch rnn be to help machine learn to create art in a manner similar to the way a human may learn it have be use in a google ai experiment to be able to sketch alongside a user while do so it can provide the user with suggestion and even complete the user s sketch when they decide to take a break sketch rnn be expose to a massive number of sketch provide through a dataset of vector drawing obtain through another google application that we will discuss later each of these sketch be tag to let the program know what object be in the sketch the datum set represent the sketch as a set of pen stroke this allow sketch rnn to then learn what aspect each sketch of a certain object have in common if a user begin to draw a cat sketch rnn could then show the user other common feature that could be on the cat this model could have many new creative application the decoder only model train on various class can assist the creative process of an artist by suggest many possible way of finish a sketch 3 the sketch rnn team even believe that give a more complex dataset the application could be use in an educational sense to teach user how to draw these application of sketch rnn couldn t be nearly as easily achieve with gan alone another method use to improve upon gan be the creative adversarial network in their paper regard adversarial network generate art several researcher discuss a new way of generate art through can the idea be that the can have two adversary network one the generator have no access to any art it have no basis to go off of when generate image the other network the discriminator be train to classify the image generate as be art or not when an image be generate the discriminator give the generator two piece of information the first be whether it believe the generate image come from the same distributor as the piece of art it be train on and the other be how the discriminator can fit the generate image into one of the category of art it be teach this technique be fantastic in that it help the generator create image that be both emulative of past work of art in the sense that it learn what be good about those image and creative in a sense that it be teach to produce new and different artistic concept this be a big difference from gan create art that emulate the training image eventually the can will learn how to produce only new and innovative artwork one final future for the vanilla gan be stackgan stackgan be a text to photo realistic image synthesizer that use stack generative adversarial network give a text description the stackgan be able to create image that be very much related to the give text this wouldn t be doable with a normal gan model as it would be much too difficult to generate photo realistic image from a text description even with a state of the art training database this be where stackgan come in it break the problem down into 2 part low resolution image be generate by our stage I gin on the top of our stage I gin we stack stage ii gan to generate realistic high resolution image condition on stage I result and text description 7 it be through the conditioning on stage I result and text description that stage ii gan can find detail that stage I gin may have miss and create high resolution image by break the problem down into small subproblem the stackgan can tackle problem that aren t possible with a regular gan on the next page be an image show the difference between a regular gan and each step of the stackgan it be through advancement like these that have be make in recent year that we can continue to push the boundary of what ai can do we have just see three way to improve upon a concept that be already quite complex and innovative each of these advancement have a practical everyday use as we continue to improve on artificial intelligence technique we will able to do more and more in regard to not just art and music but a wide variety of task to improve our life image aren t the only type of art that artificial intelligence can impact though its effect on music be be explore as we speak we will now explore some specific case and their impact on both music and artificial intelligence in do this we should be able to see how art can do as much for ai as ai do for it both field benefit heavily from the type of project that we be explore here could a machine ever be able to create a piece of music the like of johann sebastian bach in a project know as deepbach several researcher look to create piece similar to bach s chorale the beauty of deepbach be that it be able to generate coherent musical phrase and provide for instance varied reharmonization of melody without plagiarism 6 what this mean it that deepbach can create music with correct structure and be original it be just in the style of bach it isn t just a mashup of his work deepbach be create new content the developer of deepbach go on to test whether their product could actually fool listener as part of the experiment over 1 250 people be ask to vote whether piece present to they be in fact compose by bach the subject have vary degree of musical expertise the result show that as the model for deepbach s complexity increase the subject have more and more trouble distinguish the chorale of bach from those of deepbach this experiment show we that through the use of artificial intelligence and machine learning it be quite possible to recreate original work in the likeness of the great but be that the limit to what artificial intelligence can do in the field of art and music deepbach have achieve something that would have be unheard of in the not so distant past but it certainly isn t the full extent of what ai can do to benefit the field of music what if we want to create new and innovative music maybe ai can change the way music be create all together there must be project that do more to push the envelope as a matter of fact that be exactly what the team behind magenta look to do magenta be a project be conduct by the google brain team and lead by douglas eck eck have be work for google since 2010 but that isn t where his interest in music begin eck help find brain music and sound an international laboratory for brain music and sound research he be also involve at the mcgill centre for interdisciplinary research in music medium and technology and be an associate professor in computer science at the university of montreal magenta s goal be to be a research project to advance the state of the art in machine intelligence for music and art generation 2 it be an open source project that use tensorflow magenta aim to learn how to generate art and music in a way that be indeed generative it must go past just emulate exist music this be distinctly different that project along the line of deepbach which set out to emulate exist music in a way that wasn t plagiarize exist piece of music eck and company realize that art be about capture element of surprise and draw attention to certain aspect this lead to perhaps the big challenge combine generation attention and surprise to tell a compelling story so much of machine generate music and art be good in small chunk but lack any sort of long term narrative arc 2 such a perspective give computer generate music more substance and help it to become less of a gimmick one of the project the magenta team have develop be call nsynth the idea behind nsynth be to be able to create new sound that have never be hear before but beyond that to reimagine how music synthesis can be do unlike ordinary synthesizer that focus on a specific arrangement of oscillator or an algorithm for sample playback such as fm synthesis or granular synthesis 5 nsynth generate sound on an individual level to do this it use deep neural network google have even launch an experiment that allow user to really see what nsynth can do by allow they to fuse together the sound of exist instrument to create new hybrid sound that have never be hear before as an example user can take two instrument such as a banjo and a tuba and take part of each of their sound to create a totally new instrument the experiment also allow user to decide what percentage of each instrument would be use project like magenta go above and beyond in show we the full extent of what artificial intelligence can do in the way of generate music they explore new application of artificial intelligence that can generate new idea independent of human it be the close we have come to machine creativity although machine aren t yet able to truly think and express creativity they may soon be able to generate new and unique art and music for we to enjoy don t worry though eck doesn t intend to replace artist with ai instead he look to provide artist with tool to create music in an entirely new way as we look ahead to a few more of the way that ai have be use to accomplish new and innovative idea in the art space we look at project like quick draw and deep dream these project showcase amazing progress in the space while point out some issue that researcher in ai will have to work out in the year to come quick draw be an application from the google creative lab train to recognize quick drawing much like one would see in a game of pictionary the program can recognize simple object such as cat and apple base on common aspect of the many picture it be give before although the program will not get every picture right each time it be use it continue to learn from the similarity in the picture draw and the hundred of picture before it the science behind quick draw use some of the same technology that help google translate recognize your handwriting to understand handwriting or drawing you don t just look at what the person draw you look at how they actually draw it 1 it be present in the form of a game with the user draw a picture of an object choose by the application the program then have 20 second to recognize the image in each session the user be give a total of 6 object the image be then store to the database use to train application this happen to be the same database we see early in the sketch rnn application this image recognition be a very practical use of artificial intelligence in the realm of art and music it can do a lot to benefit we in our everyday life but this only begin to scratch the surface of what artificial intelligence can do in this field although this be very impressive we might point out that the application doesn t truly understand what be be draw it be just pick up on pattern in fact this distinction be part of the gap between simple ai technique and true artificial general intelligence machine that truly understand what the object in image be don t appear to be come in the near future another interesting project in the art space be google s deep dream project which use ai to create new and unique image unfortunately the deep dream generator team wouldn t go into too much detail about the technology itself mostly fear it would be too long for an email 8 they do however explain that convolutional neural network train on the famous imagenet dataset those neural network be then use to create art like image essentially deep dream take the styling of one image and use it to modify another image the result can be anything from a silly fusion to an artistic masterpiece this occur when the program identify the unique styling of an image provide by the user and impose those styling onto another image that the user provide what can easily be observe through the use of deep dream be that computer aren t yet capable of truly understand what they be do with respect to art they can be feed complex algorithm to generate image but don t fundamentally understand what it be they be generate for example a computer may see a knife cutting through an onion and assume the knife and onion be one object the lack of an ability to truly understand the content of an image be one dilemma that researcher have yet to solve perhaps as we continue to make advance in artificial intelligence we will be able to have machine that do truly understand what object be in an image and even the emotion evoke by their music the only way for this to be achieve be by reach true artificial general intelligence agi in the meantime the deep dream team believe that generative model will be able to create some really interesting piece of art and digital content for this section we will consider where artificial intelligence could be head in the art space we will take a look at how ai have impact the space and in what way it can continue to do so we will also look at way art and music could continue to impact ai in the year to come although I don t feel that we have completely master the ability to emulate the great artist of our past it be just a matter of time before that problem be solve the real task to be solve be that of create new innovation in art and music we need to work towards creation without emulation it be quite clear that we be head in that direction through project like can and magenta artificial general intelligence agi be not the only way to complete this task as a matter of fact even those who dispute the possibility of agi would have a hard time dispute the creation of unique work of art by a machine one path that may be take to far improve art and music through ai be to create more advanced dataset to use in train the complex network like sketch rnn and deep dream ai need to be train to be able to perform as expect that training have a huge impact on the result we get shouldn t we want to train our machine in the most beneficial way possible even develop software like sketch rnn to use the imagenet dataset use in deep dream could be huge in educate artist on technique for draw complex realistic image complex dataset could very well be our answer to more efficient training until our machine can think and learn like we do we will need to be very careful what datum be use to train they one of the way that art and music can help to impact ai be by provide another method of ture testing machine for those who dream of create agi what well way to test the machine s ability that to create something that test the full extent of human like creativity art be the true representation of human creativity that be in fact its essence although art be probably not the ultimate end game for artificial intelligence it could be one of the good way to test the limit of what a machine can do the day that computer can create original musical composition and create image base on description give by a user could very well be the day that we stop be able to distinguish man from machine there be many benefit to use artificial intelligence in the music space some of they have already be see in the project we have discuss so far we have see how artificial intelligence could be use for image recognition as well as their ability to turn our word into fantastic image we have also see how ai can be use to synthesize new sound that have never be hear we know that artificial intelligence can be use to create art alongside we as well as independently from we it can be teach to mimic music from the past and can create novel idea all of these accomplishment be a part of what will drive ai research into the future who know perhaps one day we will achieve artificial general intelligence and machine will be able to understand what be really in the image it be give maybe our computer will be able to understand how their art make we feel there be a clear path show we where to go from here I firmly believe that it be up to we to continue this research and test the limit of what artificial intelligence can do both in the field of art and in our everyday life from a quick cheer to a stand ovation clap to show how much you enjoy this story computer science student at louisiana tech university with an interest in anything ai share concept idea and code
Adam Geitgey,14.2K,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=tag_archive---------0----------------,machine learning be fun part 3 deep learning and convolutional neural network,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 普通话 русский 한국어 português tiếng việt or italiano be you tired of read endless news story about deep learning and not really know what that mean let s change that this time we be go to learn how to write program that recognize object in image use deep learning in other word we re go to explain the black magic that allow google photo to search your photo base on what be in the picture just like part 1 and part 2 this guide be for anyone who be curious about machine learning but have no idea where to start the goal be be accessible to anyone — which mean that there s a lot of generalization and we skip lot of detail but who care if this get anyone more interested in ml then mission accomplish if you haven t already read part 1 and part 2 read they now you might have see this famous xkcd comic before the goof be base on the idea that any 3 year old child can recognize a photo of a bird but figure out how to make a computer recognize object have puzzle the very good computer scientist for over 50 year in the last few year we ve finally find a good approach to object recognition use deep convolutional neural network that sound like a a bunch of make up word from a william gibson sci fi novel but the idea be totally understandable if you break they down one by one so let s do it — let s write a program that can recognize bird before we learn how to recognize picture of bird let s learn how to recognize something much simple — the handwritten number 8 in part 2 we learn about how neural network can solve complex problem by chain together lot of simple neuron we create a small neural network to estimate the price of a house base on how many bedroom it have how big it be and which neighborhood it be in we also know that the idea of machine learning be that the same generic algorithm can be reuse with different datum to solve different problem so let s modify this same neural network to recognize handwritten text but to make the job really simple we ll only try to recognize one letter — the numeral 8 machine learning only work when you have datum — preferably a lot of datum so we need lot and lot of handwritten 8 s to get start luckily researcher create the mnist datum set of handwritten number for this very purpose mnist provide 60 000 image of handwritten digit each as an 18x18 image here be some 8 s from the datum set the neural network we make in part 2 only take in a three number as the input 3 bedroom 2000 sq foot etc but now we want to process image with our neural network how in the world do we feed image into a neural network instead of just number the answer be incredible simple a neural network take number as input to a computer an image be really just a grid of number that represent how dark each pixel be to feed an image into our neural network we simply treat the 18x18 pixel image as an array of 324 number the handle 324 input we ll just enlarge our neural network to have 324 input node notice that our neural network also have two output now instead of just one the first output will predict the likelihood that the image be an 8 and thee second output will predict the likelihood it isn t an 8 by have a separate output for each type of object we want to recognize we can use a neural network to classify object into group our neural network be a lot big than last time 324 input instead of 3 but any modern computer can handle a neural network with a few hundred node without blink this would even work fine on your cell phone all that s leave be to train the neural network with image of 8 s and not 8 s so it learn to tell they apart when we feed in an 8 we ll tell it the probability the image be an 8 be 100 % and the probability it s not an 8 be 0 % vice versa for the counter example image here s some of our training datum we can train this kind of neural network in a few minute on a modern laptop when it s do we ll have a neural network that can recognize picture of 8 s with a pretty high accuracy welcome to the world of late 1980 s era image recognition it s really neat that simply feed pixel into a neural network actually work to build image recognition machine learning be magic right well of course it s not that simple first the good news be that our 8 recognizer really do work well on simple image where the letter be right in the middle of the image but now the really bad news our 8 recognizer totally fail to work when the letter isn t perfectly center in the image just the slight position change ruin everything this be because our network only learn the pattern of a perfectly center 8 it have absolutely no idea what an off center 8 be it know exactly one pattern and one pattern only that s not very useful in the real world real world problem be never that clean and simple so we need to figure out how to make our neural network work in case where the 8 isn t perfectly center we already create a really good program for find an 8 center in an image what if we just scan all around the image for possible 8 s in small section one section at a time until we find one this approach call a slide window it s the brute force solution it work well in some limited case but it s really inefficient you have to check the same image over and over look for object of different size we can do well than this when we train our network we only show it 8 s that be perfectly center what if we train it with more datum include 8 s in all different position and size all around the image we don t even need to collect new training datum we can just write a script to generate new image with the 8 s in all kind of different position in the image use this technique we can easily create an endless supply of training datum more datum make the problem hard for our neural network to solve but we can compensate for that by make our network big and thus able to learn more complicated pattern to make the network big we just stack up layer upon layer of node we call this a deep neural network because it have more layer than a traditional neural network this idea have be around since the late 1960 but until recently train this large of a neural network be just too slow to be useful but once we figure out how to use 3d graphic card which be design to do matrix multiplication really fast instead of normal computer processor work with large neural network suddenly become practical in fact the exact same nvidia geforce gtx 1080 video card that you use to play overwatch can be use to train neural network incredibly quickly but even though we can make our neural network really big and train it quickly with a 3d graphic card that still isn t go to get we all the way to a solution we need to be smart about how we process image into our neural network think about it it doesn t make sense to train a network to recognize an 8 at the top of a picture separately from train it to recognize an 8 at the bottom of a picture as if those be two totally different object there should be some way to make the neural network smart enough to know that an 8 anywhere in the picture be the same thing without all that extra training luckily there be as a human you intuitively know that picture have a hierarchy or conceptual structure consider this picture as a human you instantly recognize the hierarchy in this picture most importantly we recognize the idea of a child no matter what surface the child be on we don t have to re learn the idea of child for every possible surface it could appear on but right now our neural network can t do this it think that an 8 in a different part of the image be an entirely different thing it doesn t understand that move an object around in the picture doesn t make it something different this mean it have to re learn the identify of each object in every possible position that suck we need to give our neural network understanding of translation invariance — an 8 be an 8 no matter where in the picture it show up we ll do this use a process call convolution the idea of convolution be inspire partly by computer science and partly by biology I e mad scientist literally poke cat brain with weird probe to figure out how cat process image instead of feed entire image into our neural network as one grid of number we re go to do something a lot smart that take advantage of the idea that an object be the same no matter where it appear in a picture here s how it s go to work step by step — similar to our slide window search above let s pass a slide window over the entire original image and save each result as a separate tiny picture tile by do this we turn our original image into 77 equally sized tiny image tile early we feed a single image into a neural network to see if it be an 8 we ll do the exact same thing here but we ll do it for each individual image tile however there s one big twist we ll keep the same neural network weight for every single tile in the same original image in other word we be treat every image tile equally if something interesting appear in any give tile we ll mark that tile as interesting we don t want to lose track of the arrangement of the original tile so we save the result from process each tile into a grid in the same arrangement as the original image it look like this in other word we ve start with a large image and we end with a slightly small array that record which section of our original image be the most interesting the result of step 3 be an array that map out which part of the original image be the most interesting but that array be still pretty big to reduce the size of the array we downsample it use an algorithm call max pooling it sound fancy but it isn t at all we ll just look at each 2x2 square of the array and keep the big number the idea here be that if we find something interesting in any of the four input tile that make up each 2x2 grid square we ll just keep the most interesting bit this reduce the size of our array while keep the most important bit so far we ve reduce a giant image down into a fairly small array guess what that array be just a bunch of number so we can use that small array as input into another neural network this final neural network will decide if the image be or isn t a match to differentiate it from the convolution step we call it a fully connect network so from start to finish our whole five step pipeline look like this our image processing pipeline be a series of step convolution max pooling and finally a fully connect network when solve problem in the real world these step can be combine and stack as many time as you want you can have two three or even ten convolution layer you can throw in max pooling wherever you want to reduce the size of your datum the basic idea be to start with a large image and continually boil it down step by step until you finally have a single result the more convolution step you have the more complicated feature your network will be able to learn to recognize for example the first convolution step might learn to recognize sharp edge the second convolution step might recognize beak use it s knowledge of sharp edge the third step might recognize entire bird use it s knowledge of beak etc here s what a more realistic deep convolutional network like you would find in a research paper look like in this case they start a 224 x 224 pixel image apply convolution and max pooling twice apply convolution 3 more time apply max pooling and then have two fully connect layer the end result be that the image be classify into one of 1000 category so how do you know which step you need to combine to make your image classifier work honestly you have to answer this by do a lot of experimentation and testing you might have to train 100 network before you find the optimal structure and parameter for the problem you be solve machine learning involve a lot of trial and error now finally we know enough to write a program that can decide if a picture be a bird or not as always we need some datum to get start the free cifar10 datum set contain 6 000 picture of bird and 52 000 picture of thing that be not bird but to get even more datum we ll also add in the caltech ucsd bird 200 2011 datum set that have another 12 000 bird pic here s a few of the bird from our combine datum set and here s some of the 52 000 non bird image this datum set will work fine for our purpose but 72 000 low re image be still pretty small for real world application if you want google level performance you need million of large image in machine learning have more datum be almost always more important that have well algorithm now you know why google be so happy to offer you unlimited photo storage they want your sweet sweet datum to build our classifier we ll use tflearn tflearn be a wrapper around google s tensorflow deep learning library that expose a simplified api it make build convolutional neural network as easy as write a few line of code to define the layer of our network here s the code to define and train the network if you be train with a good video card with enough ram like an nvidia geforce gtx 980 ti or well this will be do in less than an hour if you be train with a normal cpu it might take a lot long as it train the accuracy will increase after the first pass I get 75 4 % accuracy after just 10 pass it be already up to 91 7 % after 50 or so pass it cap out around 95 5 % accuracy and additional training didn t help so I stop it there congrat our program can now recognize bird in image now that we have a train neural network we can use it here s a simple script that take in a single image file and predict if it be a bird or not but to really see how effective our network be we need to test it with lot of image the datum set I create hold back 15 000 image for validation when I run those 15 000 image through the network it predict the correct answer 95 % of the time that seem pretty good right well it depend our network claim to be 95 % accurate but the devil be in the detail that could mean all sort of different thing for example what if 5 % of our training image be bird and the other 95 % be not bird a program that guess not a bird every single time would be 95 % accurate but it would also be 100 % useless we need to look more closely at the number than just the overall accuracy to judge how good a classification system really be we need to look closely at how it fail not just the percentage of the time that it fail instead of think about our prediction as right and wrong let s break they down into four separate category — use our validation set of 15 000 image here s how many time our prediction fall into each category why do we break our result down like this because not all mistake be create equal imagine if we be write a program to detect cancer from an mri image if we be detect cancer we d rather have false positive than false negative false negative would be the bad possible case — that s when the program tell someone they definitely didn t have cancer but they actually do instead of just look at overall accuracy we calculate precision and recall metric precision and recall metric give we a clear picture of how well we do this tell we that 97 % of the time we guess bird we be right but it also tell we that we only find 90 % of the actual bird in the datum set in other word we might not find every bird but we be pretty sure about it when we do find one now that you know the basic of deep convolutional network you can try out some of the example that come with tflearn to get your hand dirty with different neural network architecture it even come with build in datum set so you don t even have to find your own image you also know enough now to start branch and learn about other area of machine learn why not learn how to use algorithm to train computer how to play atari game next if you like this article please consider sign up for my machine learning be fun email list I ll only email you when I have something new and awesome to share it s the good way to find out when I write more article like this you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 4 part 5 and part 6 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Adam Geitgey,15.2K,13,https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------1----------------,machine learning be fun part 4 modern face recognition with deep learning,update this article be part of a series check out the full series part 1 part 2 part 3 part 4 part 5 part 6 part 7 and part 8 you can also read this article in 普通话 русский 한국어 português tiếng việt or italiano have you notice that facebook have develop an uncanny ability to recognize your friend in your photograph in the old day facebook use to make you to tag your friend in photo by click on they and type in their name now as soon as you upload a photo facebook tag everyone for you like magic this technology be call face recognition facebook s algorithm be able to recognize your friend face after they have be tag only a few time it s pretty amazing technology — facebook can recognize face with 98 % accuracy which be pretty much as good as human can do let s learn how modern face recognition work but just recognize your friend would be too easy we can push this tech to the limit to solve a more challenging problem — tell will ferrell famous actor apart from chad smith famous rock musician so far in part 1 2 and 3 we ve use machine learning to solve isolated problem that have only one step — estimate the price of a house generate new datum base on exist datum and tell if an image contain a certain object all of those problem can be solve by choose one machine learning algorithm feed in datum and get the result but face recognition be really a series of several relate problem as a human your brain be wire to do all of this automatically and instantly in fact human be too good at recognize face and end up see face in everyday object computer be not capable of this kind of high level generalization at least not yet so we have to teach they how to do each step in this process separately we need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step in other word we will chain together several machine learning algorithm let s tackle this problem one step at a time for each step we ll learn about a different machine learning algorithm I m not go to explain every single algorithm completely to keep this from turn into a book but you ll learn the main idea behind each one and you ll learn how you can build your own facial recognition system in python use openface and dlib the first step in our pipeline be face detection obviously we need to locate the face in a photograph before we can try to tell they apart if you ve use any camera in the last 10 year you ve probably see face detection in action face detection be a great feature for camera when the camera can automatically pick out face it can make sure that all the face be in focus before it take the picture but we ll use it for a different purpose — find the area of the image we want to pass on to the next step in our pipeline face detection go mainstream in the early 2000 s when paul viola and michael jones invent a way to detect face that be fast enough to run on cheap camera however much more reliable solution exist now we re go to use a method invent in 2005 call histogram of orient gradient — or just hog for short to find face in an image we ll start by make our image black and white because we don t need color datum to find face then we ll look at every single pixel in our image one at a time for every single pixel we want to look at the pixel that directly surround it our goal be to figure out how dark the current pixel be compare to the pixel directly surround it then we want to draw an arrow showing in which direction the image be get dark if you repeat that process for every single pixel in the image you end up with every pixel be replace by an arrow these arrow be call gradient and they show the flow from light to dark across the entire image this might seem like a random thing to do but there s a really good reason for replace the pixel with gradient if we analyze pixel directly really dark image and really light image of the same person will have totally different pixel value but by only consider the direction that brightness change both really dark image and really bright image will end up with the same exact representation that make the problem a lot easy to solve but save the gradient for every single pixel give we way too much detail we end up miss the forest for the tree it would be well if we could just see the basic flow of lightness darkness at a high level so we could see the basic pattern of the image to do this we ll break up the image into small square of 16x16 pixel each in each square we ll count up how many gradient point in each major direction how many point up point up right point right etc then we ll replace that square in the image with the arrow direction that be the strong the end result be we turn the original image into a very simple representation that capture the basic structure of a face in a simple way to find face in this hog image all we have to do be find the part of our image that look the most similar to a know hog pattern that be extract from a bunch of other training face use this technique we can now easily find face in any image if you want to try this step out yourself use python and dlib here s code show how to generate and view hog representation of image whew we isolate the face in our image but now we have to deal with the problem that face turn different direction look totally different to a computer to account for this we will try to warp each picture so that the eye and lip be always in the sample place in the image this will make it a lot easy for we to compare face in the next step to do this we be go to use an algorithm call face landmark estimation there be lot of way to do this but we be go to use the approach invent in 2014 by vahid kazemi and josephine sullivan the basic idea be we will come up with 68 specific point call landmark that exist on every face — the top of the chin the outside edge of each eye the inner edge of each eyebrow etc then we will train a machine learn algorithm to be able to find these 68 specific point on any face here s the result of locate the 68 face landmark on our test image now that we know be the eye and mouth be we ll simply rotate scale and shear the image so that the eye and mouth be center as well as possible we win t do any fancy 3d warps because that would introduce distortion into the image we be only go to use basic image transformation like rotation and scale that preserve parallel line call affine transformation now no matter how the face be turn we be able to center the eye and mouth be in roughly the same position in the image this will make our next step a lot more accurate if you want to try this step out yourself use python and dlib here s the code for find face landmark and here s the code for transform the image use those landmark now we be to the meat of the problem — actually tell face apart this be where thing get really interesting the simple approach to face recognition be to directly compare the unknown face we find in step 2 with all the picture we have of people that have already be tag when we find a previously tag face that look very similar to our unknown face it must be the same person seem like a pretty good idea right there s actually a huge problem with that approach a site like facebook with billion of user and a trillion photo can t possibly loop through every previous tag face to compare it to every newly uploaded picture that would take way too long they need to be able to recognize face in millisecond not hour what we need be a way to extract a few basic measurement from each face then we could measure our unknown face the same way and find the know face with the close measurement for example we might measure the size of each ear the spacing between the eye the length of the nose etc if you ve ever watch a bad crime show like csi you know what I be talk about ok so which measurement should we collect from each face to build our know face database ear size nose length eye color something else it turn out that the measurement that seem obvious to we human like eye color don t really make sense to a computer look at individual pixel in an image researcher have discover that the most accurate approach be to let the computer figure out the measurement to collect itself deep learning do a well job than human at figure out which part of a face be important to measure the solution be to train a deep convolutional neural network just like we do in part 3 but instead of train the network to recognize picture object like we do last time we be go to train it to generate 128 measurement for each face the training process work by look at 3 face image at a time then the algorithm look at the measurement it be currently generate for each of those three image it then tweak the neural network slightly so that it make sure the measurement it generate for # 1 and # 2 be slightly close while make sure the measurement for # 2 and # 3 be slightly far apart after repeat this step million of time for million of image of thousand of different people the neural network learn to reliably generate 128 measurement for each person any ten different picture of the same person should give roughly the same measurement machine learn people call the 128 measurement of each face an embed the idea of reduce complicate raw datum like a picture into a list of computer generate number come up a lot in machine learning especially in language translation the exact approach for face we be use be invent in 2015 by researcher at google but many similar approach exist this process of train a convolutional neural network to output face embedding require a lot of datum and computer power even with an expensive nvidia telsa video card it take about 24 hour of continuous training to get good accuracy but once the network have be train it can generate measurement for any face even one it have never see before so this step only need to be do once lucky for we the fine folk at openface already do this and they publish several train network which we can directly use thank brandon amos and team so all we need to do ourselves be run our face image through their pre train network to get the 128 measurement for each face here s the measurement for our test image so what part of the face be these 128 number measure exactly it turn out that we have no idea it doesn t really matter to we all that we care be that the network generate nearly the same number when look at two different picture of the same person if you want to try this step yourself openface provide a lua script that will generate embedding all image in a folder and write they to a csv file you run it like this this last step be actually the easy step in the whole process all we have to do be find the person in our database of know people who have the close measurement to our test image you can do that by use any basic machine learning classification algorithm no fancy deep learning trick be need we ll use a simple linear svm classifier but lot of classification algorithm could work all we need to do be train a classifier that can take in the measurement from a new test image and tell which known person be the close match run this classifier take millisecond the result of the classifier be the name of the person so let s try out our system first I train a classifier with the embedding of about 20 picture each of will ferrell chad smith and jimmy falon then I run the classifier on every frame of the famous youtube video of will ferrell and chad smith pretend to be each other on the jimmy fallon show it work and look how well it work for face in different pose — even sideways face let s review the step we follow now that you know how this all work here s instruction from start to finish of how run this entire face recognition pipeline on your own computer update 4 9 2017 you can still follow the step below to use openface however I ve release a new python base face recognition library call face_recognition that be much easy to install and use so I d recommend try out face_recognition first instead of continue below I even put together a pre configure virtual machine with face_recognition opencv tensorflow and lot of other deep learning tool pre instal you can download and run it on your computer very easily give the virtual machine a shot if you don t want to install all these librarie yourself original openface instruction if you like this article please consider sign up for my machine learning be fun newsletter you can also follow I on twitter at @ageitgey email I directly or find I on linkedin I d love to hear from you if I can help you or your team with machine learning now continue on to machine learning be fun part 5 from a quick cheer to a stand ovation clap to show how much you enjoy this story interested in computer and machine learning like to write about it
Arthur Juliani,9K,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------2----------------,simple reinforcement learning with tensorflow part 0 q learning with table and neural network,for this tutorial in my reinforcement learning series we be go to be explore a family of rl algorithms call q learning algorithms these be a little different than the policy base algorithm that will be look at in the the follow tutorial part 1 3 instead of start with a complex and unwieldy deep neural network we will begin by implement a simple lookup table version of the algorithm and then show how to implement a neural network equivalent use tensorflow give that we be go back to basic it may be good to think of this as part 0 of the series it will hopefully give an intuition into what be really happen in q learning that we can then build on go forward when we eventually combine the policy gradient and q learning approach to build state of the art rl agent if you be more interested in policy network or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient method which attempt to learn function which directly map an observation to an action q learning attempt to learn the value of be in a give state and take a specific action there while both approach ultimately allow we to take intelligent action give a situation the mean of get to that action differ significantly you may have hear about deepq network which can play atari game these be really just large and more complex implementation of the q learning algorithm we be go to discuss here for this tutorial we be go to be attempt to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provide an easy way for people to experiment with their learn agent in an array of provide toy game the frozenlake environment consist of a 4x4 grid of block each one either be the start block the goal block a safe frozen block or a dangerous hole the objective be to have an agent learn to navigate from the start to the goal without move onto a hole at any give time the agent can choose to move either up down left or right the catch be that there be a wind which occasionally blow the agent onto a space they didn t choose as such perfect performance every time be impossible but learn to avoid the hole and reach the goal be certainly still doable the reward at every step be 0 except for enter the goal which provide a reward of 1 thus we will need an algorithm that learn long term expect reward this be exactly what q learning be design to provide in it s simple implementation q learning be a table of value for every state row and action column possible in the environment within each cell of the table we learn a value for how good it be to take a give action within a give state in the case of the frozenlake environment we have 16 possible state one for each block and 4 possible action the four direction of movement give we a 16x4 table of q value we start by initialize the table to be uniform all zero and then as we observe the reward we obtain for various action we update the table accordingly we make update to our q table use something call the bellman equation which state that the expect long term reward for a give action be equal to the immediate reward from the current action combine with the expect reward from the good future action take at the follow state in this way we reuse our own q table when estimate how to update our table for future action in equation form the rule look like this this say that the q value for a give state s and action a should represent the current reward r plus the maximum discount γ future reward expect accord to our own table for the next state s we would end up in the discount variable allow we to decide how important the possible future reward be compare to the present reward by update in this way the table slowly begin to obtain accurate measure of the expect future reward for a give action in a give state below be a python walkthrough of the q table algorithm implement in the frozenlake environment thank to praneet d for find the optimal hyperparameter for this approach now you may be think table be great but they don t really scale do they while it be easy to have a 16x4 table for a simple grid world the number of possible state in any modern game or real world environment be nearly infinitely large for most interesting problem table simply don t work we instead need some way to take a description of our state and produce q value for action without a table that be where neural network come in by act as a function approximator we can take any number of possible state that can be represent as a vector and learn to map they to q value in the case of the frozenlake example we will be use a one layer network which take the state encode in a one hot vector 1x16 and produce a vector of 4 q value one for each action such a simple network act kind of like a glorify table with the network weight serve as the old cell the key difference be that we can easily expand the tensorflow network with add layer activation function and different input type whereas all that be impossible with a regular table the method of update be a little different as well instead of directly update our table with a network we will be use backpropagation and a loss function our loss function will be sum of square loss where the difference between the current predict q value and the target value be compute and the gradient pass through the network in this case our q target for the choose action be the equivalent to the q value compute in equation 1 above below be the tensorflow walkthrough of implement our simple q network while the network learn to solve the frozenlake problem it turn out it doesn t do so quite as efficiently as the q table while neural network allow for great flexibility they do so at the cost of stability when it come to q learning there be a number of possible extension to our simple q network which allow for great performance and more robust learn two trick in particular be refer to as experience replay and freeze target network those improvement and other tweak be the key to get atari play deep q network and we will be explore those addition in the future for more info on the theory behind q learning see this great post by tambet matiisen I hope this tutorial have be helpful for those curious about how to implement simple q learning algorithms if this post have be valuable to you please consider donate to help support future tutorial article and implementation any contribution be greatly appreciate if you d like to follow my work on deep learning ai and cognitive science follow I on medium @arthur juliani or on twitter @awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @unity3d & cognitive neuroscience phd student explore frontier technology through the lens of artificial intelligence datum science and the shape of thing to come
Dhruv Parthasarathy,4.3K,12,https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4?source=tag_archive---------5----------------,a brief history of cnn in image segmentation from r cnn to mask r cnn,at athela we use convolutional neural network cnn for a lot more than just classification in this post we ll see how cnn can be use with great result in image instance segmentation ever since alex krizhevsky geoff hinton and ilya sutskever win imagenet in 2012 convolutional neural network cnn have become the gold standard for image classification in fact since then cnn have improve to the point where they now outperform human on the imagenet challenge while these result be impressive image classification be far simple than the complexity and diversity of true human visual understanding in classification there s generally an image with a single object as the focus and the task be to say what that image be see above but when we look at the world around we we carry out far more complex task we see complicated sight with multiple overlap object and different background and we not only classify these different object but also identify their boundary difference and relation to one another can cnn help we with such complex task namely give a more complicated image can we use cnn to identify the different object in the image and their boundary as have be show by ross girshick and his peer over the last few year the answer be conclusively yes through this post we ll cover the intuition behind some of the main technique use in object detection and segmentation and see how they ve evolve from one implementation to the next in particular we ll cover r cnn regional cnn the original application of cnn to this problem along with its descendant fast r cnn and fast r cnn finally we ll cover mask r cnn a paper release recently by facebook research that extend such object detection technique to provide pixel level segmentation here be the paper reference in this post inspire by the research of hinton s lab at the university of toronto a small team at uc berkeley lead by professor jitendra malik ask themselves what today seem like an inevitable question object detection be the task of find the different object in an image and classify they as see in the image above the team comprise of ross girshick a name we ll see again jeff donahue and trevor darrel find that this problem can be solve with krizhevsky s result by test on the pascal voc challenge a popular object detection challenge akin to imagenet they write let s now take a moment to understand how their architecture region with cnn r cnn work understand r cnn the goal of r cnn be to take in an image and correctly identify where the main object via a bounding box in the image but how do we find out where these bounding box be r cnn do what we might intuitively do as well propose a bunch of box in the image and see if any of they actually correspond to an object r cnn create these bounding box or region proposal use a process call selective search which you can read about here at a high level selective search show in the image above look at the image through window of different size and for each size try to group together adjacent pixel by texture color or intensity to identify object once the proposal be create r cnn warps the region to a standard square size and pass it through to a modified version of alexnet the win submission to imagenet 2012 that inspire r cnn as show above on the final layer of the cnn r cnn add a support vector machine svm that simply classify whether this be an object and if so what object this be step 4 in the image above improve the bounding box now have find the object in the box can we tighten the box to fit the true dimension of the object we can and this be the final step of r cnn r cnn run a simple linear regression on the region proposal to generate tight bounding box coordinate to get our final result here be the input and output of this regression model so to summarize r cnn be just the follow step r cnn work really well but be really quite slow for a few simple reason in 2015 ross girshick the first author of r cnn solve both these problem lead to the second algorithm in our short history fast r cnn let s now go over its main insight fast r cnn insight 1 roi region of interest pooling for the forward pass of the cnn girshick realize that for each image a lot of propose region for the image invariably overlap cause we to run the same cnn computation again and again ~2000 time his insight be simple — why not run the cnn just once per image and then find a way to share that computation across the ~2000 proposal this be exactly what fast r cnn do use a technique know as roipool region of interest pooling at its core roipool share the forward pass of a cnn for an image across its subregion in the image above notice how the cnn feature for each region be obtain by select a correspond region from the cnn s feature map then the feature in each region be pool usually use max pooling so all it take we be one pass of the original image as oppose to ~2000 fast r cnn insight 2 combine all model into one network the second insight of fast r cnn be to jointly train the cnn classifier and bounding box regressor in a single model where early we have different model to extract image feature cnn classify svm and tighten bounding box regressor fast r cnn instead use a single network to compute all three you can see how this be do in the image above fast r cnn replace the svm classifier with a softmax layer on top of the cnn to output a classification it also add a linear regression layer parallel to the softmax layer to output bounding box coordinate in this way all the output need come from one single network here be the input and output to this overall model even with all these advancement there be still one remain bottleneck in the fast r cnn process — the region proposer as we see the very first step to detect the location of object be generate a bunch of potential bounding box or region of interest to test in fast r cnn these proposal be create use selective search a fairly slow process that be find to be the bottleneck of the overall process in the middle 2015 a team at microsoft research compose of shaoqe ren kaime he ross girshick and jian sun find a way to make the region proposal step almost cost free through an architecture they creatively name fast r cnn the insight of fast r cnn be that region proposal depend on feature of the image that be already calculate with the forward pass of the cnn first step of classification so why not reuse those same cnn result for region proposal instead of run a separate selective search algorithm indeed this be just what the fast r cnn team achieve in the image above you can see how a single cnn be use to both carry out region proposal and classification this way only one cnn need to be train and we get region proposal almost for free the author write here be the input and output of their model how the region be generate let s take a moment to see how fast r cnn generate these region proposal from cnn feature fast r cnn add a fully convolutional network on top of the feature of the cnn create what s know as the region proposal network the region proposal network work by pass a slide window over the cnn feature map and at each window output k potential bounding box and score for how good each of those box be expect to be what do these k box represent intuitively we know that object in an image should fit certain common aspect ratio and size for instance we know that we want some rectangular box that resemble the shape of human likewise we know we win t see many box that be very very thin in such a way we create k such common aspect ratio we call anchor box for each such anchor box we output one bounding box and score per position in the image with these anchor box in mind let s take a look at the input and output to this region proposal network we then pass each such bounding box that be likely to be an object into fast r cnn to generate a classification and tightened bounding box so far we ve see how we ve be able to use cnn feature in many interesting way to effectively locate different object in an image with bounding box can we extend such technique to go one step far and locate exact pixel of each object instead of just bound box this problem know as image segmentation be what kaime he and a team of researcher include girshick explore at facebook ai use an architecture know as mask r cnn much like fast r cnn and fast r cnn mask r cnn s underlying intuition be straight forward give that fast r cnn work so well for object detection could we extend it to also carry out pixel level segmentation mask r cnn do this by add a branch to fast r cnn that output a binary mask that say whether or not a give pixel be part of an object the branch in white in the above image as before be just a fully convolutional network on top of a cnn base feature map here be its input and output but the mask r cnn author have to make one small adjustment to make this pipeline work as expect roialign realigning roipool to be more accurate when run without modification on the original fast r cnn architecture the mask r cnn author realize that the region of the feature map select by roipool be slightly misalign from the region of the original image since image segmentation require pixel level specificity unlike bound box this naturally lead to inaccuracy the author be able to solve this problem by cleverly adjust roipool to be more precisely align use a method know as roialign imagine we have an image of size 128x128 and a feature map of size 25x25 let s imagine we want feature the region correspond to the top leave 15x15 pixel in the original image see above how might we select these pixel from the feature map we know each pixel in the original image correspond to ~ 25 128 pixel in the feature map to select 15 pixel from the original image we just select 15 * 25 128 ~= 2 93 pixel in roipool we would round this down and select 2 pixel cause a slight misalignment however in roialign we avoid such round instead we use bilinear interpolation to get a precise idea of what would be at pixel 2 93 this at a high level be what allow we to avoid the misalignment cause by roipool once these mask be generate mask r cnn combine they with the classification and bounding box from fast r cnn to generate such wonderfully precise segmentation if you re interested in try out these algorithm yourself here be relevant repository fast r cnn mask r cnn in just 3 year we ve see how the research community have progress from krizhevsky et al s original result to r cnn and finally all the way to such powerful result as mask r cnn see in isolation result like mask r cnn seem like incredible leap of genius that would be unapproachable yet through this post I hope you ve see how such advancement be really the sum of intuitive incremental improvement through year of hard work and collaboration each of the idea propose by r cnn fast r cnn fast r cnn and finally mask r cnn be not necessarily quantum leap yet their sum product have lead to really remarkable result that bring we close to a human level understanding of sight what particularly excite I be that the time between r cnn and mask r cnn be just three year with continue funding focus and support how much far can computer vision improve over the next three year if you see any error or issue in this post please contact I at dhruv@getathela com and I ll immediately correct they if you re interested in apply such technique come join we at athela where we apply computer vision to blood diagnostic daily other post we ve write thank to bharath ramsundar pranav ramkrishnan tanay tandon and oliver cameron for help with this post from a quick cheer to a stand ovation clap to show how much you enjoy this story @dhruvp vp eng @athelas mit math and cs undergrad 13 mit cs master 14 previously director of ai program @ udacity blood diagnostic through deep learning http athela com
Sebastian Heinz,4.4K,13,https://medium.com/mlreview/a-simple-deep-learning-model-for-stock-price-prediction-using-tensorflow-30505541d877?source=tag_archive---------6----------------,a simple deep learning model for stock price prediction use tensorflow,for a recent hackathon that we do at statworx some of our team member scrape minutely s&p 500 datum from the google finance api the datum consist of index as well as stock price of the s&p s 500 constituent have this datum at hand the idea of develop a deep learning model for predict the s&p 500 index base on the 500 constituent price one minute ago come immediately on my mind play around with the datum and build the deep learning model with tensorflow be fun and so I decide to write my first medium com story a little tensorflow tutorial on predict s&p 500 stock price what you will read be not an in depth tutorial but more a high level introduction to the important building block and concept of tensorflow model the python code I ve create be not optimize for efficiency but understandability the dataset I ve use can be download from here 40 mb our team export the scrape stock datum from our scrape server as a csv file the dataset contain n = 41266 minute of datum range from april to august 2017 on 500 stock as well as the total s&p 500 index price index and stock be arrange in wide format the datum be already clean and prepare meaning miss stock and index price be locf ed last observation carry forward so that the file do not contain any miss value a quick look at the s&p time series use pyplot plot datum sp500 note this be actually the lead of the s&p 500 index mean its value be shift 1 minute into the future this operation be necessary since we want to predict the next minute of the index and not the current minute the dataset be split into training and test datum the training datum contain 80 % of the total dataset the data be not shuffle but sequentially slice the training datum range from april to approx end of july 2017 the test data end end of august 2017 there be a lot of different approach to time series cross validation such as roll forecast with and without refit or more elaborate concept such as time series bootstrap resample the latter involve repeat sample from the remainder of the seasonal decomposition of the time series in order to simulate sample that follow the same seasonal pattern as the original time series but be not exact copy of its value most neural network architecture benefit from scale the input sometimes also the output why because most common activation function of the network s neuron such as tanh or sigmoid be define on the 1 1 or 0 1 interval respectively nowadays rectify linear unit relu activation be commonly use activation which be unbounded on the axis of possible activation value however we will scale both the input and target anyway scaling can be easily accomplish in python use sklearn s minmaxscaler remark caution must be undertake regard what part of the data be scale and when a common mistake be to scale the whole dataset before training and test split be be apply why be this a mistake because scale invoke the calculation of statistic e g the min max of a variable when perform time series forecasting in real life you do not have information from future observation at the time of forecast therefore calculation of scale statistic have to be conduct on training datum and must then be apply to the test datum otherwise you use future information at the time of forecasting which commonly bias forecasting metric in a positive direction tensorflow be a great piece of software and currently the lead deep learning and neural network computation framework it be base on a c++ low level backend but be usually control via python there be also a neat tensorflow library for r maintain by rstudio tensorflow operate on a graph representation of the underlie computational task this approach allow the user to specify mathematical operation as element in a graph of datum variable and operator since neural network be actually graph of datum and mathematical operation tensorflow be just perfect for neural network and deep learning check out this simple example steal from our deep learning introduction from our blog in the figure above two number be suppose to be add those number be store in two variable a and b the two value be flow through the graph and arrive at the square node where they be be add the result of the addition be store into another variable c actually a b and c can be consider as placeholder any number that be feed into a and b get add and be store into c this be exactly how tensorflow work the user define an abstract representation of the model neural network through placeholder and variable afterwards the placeholder get fill with real datum and the actual computation take place the follow code implement the toy example from above in tensorflow after have import the tensorflow library two placeholder be define use tf placeholder they correspond to the two blue circle on the left of the image above afterwards the mathematical addition be define via tf add the result of the computation be c = 9 with placeholder set up the graph can be execute with any integer value for a and b of course the former problem be just a toy example the require graph and computation in a neural network be much more complex as mention before it all start with placeholder we need two placeholder in order to fit our model x contain the network s input the stock price of all s&p 500 constituent at time t = t and y the network s output the index value of the s&p 500 at time t = t + 1 the shape of the placeholder correspond to none n_stock with none mean that the input be a 2 dimensional matrix and the output be a 1 dimensional vector it be crucial to understand which input and output dimension the neural net need in order to design it properly the none argument indicate that at this point we do not yet know the number of observation that flow through the neural net graph in each batch so we keep if flexible we will later define the variable batch_size that control the number of observation per training batch besides placeholder variable be another cornerstone of the tensorflow universe while placeholder be use to store input and target datum in the graph variable be use as flexible container within the graph that be allow to change during graph execution weight and bias be represent as variable in order to adapt during training variable need to be initialize prior to model training we will get into that a litte later in more detail the model consist of four hide layer the first layer contain 1024 neuron slightly more than double the size of the inputs subsequent hide layer be always half the size of the previous layer which mean 512 256 and finally 128 neuron a reduction of the number of neuron for each subsequent layer compress the information the network identify in the previous layer of course other network architecture and neuron configuration be possible but be out of scope for this introduction level article it be important to understand the require variable dimension between input hide and output layer as a rule of thumb in multilayer perceptron mlp the type of network use here the second dimension of the previous layer be the first dimension in the current layer for weight matrix this might sound complicated but be essentially just each layer pass its output as input to the next layer the bias dimension equal the second dimension of the current layer s weight matrix which correspond the number of neuron in this layer after definition of the require weight and bias variable the network topology the architecture of the network need to be specify hereby placeholder datum and variable weigh and bias need to be combine into a system of sequential matrix multiplication furthermore the hide layer of the network be transform by activation function activation function be important element of the network architecture since they introduce non linearity to the system there be dozen of possible activation function out there one of the most common be the rectified linear unit relu which will also be use in this model the image below illustrate the network architecture the model consist of three major building block the input layer the hide layer and the output layer this architecture be call a feedforward network feedforward indicate that the batch of datum solely flow from leave to right other network architecture such as recurrent neural network also allow datum flow backwards in the network the cost function of the network be use to generate a measure of deviation between the network s prediction and the actual observed training target for regression problem the mean square error mse function be commonly use mse compute the average squared deviation between prediction and target basically any differentiable function can be implement in order to compute a deviation measure between prediction and target however the mse exhibit certain property that be advantageous for the general optimization problem to be solve the optimizer take care of the necessary computation that be use to adapt the network s weight and bias variable during train those computation invoke the calculation of so call gradient that indicate the direction in which the weight and bias have to be change during training in order to minimize the network s cost function the development of stable and speedy optimizer be a major field in neural network an deep learning research here the adam optimizer be use which be one of the current default optimizer in deep learning development adam stand for adaptive moment estimation and can be consider as a combination between two other popular optimizer adagrad and rmsprop initializer be use to initialize the network s variable before training since neural network be train use numerical optimization technique the starting point of the optimization problem be one the key factor to find good solution to the underlying problem there be different initializer available in tensorflow each with different initialization approach here I use the tf variance_scaling_initializer which be one of the default initialization strategy note that with tensorflow it be possible to define multiple initialization function for different variable within the graph however in most case a unified initialization be sufficient after have define the placeholder variable initializer cost function and optimizer of the network the model need to be train usually this be do by minibatch training during minibatch train random data sample of n = batch_size be draw from the training datum and feed into the network the training dataset get divide into n batch_size batch that be sequentially feed into the network at this point the placeholder x and y come into play they store the input and target datum and present they to the network as input and target a sample data batch of x flow through the network until it reach the output layer there tensorflow compare the model prediction against the actual observed target y in the current batch afterwards tensorflow conduct an optimization step and update the network parameter correspond to the select learning scheme after have update the weight and bias the next batch be sample and the process repeat itself the procedure continue until all batch have be present to the network one full sweep over all batch be call an epoch the training of the network stop once the maximum number of epoch be reach or another stopping criterion define by the user apply during the training we evaluate the network prediction on the test set — the datum which be not learn but set aside — for every 5th batch and visualize it additionally the image be export to disk and later combine into a video animation of the training process see below the model quickly learn the shape und location of the time series in the test datum and be able to produce an accurate prediction after some epoch nice one can see that the network rapidly adapt to the basic shape of the time series and continue to learn finer pattern of the datum this also correspond to the adam learning scheme that lower the learning rate during model training in order not to overshoot the optimization minimum after 10 epoch we have a pretty close fit to the test datum the final test mse equal 0 00078 it be very low because the target be scale the mean absolute percentage error of the forecast on the test set be equal to 5 31 % which be pretty good note that this be just a fit to the test datum no actual out of sample metric in a real world scenario please note that there be ton of way of far improve this result design of layer and neuron choose different initialization and activation scheme introduction of dropout layer of neuron early stop and so on furthermore different type of deep learning model such as recurrent neural network might achieve well performance on this task however this be not the scope of this introductory post the release of tensorflow be a landmark event in deep learning research its flexibility and performance allow researcher to develop all kind of sophisticated neural network architecture as well as other ml algorithms however flexibility come at the cost of long time to model cycle compare to high level api such as keras or mxnet nonetheless I be sure that tensorflow will make its way to the de facto standard in neural network and deep learning development in research and practical application many of our customer be already use tensorflow or start develop project that employ tensorflow model also our data science consultant at statworx be heavily use tensorflow for deep learning and neural net research and development let s see what google have plan for the future of tensorflow one thing that be miss at least in my opinion be a neat graphical user interface for design and develop neural net architecture with tensorflow backend maybe this be something google be already work on ; if you have any comment or question on my first medium story feel free to comment below I will try to answer they also feel free to use my code or share this story with your peer on social platform of your choice update I ve add both the python script as well as a zip dataset to a github repository feel free to clone and fork lastly follow I on twitter | linkedin from a quick cheer to a stand ovation clap to show how much you enjoy this story ceo @ statworx do datum science stat and ml for over a decade food wine and cocktail enthusiast check our website https www statworx com highlight from machine learning research project and learn material from and for ml scientist engineer an enthusiast
Max Pechyonkin,23K,8,https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b?source=tag_archive---------7----------------,understand hinton s capsule network part I intuition,part I intuition you be read it now part ii how capsule workpart iii dynamic routing between capsulespart iv capsnet architecture quick announcement about our new publication ai3 we be get the good writer together to talk about the theory practice and business of ai and machine learning follow it to stay up to date on the late trend last week geoffrey hinton and his team publish two paper that introduce a completely new type of neural network base on so call capsule in addition to that the team publish an algorithm call dynamic routing between capsule that allow to train such a network for everyone in the deep learning community this be huge news and for several reason first of all hinton be one of the founder of deep learning and an inventor of numerous model and algorithm that be widely use today secondly these paper introduce something completely new and this be very exciting because it will most likely stimulate additional wave of research and very cool application in this post I will explain why this new architecture be so important as well as intuition behind it in the follow post I will dive into technical detail however before talk about capsule we need to have a look at cnn which be the workhorse of today s deep learning cnns convolutional neural network be awesome they be one of the reason deep learning be so popular today they can do amazing thing that people use to think computer would not be capable of do for a long long time nonetheless they have their limit and they have fundamental drawback let we consider a very simple and non technical example imagine a face what be the component we have the face oval two eye a nose and a mouth for a cnn a mere presence of these object can be a very strong indicator to consider that there be a face in the image orientational and relative spatial relationship between these component be not very important to a cnn how do cnn work the main component of a cnn be a convolutional layer its job be to detect important feature in the image pixel layer that be deeply close to the input will learn to detect simple feature such as edge and color gradient whereas high layer will combine simple feature into more complex feature finally dense layer at the top of the network will combine very high level feature and produce classification prediction an important thing to understand be that high level feature combine low level feature as a weighted sum activation of a precede layer be multiply by the follow layer neuron s weight and add before be pass to activation nonlinearity nowhere in this setup there be pose translational and rotational relationship between simple feature that make up a high level feature cnn approach to solve this issue be to use max pooling or successive convolutional layer that reduce spacial size of the datum flow through the network and therefore increase the field of view of high layer s neuron thus allow they to detect high order feature in a large region of the input image max pooling be a crutch that make convolutional network work surprisingly well achieve superhuman performance in many area but do not be fool by its performance while cnn work well than any model before they max pooling nonetheless be lose valuable information hinton himself state that the fact that max pooling be work so well be a big mistake and a disaster of course you can do away with max pooling and still get good result with traditional cnn but they still do not solve the key problem in the example above a mere presence of 2 eye a mouth and a nose in a picture do not mean there be a face we also need to know how these object be orient relative to each other computer graphic deal with construct a visual image from some internal hierarchical representation of geometric datum note that the structure of this representation need to take into account relative position of object that internal representation be store in computer s memory as array of geometrical object and matrix that represent relative position and orientation of these object then special software take that representation and convert it into an image on the screen this be call render inspire by this idea hinton argue that brain in fact do the opposite of render he call it inverse graphic from visual information receive by eye they deconstruct a hierarchical representation of the world around we and try to match it with already learn pattern and relationship store in the brain this be how recognition happen and the key idea be that representation of object in the brain do not depend on view angle so at this point the question be how do we model these hierarchical relationship inside of a neural network the answer come from computer graphic in 3d graphic relationship between 3d object can be represent by a so call pose which be in essence translation plus rotation hinton argue that in order to correctly do classification and object recognition it be important to preserve hierarchical pose relationship between object part this be the key intuition that will allow you to understand why capsule theory be so important it incorporate relative relationship between object and it be represent numerically as a 4d pose matrix when these relationship be build into internal representation of datum it become very easy for a model to understand that the thing that it see be just another view of something that it have see before consider the image below you can easily recognize that this be the statue of liberty even though all the image show it from different angle this be because internal representation of the statue of liberty in your brain do not depend on the view angle you have probably never see these exact picture of it but you still immediately know what it be for a cnn this task be really hard because it do not have this build in understanding of 3d space but for a capsnet it be much easy because these relationship be explicitly model the paper that use this approach be able to cut error rate by 45 % as compare to the previous state of the art which be a huge improvement another benefit of the capsule approach be that it be capable of learn to achieve state of the art performance by only use a fraction of the datum that a cnn would use hinton mention this in his famous talk about what be wrong with cnn in this sense the capsule theory be much close to what the human brain do in practice in order to learn to tell digit apart the human brain need to see only a couple of dozen of example hundred at most cnn on the other hand need ten of thousand of example to achieve very good performance which seem like a brute force approach that be clearly inferior to what we do with our brain the idea be really simple there be no way no one have come up with it before and the truth be hinton have be think about this for decade the reason why there be no publication be simply because there be no technical way to make it work before one of the reason be that computer be just not powerful enough in the pre gpu base era before around 2012 another reason be that there be no algorithm that allow to implement and successfully learn a capsule network in the same fashion the idea of artificial neuron be around since 1940 s but it be not until mid 1980 s when backpropagation algorithm show up and allow to successfully train deep network in the same fashion the idea of capsule itself be not that new and hinton have mention it before but there be no algorithm up until now to make it work this algorithm be call dynamic routing between capsule this algorithm allow capsule to communicate with each other and create representation similar to scene graph in computer graphic capsule introduce a new building block that can be use in deep learning to well model hierarchical relationship inside of internal knowledge representation of a neural network intuition behind they be very simple and elegant hinton and his team propose a way to train such a network make up of capsule and successfully train it on a simple data set achieve state of the art performance this be very encouraging nonetheless there be challenge current implementation be much slow than other modern deep learning model time will show if capsule network can be train quickly and efficiently in addition we need to see if they work well on more difficult data set and in different domain in any case the capsule network be a very interesting and already work model which will definitely get more develop over time and contribute to further expansion of deep learning application domain this conclude part one of the series on capsule network in the part ii more technical part I will walk you through the capsnet s internal working step by step you can follow I on twitter let s also connect on linkedin from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learn the ai revolution be here navigate the ever change industry with our thoughtfully write article whether your a researcher engineer or entrepreneur
Slav Ivanov,3.9K,17,https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=tag_archive---------8----------------,the $ 1700 great deep learning box assembly setup and benchmark,update april 2018 use cuda 9 cudnn 7 and tensorflow 1 5 after year of use a thin client in the form of increasingly thin macbook I have get use to it so when I get into deep learning dl I go straight for the brand new at the time amazon p2 cloud server no upfront cost the ability to train many model simultaneously and the general coolness of have a machine learning model out there slowly teach itself however as time pass the aws bill steadily grow large even as I switch to 10x cheap spot instance also I didn t find myself train more than one model at a time instead I d go to lunch workout etc while the model be train and come back later with a clear head to check on it but eventually the model complexity grow and take long to train I d often forget what I do differently on the model that have just complete its 2 day training nudge by the great experience of the other folk on the fast ai forum I decide to settle down and to get a dedicated dl box at home the most important reason be save time while prototyping model — if they train fast the feedback time would be short thus it would be easy for my brain to connect the dot between the assumption I have for the model and its result then I want to save money — I be use amazon web service aws which offer p2 instance with nvidia k80 gpus lately the aws bill be around $ 60 70 month with a tendency to get large also it be expensive to store large dataset like imagenet and lastly I haven t have a desktop for over 10 year and want to see what have change in the meantime spoiler alert mostly nothing what follow be my choice inner monologue and gotcha from choose the component to benchmarke a sensible budget for I would be about 2 year worth of my current compute spending at $ 70 month for aw this put it at around $ 1700 for the whole thing you can check out all the component use the pc part picker site be also really helpful in detect if some of the component don t play well together the gpu be the most crucial component in the box it will train these deep network fast shorten the feedback cycle disclosure the follow be affiliate link to help I pay for well more gpu the choice be between a few of nvidia s card gtx 1070 gtx 1070 ti gtx 1080 gtx 1080 ti and finally the titan x the price might fluctuate especially because some gpu be great for cryptocurrency mining wink 1070 wink on performance side gtx 1080 ti and titan x be similar roughly speak the gtx 1080 be about 25 % fast than gtx 1070 and gtx 1080 ti be about 30 % fast than gtx 1080 the new gtx 1070 ti be very close in performance to gtx 1080 tim dettmer have a great article on pick a gpu for deep learning which he regularly update as new card come on the market here be the thing to consider when pick a gpu consider all of this I pick the gtx 1080 ti mainly for the training speed boost I plan to add a second 1080 ti soonish even though the gpu be the mvp in deep learning the cpu still matter for example data preparation be usually do on the cpu the number of core and thread per core be important if we want to parallelize all that data prep to stay on budget I pick a mid range cpu the intel i5 7500 it s relatively cheap but good enough to not slow thing down edit as a few people have point out probably the big gotcha that be unique to dl multi gpu be to pay attention to the pcie lane support by the cpu motherboard by andrej karpathy we want to have each gpu have 16 pcie lane so it eat datum as fast as possible 16 gb s for pcie 3 0 this mean that for two card we need 32 pcie lane however the cpu I have pick have only 16 lane so 2 gpu would run in 2x8 mode instead of 2x16 this might be a bottleneck lead to less than ideal utilization of the graphic card thus a cpu with 40 line be recommend edit 2 however tim dettmer point out that have 8 lane per card should only decrease performance by 0 10 % for two gpu so currently my recommendation be go with 16 pcie lane per video card unless it get too expensive for you otherwise 8 lane should do as well a good solution with to have for a double gpu machine would be an intel xeon processor like the e5 1620 v4 40 pcie lane or if you want to splurge go for a high end processor like the desktop i7 6850k memory ram it s nice to have a lot of memory if we be to be work with rather big dataset I get 2 stick of 16 gb for a total of 32 gb of ram and plan to buy another 32 gb later follow jeremy howard s advice I get a fast ssd disk to keep my os and current datum on and then a slow spin hdd for those huge dataset like imagenet ssd I remember when I get my first macbook air year ago how blow away be I by the ssd speed to my delight a new generation of ssd call nvme have make its way to market in the meantime a 480 gb mydigitalssd nvme drive be a great deal this baby copy file at gigabyte per second hdd 2 tb seagate while ssds have be get fast hdd have be get cheap to somebody who have use macbook with 128 gb disk for the last 7 year have this much space feel almost obscene the one thing that I keep in mind when pick a motherboard be the ability to support two gtx 1080 ti both in the number of pci express lane the minimum be 2x8 and the physical size of 2 card also make sure it s compatible with the choose cpu an asus tuf z270 do it for I msi — x99a sli plus should work great if you get an intel xeon cpu rule of thumb power supply should provide enough juice for the cpu and the gpu plus 100 watt extra the intel i5 7500 processor use 65w and the gpus 1080 ti need 250w each so I get a deepcool 750w gold psu currently unavailable evga 750 gq be similar the gold here refer to the power efficiency I e how much of the power consume be waste as heat the case should be the same form factor as the motherboard also have enough led to embarrass a burner be a bonus a friend recommend the thermaltake n23 case which I promptly get no led sadly here be how much I spend on all the component your cost may vary $ 700 gtx 1080 ti + $ 190 cpu + $ 230 ram + $ 230 ssd + $ 66 hdd + $ 130 motherboard + $ 75 psu + $ 50 case = = = = = = = = = = = = $ 1671 total add tax and fee this nicely match my preset budget of $ 1700 if you don t have much experience with hardware and fear you might break something a professional assembly might be the good option however this be a great learning opportunity that I couldn t pass even though I ve have my share of hardware relate horror story the first and important step be to read the installation manual that come with each component especially important for I as I ve do this before once or twice and I have just the right amount of inexperience to mess thing up this be do before instal the motherboard in the case next to the processor there be a lever that need to be pull up the processor be then place on the base double check the orientation finally the lever come down to fix the cpu in place but I have a quite the difficulty do this once the cpu be in position the lever wouldn t go down I actually have a more hardware capable friend of mine video walk I through the process turn out the amount of force require to get the lever lock down be more than what I be comfortable with next be fix the fan on top of the cpu the fan leg must be fully secure to the motherboard consider where the fan cable will go before instal the processor I have come with thermal paste if yours doesn t make sure to put some paste between the cpu and the cool unit also replace the paste if you take off the fan I put the power supply unit psu in before the motherboard to get the power cable snugly place in case back side pretty straight forward — carefully place it and screw it in a magnetic screwdriver be really helpful then connect the power cable and the case button and led just slide it in the m2 slot and screw it in piece of cake the memory prove quite hard to install require too much effort to properly lock in a few time I almost give up thinking I must be do it wrong eventually one of the stick click in and the other one promptly follow at this point I turn the computer on to make sure it work to my relief it start right away finally the gpu slide in effortlessly 14 pin of power later and it be run nb do not plug your monitor in the external card right away most probably it need driver to function see below finally it s complete now that we have the hardware in place only the soft part remain out with the screwdriver in with the keyboard note on dual booting if you plan to install window because you know for benchmark totally not for game it would be wise to do window first and linux second I didn t and have to reinstall ubuntu because window mess up the boot partition livewire have a detailed article on dual boot most dl framework be design to work on linux first and eventually support other operating system so I go for ubuntu my default linux distribution an old 2 gb usb drive be lay around and work great for the installation unetbootin osx or rufus window can prepare the linux thumb drive the default option work fine during the ubuntu install at the time of write ubuntu 17 04 be just release so I opt for the previous version 16 04 whose quirk be much well document online ubuntu server or desktop the server and desktop edition of ubuntu be almost identical with the notable exception of the visual interface call x not be instal with server I instal the desktop and disabled autostarte x so that the computer would boot it in terminal mode if need one could launch the visual desktop later by type startx let s get our install up to date from jeremy howard s excellent install gpu script to deep learn on our machine we need a stack of technology to use our gpu download cuda from nvidia or just run the code below update to specify version 9 of cuda thank to @zhanwenchen for the tip if you need to add later version of cuda click here after cuda have be instal the follow code will add the cuda installation to the path variable now we can verify that cuda have be instal successfully by run this should have instal the display driver as well for I nvidia smi show err as the device name so I instal the late nvidia driver as of may 2018 to fix it remove cuda nvidia driver if at any point the driver or cuda seem break as they do for I — multiple time it might be well to start over by run since version 1 5 tensorflow support cudnn 7 so we install that to download cudnn one need to register for a free developer account after download install with the follow anaconda be a great package manager for python I ve move to python 3 6 so will be use the anaconda 3 version the popular dl framework by google installation validate tensorfow install to make sure we have our stack run smoothly I like to run the tensorflow mnist example we should see the loss decrease during training keras be a great high level neural network framework an absolute pleasure to work with installation can t be easy too pytorch be a newcomer in the world of dl framework but its api be model on the successful torch which be write in lua pytorch feel new and exciting mostly great although some thing be still to be implement we install it by run jupyter be a web base ide for python which be ideal for data sciency task it s instal with anaconda so we just configure and test it now if we open http localhost 8888 we should see a jupyter screen run jupyter on boot rather than run the notebook every time the computer be restart we can set it to autostart on boot we will use crontab to do this which we can edit by run crontab e then add the following after the last line in the crontab file I use my old trusty macbook air for development so I d like to be able to log into the dl box both from my home network also when on the run ssh key it s way more secure to use a ssh key to login instead of a password digital ocean have a great guide on how to setup this ssh tunnel if you want to access your jupyter notebook from another computer the recommend way be to use ssh tunneling instead of open the notebook to the world and protect with a password let s see how we can do this 2 then to connect over ssh tunnel run the follow script on the client to test this open a browser and try http localhost 8888 from the remote machine your jupyter notebook should appear setup out of network access finally to access the dl box from the outside world we need 3 thing set up out of network access depend on the router network setup so I m not go into detail now that we have everything run smoothly let s put it to the test we ll be compare the newly build box to an aws p2 xlarge instance which be what I ve use so far for dl the test be computer vision relate mean convolutional network with a fully connect model throw in we time training model on aws p2 instance gpu k80 aw p2 virtual cpu the gtx 1080 ti and intel i5 7500 cpu andre hernandez point out that my comparison do not use tensorflow that be optimize for these cpu which would have help the they perform well check his insightful comment for more detail the hello world of computer vision the mnist database consist of 70 000 handwritten digit we run the keras example on mnist which use multilayer perceptron mlp the mlp mean that we be use only fully connect layer not convolution the model be train for 20 epoch on this dataset which achieve over 98 % accuracy out of the box we see that the gtx 1080 ti be 2 4 time fast than the k80 on aws p2 in train the model this be rather surprising as these 2 card should have about the same performance I believe this be because of the virtualization or underclocking of the k80 on aws the cpus perform 9 time slow than the gpu as we will see later it s a really good result for the processor this be due to the small model which fail to fully utilize the parallel processing power of the gpu interestingly the desktop intel i5 7500 achieve 2 3x speedup over the virtual cpu on amazon a vgg net will be finetune for the kaggle dog vs cat competition in this competition we need to tell apart picture of dog and cat run the model on cpus for the same number of batch wasn t feasible therefore we finetune for 390 batch 1 epoch on the gpu and 10 batch on the cpus the code use be on github the 1080 ti be 5 5 time fast that the aws gpu k80 the difference in the cpus performance be about the same as the previous experiment i5 be 2 6x fast however it s absolutely impractical to use cpu for this task as the cpus be take ~200x more time on this large model that include 16 convolutional layer and a couple semi wide 4096 fully connect layer on top a gan generative adversarial network be a way to train a model to generate image gan achieve this by pit two network against each other a generator which learn to create well and well image and a discriminator that try to tell which image be real and which be dream up by the generator the wasserstein gan be an improvement over the original gan we will use a pytorch implementation that be very similar to the one by the wgan author the model be train for 50 step and the loss be all over the place which be often the case with gan cpus aren t consider the gtx 1080 ti finish 5 5x fast than the aws p2 k80 which be in line with the previous result the final benchmark be on the original style transfer paper gatys et al implement on tensorflow code available style transfer be a technique that combine the style of one image a painting for example and the content of another image check out my previous post for more detail on how style transfer work the gtx 1080 ti outperform the aws k80 by a factor of 4 3 this time the cpu be 30 50 time slow than graphic card the slowdown be less than on the vgg finetune task but more than on the mnist perceptron experiment the model use mostly the early layer of the vgg network and I suspect this be too shallow to fully utilize the gpus the dl box be in the next room and a large model be train on it be it a wise investment time will tell but it be beautiful to watch the glow led in the dark and to hear its quiet hum as model be try to squeeze out that extra accuracy percentage point from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur hacker machine learn deep learning and other type of learn
Stefan Kojouharov,14.2K,7,https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?source=tag_archive---------9----------------,cheat sheet for ai neural network machine learn deep learning & big datum,over the past few month I have be collect ai cheat sheet from time to time I share they with friend and colleague and recently I have be get ask a lot so I decide to organize and share the entire collection to make thing more interesting and give context I add description and or excerpt for each major topic this be the most complete list and the big o be at the very end enjoy this machine learn cheat sheet will help you find the right estimator for the job which be the most difficult part the flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problem and how to solve it scikit learn formerly scikit learn be a free software machine learning library for the python programming language it feature various classification regression and clustering algorithm include support vector machine random forest gradient boost k mean and dbscan and be design to interoperate with the python numerical and scientific library numpy and scipy in may 2017 google announce the second generation of the tpu as well as the availability of the tpus in google compute engine 12 the second generation tpus deliver up to 180 teraflop of performance and when organize into cluster of 64 tpu provide up to 11 5 petaflop in 2017 google s tensorflow team decide to support keras in tensorflow s core library chollet explain that keras be conceive to be an interface rather than an end to end machine learning framework it present a high level more intuitive set of abstraction that make it easy to configure neural network regardless of the backend scientific computing library numpy target the cpython reference implementation of python which be a non optimize bytecode interpreter mathematical algorithm write for this version of python often run much slow than compile equivalent numpy address the slowness problem partly by provide multidimensional array and function and operator that operate efficiently on array require rewrite some code mostly inner loop use numpy the name panda be derive from the term panel datum an econometric term for multidimensional structured data set the term datum wrangler be start to infiltrate pop culture in the 2017 movie kong skull island one of the character play by actor marc evan jackson be introduce as steve woodward our data wrangler scipy build on the numpy array object and be part of the numpy stack which include tool like matplotlib panda and sympy and an expand set of scientific computing librarie this numpy stack have similar user to other application such as matlab gnu octave and scilab the numpy stack be also sometimes refer to as the scipy stack 3 matplotlib be a plotting library for the python programming language and its numerical mathematics extension numpy it provide an object orient api for embed plot into application use general purpose gui toolkit like tkinter wxpython qt or gtk+ there be also a procedural pylab interface base on a state machine like opengl design to closely resemble that of matlab though its use be discourage 2 scipy make use of matplotlib pyplot be a matplotlib module which provide a matlab like interface 6 matplotlib be design to be as usable as matlab with the ability to use python with the advantage that it be free > > > if you like this list you can let I know here < < < stefan be the founder of chatbot s life a chatbot medium and consult firm chatbot s life have grow to over 150k view per month and have become the premium place to learn about bot & ai online chatbot s life have also consult many of the top bot company like swelly instav outbrain neargroup and a number of enterprise big o algorithm cheat sheet http bigocheatsheet com bokeh cheat sheet https s3 amazonaw com asset datacamp com blog_asset python_bokeh_cheat_sheet pdf datum science cheat sheet https www datacamp com community tutorial python data science cheat sheet basic datum wrangle cheat sheet https www rstudio com wp content upload 2015 02 datum wrangle cheatsheet pdf datum wrangle https en wikipedia org wiki data_wrangle ggplot cheat sheet https www rstudio com wp content upload 2015 03 ggplot2 cheatsheet pdf keras cheat sheet https www datacamp com community blog keras cheat sheet#gs drkenms keras https en wikipedia org wiki keras machine learn cheat sheet https ai icymi email new machinelearning cheat sheet by emily barry abdsc machine learning cheat sheet https doc microsoft com en in azure machine learning machine learn algorithm cheat sheet ml cheat sheet http peekaboo vision blogspot com 2013 01 machine learn cheat sheet for scikit html matplotlib cheat sheet https www datacamp com community blog python matplotlib cheat sheet#gs uekyspy matpotlib https en wikipedia org wiki matplotlib neural network cheat sheet http www asimovinstitute org neural network zoo neural network graph cheat sheet http www asimovinstitute org blog neural network https www quora com where can find a cheat sheet for neural network numpy cheat sheet https www datacamp com community blog python numpy cheat sheet#gs ak5zbge numpy https en wikipedia org wiki numpy panda cheat sheet https www datacamp com community blog python panda cheat sheet#gs oundfxm panda https en wikipedia org wiki panda _ software panda cheat sheet https www datacamp com community blog panda cheat sheet python#gs hpforic pyspark cheat sheet https www datacamp com community blog pyspark cheat sheet python#gs l = j1zxq scikit cheat sheet https www datacamp com community blog scikit learn cheat sheet scikit learn https en wikipedia org wiki scikit learn scikit learn cheat sheet http peekaboo vision blogspot com 2013 01 machine learn cheat sheet for scikit html scipy cheat sheet https www datacamp com community blog python scipy cheat sheet#gs jdsg3oi scipy https en wikipedia org wiki scipy tesorflow cheat sheet https www altoros com tensorflow cheat sheet html tensor flow https en wikipedia org wiki tensorflow from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of chatbot life I help company create great chatbot & ai system and share my insight along the way late news info and tutorial on artificial intelligence machine learn deep learn big datum and what it mean for humanity
Netflix Technology Blog,99,11,https://medium.com/netflix-techblog/distributed-neural-networks-with-gpus-in-the-aws-cloud-ccf71e82056b?source=tag_archive---------0----------------,distribute neural network with gpu in the aws cloud,by alex chen justin basilico and xavier amatriain as we have describe previously on this blog at netflix we be constantly innovate by look for well way to find the good movie and tv show for our member when a new algorithmic technique such as deep learning show promise result in other domain e g image recognition neuro imaging language model and speech recognition it should not come as a surprise that we would try to figure out how to apply such technique to improve our product in this post we will focus on what we have learn while build infrastructure for experiment with these approach at netflix we hope that this will be useful for other work on similar algorithm especially if they be also leverage the amazon web service aws infrastructure however we will not detail how we be use variant of artificial neural network for personalization since it be an active area of research many researcher have point out that most of the algorithmic technique use in the trendy deep learning approach have be know and available for some time much of the more recent innovation in this area have be around make these technique feasible for real world application this involve design and implement architecture that can execute these technique use a reasonable amount of resource in a reasonable amount of time the first successful instance of large scale deep learning make use of 16000 cpu core in 1000 machine in order to train an artificial neural network in a matter of day while that be a remarkable milestone the require infrastructure cost and computation time be still not practical andrew ng and his team address this issue in follow up work their implementation use gpu as a powerful yet cheap alternative to large cluster of cpu use this architecture they be able to train a model 6 5 time large in a few day use only 3 machine in another study schwenk et al show that train these model on gpu can improve performance dramatically even when compare to high end multicore cpus give our well know approach and leadership in cloud computing we seek out to implement a large scale neural network training system that leverage both the advantage of gpu and the aws cloud we want to use a reasonable number of machine to implement a powerful machine learning solution use a neural network approach we also want to avoid need special machine in a dedicated data center and instead leverage the full on demand computing power we can obtain from aws in architecte our approach for leverage computing power in the cloud we seek to strike a balance that would make it fast and easy to train neural network by look at the entire training process for computing resource we have the capacity to use many gpu core cpu core and aw instance which we would like to use efficiently for an application such as this we typically need to train not one but multiple model either from different dataset or configuration e g different international region for each configuration we need to perform hyperparameter tuning where each combination of parameter require train a separate neural network in our solution we take the approach of use gpu base parallelism for training and use distribute computation for handle hyperparameter tuning and different configuration some of you might be think that the scenario describe above be not what people think of as a distribute machine learning in the traditional sense for instance in the work by ng et al cite above they distribute the learning algorithm itself between different machine while that approach might make sense in some case we have find that to be not always the norm especially when a dataset can be store on a single instance to understand why we first need to explain the different level at which a model training process can be distribute in a standard scenario we will have a particular model with multiple instance those instance might correspond to different partition in your problem space a typical situation be to have different model train for different country or region since the feature distribution and even the item space might be very different from one region to the other this represent the first initial level at which we can decide to distribute our learning process we could have for example a separate machine train each of the 41 country where netflix operate since each region can be train entirely independently however as explain above train a single instance actually imply training and test several model each corresponding to a different combination of hyperparameter this represent the second level at which the process can be distribute this level be particularly interesting if there be many parameter to optimize and you have a good strategy to optimize they like bayesian optimization with gaussian process the only communication between run be hyperparameter setting and test evaluation metric finally the algorithm train itself can be distribute while this be also interesting it come at a cost for example train ann be a comparatively communication intensive process give that you be likely to have thousand of core available in a single gpu instance it be very convenient if you can squeeze the most out of that gpu and avoid get into costly across machine communication scenario this be because communication within a machine use memory be usually much fast than communication over a network the follow pseudo code below illustrate the three level at which an algorithm training process like we can be distribute in this post we will explain how we address level 1 and 2 distribution in our use case note that one of the reason we do not need to address level 3 distribution be because our model have million of parameter compare to the billion in the original paper by ng before we address distribution problem though we have to make sure the gpu base parallel training be efficient we approach this by first get a proof of concept to work on our own development machine and then address the issue of how to scale and use the cloud as a second stage we start by use a lenovo s20 workstation with a nvidia quadro 600 gpu this gpu have 98 core and provide a useful baseline for our experiment ; especially consider that we plan on use a more powerful machine and gpu in the aws cloud our first attempt to train our neural network model take 7 hour we then run the same code to train the model in on a ec2 s cg1 4xlarge instance which have a more powerful tesla m2050 with 448 core however the training time jump from 7 to over 20 hour profiling show that most of the time be spend on the function call to nvidia performance primitive library e g nppsmulc_32f_i nppsexp_32f_i call the npp function repeatedly take 10x more system time on the cg1 instance than in the lenovo s20 while we try to uncover the root cause we work our way around the issue by reimplemente the npp function use the customize cuda kernel e g replace nppsmulc_32f_i function with replace all npp function in this way for the neural network code reduce the total training time on the cg1 instance from over 20 hour to just 47 minute when train on 4 million sample train 1 million sample take 96 second of gpu time use the same approach on the lenovo s20 the total training time also reduce from 7 hour to 2 hour this make we believe that the implementation of these function be suboptimal regardless of the card specific while we be implement this hack we also work with the aws team to find a principled solution that would not require a kernel patch in do so we find that the performance degradation be relate to the nvreg_checkpciconfigspace parameter of the kernel accord to redhat set this parameter to 0 disable very slow access to the pci configuration space in a virtualized environment such as the aws cloud these access cause a trap in the hypervisor that result in even slow access nvreg_checkpciconfigspace be a parameter of kernel module nvidia current that can be set use we test the effect of change this parameter use a benchmark that call mulc repeatedly 128x1000 time below be the result runtime in sec on our cg1 4xlarge instance as you can see disable access to pci space have a spectacular effect in the original npp function decrease the runtime by 95 % the effect be significant even in our optimize kernel function save almost 25 % in runtime however it be important to note that even when the pci access be disable our customize function perform almost 60 % well than the default one we should also point out that there be other option which we have not explore so far but could be useful for other first we could look at optimize our code by apply a kernel fusion trick that combine several computation step into one kernel to reduce the memory access finally we could think about use theano the gpu match compiler in python which be suppose to also improve performance in these case while our initial work be do use cg1 4xlarge ec2 instance we be interested in move to the new ec2 gpu g2 2xlarge instance type which have a grid k520 gpu gk104 chip with 1536 core currently our application be also bound by gpu memory bandwidth and the grid k520 s memory bandwidth be 198 gb sec which be an improvement over the tesla m2050 s at 148 gb sec of course use a gpu with fast memory would also help e g titan s memory bandwidth be 288 gb sec we repeat the same comparison between the default npp function and our customize one with and without pci space access on the g2 2xlarge instance one initial surprise be that we measure bad performance for npp on the g2 instance than the cg1 when pci space access be enable however disable it improve performance between 45 % and 65 % compare to the cg1 instance again our kernelmulc customize function be over 70 % well with benchmark time under a second thus switch to g2 with the right configuration allow we to run our experiment fast or alternatively large experiment in the same amount of time once we have optimize the single node training and testing operation we be ready to tackle the issue of hyperparameter optimization if you be not familiar with this concept here be a simple explanation most machine learning algorithm have parameter to tune which be call often call hyperparameter to distinguish they from model parameter that be produce as a result of the learning algorithm for example in the case of a neural network we can think about optimize the number of hide unit the learning rate or the regularization weight in order to tune these you need to train and test several different combination of hyperparameter and pick the good one for your final model a naive approach be to simply perform an exhaustive grid search over the different possible combination of reasonable hyperparameter however when face with a complex model where train each one be time consume and there be many hyperparameter to tune it can be prohibitively costly to perform such exhaustive grid search luckily you can do well than this by think of parameter tuning as an optimization problem in itself one way to do this be to use a bayesian optimization approach where an algorithm s performance with respect to a set of hyperparameter be model as a sample from a gaussian process gaussian process be a very effective way to perform regression and while they can have trouble scale to large problem they work well when there be a limited amount of datum like what we encounter when perform hyperparameter optimization we use package spearmint to perform bayesian optimization and find the good hyperparameter for the neural network training algorithm we hook up spearmint with our training algorithm by have it choose the set of hyperparameter and then train a neural network with those parameter use our gpu optimize code this model be then test and the test metric result use to update the next hyperparameter choice make by spearmint we ve squeeze high performance from our gpu but we only have 1 2 gpu card per machine so we would like to make use of the distribute computing power of the aws cloud to perform the hyperparameter tuning for all configuration such as different model per international region to do this we use the distribute task queue celery to send work to each of the gpu each worker process listen to the task queue and run the training on one gpu this allow we for example to tune train and update several model daily for all international region although the spearmint + celery system be work we be currently evaluate more complete and flexible solution use htcondor or starcluster htcondor can be use to manage the workflow of any direct acyclic graph dag it handle input output file transfer and resource management in order to use condor we need each compute node register into the manager with a give classad e g slot1_has_gpu = true ; stard_attrs = has_gpu then the user can submit a job with a configuration requirement = has_gpu so that the job only run on aw instance that have an available gpu the main advantage of use condor be that it also manage the distribution of the datum need for the training of the different model condor also allow we to run the spearmint bayesian optimization on the manager instead of have to run it on each of the worker another alternative be to use starcluster which be an open source cluster computing framework for aws ec2 develop at mit starcluster run on the oracle grid engine formerly sun grid engine in a fault tolerant way and be fully support by spearmint finally we be also look into integrate spearmint with jobman in order to well manage the hyperparameter search workflow figure below illustrate the generalize setup use spearmint plus celery condor or starcluster implement bleed edge solution such as use gpu to train large scale neural network can be a daunt endeavour if you need to do it in your own custom infrastructure the cost and the complexity might be overwhelming lever the public aws cloud can have obvious benefit provide care be take in the customization and use of the instance resource by share our experience we hope to make it much easy and straightforward for other to develop similar application we be always look for talented researcher and engineer to join our team so if you be interested in solve these type of problem please take a look at some of our open position on the netflix job page originally publish at techblog netflix com on february 10 2014 from a quick cheer to a stand ovation clap to show how much you enjoy this story learn more about how netflix design build and operate our system and engineering organization learn about netflix s world class engineering effort company culture product development and more
Francesco Gadaleta,3,4,https://hackernoon.com/gradient-descent-vs-coordinate-descent-9b5657f1c59f?source=tag_archive---------1----------------,gradient descent vs coordinate descent hacker noon,when it come to function minimization it s time to open a book of optimization and linear algebra I be currently work on variable selection and lasso base solution in genetic what lasso do be basically minimize the loss function and an penalty in order to set to zero some regression coefficient and select only those covariate that be really associate with the response pheew the short summary of lasso ever we all know that provide the function to be minimize be convex a good direction to follow in order to find a local minimum be towards the negative gradient of the function now my question be how good or bad be follow the negative gradient with respect to a coordinate descent approach that loop across all dimension and minimize along each there be no well way to try this with real code and start measure hence I write some code that implement both gradient descent and coordinate descent the comparison might not be completely fair because the learning rate in the gradient descent procedure be fix at 0 1 which in some case might be slow indeed but even with some tuning maybe with some linear search or adaptive learning rate it s quite common to see that coordinate descent overcome its brother gradient descent many time this occur much more often when the number of covariate become very high as in many computational biology problem in the figure below I plot the analytical solution in red the gradient descent minimisation in blue and the coordinate descent in green across a number of iteration a small explanation be probably necessary to read the function that perform coordinate descent for a more mathematical explanation refer to the original post coordinate descent will update each variable in a round robin fashion despite the learning rate of the gradient descent procedure which could indeed speed up convergence the comparison between the two be fair at least in term of complexity coordinate descent need to perform operation for each coordinate update gradient descent perform the same number of operation the r code that perform this comparison and generate the plot above be give below feel free to download this code remember to cite I and send I some cookie happy descent originally publish at worldofpiggy com on may 31 2014 from a quick cheer to a stand ovation clap to show how much you enjoy this story machine learn math crypto blockchain fitchain io how hacker start their afternoon
Milo Spencer-Harper,2.2K,3,https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a?source=tag_archive---------0----------------,how to build a multi layer neural network in python,in my last blog post thank to an excellent blog post by andrew trask I learn how to build a neural network for the first time it be super simple 9 line of python code model the behaviour of a single neuron but what if we be face with a more difficult problem can you guess what the should be the trick be to notice that the third column be irrelevant but the first two column exhibit the behaviour of a xor gate if either the first column or the second column be 1 then the output be 1 however if both column be 0 or both column be 1 then the output be 0 so the correct answer be 0 however this would be too much for our single neuron to handle this be consider a nonlinear pattern because there be no direct one to one relationship between the input and the output instead we must create an additional hidden layer consist of four neuron layer 1 this layer enable the neural network to think about combination of input you can see from the diagram that the output of layer 1 feed into layer 2 it be now possible for the neural network to discover correlation between the output of layer 1 and the output in the training set as the neural network learn it will amplify those correlation by adjust the weight in both layer in fact image recognition be very similar there be no direct relationship between pixel and apple but there be a direct relationship between combination of pixel and apple the process of add more layer to a neural network so it can think about combination be call deep learning ok be we ready for the python code first I ll give you the code and then I ll explain far also available here https github com miloharper multi layer neural network this code be an adaptation from my previous neural network so for a more comprehensive explanation it s worth look back at my early blog post what s different this time be that there be multiple layer when the neural network calculate the error in layer 2 it propagate the error backwards to layer 1 adjust the weight as it go this be call back propagation ok let s try run it use the terminal command python main py you should get a result that look like this first the neural network assign herself random weight to her synaptic connection then she train herself use the training set then she consider a new situation 1 1 0 that she hadn t see before and predict 0 0078876 the correct answer be 0 so she be pretty close you might have notice that as my neural network have become smart I ve inadvertently personify she by use she instead of it that s pretty cool but the computer be do lot of matrix multiplication behind the scene which be hard to visualise in my next blog post I ll visually represent our neural network with an animate diagram of her neuron and synaptic connection so we can see her thinking from a quick cheer to a stand ovation clap to show how much you enjoy this story study economic at oxford university founder of www moju io interested in politic and ai technology trend and new invention follow this collection to update the late trend update as a collection editor I don t have any permission to add your article in the wild please submit your article and I will approve also follow this collection please
Jim Fleming,294,3,https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f?source=tag_archive---------1----------------,load a tensorflow graph with the c++ api jim fleme medium,check out the related post loading tensorflow graph from node js use the c api the current documentation around load a graph with c++ be pretty sparse so I spend some time set up a barebone example in the tensorflow repo there be more involved example such as build a graph in c++ however the c++ api for construct graph be not as complete as the python api many feature include automatic gradient computation be not available from c++ yet another example in the repo demonstrate define your own operation but most user will never need this I imagine the most common use case for the c++ api be for load pre train graph to be standalone or embed in other application be aware there be some caveat to this approach that I ll cover at the end let s start by create a minimal tensorflow graph and write it out as a protobuf file make sure to assign name to your input and operation so they re easy to assign when we execute the graph later the node s do have default name but they aren t very useful variable_1 or mul_3 here s an example create with jupyter let s create a new folder like tensorflow tensorflow < my project name > for your binary or library to live I m go to call the project loader since it will be load a graph inside this project folder we ll create a new file call < my project name > cc e g loader cc if you re curious the cc extension be essentially the same as cpp but be prefer by google s code guideline inside loader cc we re go to do a few thing now we create a build file for our project this tell bazel what to compile inside we want to define a cc_binary for our program you can also use the linkshared option on the binary to produce a share library or the cc_library rule if you re go to link it use bazel here s the final directory structure you could also call bazel run loader to run the executable directly however the work directory for bazel run be bury in a temporary folder and readbinaryproto look in the current work directory for relative path and that should be all we need to do to compile and run c++ code for tensorflow the last thing to cover be the caveat I mention hopefully someone can shed some light on these last point so we can begin to embed tensorflow graph in application if you be that person message I on twitter or email if you d like help deploy tensorflow in production I do consult from a quick cheer to a stand ovation clap to show how much you enjoy this story cto and lead ml engineer at fomoro — focus on machine learning and apply cut edge research for business — previously @rdio what I m work on
Milo Spencer-Harper,1.8K,4,https://medium.com/deep-learning-101/how-to-generate-a-video-of-a-neural-network-learning-in-python-62f5c520e85c?source=tag_archive---------2----------------,video of a neural network learn deep learn 101 medium,as part of my quest to learn about ai I generate a video of a neural network learn many of the example on the internet use matrix grid of number to represent a neural network this method be favour because it be however it s difficult to understand what be happen from a learning perspective be able to visually see a neural network be hugely beneficial the video you be about to see show a neural network try to solve this pattern can you work it out it s the same problem I pose in my previous blog post the trick be to notice that the third column be irrelevant but the first two column exhibit the behaviour of a xor gate if either the first column or the second column be 1 then the output be 1 however if both column be 0 or both column be 1 then the output be 0 so the correct answer be 0 our neural network will cycle through these 7 example 60 000 time to speed up the video I will only show you 13 of these cycle pause for a second on each frame why the number 13 it ensure the video last exactly as long as the music each time she consider an example in the training set you will see she think you will see her neuron and her synaptic connection glow she will then calculate the error the difference between the output and the desire output she will then propagate this error backwards adjust her synaptic connection green synaptic connection represent positive weight a signal flow through this synapse will excite the next neuron to fire red synaptic connection represent negative weight a signal flow through this synapse will inhibit the next neuron from fire thick synapsis represent strong connection large weight in the beginning her synaptic weight be randomly assign notice how some synapsis be green positive and other be red negative if these synapsis turn out to be beneficial in calculate the right answer she will strengthen they over time however if they be unhelpful these synapsis will wither it s even possible for a synapse which be originally positive to become negative and vice versa an example of this be the first synapse into the output neuron — early on in the video it turn from red to green in the beginning her brain look like this do you notice that all her neuron be dark this be because she isn t currently think about anything the number to the right of each neuron represent the level of neural activity and vary between 0 and 1 ok now she be go to think about the pattern we see early watch the video carefully to see her synapsis grow thick as she learn do you notice how I slow the video down at the beginning by skip only a small number of cycle when I first shoot the video I didn t do this however I realise that learning be subject to the law of diminish return the neural network change more rapidly during the initial stage of training which be why I slow this bit down now that she have learn about the pattern use the 7 example in the training set let s examine her brain again do you see how she have strengthen some of her synapsis at the expense of other for instance do you remember how the third column in the training set be irrelevant in determine the answer you can see she have discover this because the synapsis come out of her third input neuron have almost wither away relative to the other let s give she a new situation 1 1 0 to think about you can see her neural pathway light up she have estimate 0 01 the correct answer be 0 so she be very close pretty cool traditional computer program can t learn but neural network can learn and adapt to new situation just like the human mind how do I do it I use the python library matplotlib which provide method for drawing and animation I create the glow effect use alpha transparency you can view my full source code here thank for reading if you enjoy read this article please click the heart icon to recommend from a quick cheer to a stand ovation clap to show how much you enjoy this story study economic at oxford university founder of www moju io interested in politic and ai fundamental and late development in # deeplearne
Christian Hernandez,364,7,https://medium.com/crossing-the-pond/into-the-age-of-context-f0aed15171d7?source=tag_archive---------3----------------,into the age of context cross the pond medium,I spend most of my early career proclaim that this be the year of mobile the year of mobile be actually 2007 when the iphone launch and accelerate a revolution around mobile computing as the economist recently put it just eight year later apple s iphone exemplify the early 21st century s define technology it s not a question of whether smartphone have become our primary computing interaction device it s a question of by how much relative to other interaction medium so let s agree that we be currently live in the era of mobile look forward to the next 5 year though I personally believe we will move from the era of mobile to the age of context credit to robert scoble and shel israel for their book with that same term let I first define what I mean by age of context in the age of context personal datum ex calendar and email location and time be integrate with publicly available datum ex traffic data pollution level and app level data ex uber surge pricing number of step track by my fitbit to intelligently drive I towards an action ex get I to walk to my next meeting instead of order a car it be an age in which we and the device and sensor around we generate massive ream of datum and in which self teaching algorithms drill into that datum to derive insight and recommend or auto generate an action it be an era in which our biological computational capacity and action be enhance and improve by digital service the age of context be be bring about by a number of technology trend which have be accelerate in a parallel and be now come together the first and most obvious trend be the proliferation of supercomputer in our pocket industry analyst forecast 1 87 billion phone will be ship by 2018 these device carry not only a grow amount of processing power but also the ecosystem of application and service which integrate with sensor and functionality on the device to allow we to literally remote control our life in the evolution from the current era of mobile to the future age of context the supercomputer in our pocket evolve from information delivery and application interaction layer to notification context aware action driver smartphone will soon be complement by wearable computing device be that the apple watch or a future evolution of google glass these new form factor be ideally suit for an era in which data need to be compile into succinct notification and action enabler in the last 10 year the web have evolve into a social web on top of which identity and deep insight into each of we power service and experience it allow goodread to associate book with my identity vivino to determine that I like earthy red wine unilever to good target I for an ad on facebook and netflix to mine my datum to then commission a show it know I will like this identity layer be now be overlay with a financial layer in which associate with my digital identity I also have a secure digital payment mechanism this transactional financial layer will begin to enable seamless transaction in the age of context the starbuck app will know that I usually emerge from the tube at 9 10 am and walk to their local store to order a tall americano extra shot at 9 11 as I reach street level my phone or watch or wearable computing device will know where I be close to starbuck and to the office know my routine have my payment information store and simply generate an action driver that say tall americano extra shot order a few minute later I can pick up my coffee which have already be pay for these service be already possible today a parallel and accelerate trend which will power the age of context be the proliferation of intelligent and connected sensor around we call that internet of thing or call it simply a democratization and consumerization of device that capture datum for now and act on datum eventually while the end number vary industry analyst all believe the number of connect device start to get very big very fast gartner predict that by 2020 there will be 25 billion connect device with the vast majority of those be consumer centric today my jawbone be a fairly basic data collection device it know that I walk 8 000 step and sleep too little but it doesn t drive I to action other than provide I with a visualization of the datum in the age of context this will change as large and large datum set of sensor datum combine with other datum combine with intelligent analytic allow datum to become actionable in the future my jawbone win t simply count my step it will also be able to integrate with other datum set to generate personal health insight it will have track over time that my blood pressure rise every morning at 9 20 after I have consume the third coffee of the day compare my blood rate to thousand of other of my age range and demographic background it will know that the level be unhealthy and it will help I take a conscious decision not to consume that extra coffee through a notification datum will derive insight and that insight will hopefully drive action one could argue that the parallel trend of mobile sensor and the social web be already mainstream what then be bring they together to weave the age of context the glue be datum the massive amount of datum the grow number of internet user and connected device generate each day more critically the cost of store this datum have drop to nearly zero deloitte estimate that in 1992 the cost of store a gigabyte of datum be $ 569 and that by 2012 the cost have drop to $ 0 03 but datum by itself be just bit and byte the second key trend that be weave the age of context be the breakthrough in algorithm and model to analyze this datum in close to real time for the age of context to come about system must know how to query and how to act on all the possible contextual datum point to drive the simplified action outline in the example above the advance and investment into machine learning and ai be the final piece of the puzzle need to turn datum from information to action the most visible example of the age of context today be google now google have a lot of information about I it know what work be as I spend most of the time there between 9 am and 7 pm it know what home be as I spend most of the evening there since I use google app it know what my first meeting be since I search for duke basketball on a regular basis it know I care about the score since I usually take the tube and google have access to the london tfl datum it know that I will be late to my next meeting but even though google now recently open up its api to third party developer it be still fairly google biased and google optimize for the age of context to thrive the platform that power it must be interlink across datum and application whether this age come about through intelligent agent like siri or viv or the character from she or a meta app layer sit across vertical app and service be still unclear the miss piece for much of this to come about be a common meta language for vertical and punctual app to share datum and action this common language will likely be an evolution of the various deep linking standard be develop facebook have a flavour android have a flavour and a myriad of startup have flavour an emerge standard will not only enable the age of context but also probably crown the champion of this new era as the standard will also own the interaction the interlinkage and the path to monetization across device and experience the trend above be all happen around we the standard and algorithm be all be build by brilliant mind across the world the interface layer and device be already with we the age of context be be create at an accelerate pace and I can t wait to see what get build and how our day to day life be enhance by this new era thank to john henderson for his feedback and thought on this post from a quick cheer to a stand ovation clap to show how much you enjoy this story co founder and manage partner @whitestarvc former product and mobile guy at smallish company that become big salvadoran bear londoner # ygl of the @wef story from the white star capital team and our portfolio company on entrepreneurship and scale globally
Venture Scanner,207,5,https://medium.com/@VentureScanner/the-state-of-artificial-intelligence-in-six-visuals-8bc6e9bf8f32?source=tag_archive---------4----------------,the state of artificial intelligence in six visual,we cover many emerge market in the startup ecosystem previously we publish post that summarize financial technology internet of thing bitcoin and martech in six visual this week we do the same with artificial intelligence ai at this time we be track 855 ai company across 13 category with a combine funding amount of $ 8 75billion to see all of our ai relate post check out our blog the six artificial intelligence visual below help make sense of this dynamic market deep learning machine learning application machine learning be the technology of computer algorithm that operate base on its learning from exist datum deep learning be a subset of machine learning that focus on deeply layer neural network the follow company utilize deep learning machine learning technology in a specific way or use case in their product computer vision image recognition computer vision be the method of process and analyze image to understand and produce information from they image recognition be the process of scan image to identify object and face the follow company either build computer vision image recognition technology or utilize it as the core offering in their product deep learning machine learn general machine learning be the technology of computer algorithm that operate base on its learning from exist datum deep learning be a subset of machine learning that focus on deeply layer neural network the follow company either build deep learning machine learning technology or utilize it as the core offering of their product natural language processing natural language processing be the method through which computer process human language input and convert into understandable representation to derive meaning from they the follow company either build natural language processing technology or utilize it as the core offering in their product exclude all speech recognition company smart robot smart robot company build robot that can learn from their experience and act and react autonomously base on the condition of their environment virtual personal assistant virtual personal assistant be software agent that use artificial intelligence to perform task and service for an individual such as customer service etc natural language processing speech recognition speech recognition be a subset of natural language processing that focus on process a sound clip of human speech and derive meaning from it computer vision image recognition computer vision be the method of process and analyze image to understand and produce information from they image recognition be the process of scan image to identify object and face the follow company utilize computer vision image recognition technology in a specific way or use case in their product recommendation engine and collaborative filtering recommendation engine be system that predict the preference and interest of user for certain item movie restaurant and deliver personalized recommendation to they collaborative filtering be a method of predict a user s preference and interest by collect the preference information from many other similar user gesture control gesture control be the process through which human interact and communicate with computer with their gesture which be recognize and interpret by the computer video automatic content recognition video automatic content recognition be the process through which the computer compare a sampling of video content with a source content file to identify what the content be through its unique characteristic context aware computing context aware computing be the process through which computer become aware of their environment and their context of use such as location orientation lighting and adapt their behavior accordingly speech to speech transition speech to speech translation be the process through which human speech in one language be process by the computer and translate into another language instantly the bar graph above summarize the number of company in each artificial intelligence category to show which be dominate the current market currently the deep learning machine learning application category be lead the way with a total of 200 company follow by natural language processing speech recognition with 130 company the bar graph above summarize the average company funding per artificial intelligence category again the deep learning machine learning application category lead the way with an average of $ 13 8 m per fund company the sem category include company that help marketer with manage and scale their pay search program the graph above compare total venture funding in artificial intelligence to the number of company in each category deep learning machine learning application seem to be the category with the most traction the follow infographic be an update heat map indicate where artificial intelligence startup exist across 62 country currently the united states be lead the way with 415 company the united kingdom be in second with 67 company follow by canada with 29 the bar graph above summarize artificial intelligence by median age of category the speech recognition and video content recognition category have the high median age at 8 year follow by computer vision general at 6 5 year as artificial intelligence continue to develop so too will its move part we hope this post provide some big picture clarity on this booming industry venture scanner enable corporation to research identify and connect with the most innovative technology and company we do this through a unique combination of our data technology and expert analyst if you have any question reach out to info@venturescanner com from a quick cheer to a stand ovation clap to show how much you enjoy this story technology and analyst power research firm visit we at www venturescanner com
Illia Polosukhin,108,3,https://medium.com/@ilblackdragon/tensorflow-tutorial-part-2-9ffe47049c92?source=tag_archive---------5----------------,tensorflow tutorial — part 2 illia polosukhin medium,in the previous part 1 of this tutorial I introduce a bit of tensorflow and scikit flow and show how to build a simple logistic regression model on titanic dataset in this part let s go deeply and try multi layer fully connect neural network write your custom model to plug into the scikit flow and top it with try out convolutional network of course there be not much point of yet another linear logistic regression framework an idea behind tensorflow and many other deep learning framework be to be able to connect differentiable part of the model together and optimize they give the same cost or loss function scikit flow already implement a convenient wrapper around tensorflow api for create many layer of fully connect unit so it s simple to start with deep model by just swap classifier in our previous model to the tensorflowdnnclassifier and specify hidden unit per layer this will create 3 layer of fully connect unit with 10 20 and 10 hide unit respectively with default rectify linear unit activation we will be able to customize this setup in the next part I didn t play much with hyperparameter but previous dnn model actually yield bad accuracy then a logistic regression we can explore if this be due to overfitte on under fitting in a separate post for the sake of this example I though want to show how to switch to the custom model where you can have more control this model be very similar to the previous one but we change the activation function from a rectified linear unit to a hyperbolic tangent rectify linear unit and hyperbolic tangent be most popular activation function for neural network as you can see create a custom model be as easy as write a function that take x and y input which be tensor and return two tensor prediction and loss this be where you can start learn tensorflow api to create part of sub graph what kind of tensorflow tutorial would this be without an example of digit recognition this be just an example how you can try different type of dataset and model not limit to only float number feature here we take digit dataset and write a custom model we ve create conv_model function that give tensor x and y run 2d convolutional layer with the most simple max pooling — just maximum the result be pass as feature to skflow model logistic_regression which handle classification to require number of class by attach softmax over class and computing cross entropy loss it s easy now to modify this code to add as many layer as you want some of the state of the art image recognition model be hundred+ layer of convolution max pool dropout and etc the part 3 be expand the model for titanic dataset with handle categorical variable ps thank to vlad frolov for help with miss article and point mistake in the draft from a quick cheer to a stand ovation clap to show how much you enjoy this story co founder @ near ai — teach machine to code I m tweet as @ilblackdragon
Derrick Harris,124,9,https://medium.com/s-c-a-l-e/how-baidu-mastered-mandarin-with-deep-learning-and-lots-of-data-1d94032564a5?source=tag_archive---------6----------------,baidu explain how it s master mandarin with deep learning,on aug 8 at the international neural network society conference on big datum in san francisco baidu senior research engineer awni hannun present on a new model that the chinese search giant have develop for handle voice query in mandarin the model which be accurate 94 percent of the time in test be base on a powerful deep learning system call deep speech that baidu first unveil in december 2014 in this lightly edit interview hannun explain why his new research be important why mandarin be such a tough language to learn and where we can expect to see future advance in deep learning method scale how accurate be deep speech at translate mandarin awni hannun it have a 6 percent character error rate which essentially mean that it get wrong 6 out of 100 character to put that in context this be in my opinion — and to the good of our lab s knowledge — the good system at transcribe mandarin voice query in the world in fact we run an experiment where we have a few people at the lab who speak chinese transcribe some of the example that we be test the system on it turn out that our system be well at transcribe example than they be — if we restrict it to transcribe without the help of the internet and such thing what be it about mandarin that make it such a challenge compare with other language there be a couple of difference with mandarin that make we think it would be very difficult to have our english speech system work well with it one be that it s a tonal language so when you say a word in a different pitch it change the meaning of the word which be definitely not the case in english in traditional speech recognition it s actually a desirable property that there be some pitch invariance which essentially mean that it try to ignore pitch when it do the transcription so you have to change a bunch of thing to get a system to work with mandarin or any chinese for that matter however for we it be not the case that we have to change a whole bunch of thing because our pipeline be much simple than the traditional speech pipeline we don t do a whole lot of pre processing on the audio in order to make it pitch invariant but rather just let the model learn what s relevant from the datum to most effectively transcribe it properly it be actually able to do that fine in mandarin without have to change the input the other thing that be very different about chinese — mandarin in this case — be the character set the english alphabet be 26 letter whereas in chinese it s something like 80 000 different character our system directly output a character at a time as it s build its transcription so we speculate it would be very challenging to have to do that on 80 000 character at each step versus 26 that s a challenge we be able to overcome just by use character that people commonly say which be a small subset baidu have be handle a fairly high volume of voice search for a while now how be the deep speech system well than the previous system for handle query in mandarin baidu have a very active system for voice search in mandarin and it work pretty well I think in term of total query activity it s still a relatively small percentage we want to make that share large or at least enable people to use it more by make the accuracy of the system well can you describe the difference between a search base system like deep speech and something like microsoft s skype translate which be also base on deep learning typically the way it s do be there be three module in the pipeline the first be a speech transcription module the second be the machine translation module and the third would be the speech synthesis module what we re talk about specifically be just the speech transcription module and I m sure microsoft have one as part of skype translate our system be different than that system in that it s more what we call end to end rather than have a lot of human engineer component that have be develop over decade of speech research — by look at the system and say what what feature be important or which phoneme the model should predict — we just have some input datum which be an audio wav file on which we do very little pre processing and then we have a big deep neural network that output directly to character we give it enough datum that it s able to learn what s relevant from the input to correctly transcribe the output with as little human intervention as possible one thing that s pleasantly surprising to we be that we have to do very little change to it — other than scale it and give it the right datum — to make this system we show in december that work really well on english work remarkably well in chinese as well what s the usual timeline to get this type of system from r&d into production it s not an easy process but I think it s easy than the process of get a model to be very accurate — in the sense that it s more of an engineering problem than a research problem we re actively work on that now and I m hopeful our research system will be in production in the near term baidu have plan — and product — in other area include wearable and other embed form of speech recognition do the work you re do on search relate to these other initiative we want to build a speech system that can be use as the interface to any smart device not just voice search it turn out that voice search be a very important part of baidu s ecosystem so that s one place we can have a lot of impact right now be the pace of progress and significant advance in deep learning as fast it seem I think right now it do feel like the pace be increase because people be recognize that if you take task where you have some input and be try to produce some output you can apply deep learning to that task if it be some old machine learning task such as machine translation or speech recognition which have be heavily engineer for the past several decade you can make significant advance if you try to simplify that pipeline with deep learning and increase the amount of datum we re just on the crest of that in particular processing sequential datum with deep learning be something that we re just figure out how to do really well we ve come up with model that seem to work well and we re at the point where we re go to start squeeze a lot of performance out of these model and then you ll see that right and leave benchmark will be drop when it come to sequential datum beyond that I don t know it s possible we ll start to plateau or we ll start invent new architecture to do new task I think the moral of this story be where there s a lot of datum and where it make sense to use a deep learning model success be with high probability go to happen that s why it feel like progress be happen so rapidly right now it really become a story of how can we get right datum when deep learning be involve that become the big challenge architecturally deep speech run on a powerful gpu base system where be the opportunity to move deep learning algorithm onto small system such as smartphone in order to offload processing from baidu s or anyone else s server that s something I think about a lot actually and I think the future be bright in that regard it s certainly the case that deep learning model be get big and big but typically it also have also be the case that the size and expressivity of the model be more necessary during training than it be during test there have be a lot of work that show that if you take a model that have be train at say 32 bit float point precision and then compress it to 8 bit fix point precision it work just as well at test time or it work almost as well you can reduce it by a factor of four and still have it work just as well there s also a lot of work in compress exist model like how can we take a giant model that we ve train to soak up a lot of datum and then say train another much small model to duplicate what that large model do but that small model we can actually put into an embed device somewhere often the hard part be in train the system in those case it need to be really big and the server have to be really beefy but I do think there s a lot of promise work with which we can make the model a lot small and there s a future in term of embed they in different place of course something like search have to go back to cloud server unless you ve somehow index the whole web on your smartphone right yeah that would be challenge for some additional context on just how powerful a system deep speech be — and why baidu put so much emphasis on system architecture for its deep learning effort — consider this explanation offer by baidu system research scientist bryan catanzaro from a quick cheer to a stand ovation clap to show how much you enjoy this story founder editor writer of architecht day job be run content at replicated formerly at gigaom mesosphere fortune what s next in computing tell by the people behind the software
Kyle McDonald,109,6,https://medium.com/@kcimc/comparing-artificial-artists-7d889428fce4?source=tag_archive---------7----------------,compare artificial artist kyle mcdonald medium,last wednesday a neural algorithm of artistic style be post to arxiv feature some of the most compelling imagery generate by deep convolutional neural network dcnns since google research s deepdream post on sunday kai sheng tai post the first public implementation I immediately stop work on my implementation and start play with his unfortunately his result don t quite match the paper and it s unclear why I m just get start with this topic so as I learn I want to share my understanding of the algorithm here along with some result I get from test his code in two part the paper describe an algorithm for render a photo in the style of a give paint 2 instead of try to match the activation exactly try to match the correlation of the activation they call this style reconstruction and depend on the layer you reconstruct you get vary level of abstraction the correlation feature they use be call a gram matrix the dot product between the vectorized feature activation matrix and its transpose if this sound confusing see the footnote finally instead of optimize for just one of these thing they optimize for both simultaneously the style of one image and the content of another image here be an attempt to recreate the result from the paper use kai s implementation not quite the same and possibly explain by a few difference between kai s implementation and the original paper as a final comparison consider the image andrej karpathy post from his own implementation the same large scale high level feature be miss here just like in the style reconstruction of seat nude above beside s kai s I ve see one more implementation from a phd student name satoshi a brief example in python with chainer I haven t spend as much time with it as I have to adapt it to run on my cpu due to lack of memory but I do notice after run tübingen in the style of the starry night with a 1 10e3 ratio and 100 iteration it seem to converge on something match the general structure but lack the overall palette I d like to understand this algorithm well enough to generalize it to other medium mainly think about sound right now so if you have an insight or other implementation please share they in the comment I ve start test another implementation that pop up this morning from justin johnson his follow the original paper very closely except for use unequal weight when balance different layer use for style reconstruction all the follow example be run for 100 iteration with the default ratio of 1 10e0 justin switch his implementation to use l bfgs and equally weight layer and to my eye this match the result in the original paper here be his result for one of the hard content style pair other implementation that look great but I haven t test enough the definition of the gram matrix confuse I at first so I write it out as code use a literal translation of equation 3 in the paper you would write in python with numpy it turn out that the original description be compute more efficiently than this literal translation for example kai write in lua with torch satoshi compute it for all the layer simultaneously in python with chainer or again in python with numpy and caffe layer from a quick cheer to a stand ovation clap to show how much you enjoy this story artist work with code
Jim Fleming,165,4,https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa?source=tag_archive---------8----------------,highway network with tensorflow jim fleme medium,this week I implement highway network to get an intuition for how they work highway network inspire by lstms be a method of construct network with hundred even thousand of layer let s see how we construct they use tensorflow tl;dr fully connected highway repo and convolutional highway repo for comparison let s start with a standard fully connected or dense layer we need a weight matrix and a bias vector then we ll compute the following for the layer output here s what a dense layer look like as a graph in tensorboard for the highway layer what we want be two gate that control the flow of information the transform gate control how much of the activation we pass through and the carry gate control how much of the unmodified input we pass through otherwise the layer largely resemble a dense layer with a few addition what happen be that when the transform gate be 1 we pass through our activation h and suppress the carry gate since it will be 0 when the carry gate be 1 we pass through the unmodified input x while the activation be suppress here s what the highway layer graph look in tensorboard use a highway layer in a network be also straightforward one detail to keep in mind be that consecutive highway layer must be the same size but you can use fully connect layer to change dimensionality this become especially complicated in convolutional layer where each layer can change the output dimension we can use padding same to maintain each layer dimensionality otherwise by simply use hyperparameter from the tensorflow doc I e no hyperparameter search the fully connect highway network perform much well than a fully connect network use mnist as my simple trial now that we have a highway network I want to answer a few question that come up for I while read the paper for instance how deep will the network converge the paper briefly mention 1000 layer can we train with 1000 layer on mnist yes also reach around 95 % accuracy try it out with a carry bias around 20 0 for mnist from the paper the network will only utilize ~15 layer anyway the network can probably even go deeply since the it s just learn to carry the last 980 layer or so we can t do much useful at or past 1000 layer so that seem sufficient for now what happen if you set very low or very high carry bias in either extreme the network simply fail to converge in a reasonable amount of time in the case of low bias more positive the network start as if the carry gate aren t present at all in the case of high bias more negative we re put more emphasis on carry and the network can take a long time to overcome that otherwise the bias don t seem to need to be exact at least on this simple example when in doubt start with high bias more negative since it s easy to learn to overcome carry than without carry gate which be just a plain network overall I be happy with how easy highway network be to implement they re fully differentiable with only a single additional hyperparameter for the initial carry bias one downside be that highway layer do require additional parameter for the transform weight and bias however since we can go deeply the layer do not need to be as wide which can compensate here s be the complete notebook if you want to play with the code fully connect highway repo and convolutional highway repo follow I on twitter for more post like these if you d like build very deep network in production I do consult from a quick cheer to a stand ovation clap to show how much you enjoy this story cto and lead ml engineer at fomoro — focus on machine learning and apply cut edge research for business — previously @rdio what I m work on
Nathan Benaich,264,10,https://medium.com/@NathanBenaich/investing-in-artificial-intelligence-a-vc-perspective-afaf6adc82ea?source=tag_archive---------9----------------,invest in artificial intelligence nathan benaich medium,my expand talking point from a presentation I give at the re work invest in deep learning dinner in london on 1st december 2015 tl;dr check out the slide here it s my belief that artificial intelligence be one of the most exciting and transformative opportunity of our time there s a few reason why that s so consumer worldwide carry 2 billion smartphone they re increasingly addicted to these device and 40 % of the world be online kpcb this mean we re create new datum asset that never exist before user behavior preference interest knowledge connection the cost of compute and storage be both plummet by order of magnitude while the computational capacity of today s processor be grow we ve see improvement in learn method architecture and software infrastructure the pace of innovation can therefore only be accelerate indeed we don t fully appreciate what tomorrow will look and feel like ai drive product be already out in the wild and improve the performance of search engine recommender system e g e commerce music ad serve and financial trading amongst other company with the resource to invest in ai be already create an impetus for other to follow suit or risk not have a competitive seat at the table together therefore the community have a well understanding and be equip with more capable tool with which to build learning system for a wide range of increasingly complex task more on this discussion here a key consideration in my view be that the open sourcing of technology by large incumbent google microsoft intel ibm and the range of company productise technology for cheap mean that technical barrier be erode fast what end up move the needle be proprietary datum access creation experience talent and addictive product operational commercial financial there be two big factor that make involve the user in an ai drive product paramount 1 machine don t yet recapitulate human cognition in order to pick up where software fall short we need to call on the user for help 2 buyer user of software product have more choice today than ever as such they re often fickle avg 90 day retention for app be 35 % return expect value out of the box be key to building habit hyperparameter optimisation can help here be some great example of product which prove that involve the user in the loop improve performance we can even go a step far I think by explain how machine generate result be obtain for example ibm watson surface relevant literature when support a patient diagnosis in the oncology clinic do so improve user satisfaction and help build confidence in the system to encourage long term use and investment remember it s generally hard for we to trust something we don t truly understand to put this discussion into context let s first look at the global vc market q1 q3 2015 see $ 47 2bn invest a volume high than each of the full year total for 17 of the last 20 year nvca we re likely to breach $ 55bn by year end there be circa 900 company work in the ai field most of which tackle problem in business intelligence finance and security q4 2014 see a flurry of deal into ai company start by well respected and achieve academic vicarious scale inference metamind and sentient technology so far we ve see circa 300 deal into ai company define as business whose description include keyword artificial intelligence machine learn computer vision nlp data science neural network deep learning from jan 1st 2015 thru 1st dec 2015 cb insight in the uk company like ravelin signal and gluru raise seed round circa $ 2bn be invest albeit bloat by large venture debt or credit line for consumer business loan provider avant $ 339 m debt+credit zestfinance $ 150 m debt liftforward $ 250 m credit and argon credit $ 75 m credit importantly 80 % of deal be < $ 5 m in size and 90 % of the cash be invest into we company vs 13 % in europe 75 % of round be in the us the exit market have see 33 m&a transaction and 1 ipo adgorithm on the lse six event be for european company 1 in asia and the rest be account for by american company the large transaction be tellapart twitter $ 532 m ; $ 17 m raise elastica blue coat system $ 280 m ; $ 45 m raise and supersonicad ironsource $ 150 m ; $ 21 m raise which return solid multiple of invest capital the remain transaction be mostly for talent give that median team size at the time of the acquisition be 7ppl median altogether ai investment will have account for circa 5 % of total vc investment for 2015 that s high than the 2 % claim in 2013 but still track far behind compete category like adtech mobile and bi software the key takeaway point be a the financing and exit market for ai company be still nascent as exemplify by the small round and low deal volume and b the vast majority of activity take place in the us business must therefore have exposure to this market I spend a number of summer in university and 3 year in grad school research the genetic factor govern the spread of cancer around the body a key takeaway I leave with be the follow therapeutic development be a very challenging expensive lengthy regulate and ultimately offer a transient solution to treat disease instead I truly believe that what we need to improve healthcare outcome be granular and longitudinal monitoring of physiology and lifestyle this should enable early detection of health condition in near real time drive down cost of care over a patient s lifetime while consequently improve outcome consider the digitally connect lifestyle we lead today the device some of we interact with on a daily basis be able to track our movement vital sign exercise sleep and even reproductive health we re disconnect for few hour of the day than we re online and I think we re less apprehensive to store various datum type in the cloud where they can be access with consent by 3rd party sure the news might paint a different but the fact be that we re still use the web and it s wealth of product on a population level therefore we have the chance to interrogate data set that have never before exist from these we could glean insight into how nature and nurture influence the genesis and development of disease that s huge look at today s clinical model a patient present into the hospital when they feel something be wrong the doctor have to conduct a battery of test to derive a diagnosis these test address a single often late stage time point at which moment little can be do to reverse damage e g in the case of cancer now imagine the future in a world of continuous non invasive monitoring of physiology and lifestyle we could predict disease onset and outcome understand which condition a patient likely suffer from and how they ll respond to various therapeutic modality there s load of application for artificial intelligence here intelligence sensor signal process anomaly detection multivariate classifier deep learning on molecular interaction some company be already hack away at this problem a point worth note be that the uk have a slight leg up on the data access front initiative like the uk biobank 500k patient record genomics england 100k genome sequence hipsci stem cell and the nhs care data programme be lead the way in create centralised datum repository for public health and therapeutic research cheer for point out hari arul could business ever conceivably run themselves ai enable automation of knowledge work could cut employment cost by $ 9tn by 2020 baml couple to the efficiency gain worth $ 1 9tn drive by robot I reckon there s a chance for near complete automation of core repetitive business function in the future think of all the productise saas tool that be available off the shelf for crm marketing billing payment logistic web development customer interaction finance hiring and bi then consider tool like zapier or tray io which help connect application and program business logic these could be far expand by leverage contextual datum point that inform decision make perhaps we could eventually re image the new ebay where you ll have fully automate inventory procurement pricing list generation translation recommendation transaction processing customer interaction packaging fulfilment and shipping of course probably a way off I m bullish on the value to be create with artificial intelligence across our personal and professional life I think there s currently low vc risk tolerance for this sector especially give shortening investment horizon for value to be create more support be need for company drive long term innovation especially that far less be occur within university vc be bear to fund moonshot we must remember that access to technology will over time become commoditise it s therefore key to understand your use case your user the value you bring and how it s experience and assess this get to the point of find a strategy to build a sustainable advantage such that other find it hard to replicate your offering aspect of this strategy may in fact be non ai and non technical in nature e g the user experience layer — thank for highlight this hari arul as such there s a renew focus on core principle build a solution to an unsolved poorly serve high value persistent problem for consumer or business finally you must have exposure to the us market where the lion s share of value be create and realise we have an opportunity to catalyse the growth of the ai sector in europe but not without keep close tab on what work doesn t work across the pond first hand work in the space we d love to get to know you sign up to my newsletter covering ai news and analysis from the tech world research lab and private public company market I m an investor at playfair capital a london base investment firm focus on early stage technology company that change the way we live work and play we invest across europe and the us and our focus be on core technology and user experience 25 % of our portfolio be ai mapillary duedil jukedeck seldon clarify gluru and ravelin we want to take risk on technologist create new market or reinvent exist one from a quick cheer to a stand ovation clap to show how much you enjoy this story advance human progress with intelligent system venture partner @pointninecap former scientist photographer perpetual foodie nathan ai @ldn_ai @twentybn
Tal Perry,2.6K,17,https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02?source=tag_archive---------3----------------,deep learn the stock market tal perry medium,update 25 1 17 — take I a while but here be an ipython notebook with a rough implementation in the past few month I ve be fascinate with deep learning especially its application to language and text I ve spend the bulk of my career in financial technology mostly in algorithmic trading and alternative datum service you can see where this be go I write this to get my idea straight in my head while I ve become a deep learning enthusiast I don t have too many opportunity to brain dump an idea in most of its messy glory I think that a decent indication of a clear thought be the ability to articulate it to people not from the field I hope that I ve succeed in do that and that my articulation be also a pleasurable read why nlp be relevant to stock prediction in many nlp problem we end up take a sequence and encode it into a single fix size representation then decode that representation into another sequence for example we might tag entity in the text translate from english to french or convert audio frequency to text there be a torrent of work come out in these area and a lot of the result be achieve state of the art performance in my mind the big difference between the nlp and financial analysis be that language have some guarantee of structure it s just that the rule of the structure be vague market on the other hand don t come with a promise of a learnable structure that such a structure exist be the assumption that this project would prove or disprove rather it might prove or disprove if I can find that structure assume the structure be there the idea of summarize the current state of the market in the same way we encode the semantic of a paragraph seem plausible to I if that doesn t make sense yet keep read it will you shall know a word by the company it keep firth j r 1957 11 there be ton of literature on word embedding richard socher s lecture be a great place to start in short we can make a geometry of all the word in our language and that geometry capture the meaning of word and relationship between they you may have see the example of king man + woman = queen or something of the sort embedding be cool because they let we represent information in a condensed way the old way of represent word be hold a vector a big list of number that be as long as the number of word we know and set a 1 in a particular place if that be the current word we be look at that be not an efficient approach nor do it capture any meaning with embedding we can represent all of the word in a fix number of dimension 300 seem to be plenty 50 work great and then leverage their high dimensional geometry to understand they the picture below show an example an embedding be train on more or less the entire internet after a few day of intensive calculation each word be embed in some high dimensional space this space have a geometry concept like distance and so we can ask which word be close together the author inventor of that method make an example here be the word that be close to frog but we can embed more than just word we can do say stock market embedding market2vec the first word embed algorithm I hear about be word2vec I want to get the same effect for the market though I ll be use a different algorithm my input datum be a csv the first column be the date and there be 4 * 1000 column correspond to the high low open closing price of 1000 stock that be my input vector be 4000 dimensional which be too big so the first thing I m go to do be stuff it into a low dimensional space say 300 because I like the movie take something in 4000 dimension and stuff it into a 300 dimensional space my sound hard but its actually easy we just need to multiply matrix a matrix be a big excel spreadsheet that have number in every cell and no format problem imagine an excel table with 4000 column and 300 row and when we basically bang it against the vector a new vector come out that be only of size 300 I wish that s how they would have explain it in college the fanciness start here as we re go to set the number in our matrix at random and part of the deep learning be to update those number so that our excel spreadsheet change eventually this matrix spreadsheet I ll stick with matrix from now on will have number in it that bang our original 4000 dimensional vector into a concise 300 dimensional summary of itself we re go to get a little fancier here and apply what they call an activation function we re go to take a function and apply it to each number in the vector individually so that they all end up between 0 and 1 or 0 and infinity it depend why it make our vector more special and make our learning process able to understand more complicated thing how so what what I m expect to find be that that new embedding of the market price the vector into a small space capture all the essential information for the task at hand without waste time on the other stuff so I d expect they d capture correlation between other stock perhaps notice when a certain sector be decline or when the market be very hot I don t know what trait it will find but I assume they ll be useful now what let put aside our market vector for a moment and talk about language model andrej karpathy write the epic post the unreasonable effectiveness of recurrent neural network if I d summarize in the most liberal fashion the post boil down to and then as a punchline he generate a bunch of text that look like shakespeare and then he do it again with the linux source code and then again with a textbook on algebraic geometry so I ll get back to the mechanic of that magic box in a second but let I remind you that we want to predict the future market base on the past just like he predict the next word base on the previous one where karpathy use character we re go to use our market vector and feed they into the magic black box we haven t decide what we want it to predict yet but that be okay we win t be feed its output back into it either go deeply I want to point out that this be where we start to get into the deep part of deep learning so far we just have a single layer of learn that excel spreadsheet that condense the market now we re go to add a few more layer and stack they to make a deep something that s the deep in deep learning so karpathy show we some sample output from the linux source code this be stuff his black box write notice that it know how to open and close parenthesis and respect indentation convention ; the content of the function be properly indent and the multi line printk statement have an inner indentation that mean that this magic box understand long range dependency when it s indent within the print statement it know it s in a print statement and also remember that it s in a function or at least another indented scope that s nuts it s easy to gloss over that but an algorithm that have the ability to capture and remember long term dependency be super useful because we want to find long term dependency in the market inside the magical black box what s inside this magical black box it be a type of recurrent neural network rnn call an lstm an rnn be a deep learning algorithm that operate on sequence like sequence of character at every step it take a representation of the next character like the embedding we talk about before and operate on the representation with a matrix like we see before the thing be the rnn have some form of internal memory so it remember what it see previously it use that memory to decide how exactly it should operate on the next input use that memory the rnn can remember that it be inside of an intended scope and that be how we get properly nest output text a fancy version of an rnn be call a long short term memory lstm lstm have cleverly design memory that allow it to so an lstm can see a { and say to itself oh yeah that s important I should remember that and when it do it essentially remember an indication that it be in a nested scope once it see the corresponding } it can decide to forget the original opening brace and thus forget that it be in a nested scope we can have the lstm learn more abstract concept by stack a few of they on top of each other that would make we deep again now each output of the previous lstm become the input of the next lstm and each one go on to learn high abstraction of the datum come in in the example above and this be just illustrative speculation the first layer of lstms might learn that character separate by a space be word the next layer might learn word type like static void action_new_function the next layer might learn the concept of a function and its argument and so on it s hard to tell exactly what each layer be do though karpathy s blog have a really nice example of how he do visualize exactly that connect market2vec and lstms the studious reader will notice that karpathy use character as his input not embedding technically a one hot encoding of character but lar eidne actually use word embedding when he write auto generating clickbait with recurrent neural network the figure above show the network he use ignore the softmax part we ll get to it later for the moment check out how on the bottom he put in a sequence of word vector at the bottom and each one remember a word vector be a representation of a word in the form of a bunch of number like we see in the beginning of this post lar input a sequence of word vector and each one of they we re go to do the same thing with one difference instead of word vector we ll input marketvector those market vector we describe before to recap the marketvector should contain a summary of what s happen in the market at a give point in time by put a sequence of they through lstms I hope to capture the long term dynamic that have be happen in the market by stack together a few layer of lstms I hope to capture high level abstraction of the market s behavior what come out thus far we haven t talk at all about how the algorithm actually learn anything we just talk about all the clever transformation we ll do on the datum we ll defer that conversation to a few paragraph down but please keep this part in mind as it be the se up for the punch line that make everything else worthwhile in karpathy s example the output of the lstms be a vector that represent the next character in some abstract representation in eidne example the output of the lstms be a vector that represent what the next word will be in some abstract space the next step in both case be to change that abstract representation into a probability vector that be a list that say how likely each character or word respectively be likely to appear next that s the job of the softmax function once we have a list of likelihood we select the character or word that be the most likely to appear next in our case of predict the market we need to ask ourselves what exactly we want to market to predict some of the option that I think about be 1 and 2 be regression problem where we have to predict an actual number instead of the likelihood of a specific event like the letter n appear or the market go up those be fine but not what I want to do 3 and 4 be fairly similar they both ask to predict an event in technical jargon — a class label an event could be the letter n appear next or it could be move up 5 % while not go down more than 3 % in the last 10 minute the trade off between 3 and 4 be that 3 be much more common and thus easy to learn about while 4 be more valuable as not only be it an indicator of profit but also have some constraint on risk 5 be the one we ll continue with for this article because it s similar to 3 and 4 but have mechanic that be easy to follow the vix be sometimes call the fear index and it represent how volatile the stock in the s&p500 be it be derive by observe the imply volatility for specific option on each of the stock in the index sidenote — why predict the vix what make the vix an interesting target be that back to our lstm output and the softmax how do we use the formulation we see before to predict change in the vix a few minute in the future for each point in our dataset we ll look what happen to the vix 5 minute later if it go up by more than 1 % without go down more than 0 5 % during that time we ll output a 1 otherwise a 0 then we ll get a sequence that look like we want to take the vector that our lstms output and squish it so that it give we the probability of the next item in our sequence be a 1 the squishing happen in the softmax part of the diagram above technically since we only have 1 class now we use a sigmoid so before we get into how this thing learn let s recap what we ve do so far how do this thing learn now the fun part everything we do until now be call the forward pass we d do all of those step while we train the algorithm and also when we use it in production here we ll talk about the backward pass the part we do only while in training that make our algorithm learn so during train not only do we prepare year worth of historical datum we also prepare a sequence of prediction target that list of 0 and 1 that show if the vix move the way we want it to or not after each observation in our datum to learn we ll feed the market datum to our network and compare its output to what we calculate compare in our case will be simple subtraction that be we ll say that our model s error be or in english the square root of the square of the difference between what actually happen and what we predict here s the beauty that s a differential function that be we can tell by how much the error would have change if our prediction would have change a little our prediction be the outcome of a differentiable function the softmax the input to the softmax the lstms be all mathematical function that be differentiable now all of these function be full of parameter those big excel spreadsheet I talk about age ago so at this stage what we do be take the derivative of the error with respect to every one of the million of parameter in all of those excel spreadsheet we have in our model when we do that we can see how the error will change when we change each parameter so we ll change each parameter in a way that will reduce the error this procedure propagate all the way to the beginning of the model it tweak the way we embed the input into marketvector so that our marketvector represent the most significant information for our task it tweak when and what each lstm choose to remember so that their output be the most relevant to our task it tweak the abstraction our lstms learn so that they learn the most important abstraction for our task which in my opinion be amazing because we have all of this complexity and abstraction that we never have to specify anywhere it s all infer mathamagically from the specification of what we consider to be an error what s next now that I ve lay this out in writing and it still make sense to I I want so if you ve come this far please point out my error and share your input other thought here be some mostly more advanced thought about this project what other thing I might try and why it make sense to I that this may actually work liquidity and efficient use of capital generally the more liquid a particular market be the more efficient that be I think this be due to a chicken and egg cycle whereas a market become more liquid it be able to absorb more capital move in and out without that capital hurt itself as a market become more liquid and more capital can be use in it you ll find more sophisticated player move in this be because it be expensive to be sophisticated so you need to make return on a large chunk of capital in order to justify your operational cost a quick corollary be that in less liquid market the competition isn t quite as sophisticated and so the opportunity a system like this can bring may not have be trade away the point being be I to try and trade this I would try and trade it on less liquid segment of the market that be maybe the tase 100 instead of the s&p 500 this stuff be new the knowledge of these algorithm the framework to execute they and the computing power to train they be all new at least in the sense that they be available to the average joe such as myself I d assume that top player have figure this stuff out year ago and have have the capacity to execute for as long but as I mention in the above paragraph they be likely execute in liquid market that can support their size the next tier of market participant I assume have a slow velocity of technological assimilation and in that sense there be or soon will be a race to execute on this in as yet untapped market multiple time frame while I mention a single stream of input in the above I imagine that a more efficient way to train would be to train market vector at least on multiple time frame and feed they in at the inference stage that be my low time frame would be sample every 30 second and I d expect the network to learn dependency that stretch hour at most I don t know if they be relevant or not but I think there be pattern on multiple time frame and if the cost of computation can be bring low enough then it be worthwhile to incorporate they into the model I m still wrestle with how good to represent these on the computational graph and perhaps it be not mandatory to start with marketvector when use word vector in nlp we usually start with a pretraine model and continue adjust the embedding during training of our model in my case there be no pretraine market vector available nor be tehre a clear algorithm for train they my original consideration be to use an auto encoder like in this paper but end to end training be cool a more serious consideration be the success of sequence to sequence model in translation and speech recognition where a sequence be eventually encode as a single vector and then decode into a different representation like from speech to text or from english to french in that view the entire architecture I describe be essentially the encoder and I haven t really lay out a decoder but I want to achieve something specific with the first layer the one that take as input the 4000 dimensional vector and output a 300 dimensional one I want it to find correlation or relation between various stock and compose feature about they the alternative be to run each input through an lstm perhaps concatenate all of the output vector and consider that output of the encoder stage I think this will be inefficient as the interaction and correlation between instrument and their feature will be lose and thre will be 10x more computation require on the other hand such an architecture could naively be parallel across multiple gpu and host which be an advantage cnns recently there have be a spur of paper on character level machine translation this paper catch my eye as they manage to capture long range dependency with a convolutional layer rather than an rnn I haven t give it more than a brief read but I think that a modification where I d treat each stock as a channel and convolve over channel first like in rgb image would be another way to capture the market dynamic in the same way that they essentially encode semantic meaning from character from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of https lighttag io platform to annotate text for nlp google developer expert in ml I do deep learning on text for a living and for fun
Andrej Karpathy,9.2K,7,https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=tag_archive---------4----------------,yes you should understand backprop andrej karpathy medium,when we offer cs231n deep learning class at stanford we intentionally design the programming assignment to include explicit calculation involve in backpropagation on the low level the student have to implement the forward and the backward pass of each layer in raw numpy inevitably some student complain on the class message board this be seemingly a perfectly sensible appeal if you re never go to write backward pass once the class be over why practice write they be we just torture the student for our own amusement some easy answer could make argument along the line of it s worth know what s under the hood as an intellectual curiosity or perhaps you might want to improve on the core algorithm later but there be a much strong and practical argument which I want to devote a whole post to > the problem with backpropagation be that it be a leaky abstraction in other word it be easy to fall into the trap of abstract away the learning process — believe that you can simply stack arbitrary layer together and backprop will magically make they work on your datum so let look at a few explicit example where this be not the case in quite unintuitive way we re start off easy here at one point it be fashionable to use sigmoid or tanh non linearity in the fully connect layer the tricky part people might not realize until they think about the backward pass be that if you be sloppy with the weight initialization or datum preprocesse these non linearity can saturate and entirely stop learn — your training loss will be flat and refuse to go down for example a fully connect layer with sigmoid non linearity compute use raw numpy if your weight matrix w be initialize too large the output of the matrix multiply could have a very large range e g number between 400 and 400 which will make all output in the vector z almost binary either 1 or 0 but if that be the case z * 1 z which be local gradient of the sigmoid non linearity will in both case become zero vanish make the gradient for both x and w be zero the rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule another non obvious fun fact about sigmoid be that its local gradient z * 1 z achieve a maximum at 0 25 when z = 0 5 that mean that every time the gradient signal flow through a sigmoid gate its magnitude always diminish by one quarter or more if you re use basic sgd this would make the low layer of a network train much slow than the high one tldr if you re use sigmoid or tanh non linearity in your network and you understand backpropagation you should always be nervous about make sure that the initialization doesn t cause they to be fully saturate see a long explanation in this cs231n lecture video another fun non linearity be the relu which threshold neuron at zero from below the forward and backward pass for a fully connect layer that use relu would at the core include if you stare at this for a while you ll see that if a neuron get clamp to zero in the forward pass I e z=0 it doesn t fire then its weight will get zero gradient this can lead to what be call the dead relu problem where if a relu neuron be unfortunately initialize such that it never fire or if a neuron s weight ever get knock off with a large update during training into this regime then this neuron will remain permanently dead it s like permanent irrecoverable brain damage sometimes you can forward the entire training set through a train network and find that a large fraction e g 40 % of your neuron be zero the entire time tldr if you understand backpropagation and your network have relus you re always nervous about dead relus these be neuron that never turn on for any example in your entire training set and will remain permanently dead neuron can also die during training usually as a symptom of aggressive learning rate see a long explanation in cs231n lecture video vanilla rnn feature another good example of unintuitive effect of backpropagation I ll copy paste a slide from cs231n that have a simplified rnn that do not take any input x and only compute the recurrence on the hidden state equivalently the input x could always be zero this rnn be unrolled for t time step when you stare at what the backward pass be do you ll see that the gradient signal go backwards in time through all the hidden state be always be multiply by the same matrix the recurrence matrix whh intersperse with non linearity backprop what happen when you take one number a and start multiply it by some other number b I e a*b*b*b*b*b*b this sequence either go to zero if |b| < 1 or explode to infinity when |b|>1 the same thing happen in the backward pass of an rnn except b be a matrix and not just a number so we have to reason about its large eigenvalue instead tldr if you understand backpropagation and you re use rnn you be nervous about have to do gradient clipping or you prefer to use an lstm see a long explanation in this cs231n lecture video let look at one more — the one that actually inspire this post yesterday I be browse for a deep q learning implementation in tensorflow to see how other deal with compute the numpy equivalent of q a where a be an integer vector — turn out this trivial operation be not support in tf anyway I search dqn tensorflow click the first link and find the core code here be an excerpt if you re familiar with dqn you can see that there be the target_q_t which be just reward * gamma argmax_a q s a and then there be q_acte which be q s a of the action that be take the author here subtract the two into variable delta which they then want to minimize on line 295 with the l2 loss with tf reduce_mean tf square so far so good the problem be on line 291 the author be try to be robust to outlier so if the delta be too large they clip it with tf clip_by_value this be well intentione and look sensible from the perspective of the forward pass but it introduce a major bug if you think about the backward pass the clip_by_value function have a local gradient of zero outside of the range min_delta to max_delta so whenever the delta be above min max_delta the gradient become exactly zero during backprop the author be clip the raw q delta when they be likely try to clip the gradient for add robustness in that case the correct thing to do be to use the huber loss in place of tf square it s a bit gross in tensorflow because all we want to do be clip the gradient if it be above a threshold but since we can t meddle with the gradient directly we have to do it in this round about way of define the huber loss in torch this would be much more simple I submit an issue on the dqn repo and this be promptly fix backpropagation be a leaky abstraction ; it be a credit assignment scheme with non trivial consequence if you try to ignore how it work under the hood because tensorflow automagically make my network learn you will not be ready to wrestle with the danger it present and you will be much less effective at building and debug neural network the good news be that backpropagation be not that difficult to understand if present properly I have relatively strong feeling on this topic because it seem to I that 95 % of backpropagation material out there present it all wrong filling page with mechanical math instead I would recommend the cs231n lecture on backprop which emphasize intuition yay for shameless self advertising and if you can spare the time as a bonus work through the cs231n assignment which get you to write backprop manually and help you solidify your understanding that s it for now I hope you ll be much more suspicious of backpropagation go forward and think carefully through what the backward pass be do also I m aware that this post have unintentionally turn into several cs231n ad apology for that from a quick cheer to a stand ovation clap to show how much you enjoy this story director of ai at tesla previously research scientist at openai and phd student at stanford I like to train deep neural net on large dataset
Erik Hallström,2.5K,7,https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767?source=tag_archive---------5----------------,how to build a recurrent neural network in tensorflow 1 7,in this tutorial I ll explain how to build a simple work recurrent neural network in tensorflow this be the first in a series of seven part where various aspect and technique of build recurrent neural network in tensorflow be cover a short introduction to tensorflow be available here for now let s get start with the rnn it be short for recurrent neural network and be basically a neural network that can be use when your datum be treat as a sequence where the particular order of the data point matter more importantly this sequence can be of arbitrary length the most straight forward example be perhaps a time series of number where the task be to predict the next value give previous value the input to the rnn at every time step be the current value as well as a state vector which represent what the network have see at time step before this state vector be the encode memory of the rnn initially set to zero the good and most comprehensive article explain rnn s I ve find so far be this article by researcher at ucsd highly recommend for now you only need to understand the basic read it until the modern rnn architecture section that will be cover later although this article contain some explanation it be mostly focus on the practical part how to build it you be encourage to look up more theory on the internet there be plenty of good explanation we will build a simple echo rnn that remember the input datum and then echo it after a few time step first let s set some constant we ll need what they mean will become clear in a moment now generate the training datum the input be basically a random binary vector the output will be the echo of the input shift echo_step step to the right notice the reshaping of the datum into a matrix with batch_size row neural network be train by approximate the gradient of loss function with respect to the neuron weight by look at only a small subset of the datum also know as a mini batch the theoretical reason for do this be far elaborate in this question the reshaping take the whole dataset and put it into a matrix that later will be slice up into these mini batch tensorflow work by first building up a computational graph that specify what operation will be do the input and output of this graph be typically multidimensional array also know as tensor the graph or part of it can then be execute iteratively in a session this can either be do on the cpu gpu or even a resource on a remote server the two basic tensorflow data structure that will be use in this example be placeholder and variable on each run the batch datum be feed to the placeholder which be start node of the computational graph also the rnn state be supply in a placeholder which be save from the output of the previous run the weight and bias of the network be declare as tensorflow variable which make they persistent across run and enable they to be update incrementally for each batch the figure below show the input data matrix and the current batch batchx_placeholder be in the dashed rectangle as we will see later this batch window be slide truncated_backprop_length step to the right at each run hence the arrow in our example below batch_size = 3 truncated_backprop_length = 3 and total_series_length = 36 note that these number be just for visualization purpose the value be different in the code the series order index be show as number in a few of the data point now it s time to build the part of the graph that resemble the actual rnn computation first we want to split the batch datum into adjacent time step as you can see in the picture below that be do by unpack the column axis = 1 of the batch into a python list the rnn will simultaneously be train on different part in the time series ; step 4 to 6 16 to 18 and 28 to 30 in the current batch example the reason for use the variable name plural _ series be to emphasize that the variable be a list that represent a time series with multiple entry at each step the fact that the training be do on three place simultaneously in our time series require we to save three instance of state when propagate forward that have already be account for as you see that the init_state placeholder have batch_size row next let s build the part of the graph that do the actual rnn computation notice the concatenation on line 6 what we actually want to do be calculate the sum of two affine transform current_input * wa + current_state * wb in the figure below by concatenate those two tensor you will only use one matrix multiplication the addition of the bias b be broadcast on all sample in the batch you may wonder the variable name truncated_backprop_length be suppose to mean when a rnn be train it be actually treat as a deep neural network with reoccurre weight in every layer these layer will not be unrolled to the beginning of time that would be too computationally expensive and be therefore truncate at a limited number of time step in our sample schematic above the error be backpropagate three step in our batch this be the final part of the graph a fully connect softmax layer from the state to the output that will make the class one hot encode and then calculate the loss of the batch the last line be add the training functionality tensorflow will perform back propagation for we automatically — the computation graph be execute once for each mini batch and the network weight be update incrementally notice the api call to sparse_softmax_cross_entropy_with_logit it automatically calculate the softmax internally and then compute the cross entropy in our example the class be mutually exclusive they be either zero or one which be the reason for use the sparse softmax you can read more about it in the api the usage be to havelogits be of shape batch_size num_classe and label of shape batch_size there be a visualization function so we can se what s go on in the network as we train it will plot the loss over the time show training input training output and the current prediction by the network on different sample series in a training batch it s time to wrap up and train the network in tensorflow the graph be execute in a session new datum be generate on each epoch not the usual way to do it but it work in this case since everything be predictable you can see that we be move truncated_backprop_length step forward on each iteration line 15 19 but it be possible have different stride this subject be far elaborate in this article the downside with do this be that truncated_backprop_length need to be significantly large than the time dependencie three step in our case in order to encapsulate the relevant training datum otherwise there might a lot of miss as you can see on the figure below also realize that this be just simple example to explain how a rnn work this functionality could easily be program in just a few line of code the network will be able to exactly learn the echo behavior so there be no need for test datum the program will update the plot as training progress show in the picture below blue bar denote a training input signal binary one red bar show echos in the training output and green bar be the echos the net be generate the different bar plot show different sample series in the current batch our algorithm will fairly quickly learn the task the graph in the top left corner show the output of the loss function but why be there spike in the curve think of it for a moment answer be below the reason for the spike be that we be start on a new epoch and generate new datum since the matrix be reshape the first element on each row be adjacent to the last element in the previous row the first few element on all row except the first have dependency that will not be include in the state so the net will always perform badly on the first batch this be the whole runnable program just copy paste and run after each part in the article series the whole runnable program will be present if a line be reference by number these be the line number that we mean in the next post in this series we will be simplify the computational graph creation by use the native tensorflow rnn api from a quick cheer to a stand ovation clap to show how much you enjoy this story study engineering physic and in machine learning at royal institute of technology in stockholm also be live in taiwan 學習中文 interested in deep learning
Stefan Kojouharov,1.5K,23,https://chatbotslife.com/ultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c?source=tag_archive---------6----------------,ultimate guide to leverage nlp & machine learning for your chatbot,code snippet and github include over the past few month I have be collect the good resource on nlp and how to apply nlp and deep learning to chatbot every once in awhile I would run across an exception piece of content and I quickly start put together a master list soon I find myself share this list and some of the most useful article with developer and other people in bot community in process my list become a guide and after some urge I have decide to share it or at least a condensed version of it for length reason this guide be mostly base on the work do by denny britz who have do a phenomenal job explore the depth of deep learning for bot code snippet and github include without further ado let we begin chatbot be a hot topic and many company be hope to develop bot to have natural conversation indistinguishable from human one and many be claim to be use nlp and deep learning technique to make this possible but with all the hype around ai it s sometimes difficult to tell fact from fiction in this series I want to go over some of the deep learning technique that be use to build conversational agent start off by explain where we be right now what s possible and what will stay nearly impossible for at least a little while retrieval base model easier use a repository of predefined response and some kind of heuristic to pick an appropriate response base on the input and context the heuristic could be as simple as a rule base expression match or as complex as an ensemble of machine learning classifier these system don t generate any new text they just pick a response from a fix set generative model hard don t rely on pre define response they generate new response from scratch generative model be typically base on machine translation technique but instead of translate from one language to another we translate from an input to an output response both approach have some obvious pro and con due to the repository of handcrafted response retrieval base method don t make grammatical mistake however they may be unable to handle unseen case for which no appropriate predefined response exist for the same reason these model can t refer back to contextual entity information like name mention early in the conversation generative model be smart they can refer back to entity in the input and give the impression that you re talk to a human however these model be hard to train be quite likely to make grammatical mistake especially on long sentence and typically require huge amount of training datum deep learning technique can be use for both retrieval base or generative model but research seem to be move into the generative direction deep learning architecture likesequence to sequence be uniquely suit for generating text and researcher be hope to make rapid progress in this area however we re still at the early stage of build generative model that work reasonably well production system be more likely to be retrieval base for now long vs short conversation the long the conversation the more difficult to automate it on one side of the spectrum areshort text conversation easy where the goal be to create a single response to a single input for example you may receive a specific question from a user and reply with an appropriate answer then there be long conversation hard where you go through multiple turn and need to keep track of what have be say customer support conversation be typically long conversational thread with multiple question in an open domain hard set the user can take the conversation anywhere there isn t necessarily have a well define goal or intention conversation on social medium site like twitter and reddit be typically open domain — they can go into all kind of direction the infinite number of topic and the fact that a certain amount of world knowledge be require to create reasonable response make this a hard problem in a closed domain easy set the space of possible input and output be somewhat limited because the system be try to achieve a very specific goal technical customer support or shopping assistant be example of closed domain problem these system don t need to be able to talk about politic they just need to fulfill their specific task as efficiently as possible sure user can still take the conversation anywhere they want but the system isn t require to handle all these case — and the user don t expect it to there be some obvious and not so obvious challenge when build conversational agent most of which be active research area to produce sensible response system may need to incorporate both linguistic context andphysical context in long dialog people keep track of what have be say and what information have be exchange that s an example of linguistic context the most common approach be toembe the conversation into a vector but do that with long conversation be challenge experiment in build end to end dialogue system use generative hierarchical neural network model and attention with intention for a neural network conversation model both go into that direction one may also need to incorporate other kind of contextual datum such as date time location or information about a user when generate response the agent should ideally produce consistent answer to semantically identical input for example you want to get the same reply to how old be you and what be your age this may sound simple but incorporate such fix knowledge or personality into model be very much a research problem many system learn to generate linguistic plausible response but they be not train to generate semantically consistent one usually that s because they be train on a lot of datum from multiple different user model like that in a persona base neural conversation model be make first step into the direction of explicitly model a personality the ideal way to evaluate a conversational agent be to measure whether or not it be fulfil its task e g solve a customer support problem in a give conversation but such label be expensive to obtain because they require human judgment and evaluation sometimes there be no well define goal as be the case with open domain model common metric such as bleuthat be use for machine translation and be base on text match aren t well suited because sensible response can contain completely different word or phrase in fact in how not to evaluate your dialogue system an empirical study of unsupervised evaluation metric for dialogue response generation researcher find that none of the commonly use metric really correlate with human judgment a common problem with generative system be that they tend to produce generic response like that s great or I don t know that work for a lot of input case early version of google s smart reply tend to respond with I love you to almost anything that s partly a result of how these system be train both in term of datum and in term of actual training objective algorithm some researcher have try to artificially promote diversity through various objective function however human typically produce response that be specific to the input and carry an intention because generative system and particularly open domain system aren t train to have specific intention they lack this kind of diversity give all the cut edge research right now where be we and how well do these system actually work let s consider our taxonomy again a retrieval base open domain system be obviously impossible because you can never handcraft enough response to cover all case a generative open domain system be almost artificial general intelligence agi because it need to handle all possible scenario we re very far away from that as well but a lot of research be go on in that area this leave we with problem in restricted domain where both generative and retrieval base method be appropriate the long the conversation and the more important the context the more difficult the problem become in a recent interview andrew ng now chief scientist of baidu put it well many company start off by outsource their conversation to human worker and promise that they can automate it once they ve collect enough datum that s likely to happen only if they be operate in a pretty narrow domain — like a chat interface to call an uber for example anything that s a bit more open domain like sale email be beyond what we can currently do however we can also use these system to assist human worker by propose and correct response that s much more feasible grammatical mistake in production system be very costly and may drive away user that s why most system be probably well off use retrieval base method that be free of grammatical error and offensive response if company can somehow get their hand on huge amount of datum then generative model become feasible — but they must be assist by other technique to prevent they from go off the rail like microsoft s tay do the code and datum for this tutorial be on github the vast majority of production system today be retrieval base or a combination of retrieval base and generative google s smart reply be a good example generative model be an active area of research but we re not quite there yet if you want to build a conversational agent today your good bet be most likely a retrieval base model in this post we ll work with the ubuntu dialog corpus paper github the ubuntu dialog corpus udc be one of the large public dialog dataset available it s base on chat log from the ubuntu channel on a public irc network the paper go into detail on how exactly the corpus be create so I win t repeat that here however it s important to understand what kind of datum we re work with so let s do some exploration first the training data consist of 1 000 000 example 50 % positive label 1 and 50 % negative label 0 each example consist of a context the conversation up to this point and an utterance a response to the context a positive label mean that an utterance be an actual response to a context and a negative label mean that the utterance wasn t — it be pick randomly from somewhere in the corpus here be some sample datum note that the dataset generation script have already do a bunch of preprocesse for we — it hastokenize stem and lemmatize the output use the nltk tool the script also replace entity like name location organization url and system path with special token this preprocesse isn t strictly necessary but it s likely to improve performance by a few percent the average context be 86 word long and the average utterance be 17 word long check out the jupyter notebook to see the data analysis the datum set come with test and validation set the format of these be different from that of the training datum each record in the test validation set consist of a context a ground truth utterance the real response and 9 incorrect utterance call distractor the goal of the model be to assign the high score to the true utterance and low score to wrong utterance the be various way to evaluate how well our model do a commonly use metric be recall@k recall@k mean that we let the model pick the k good response out of the 10 possible response 1 true and 9 distractor if the correct one be among the pick one we mark that test example as correct so a large k mean that the task become easy if we set k=10 we get a recall of 100 % because we only have 10 response to pick from if we set k=1 the model have only one chance to pick the right response at this point you may be wonder how the 9 distractor be choose in this datum set the 9 distractor be pick at random however in the real world you may have million of possible response and you don t know which one be correct you can t possibly evaluate a million potential response to pick the one with the high score — that d be too expensive google ssmart reply use cluster technique to come up with a set of possible response to choose from first or if you only have a few hundred potential response in total you could just evaluate all of they before start with fancy neural network model let s build some simple baseline model to help we understand what kind of performance we can expect we ll use the follow function to evaluate our recall@k metric here y be a list of our prediction sort by score in descend order and y_t be the actual label for example a y of 0 3 1 2 5 6 4 7 8 9 would mean that the utterance number 0 get the high score and utterance 9 get the low score remember that we have 10 utterance for each test example and the first one index 0 be always the correct one because the utterance column come before the distractor column in our datum intuitively a completely random predictor should get a score of 10 % for recall@1 a score of 20 % for recall@2 and so on let s see if that s the case great seem to work of course we don t just want a random predictor another baseline that be discuss in the original paper be a tf idf predictor tf idf stand for term frequency — inverse document frequency and it measure how important a word in a document be relative to the whole corpus without go into too much detail you can find many tutorial about tf idf on the web document that have similar content will have similar tf idf vector intuitively if a context and a response have similar word they be more likely to be a correct pair at least more likely than random many library out there such as scikit learn come with build in tf idf function so it s very easy to use let s build a tf idf predictor and see how well it perform we can see that the tf idf model perform significantly well than the random model it s far from perfect though the assumption we make aren t that great first of all a response doesn t necessarily need to be similar to the context to be correct secondly tf idf ignore word order which can be an important signal with a neural network model we can do a bit well the deep learning model we will build in this post be call a dual encoder lstm network this type of network be just one of many we could apply to this problem and it s not necessarily the good one you can come up with all kind of deep learning architecture that haven t be try yet — it s an active research area for example the seq2seq model often use in machine translation would probably do well on this task the reason we be go for the dual encoder be because it have be report to give decent performance on this datum set this mean we know what to expect and can be sure that our implementation be correct apply other model to this problem would be an interesting project the dual encoder lstm we ll build look like this paper it roughly work as follow to train the network we also need a loss cost function we ll use the binary cross entropy loss common for classification problem let s call our true label for a context response pair y this can be either 1 actual response or 0 incorrect response let s call our predict probability from 4 above y then the cross entropy loss be calculate as l= −y * ln y − 1 − y * ln 1−y the intuition behind this formula be simple if y=1 we be leave with l = ln y which penalize a prediction far away from 1 and if y=0 we be leave with l= −ln 1−y which penalize a prediction far away from 0 for our implementation we ll use a combination of numpy panda tensorflow and tf learn a combination of high level convenience function for tensorflow the dataset originally come in csv format we could work directly with csvs but it s well to convert our datum into tensorflow s proprietary example format quick side note there s alsotf sequenceexample but it doesn t seem to be support by tf learn yet the main benefit of this format be that it allow we to load tensor directly from the input file and let tensorflow handle all the shuffle batch and queue of input as part of the preprocessing we also create a vocabulary this mean we map each word to an integer number e g cat may become 2631 the tfrecord file we will generate store these integer number instead of the word string we will also save the vocabulary so that we can map back from integer to word later on each example contain the follow field the preprocessing be do by the prepare_data py python script which generate 3 file train tfrecord validation tfrecord and test tfrecord you can run the script yourself or download the datum file here in order to use tensorflow s build in support for training and evaluation we need to create an input function — a function that return batch of our input datum in fact because our training and test datum have different format we need different input function for they the input function should return a batch of feature and label if available something along the line of because we need different input function during training and evaluation and because we hate code duplication we create a wrapper call create_input_fn that create an input function for the appropriate mode it also take a few other parameter here s the definition we re use the complete code can be find in udc_input py on a high level the function do the following we already mention that we want to use the recall@k metric to evaluate our model luckily tensorflow already come with many standard evaluation metric that we can use include recall@k to use these metric we need to create a dictionary that map from a metric name to a function that take the prediction and label as argument above we use functool partial to convert a function that take 3 argument to one that only take 2 argument don t let the name streaming_sparse_recall_at_k confuse you stream just mean that the metric be accumulate over multiple batch and sparse refer to the format of our label this bring be to an important point what exactly be the format of our prediction during evaluation during training we predict the probability of the example be correct but during evaluation our goal be to score the utterance and 9 distractor and pick the good one — we don t simply predict correct incorrect this mean that during evaluation each example should result in a vector of 10 score e g 0 34 0 11 0 22 0 45 0 01 0 02 0 03 0 08 0 33 0 11 where the score correspond to the true response and the 9 distractor respectively each utterance be score independently so the probability don t need to add up to 1 because the true response be always element 0 in array the label for each example be 0 the example above would be count as classify incorrectly by recall@1because the third distractor get a probability of 0 45 while the true response only get 0 34 it would be score as correct by recall@2 however before write the actual neural network code I like to write the boilerplate code for training and evaluate the model that s because as long as you adhere to the right interface it s easy to swap out what kind of network you be use let s assume we have a model functionmodel_fn that take as input our batch feature label and mode train or evaluation and return the prediction then we can write general purpose code to train our model as follow here we create an estimator for our model_fn two input function for training and evaluation datum and our evaluation metric dictionary we also define a monitor that evaluate our model every flag eval_every step during training finally we train the model the training run indefinitely but tensorflow automatically save checkpoint file in model_dir so you can stop the training at any time a more fancy technique would be to use early stopping which mean you automatically stop train when a validation set metric stop improve I e you be start to overfit you can see the full code in udc_train py two thing I want to mention briefly be the usage of flag this be a way to give command line parameter to the program similar to python s argparse hparam be a custom object we create in hparams py that hold hyperparameter nob we can tweak of our model this hparams object be give to the model when we instantiate it now that we have set up the boilerplate code around inputs parse evaluation and training it s time to write code for our dual lstm neural network because we have different format of training and evaluation datum I ve write a create_model_fn wrapper that take care of bring the datum into the right format for we it take a model_impl argument which be a function that actually make prediction in our case it s the dual encoder lstm we describe above but we could easily swap it out for some other neural network let s see what that look like the full code be in dual_encoder py give this we can now instantiate our model function in the main routine in udc_train py that we define early that s it we can now run python udc_train py and it should start train our network occasionally evaluate recall on our validation datum you can choose how often you want to evaluate use the — eval_every switch to get a complete list of all available command line flag that we define use tf flag and hparam you can run python udc_train py — help info tensorflow result after 270 step 0 248 sec batch recall_at_1 = 0 507581018519 recall_at_2 = 0 689699074074 recall_at_5 = 0 913020833333 recall_at_10 = 1 0 loss = 0 5383 after you ve train the model you can evaluate it on the test set use python udc_test py — model_dir=$model_dir_from_traine e g python udc_test py — model_dir=~ github chatbot retrieval run 1467389151 this will run the recall@k evaluation metric on the test set instead of the validation set note that you must call udc_t py with the same parameter you use during training so if you train with — embedding_size=128 you need to call the test script with the same after train for about 20 000 step around an hour on a fast gpu our model get the follow result on the test set while recall@1 be close to our tfidf model recall@2 and recall@5 be significantly well suggest that our neural network assign high score to the correct answer the original paper report 0 55 0 72 and 0 92 for recall@1 recall@2 and recall@5 respectively but I haven t be able to reproduce score quite as high perhaps additional datum preprocessing or hyperparameter optimization may bump score up a bit more you can modify and run udc_predict py to get probability score for unseen datum for example python udc_predict py — model_dir= run 1467576365 output you could imagine feed in 100 potential response to a context and then pick the one with the high score in this post we ve implement a retrieval base neural network model that can assign score to potential response give a conversation context there be still a lot of room for improvement however one can imagine that other neural network do well on this task than a dual lstm encoder there be also a lot of room for hyperparameter optimization or improvement to the preprocesse step the code and datum for this tutorial be on github so check it out denny s blogs http blog dennybritz com & http www wildml com mark clark https www linkedin com in markwclark I hope you have find this condense nlp guide helpful I want to publish a long version imagine if this be 5x long however I don t want to scare the reader away as someone who develop the front end of bot user experience personality flow etc I find it extremely helpful to the understand the stack know the technological pro and con and so to be able to effectively design around nlp nlu limitation ultimately a lot of the issue bot face today eg context can be design around effectively if you have any suggestion on regard this article and how it can be improve feel free to drop I a line creator of 10 + bot include smart note bot founder of chatbot s life where we help company create great chatbot and share our insight along the way want to talk bot good way to chat directly and see my late project be via my personal bot stefan s bot currently I m consult a number of company on their chatbot project to get feedback on your chatbot project or to start a chatbot project contact I from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of chatbot life I help company create great chatbot & ai system and share my insight along the way good place to learn about chatbot we share the late bot news info ai & nlp tool tutorial & more
Arthur Juliani,3.5K,8,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=tag_archive---------7----------------,simple reinforcement learning with tensorflow part 8 asynchronous actor critic agent a3c,in this article I want to provide a tutorial on implement the asynchronous advantage actor critic a3c algorithm in tensorflow we will use it to solve a simple challenge in a 3d doom environment with the holiday right around the corner this will be my final post for the year and I hope it will serve as a culmination of all the previous topic in the series if you haven t yet or be new to deep learning and reinforcement learning I suggest check out the early entry in the series before go through this post in order to understand all the building block which will be utilize here if you have be follow the series thank you I have learn so much about rl in the past year and be happy to have share it with everyone through this article series so what be a3c the a3c algorithm be release by google s deepmind group early this year and it make a splash by essentially obsolete dqn it be fast simple more robust and able to achieve much well score on the standard battery of deep rl task on top of all that it could work in continuous as well as discrete action space give this it have become the go to deep rl algorithm for new challenging problem with complex state and action space in fact openai just release a version of a3c as their universal starter agent for work with their new and very diverse set of universe environment asynchronous advantage actor critic be quite a mouthful let s start by unpack the name and from there begin to unpack the mechanic of the algorithm itself asynchronous unlike dqn where a single agent represent by a single neural network interact with a single environment a3c utilize multiple incarnation of the above in order to learn more efficiently in a3c there be a global network and multiple worker agent which each have their own set of network parameter each of these agent interact with it s own copy of the environment at the same time as the other agent be interact with their environment the reason this work well than have a single agent beyond the speedup of get more work do be that the experience of each agent be independent of the experience of the other in this way the overall experience available for training become more diverse actor critic so far this series have focus on value iteration method such as q learning or policy iteration method such as policy gradient actor critic combine the benefit of both approach in the case of a3c our network will estimate both a value function v s how good a certain state be to be in and a policy π s a set of action probability output these will each be separate fully connect layer sit at the top of the network critically the agent use the value estimate the critic to update the policy the actor more intelligently than traditional policy gradient method advantage if we think back to our implementation of policy gradient the update rule use the discount return from a set of experience in order to tell the agent which of its action be good and which be bad the network be then update in order to encourage and discourage action appropriately the insight of use advantage estimate rather than just discount return be to allow the agent to determine not just how good its action be but how much well they turn out to be than expect intuitively this allow the algorithm to focus on where the network s prediction be lack if you recall from the duel q network architecture the advantage function be as follow since we win t be determine the q value directly in a3c we can use the discount return r as an estimate of q s a to allow we to generate an estimate of the advantage in this tutorial we will go even far and utilize a slightly different version of advantage estimation with low variance refer to as generalize advantage estimation in the process of build this implementation of the a3c algorithm I use as reference the quality implementation by dennybritz and openai both of which I highly recommend if you d like to see alternative to my code here each section embed here be take out of context for instructional purpose and win t run on its own to view and run the full functional a3c implementation see my github repository the general outline of the code architecture be the a3c algorithm begin by construct the global network this network will consist of convolutional layer to process spatial dependency follow by an lstm layer to process temporal dependency and finally value and policy output layer below be example code for establish the network graph itself next a set of worker agent each with their own network and environment be create each of these worker be run on a separate processor thread so there should be no more worker than there be thread on your cpu ~ from here we go asynchronous ~ each worker begin by set its network parameter to those of the global network we can do this by construct a tensorflow op which set each variable in the local worker network to the equivalent variable value in the global network each worker then interact with its own copy of the environment and collect experience each keep a list of experience tuple observation action reward do value that be constantly add to from interaction with the environment once the worker s experience history be large enough we use it to determine discount return and advantage and use those to calculate value and policy loss we also calculate an entropy h of the policy this correspond to the spread of action probability if the policy output action with relatively similar probability then entropy will be high but if the policy suggest a single action with a large probability then entropy will be low we use the entropy as a means of improve exploration by encourage the model to be conservative regard its sureness of the correct action a worker then use these loss to obtain gradient with respect to its network parameter each of these gradient be typically clip in order to prevent overly large parameter update which can destabilize the policy a worker then use the gradient to update the global network parameter in this way the global network be constantly be update by each of the agent as they interact with their environment once a successful update be make to the global network the whole process repeat the worker then reset its own network parameter to those of the global network and the process begin again to view the full and functional code see the github repository here the robustness of a3c allow we to tackle a new generation of reinforcement learning challenge one of which be 3d environment we have come a long way from multi armed bandit and grid world and in this tutorial I have set up the code to allow for play through the first vizdoom challenge vizdoom be a system to allow for rl research use the classic doom game engine the maintainer of vizdoom recently create a pip package so instal it be as simple as pip install vizdoom once it be instal we will be use the basic wad environment which be provide in the github repository and need to be place in the work directory the challenge consist of control an avatar from a first person perspective in a single square room there be a single enemy on the opposite side of the room which appear in a random location each episode the agent can only move to the left or right and fire a gun the goal be to shoot the enemy as quickly as possible use as few bullet as possible the agent have 300 time step per episode to shoot the enemy shoot the enemy yield a reward of 1 and each time step as well as each shot yield a small penalty after about 500 episode per worker agent the network learn a policy to quickly solve the challenge feel free to adjust parameter such as learn rate clip magnitude update frequency etc to attempt to achieve ever great performance or utilize a3c in your own rl task I hope this tutorial have be helpful to those new to a3c and asynchronous reinforcement learning now go forth and build ais there be a lot of move part in a3c so if you discover a bug or find a well way to do something please don t hesitate to bring it up here or in the github I be more than happy to incorporate change and feedback to improve the algorithm if you d like to follow my writing on deep learning ai and cognitive science follow I on medium @arthur juliani or on twitter @awjuliani if this post have be valuable to you please consider donate to help support future tutorial article and implementation any contribution be greatly appreciate more from my simple reinforcement learning with tensorflow series from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @unity3d & cognitive neuroscience phd student explore frontier technology through the lens of artificial intelligence datum science and the shape of thing to come
Alexandr Honchar,1.91K,7,https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-part-one-simple-time-series-forecasting-f992daa1045a?source=tag_archive---------8----------------,neural network for algorithmic trading simple time series forecasting,ciao people this be first part of my experiment on application of deep learning to finance in particular to algorithmic trading I want to implement trading system from scratch base only on deep learning approach so for any problem we have here price prediction trading strategy risk management we go to use different variation of artificial neural network ann and check how well they can handle this now I plan to work on next section I highly recommend you to check out code and ipython notebook in this repository in this first part I want to show how mlps cnn and rnn can be use for financial time series prediction in this part we be not go to use any feature engineering let s just consider historical dataset of s&p 500 index price movement we have information from 1950 to 2016 about open close high low price for every day in the year and volume of trade first we will try just to predict close price in the end of the next day second we will try to predict return close price — open price download the dataset from yahoo finance or from this repository we will consider our problem as 1 regression problem try to forecast exactly close price or return next day 2 binary classification problem price will go up 1 ; 0 or down 0 ; 1 for train nns we go to use framework keras first let s prepare our datum for training we want to predict t+1 value base on n previous day information for example have close price from past 30 day on the market we want to predict what price will be tomorrow on the 31st day we use first 90 % of time series as training set consider it as historical datum and last 10 % as testing set for model evaluation here be example of load splitting into training sample and preprocessing of raw input datum it will be just 2 hide layer perceptron number of hide neuron be choose empirically we will work on hyperparameter optimization in next section between two hide layer we add one dropout layer to prevent overfitte important thing be dense 1 activation linear and mse in compile section we want one output that can be in any range we predict real value and our loss function be define as mean square error let s see what happen if we just pass chunk of 20 day close price and predict price on 21st day final mse= 46 3635263557 but it s not very representative information below be plot of prediction for first 150 point of test dataset black line be actual datum blue one — predict we can clearly see that our algorithm be not even close by value but can learn the trend let s scale our datum use sklearn s method preprocessing scale to have our time series zero mean and unit variance and train the same mlp now we have mse = 0 0040424330518 but it be on scale datum on the plot below you can see actual scale time series black and our forecast blue for it for use this model in real world we should return back to unscaled time series we can do it by multiply or prediction by standard deviation of time series we use to make prediction 20 unscaled time step and add it s mean value mse in this case equal 937 963649937 here be the plot of restore prediction red and real datum green not bad isn t it but let s try more sophisticated algorithm for this problem I be not go to dive into theory of convolutional neural network you can check out this amazing resourse let s define 2 layer convolutional neural network combination of convolution and max pooling layer with one fully connect layer and the same output as early let s check out result mse for scale and restore datum be 0 227074542433 ; 935 520550172 plot be below even look on mse on scale datum this network learn much bad most probably deep architecture need more datum for training or it just overfitte due to too high number of filter or layer we will consider this issue later as recurrent architecture I want to use two stack lstm layer read more about lstms here plot of forecast be below ms = 0 0246238639582 ; 939 948636707 rnn forecasting look more like move average model it can t learn and predict all fluctuation so it s a bit unexpectable result but we can see that mlp work well for this time series forecasting let s check out what will happen if we swith from regression to classification problem now we will use not close price but daily return close price open price and we want to predict if close price be high or low than open price base on last 20 day return code be change just a bit — we change our last dense layer to have output 0 ; 1 or 1 ; 0 and add softmax output to expect probabilistic output to load binary output change in the code follow line also we change loss function to binary cross entopy and add accuracy metric oh it s not well than random guess 50 % accuracy let s try something well check out the result below we can see that treat financial time series prediction as regression problem be well approach it can learn the trend and price close to the actual what be surprising for I that mlp be treat sequence datum well as cnn or rnn which be suppose to work well with time series I explain it with pretty small dataset ~16k time stamp and dummy hyperparameter choice you can reproduce result and get well use code from repository I think we can get well result both in regression and classification use different feature not only scale time series like some technical indicator volume of sale also we can try more frequent datum let s say minute by minute tick to have more training datum all these thing I m go to do later so stay tune from a quick cheer to a stand ovation clap to show how much you enjoy this story 🇺 🇦 🇮 🇹 ai entrepreneur blogger and researcher make machine work 💻 learn 📕 and like 👍 but human create 🎨 discover 🚀 and love ❤ ️ the good about machine learn computer vision deep learn natural language processing and other
Arthur Juliani,1.7K,8,https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df?source=tag_archive---------9----------------,simple reinforcement learning with tensorflow part 4 deep q network and beyond,welcome to the late installment of my reinforcement learning series in this tutorial we will be walk through the creation of a deep q network it will be build upon the simple one layer q network we create in part 0 so I would recommend read that first if you be new to reinforcement learning while our ordinary q network be able to barely perform as well as the q table in a simple game environment deep q network be much more capable in order to transform an ordinary q network into a dqn we will be make the follow improvement it be these three innovation that allow the google deepmind team to achieve superhuman performance on dozen of atari game use their dqn agent we will be walk through each individual improvement and show how to implement it we win t stop there though the pace of deep learning research be extremely fast and the dqn of 2014 be no long the most advanced agent around anymore I will discuss two simple additional improvement to the dqn architecture double dqn and duel dqn that allow for improved performance stability and fast training time in the end we will have a network that can tackle a number of challenge atari game and we will demonstrate how to train the dqn to learn a basic navigation task since our agent be go to be learn to play video game it have to be able to make sense of the game s screen output in a way that be at least similar to how human or other intelligent animal be able to instead of consider each pixel independently convolutional layer allow we to consider region of an image and maintain spatial relationship between the object on the screen as we send information up to high level of the network in this way they act similarly to human receptive field indeed there be a body of research show that convolutional neural network learn representation that be similar to those of the primate visual cortex as such they be ideal for the first few element within our network in tensorflow we can utilize the tf contrib layer convolution2d function to easily create a convolutional layer we write for function as follow here num_out refer to how many filter we would like to apply to the previous layer kernel_size refer to how large a window we would like to slide over the previous layer stride refer to how many pixel we want to skip as we slide the window across the layer finally pad refer to whether we want our window to slide over just the bottom layer valid or add padding around it same in order to ensure that the convolutional layer have the same dimension as the previous layer for more information see the tensorflow documentation the second major addition to make dqns work be experience replay the basic idea be that by store an agent s experience and then randomly draw batch of they to train the network we can more robustly learn to perform well in the task by keep the experience we draw random we prevent the network from only learn about what it be immediately do in the environment and allow it to learn from a more varied array of past experience each of these experience be store as a tuple of < state action reward next state > the experience replay buffer store a fix number of recent memory and as new one come in old one be remove when the time come to train we simply draw a uniform batch of random memory from the buffer and train our network with they for our dqn we will build a simple class that handle store and retrieve memory the third major addition to the dqn that make it unique be the utilization of a second network during the training procedure this second network be use to generate the target q value that will be use to compute the loss for every action during training why not use just use one network for both estimation the issue be that at every step of train the q network s value shift and if we be use a constantly shift set of value to adjust our network value then the value estimation can easily spiral out of control the network can become destabilize by fall into feedback loop between the target and estimate q value in order to mitigate that risk the target network s weight be fix and only periodically or slowly update to the primary q network value in this way training can proceed in a more stable manner instead of update the target network periodically and all at once we will be update it frequently but slowly this technique be introduce in another deepmind paper early this year where they find that it stabilize the training process with the addition above we have everything we need to replicate the dwn of 2014 but the world move fast and a number of improvement above and beyond the dqn architecture describe by deepmind have allow for even great performance and stability before train your new dqn on your favorite atari game I would suggest check the new addition out I will provide a description and some code for two of they double dqn and duel dqn both be simple to implement and by combine both technique we can achieve well performance with fast training time the main intuition behind double dqn be that the regular dqn often overestimate the q value of the potential action to take in a give state while this would be fine if all action be always overestimate equally there be reason to believe this wasn t the case you can easily imagine that if certain suboptimal action regularly be give high q value than optimal action the agent would have a hard time ever learn the ideal policy in order to correct for this the author of ddqn paper propose a simple trick instead of take the max over q value when compute the target q value for our training step we use our primary network to chose an action and our target network to generate the target q value for that action by decouple the action choice from the target q value generation we be able to substantially reduce the overestimation and train fast and more reliably below be the new ddqn equation for update the target value in order to explain the reasoning behind the architecture change that duel dqn make we need to first explain some a few additional reinforcement learning term the q value that we have be discuss so far correspond to how good it be to take a certain action give a certain state this can be write as q s a this action give state can actually be decompose into two more fundamental notion of value the first be the value function v s which say simple how good it be to be in any give state the second be the advantage function a a which tell how much well take a certain action would be compare to the other we can then think of q as be the combination of v and a more formally the goal of duel dqn be to have a network that separately compute the advantage and value function and combine they back into a single q function only at the final layer it may seem somewhat pointless to do this at first glance why decompose a function that we will just put back together the key to realize the benefit be to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any give time for example imagine sit outside in a park watch the sunset it be beautiful and highly rewarding to be sit there no action need to be take and it doesn t really make sense to think of the value of sit there as be condition on anything beyond the environmental state you be in we can achieve more robust estimate of state value by decouple it from the necessity of be attach to specific action now that we have learn all the trick to get the most out of our dqn let s actually try it on a game environment while the dqn we have describe above could learn atari game with enough training get the network to perform well on those game take at least a day of training on a powerful machine for educational purpose I have build a simple game environment which our dqn learn to master in a couple hour on a moderately powerful machine I be use a gtx970 in the environment the agent control a blue square and the goal be to navigate to the green square reward +1 while avoid the red square reward 1 at the start of each episode all square be randomly place within a 5x5 grid world the agent have 50 step to achieve as large a reward as possible because they be randomly position the agent need to do more than simply learn a fix path as be the case in the frozenlake environment from tutorial 0 instead the agent must learn a notion of spatial relationship between the block and indeed it be able to do just that the game environment output 84x84x3 color image and use function call as similar to the openai gym as possible in do so it should be easy to modify this code to work on any of the openai atari game I encourage those with the time and computing resource necessary to try get the agent to perform well in an atari game the hyperparameter may need some tuning but it be definitely possible good luck if this post have be valuable to you please consider donate to help support future tutorial article and implementation any contribution be greatly appreciate if you d like to follow my work on deep learning ai and cognitive science follow I on medium @arthur juliani or on twitter @awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @unity3d & cognitive neuroscience phd student
Vishal Maini,32K,10,https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12?source=tag_archive---------0----------------,a beginner s guide to ai ml 🤖 👶 machine learn for human medium,part 1 why machine learning matter the big picture of artificial intelligence and machine learning — past present and future part 2 1 supervised learning learn with an answer key introduce linear regression loss function overfitte and gradient descent part 2 2 supervise learn ii two method of classification logistic regression and svms part 2 3 supervise learn iii non parametric learner k near neighbor decision tree random forest introduce cross validation hyperparameter tuning and ensemble model part 3 unsupervised learning cluster k mean hierarchical dimensionality reduction principal component analysis pca singular value decomposition svd part 4 neural network & deep learning why where and how deep learning work draw inspiration from the brain convolutional neural network cnns recurrent neural network rnn real world application part 5 reinforcement learning exploration and exploitation markov decision process q learning policy learning and deep reinforcement learn the value learn problem appendix the good machine learning resource a curate list of resource for create your machine learning curriculum this guide be intend to be accessible to anyone basic concept in probability statistic program linear algebra and calculus will be discuss but it isn t necessary to have prior knowledge of they to gain value from this series artificial intelligence will shape our future more powerfully than any other innovation this century anyone who do not understand it will soon find themselves feel leave behind wake up in a world full of technology that feel more and more like magic the rate of acceleration be already astounding after a couple of ai winter and period of false hope over the past four decade rapid advance in datum storage and computer processing power have dramatically change the game in recent year in 2015 google train a conversational agent ai that could not only convincingly interact with human as a tech support helpdesk but also discuss morality express opinion and answer general fact base question the same year deepmind develop an agent that surpass human level performance at 49 atari game receive only the pixel and game score as input soon after in 2016 deepmind obsolete their own achievement by release a new state of the art gameplay method call a3c meanwhile alphago defeat one of the good human player at go — an extraordinary achievement in a game dominate by human for two decade after machine first conquer chess many master could not fathom how it would be possible for a machine to grasp the full nuance and complexity of this ancient chinese war strategy game with its 10170 possible board position there be only 1080atoms in the universe in march 2017 openai create agent that invent their own language to cooperate and more effectively achieve their goal soon after facebook reportedly successfully train agent to negotiate and even lie just a few day ago as of this writing on august 11 2017 openai reach yet another incredible milestone by defeat the world s top professional in 1v1 match of the online multiplayer game dota 2 much of our day to day technology be power by artificial intelligence point your camera at the menu during your next trip to taiwan and the restaurant s selection will magically appear in english via the google translate app today ai be use to design evidence base treatment plan for cancer patient instantly analyze result from medical test to escalate to the appropriate specialist immediately and conduct scientific research for drug discovery in everyday life it s increasingly commonplace to discover machine in role traditionally occupy by human really don t be surprised if a little housekeeping delivery bot show up instead of a human next time you call the hotel desk to send up some toothpaste in this series we ll explore the core machine learning concept behind these technology by the end you should be able to describe how they work at a conceptual level and be equip with the tool to start build similar application yourself artificial intelligence be the study of agent that perceive the world around they form plan and make decision to achieve their goal its foundation include mathematics logic philosophy probability linguistic neuroscience and decision theory many field fall under the umbrella of ai such as computer vision robotic machine learning and natural language processing machine learning be a subfield of artificial intelligence its goal be to enable computer to learn on their own a machine s learn algorithm enable it to identify pattern in observed datum build model that explain the world and predict thing without have explicit pre program rule and model the technology discuss above be example of artificial narrow intelligence ani which can effectively perform a narrowly define task meanwhile we re continue to make foundational advance towards human level artificial general intelligence agi also know as strong ai the definition of an agi be an artificial intelligence that can successfully perform any intellectual task that a human being can include learn planning and decision making under uncertainty communicate in natural language make joke manipulate people trade stock or reprogramme itself and this last one be a big deal once we create an ai that can improve itself it will unlock a cycle of recursive self improvement that could lead to an intelligence explosion over some unknown time period range from many decade to a single day you may have hear this point refer to as the singularity the term be borrow from the gravitational singularity that occur at the center of a black hole an infinitely dense one dimensional point where the law of physics as we understand they start to break down a recent report by the future of humanity institute survey a panel of ai researcher on timeline for agi and find that researcher believe there be a 50 % chance of ai outperform human in all task in 45 year grace et al 2017 we ve personally speak with a number of sane and reasonable ai practitioner who predict much long timeline the upper limit be never and other whose timeline be alarmingly short — as little as a few year the advent of great than human level artificial superintelligence asi could be one of the good or bad thing to happen to our specie it carry with it the immense challenge of specify what ais will want in a way that be friendly to human while it s impossible to say what the future hold one thing be certain 2017 be a good time to start understand how machine think to go beyond the abstraction of a philosopher in an armchair and intelligently shape our roadmap and policy with respect to ai we must engage with the detail of how machine see the world — what they want their potential bias and failure mode their temperamental quirk — just as we study psychology and neuroscience to understand how human learn decide act and feel machine learning be at the core of our journey towards artificial general intelligence and in the meantime it will change every industry and have a massive impact on our day to day live that s why we believe it s worth understand machine learning at least at a conceptual level — and we design this series to be the good place to start you don t necessarily need to read the series cover to cover to get value out of it here be three suggestion on how to approach it depend on your interest and how much time you have vishal most recently lead growth at upstart a lending platform that utilize machine learning to price credit automate the borrowing process and acquire user he spend his time think about startup apply cognitive science moral philosophy and the ethic of artificial intelligence samer be a master s student in computer science and engineering at ucsd and co founder of conigo lab prior to grad school he found tablescribe a business intelligence tool for smb and spend two year advise fortune 100 company at mckinsey samer previously study computer science and ethic politic and economic at yale most of this series be write during a 10 day trip to the united kingdom in a frantic blur of train plane cafe pub and wherever else we could find a dry place to sit our aim be to solidify our own understanding of artificial intelligence machine learning and how the method therein fit together — and hopefully create something worth share in the process and now without further ado let s dive into machine learning with part 2 1 supervised learning more from machine learning for human 🤖 👶 a special thank to jonathan eng edoardo conti grant schneider sunny kumar stephanie he tarun wadhwa and sachin maini series editor for their significant contribution and feedback from a quick cheer to a stand ovation clap to show how much you enjoy this story research comms @deepmindai previously @upstart @yale @trueventurestec demystify artificial intelligence & machine learning discussion on safe and intentional application of ai for positive social impact
Tim Anglade,7K,23,https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3?source=tag_archive---------1----------------,how hbo s silicon valley build not hotdog with mobile tensorflow keras & react native,the hbo show silicon valley release a real ai app that identify hotdog — and not hotdog — like the one show on season 4 s 4th episode the app be now available on android as well as io to achieve this we design a bespoke neural architecture that run directly on your phone and train it with tensorflow keras & nvidia gpus while the use case be farcical the app be an approachable example of both deep learning and edge computing all ai work be power 100 % by the user s device and image be process without ever leave their phone this provide user with a snappy experience no round trip to the cloud offline availability and well privacy this also allow we to run the app at a cost of $ 0 even under the load of a million user provide significant saving compare to traditional cloud base ai approach the app be develop in house by the show by a single developer run on a single laptop & attach gpu use hand curate datum in that respect it may provide a sense of what can be achieve today with a limited amount of time & resource by non technical company individual developer and hobbyist alike in that spirit this article attempt to give a detailed overview of step involve to help other build their own app if you haven t see the show or try the app you should the app let you snap a picture and then tell you whether it think that image be of a hotdog or not it s a straightforward use case that pay homage to recent ai research and application in particular imagenet while we ve probably dedicate more engineering resource to recognize hotdog than anyone else the app still fail in horrible and or subtle way conversely it s also sometimes able to recognize hotdog in complex situation accord to engadget it s incredible I ve have more success identify food with the app in 20 minute than I have have tag and identify song with shazam in the past two year have you ever find yourself read hacker news thinking they raise a 10 m series a for that I could build it in one weekend this app probably feel a lot like that and the initial prototype be indeed build in a single weekend use google cloud platform s vision api and react native but the final app we end up release on the app store require month of additional part time work to deliver meaningful improvement that would be difficult for an outsider to appreciate we spend week optimize overall accuracy training time inference time iterate on our setup & tooling so we could have a fast development iteration and spend a whole weekend optimize the user experience around ios & android permission don t even get I start on that one all too often technical blog post or academic paper skip over this part prefer to present the final choose solution in the interest of help other learn from our mistake & choice we will present an abridge view of the approach that didn t work for we before we describe the final architecture we end up shipping in the next section we choose react native to build the prototype as it would give we an easy sandbox to experiment with and would help we quickly support many device the experience end up be a good one and we keep react native for the remainder of the project it didn t always make thing easy and the design for the app be purposefully limited but in the end react native get the job do the other main component we use for the prototype — google cloud s vision api be quickly abandon there be 3 main factor for these reason we start experiment with what s trendily call edge computing which for our purpose mean that after train our neural network on our laptop we would export it and embe it directly into our mobile app so that the neural network execution phase or inference would run directly inside the user s phone through a chance encounter with pete warden of the tensorflow team we have become aware of its ability to run tensorflow directly embed on an ios device and start explore that path after react native tensorflow become the second fix part of our stack it only take a day of work to integrate tensorflow s objective c++ camera example in our react native shell it take slightly long to use their transfer learning script which help you retrain the inception architecture to deal with a more specific image problem inception be the name of a family of neural architecture build by google to deal with image recognition problem inception be available pre train which mean the training phase have be complete and the weight be set most often for image recognition network they have be train on imagenet a dataset contain over 20 000 different type of object hotdog be one of they however much like google cloud s vision api imagenet training reward breadth as much as depth here and out of the box accuracy on a single one of the 20 000 + category can be lack as such retraining also call transfer learning aim to take a full train neural net and retrain it to perform well on the specific problem you d like to handle this usually involve some degree of forget either by excise entire layer from the stack or by slowly erase the network s ability to distinguish a type of object e g chair in favor of well accuracy at recognize the one you care about I e hotdog while the network inception in this case may have be train on the 14 m image contain in imagenet we be able to retrain it on a just a few thousand hotdog image to get drastically enhance hotdog recognition the big advantage of transfer learning be you will get well result much fast and with less datum than if you train from scratch a full training might take month on multiple gpu and require million of image while retraining can conceivably be do in hour on a laptop with a couple thousand image one of the big challenge we encounter be understand exactly what should count as a hotdog and what should not define what a hotdog be end up be surprisingly difficult do cut up sausage count and if so which kind and subject to cultural interpretation similarly the open world nature of our problem mean we have to deal with an almost infinite number of inputs while certain computer vision problem have relatively limit input say x ray of bolt with or without a mechanical default we have to prepare the app to be feed selfie nature shot and any number of food suffice to say this approach be promise and do lead to some improved result however it have to be abandon for a couple of reason first the nature of our problem mean a strong imbalance in training datum there be many more example of thing that be not hotdog than thing that be hotdog in practice this mean that if you train your algorithm on 3 hotdog image and 97 non hotdog image and it recognize 0 % of the former but 100 % of the latter it will still score 97 % accuracy by default this be not straightforward to solve out of the box use tensorflow s retrain tool and basically necessitate set up a deep learning model from scratch import weight and train in a more control manner at this point we decide to bite the bullet and get something start with keras a deep learning library that provide nice easy to use abstraction on top of tensorflow include pretty awesome training tool and a class_weight option which be ideal to deal with this sort of dataset imbalance we be deal with we use that opportunity to try other popular neural architecture like vgg but one problem remain none of they could comfortably fit on an iphone they consume too much memory which lead to app crash and would sometime take up to 10 second to compute which be not ideal from a ux standpoint many thing be attempt to mitigate that but in the end it these architecture be just too big to run efficiently on mobile to give you a context out of time this be roughly the mid way point of the project by that time the ui be 90%+ do and very little of it be go to change but in hindsight the neural net be at well 20 % do we have a good sense of challenge & a good dataset but 0 line of the final neural architecture have be write none of our neural code could reliably run on mobile and even our accuracy be go to improve drastically in the week to come the problem directly ahead of we be simple if inception and vgg be too big be there a simple pre train neural network we could retrain at the suggestion of the always excellent jeremy p howard where have that guy be all our life we explore xception enet and squeezenet we quickly settle on squeezenet due to its explicit positioning as a solution for embed deep learning and the availability of a pre train keras model on github yay open source so how big of a difference do this make an architecture like vgg use about 138 million parameter essentially the number of number necessary to model the neuron and value between they inception be already a massive improvement require only 23 million parameter squeezenet in comparison only require 1 25 million this have two advantage there be tradeoff of course during this phase we start experiment with tune the neural network architecture in particular we start use batch normalization and try different activation function after add batch normalization and elu to squeezenet we be able to train neural network that achieve 90%+ accuracy when train from scratch however they be relatively brittle mean the same network would overfit in some case or underfit in other when confront to real life testing even add more example to the dataset and play with data augmentation fail to deliver a network that meet expectation so while this phase be promise and for the first time give we a function app that could work entirely on an iphone in less than a second we eventually move to our 4th & final architecture our final architecture be spur in large part by the publication on april 17 of google s mobilenet paper promise a new neural architecture with inception like accuracy on simple problem like ours with only 4 m or so parameter this mean it sit in an interesting sweet spot between a squeezenet that have maybe be overly simplistic for our purpose and the possibly overwrought elephant try to squeeze in a tutu of use inception or vgg on mobile the paper introduce some capacity to tune the size & complexity of network specifically to trade memory cpu consumption against accuracy which be very much top of mind for we at the time with less than a month to go before the app have to launch we endeavor to reproduce the paper s result this be entirely anticlimactic as within a day of the paper be publish a keras implementation be already offer publicly on github by refik can malli a student at istanbul technical university whose work we have already benefit from when we take inspiration from his excellent keras squeezenet implementation the depth & openness of the deep learning community and the presence of talente mind like r c be what make deep learning viable for application today — but they also make work in this field more thrilling than any tech trend we ve be involve with our final architecture end up make significant departure from the mobilenet architecture or from convention in particular so how do this stack work exactly deep learning often get a bad rap for be a black box and while it s true many component of it can be mysterious the network we use often leak information about how some of their magic work we can look at the layer of this stack and how they activate on specific input image give we a sense of each layer s ability to recognize sausage bun or other particularly salient hotdog feature data quality be of the utmost importance a neural network can only be as good as the datum that train it and improve training set quality be probably one of the top 3 thing we spend time on during this project the key thing we do to improve this be the final composition of our dataset be 150k image of which only 3k be hotdog there be only so many hotdog you can look at but there be many not hotdog to look at the 49 1 imbalance be deal with by say a keras class weight of 49 1 in favor of hotdog of the remain 147k image most be of food with just 3k photo of non food item to help the network generalize a bit more and not get trick into see a hotdog if present with an image of a human in a red outfit our data augmentation rule be as follow these number be derive intuitively base on experiment and our understanding of the real life usage of our app as oppose to careful experimentation the final key to our data pipeline be use patrick rodriguez s multiprocess image datum generator for keras while keras do have a build in multi thread and multiprocess implementation we find patrick s library to be consistently fast in our experiment for reason we do not have time to investigate this library cut our training time to a third of what it use to be the network be train use a 2015 macbook pro and attach external gpu egpu specifically an nvidia gtx 980 ti we d probably buy a 1080 ti if we be start today we be able to train the network on batch of 128 image at a time the network be train for a total of 240 epoch mean we run all 150k image through the network 240 time this take about 80 hour we train the network in 3 phase while learn rate be identify by run the linear experiment recommend by the clr paper they seem to intuitively make sense in that the max for each phase be within a factor of 2 of the previous minimum which be align with the industry standard recommendation of halve your learning rate if your accuracy plateaus during training in the interest of time we perform some training run on a paperspace p5000 instance run ubuntu in those case we be able to double the batch size and find that optimal learning rate for each phase be roughly double as well even have design a relatively compact neural architecture and have train it to handle situation it may find in a mobile context we have a lot of work leave to make it run properly try to run a top of the line neural net architecture out of the box can quickly burn hundred megabyte of ram which few mobile device can spare today beyond network optimization it turn out the way you handle image or even load tensorflow itself can have a huge impact on how quickly your network run how little ram it use and how crash free the experience will be for your user this be maybe the most mysterious part of this project relatively little information can be find about it possibly due to the dearth of production deep learning application run on mobile device as of today however we must commend the tensorflow team and particularly pete warden andrew harp and chad whipkey for the exist documentation and their kindness in answer our inquiry instead of use tensorflow on io we look at use apple s build in deep learning librarie instead bnn mpscnn and later on coreml we would have design the network in keras train it with tensorflow export all the weight value re implement the network with bnns or mpscnn or import it via coreml and load the parameter into that new implementation however the big obstacle be that these new apple library be only available on io 10 + and we want to support old version of io as io 10 + adoption and these framework continue to improve there may not be a case for use tensorflow on device in the near future if you think inject javascript into your app on the fly be cool try inject neural net into your app the last production trick we use be to leverage codepush and apple s relatively permissive term of service to live inject new version of our neural network after submission to the app store while this be mostly do to help we quickly deliver accuracy improvement to our user after release you could conceivably use this approach to drastically expand or alter the feature set of your app without go through an app store review again there be a lot of thing that didn t work or we didn t have time to do and these be the idea we d investigate in the future finally we d be remiss not to mention the obvious and important influence of user experience developer experience and build in bias in develop an ai app each probably deserve their own post or their own book but here be the very concrete impact of these 3 thing in our experience ux user experience be arguably more critical at every stage of the development of an ai app than for a traditional application there be no deep learning algorithm that will give you perfect result right now but there be many situation where the right mix of deep learning + ux will lead to result that be indistinguishable from perfect proper ux expectation be irreplaceable when it come to set developer on the right path to design their neural network set the proper expectation for user when they use the app and gracefully handle the inevitable ai failure building ai app without a ux first mindset be like train a neural net without stochastic gradient descent you will end up stuck in the local minima of the uncanny valley on your way to build the perfect ai use case dx developer experience be extremely important as well because deep learning training time be the new horsing around while wait for your program to compile we suggest you heavily favor dx first hence keras as it s always possible to optimize runtime for later run manual gpu parallelization multi process data augmentation tensorflow pipeline even re implement for caffe2 pytorch even project with relatively obtuse apis & documentation like tensorflow greatly improve dx by provide a highly test highly use well maintain environment for training & run neural network for the same reason it s hard to beat both the cost as well as the flexibility of have your own local gpu for development be able to look at edit image locally edit code with your prefer tool without delay greatly improve the development quality & speed of building ai project most ai app will hit more critical cultural bias than ours but as an example even our straightforward use case catch we flat foot with build in bias in our initial dataset that make the app unable to recognize french style hotdog asian hotdog and more oddity we do not have immediate personal experience with it s critical to remember that ai do not make well decision than human — they be infect by the same human bias we fall prey to via the training set human provide thank to mike judge alec berg clay tarver todd silverstein jonathan dotan lisa schomas amy solomon dorothy street & rich toyon and all the writer of the show — the app would simply not exist without they meaghan dana david jay and everyone at hbo scale venture partner & gitlab rachel thomas and jeremy howard & fast ai for all that they have teach I and for kindly review a draft of this post check out their free online deep learning course it s awesome jp simard for his help on io and finally the tensorflow team & r machinelearning for their help & inspiration and thank to everyone who use & share the app it make stare at picture of hotdog for month on end totally worth it 😅 from a quick cheer to a stand ovation clap to show how much you enjoy this story a i startup & hbo s silicon valley get in touch timanglade@gmail com
Dhruv Parthasarathy,4.3K,12,https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4?source=tag_archive---------2----------------,a brief history of cnn in image segmentation from r cnn to mask r cnn,at athela we use convolutional neural network cnn for a lot more than just classification in this post we ll see how cnn can be use with great result in image instance segmentation ever since alex krizhevsky geoff hinton and ilya sutskever win imagenet in 2012 convolutional neural network cnn have become the gold standard for image classification in fact since then cnn have improve to the point where they now outperform human on the imagenet challenge while these result be impressive image classification be far simple than the complexity and diversity of true human visual understanding in classification there s generally an image with a single object as the focus and the task be to say what that image be see above but when we look at the world around we we carry out far more complex task we see complicated sight with multiple overlap object and different background and we not only classify these different object but also identify their boundary difference and relation to one another can cnn help we with such complex task namely give a more complicated image can we use cnn to identify the different object in the image and their boundary as have be show by ross girshick and his peer over the last few year the answer be conclusively yes through this post we ll cover the intuition behind some of the main technique use in object detection and segmentation and see how they ve evolve from one implementation to the next in particular we ll cover r cnn regional cnn the original application of cnn to this problem along with its descendant fast r cnn and fast r cnn finally we ll cover mask r cnn a paper release recently by facebook research that extend such object detection technique to provide pixel level segmentation here be the paper reference in this post inspire by the research of hinton s lab at the university of toronto a small team at uc berkeley lead by professor jitendra malik ask themselves what today seem like an inevitable question object detection be the task of find the different object in an image and classify they as see in the image above the team comprise of ross girshick a name we ll see again jeff donahue and trevor darrel find that this problem can be solve with krizhevsky s result by test on the pascal voc challenge a popular object detection challenge akin to imagenet they write let s now take a moment to understand how their architecture region with cnn r cnn work understand r cnn the goal of r cnn be to take in an image and correctly identify where the main object via a bounding box in the image but how do we find out where these bounding box be r cnn do what we might intuitively do as well propose a bunch of box in the image and see if any of they actually correspond to an object r cnn create these bounding box or region proposal use a process call selective search which you can read about here at a high level selective search show in the image above look at the image through window of different size and for each size try to group together adjacent pixel by texture color or intensity to identify object once the proposal be create r cnn warps the region to a standard square size and pass it through to a modified version of alexnet the win submission to imagenet 2012 that inspire r cnn as show above on the final layer of the cnn r cnn add a support vector machine svm that simply classify whether this be an object and if so what object this be step 4 in the image above improve the bounding box now have find the object in the box can we tighten the box to fit the true dimension of the object we can and this be the final step of r cnn r cnn run a simple linear regression on the region proposal to generate tight bounding box coordinate to get our final result here be the input and output of this regression model so to summarize r cnn be just the follow step r cnn work really well but be really quite slow for a few simple reason in 2015 ross girshick the first author of r cnn solve both these problem lead to the second algorithm in our short history fast r cnn let s now go over its main insight fast r cnn insight 1 roi region of interest pooling for the forward pass of the cnn girshick realize that for each image a lot of propose region for the image invariably overlap cause we to run the same cnn computation again and again ~2000 time his insight be simple — why not run the cnn just once per image and then find a way to share that computation across the ~2000 proposal this be exactly what fast r cnn do use a technique know as roipool region of interest pooling at its core roipool share the forward pass of a cnn for an image across its subregion in the image above notice how the cnn feature for each region be obtain by select a correspond region from the cnn s feature map then the feature in each region be pool usually use max pooling so all it take we be one pass of the original image as oppose to ~2000 fast r cnn insight 2 combine all model into one network the second insight of fast r cnn be to jointly train the cnn classifier and bounding box regressor in a single model where early we have different model to extract image feature cnn classify svm and tighten bounding box regressor fast r cnn instead use a single network to compute all three you can see how this be do in the image above fast r cnn replace the svm classifier with a softmax layer on top of the cnn to output a classification it also add a linear regression layer parallel to the softmax layer to output bounding box coordinate in this way all the output need come from one single network here be the input and output to this overall model even with all these advancement there be still one remain bottleneck in the fast r cnn process — the region proposer as we see the very first step to detect the location of object be generate a bunch of potential bounding box or region of interest to test in fast r cnn these proposal be create use selective search a fairly slow process that be find to be the bottleneck of the overall process in the middle 2015 a team at microsoft research compose of shaoqe ren kaime he ross girshick and jian sun find a way to make the region proposal step almost cost free through an architecture they creatively name fast r cnn the insight of fast r cnn be that region proposal depend on feature of the image that be already calculate with the forward pass of the cnn first step of classification so why not reuse those same cnn result for region proposal instead of run a separate selective search algorithm indeed this be just what the fast r cnn team achieve in the image above you can see how a single cnn be use to both carry out region proposal and classification this way only one cnn need to be train and we get region proposal almost for free the author write here be the input and output of their model how the region be generate let s take a moment to see how fast r cnn generate these region proposal from cnn feature fast r cnn add a fully convolutional network on top of the feature of the cnn create what s know as the region proposal network the region proposal network work by pass a slide window over the cnn feature map and at each window output k potential bounding box and score for how good each of those box be expect to be what do these k box represent intuitively we know that object in an image should fit certain common aspect ratio and size for instance we know that we want some rectangular box that resemble the shape of human likewise we know we win t see many box that be very very thin in such a way we create k such common aspect ratio we call anchor box for each such anchor box we output one bounding box and score per position in the image with these anchor box in mind let s take a look at the input and output to this region proposal network we then pass each such bounding box that be likely to be an object into fast r cnn to generate a classification and tightened bounding box so far we ve see how we ve be able to use cnn feature in many interesting way to effectively locate different object in an image with bounding box can we extend such technique to go one step far and locate exact pixel of each object instead of just bound box this problem know as image segmentation be what kaime he and a team of researcher include girshick explore at facebook ai use an architecture know as mask r cnn much like fast r cnn and fast r cnn mask r cnn s underlying intuition be straight forward give that fast r cnn work so well for object detection could we extend it to also carry out pixel level segmentation mask r cnn do this by add a branch to fast r cnn that output a binary mask that say whether or not a give pixel be part of an object the branch in white in the above image as before be just a fully convolutional network on top of a cnn base feature map here be its input and output but the mask r cnn author have to make one small adjustment to make this pipeline work as expect roialign realigning roipool to be more accurate when run without modification on the original fast r cnn architecture the mask r cnn author realize that the region of the feature map select by roipool be slightly misalign from the region of the original image since image segmentation require pixel level specificity unlike bound box this naturally lead to inaccuracy the author be able to solve this problem by cleverly adjust roipool to be more precisely align use a method know as roialign imagine we have an image of size 128x128 and a feature map of size 25x25 let s imagine we want feature the region correspond to the top leave 15x15 pixel in the original image see above how might we select these pixel from the feature map we know each pixel in the original image correspond to ~ 25 128 pixel in the feature map to select 15 pixel from the original image we just select 15 * 25 128 ~= 2 93 pixel in roipool we would round this down and select 2 pixel cause a slight misalignment however in roialign we avoid such round instead we use bilinear interpolation to get a precise idea of what would be at pixel 2 93 this at a high level be what allow we to avoid the misalignment cause by roipool once these mask be generate mask r cnn combine they with the classification and bounding box from fast r cnn to generate such wonderfully precise segmentation if you re interested in try out these algorithm yourself here be relevant repository fast r cnn mask r cnn in just 3 year we ve see how the research community have progress from krizhevsky et al s original result to r cnn and finally all the way to such powerful result as mask r cnn see in isolation result like mask r cnn seem like incredible leap of genius that would be unapproachable yet through this post I hope you ve see how such advancement be really the sum of intuitive incremental improvement through year of hard work and collaboration each of the idea propose by r cnn fast r cnn fast r cnn and finally mask r cnn be not necessarily quantum leap yet their sum product have lead to really remarkable result that bring we close to a human level understanding of sight what particularly excite I be that the time between r cnn and mask r cnn be just three year with continue funding focus and support how much far can computer vision improve over the next three year if you see any error or issue in this post please contact I at dhruv@getathela com and I ll immediately correct they if you re interested in apply such technique come join we at athela where we apply computer vision to blood diagnostic daily other post we ve write thank to bharath ramsundar pranav ramkrishnan tanay tandon and oliver cameron for help with this post from a quick cheer to a stand ovation clap to show how much you enjoy this story @dhruvp vp eng @athelas mit math and cs undergrad 13 mit cs master 14 previously director of ai program @ udacity blood diagnostic through deep learning http athela com
Sebastian Heinz,4.4K,13,https://medium.com/mlreview/a-simple-deep-learning-model-for-stock-price-prediction-using-tensorflow-30505541d877?source=tag_archive---------3----------------,a simple deep learning model for stock price prediction use tensorflow,for a recent hackathon that we do at statworx some of our team member scrape minutely s&p 500 datum from the google finance api the datum consist of index as well as stock price of the s&p s 500 constituent have this datum at hand the idea of develop a deep learning model for predict the s&p 500 index base on the 500 constituent price one minute ago come immediately on my mind play around with the datum and build the deep learning model with tensorflow be fun and so I decide to write my first medium com story a little tensorflow tutorial on predict s&p 500 stock price what you will read be not an in depth tutorial but more a high level introduction to the important building block and concept of tensorflow model the python code I ve create be not optimize for efficiency but understandability the dataset I ve use can be download from here 40 mb our team export the scrape stock datum from our scrape server as a csv file the dataset contain n = 41266 minute of datum range from april to august 2017 on 500 stock as well as the total s&p 500 index price index and stock be arrange in wide format the datum be already clean and prepare meaning miss stock and index price be locf ed last observation carry forward so that the file do not contain any miss value a quick look at the s&p time series use pyplot plot datum sp500 note this be actually the lead of the s&p 500 index mean its value be shift 1 minute into the future this operation be necessary since we want to predict the next minute of the index and not the current minute the dataset be split into training and test datum the training datum contain 80 % of the total dataset the data be not shuffle but sequentially slice the training datum range from april to approx end of july 2017 the test data end end of august 2017 there be a lot of different approach to time series cross validation such as roll forecast with and without refit or more elaborate concept such as time series bootstrap resample the latter involve repeat sample from the remainder of the seasonal decomposition of the time series in order to simulate sample that follow the same seasonal pattern as the original time series but be not exact copy of its value most neural network architecture benefit from scale the input sometimes also the output why because most common activation function of the network s neuron such as tanh or sigmoid be define on the 1 1 or 0 1 interval respectively nowadays rectify linear unit relu activation be commonly use activation which be unbounded on the axis of possible activation value however we will scale both the input and target anyway scaling can be easily accomplish in python use sklearn s minmaxscaler remark caution must be undertake regard what part of the data be scale and when a common mistake be to scale the whole dataset before training and test split be be apply why be this a mistake because scale invoke the calculation of statistic e g the min max of a variable when perform time series forecasting in real life you do not have information from future observation at the time of forecast therefore calculation of scale statistic have to be conduct on training datum and must then be apply to the test datum otherwise you use future information at the time of forecasting which commonly bias forecasting metric in a positive direction tensorflow be a great piece of software and currently the lead deep learning and neural network computation framework it be base on a c++ low level backend but be usually control via python there be also a neat tensorflow library for r maintain by rstudio tensorflow operate on a graph representation of the underlie computational task this approach allow the user to specify mathematical operation as element in a graph of datum variable and operator since neural network be actually graph of datum and mathematical operation tensorflow be just perfect for neural network and deep learning check out this simple example steal from our deep learning introduction from our blog in the figure above two number be suppose to be add those number be store in two variable a and b the two value be flow through the graph and arrive at the square node where they be be add the result of the addition be store into another variable c actually a b and c can be consider as placeholder any number that be feed into a and b get add and be store into c this be exactly how tensorflow work the user define an abstract representation of the model neural network through placeholder and variable afterwards the placeholder get fill with real datum and the actual computation take place the follow code implement the toy example from above in tensorflow after have import the tensorflow library two placeholder be define use tf placeholder they correspond to the two blue circle on the left of the image above afterwards the mathematical addition be define via tf add the result of the computation be c = 9 with placeholder set up the graph can be execute with any integer value for a and b of course the former problem be just a toy example the require graph and computation in a neural network be much more complex as mention before it all start with placeholder we need two placeholder in order to fit our model x contain the network s input the stock price of all s&p 500 constituent at time t = t and y the network s output the index value of the s&p 500 at time t = t + 1 the shape of the placeholder correspond to none n_stock with none mean that the input be a 2 dimensional matrix and the output be a 1 dimensional vector it be crucial to understand which input and output dimension the neural net need in order to design it properly the none argument indicate that at this point we do not yet know the number of observation that flow through the neural net graph in each batch so we keep if flexible we will later define the variable batch_size that control the number of observation per training batch besides placeholder variable be another cornerstone of the tensorflow universe while placeholder be use to store input and target datum in the graph variable be use as flexible container within the graph that be allow to change during graph execution weight and bias be represent as variable in order to adapt during training variable need to be initialize prior to model training we will get into that a litte later in more detail the model consist of four hide layer the first layer contain 1024 neuron slightly more than double the size of the inputs subsequent hide layer be always half the size of the previous layer which mean 512 256 and finally 128 neuron a reduction of the number of neuron for each subsequent layer compress the information the network identify in the previous layer of course other network architecture and neuron configuration be possible but be out of scope for this introduction level article it be important to understand the require variable dimension between input hide and output layer as a rule of thumb in multilayer perceptron mlp the type of network use here the second dimension of the previous layer be the first dimension in the current layer for weight matrix this might sound complicated but be essentially just each layer pass its output as input to the next layer the bias dimension equal the second dimension of the current layer s weight matrix which correspond the number of neuron in this layer after definition of the require weight and bias variable the network topology the architecture of the network need to be specify hereby placeholder datum and variable weigh and bias need to be combine into a system of sequential matrix multiplication furthermore the hide layer of the network be transform by activation function activation function be important element of the network architecture since they introduce non linearity to the system there be dozen of possible activation function out there one of the most common be the rectified linear unit relu which will also be use in this model the image below illustrate the network architecture the model consist of three major building block the input layer the hide layer and the output layer this architecture be call a feedforward network feedforward indicate that the batch of datum solely flow from leave to right other network architecture such as recurrent neural network also allow datum flow backwards in the network the cost function of the network be use to generate a measure of deviation between the network s prediction and the actual observed training target for regression problem the mean square error mse function be commonly use mse compute the average squared deviation between prediction and target basically any differentiable function can be implement in order to compute a deviation measure between prediction and target however the mse exhibit certain property that be advantageous for the general optimization problem to be solve the optimizer take care of the necessary computation that be use to adapt the network s weight and bias variable during train those computation invoke the calculation of so call gradient that indicate the direction in which the weight and bias have to be change during training in order to minimize the network s cost function the development of stable and speedy optimizer be a major field in neural network an deep learning research here the adam optimizer be use which be one of the current default optimizer in deep learning development adam stand for adaptive moment estimation and can be consider as a combination between two other popular optimizer adagrad and rmsprop initializer be use to initialize the network s variable before training since neural network be train use numerical optimization technique the starting point of the optimization problem be one the key factor to find good solution to the underlying problem there be different initializer available in tensorflow each with different initialization approach here I use the tf variance_scaling_initializer which be one of the default initialization strategy note that with tensorflow it be possible to define multiple initialization function for different variable within the graph however in most case a unified initialization be sufficient after have define the placeholder variable initializer cost function and optimizer of the network the model need to be train usually this be do by minibatch training during minibatch train random data sample of n = batch_size be draw from the training datum and feed into the network the training dataset get divide into n batch_size batch that be sequentially feed into the network at this point the placeholder x and y come into play they store the input and target datum and present they to the network as input and target a sample data batch of x flow through the network until it reach the output layer there tensorflow compare the model prediction against the actual observed target y in the current batch afterwards tensorflow conduct an optimization step and update the network parameter correspond to the select learning scheme after have update the weight and bias the next batch be sample and the process repeat itself the procedure continue until all batch have be present to the network one full sweep over all batch be call an epoch the training of the network stop once the maximum number of epoch be reach or another stopping criterion define by the user apply during the training we evaluate the network prediction on the test set — the datum which be not learn but set aside — for every 5th batch and visualize it additionally the image be export to disk and later combine into a video animation of the training process see below the model quickly learn the shape und location of the time series in the test datum and be able to produce an accurate prediction after some epoch nice one can see that the network rapidly adapt to the basic shape of the time series and continue to learn finer pattern of the datum this also correspond to the adam learning scheme that lower the learning rate during model training in order not to overshoot the optimization minimum after 10 epoch we have a pretty close fit to the test datum the final test mse equal 0 00078 it be very low because the target be scale the mean absolute percentage error of the forecast on the test set be equal to 5 31 % which be pretty good note that this be just a fit to the test datum no actual out of sample metric in a real world scenario please note that there be ton of way of far improve this result design of layer and neuron choose different initialization and activation scheme introduction of dropout layer of neuron early stop and so on furthermore different type of deep learning model such as recurrent neural network might achieve well performance on this task however this be not the scope of this introductory post the release of tensorflow be a landmark event in deep learning research its flexibility and performance allow researcher to develop all kind of sophisticated neural network architecture as well as other ml algorithms however flexibility come at the cost of long time to model cycle compare to high level api such as keras or mxnet nonetheless I be sure that tensorflow will make its way to the de facto standard in neural network and deep learning development in research and practical application many of our customer be already use tensorflow or start develop project that employ tensorflow model also our data science consultant at statworx be heavily use tensorflow for deep learning and neural net research and development let s see what google have plan for the future of tensorflow one thing that be miss at least in my opinion be a neat graphical user interface for design and develop neural net architecture with tensorflow backend maybe this be something google be already work on ; if you have any comment or question on my first medium story feel free to comment below I will try to answer they also feel free to use my code or share this story with your peer on social platform of your choice update I ve add both the python script as well as a zip dataset to a github repository feel free to clone and fork lastly follow I on twitter | linkedin from a quick cheer to a stand ovation clap to show how much you enjoy this story ceo @ statworx do datum science stat and ml for over a decade food wine and cocktail enthusiast check our website https www statworx com highlight from machine learning research project and learn material from and for ml scientist engineer an enthusiast
Max Pechyonkin,23K,8,https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b?source=tag_archive---------4----------------,understand hinton s capsule network part I intuition,part I intuition you be read it now part ii how capsule workpart iii dynamic routing between capsulespart iv capsnet architecture quick announcement about our new publication ai3 we be get the good writer together to talk about the theory practice and business of ai and machine learning follow it to stay up to date on the late trend last week geoffrey hinton and his team publish two paper that introduce a completely new type of neural network base on so call capsule in addition to that the team publish an algorithm call dynamic routing between capsule that allow to train such a network for everyone in the deep learning community this be huge news and for several reason first of all hinton be one of the founder of deep learning and an inventor of numerous model and algorithm that be widely use today secondly these paper introduce something completely new and this be very exciting because it will most likely stimulate additional wave of research and very cool application in this post I will explain why this new architecture be so important as well as intuition behind it in the follow post I will dive into technical detail however before talk about capsule we need to have a look at cnn which be the workhorse of today s deep learning cnns convolutional neural network be awesome they be one of the reason deep learning be so popular today they can do amazing thing that people use to think computer would not be capable of do for a long long time nonetheless they have their limit and they have fundamental drawback let we consider a very simple and non technical example imagine a face what be the component we have the face oval two eye a nose and a mouth for a cnn a mere presence of these object can be a very strong indicator to consider that there be a face in the image orientational and relative spatial relationship between these component be not very important to a cnn how do cnn work the main component of a cnn be a convolutional layer its job be to detect important feature in the image pixel layer that be deeply close to the input will learn to detect simple feature such as edge and color gradient whereas high layer will combine simple feature into more complex feature finally dense layer at the top of the network will combine very high level feature and produce classification prediction an important thing to understand be that high level feature combine low level feature as a weighted sum activation of a precede layer be multiply by the follow layer neuron s weight and add before be pass to activation nonlinearity nowhere in this setup there be pose translational and rotational relationship between simple feature that make up a high level feature cnn approach to solve this issue be to use max pooling or successive convolutional layer that reduce spacial size of the datum flow through the network and therefore increase the field of view of high layer s neuron thus allow they to detect high order feature in a large region of the input image max pooling be a crutch that make convolutional network work surprisingly well achieve superhuman performance in many area but do not be fool by its performance while cnn work well than any model before they max pooling nonetheless be lose valuable information hinton himself state that the fact that max pooling be work so well be a big mistake and a disaster of course you can do away with max pooling and still get good result with traditional cnn but they still do not solve the key problem in the example above a mere presence of 2 eye a mouth and a nose in a picture do not mean there be a face we also need to know how these object be orient relative to each other computer graphic deal with construct a visual image from some internal hierarchical representation of geometric datum note that the structure of this representation need to take into account relative position of object that internal representation be store in computer s memory as array of geometrical object and matrix that represent relative position and orientation of these object then special software take that representation and convert it into an image on the screen this be call render inspire by this idea hinton argue that brain in fact do the opposite of render he call it inverse graphic from visual information receive by eye they deconstruct a hierarchical representation of the world around we and try to match it with already learn pattern and relationship store in the brain this be how recognition happen and the key idea be that representation of object in the brain do not depend on view angle so at this point the question be how do we model these hierarchical relationship inside of a neural network the answer come from computer graphic in 3d graphic relationship between 3d object can be represent by a so call pose which be in essence translation plus rotation hinton argue that in order to correctly do classification and object recognition it be important to preserve hierarchical pose relationship between object part this be the key intuition that will allow you to understand why capsule theory be so important it incorporate relative relationship between object and it be represent numerically as a 4d pose matrix when these relationship be build into internal representation of datum it become very easy for a model to understand that the thing that it see be just another view of something that it have see before consider the image below you can easily recognize that this be the statue of liberty even though all the image show it from different angle this be because internal representation of the statue of liberty in your brain do not depend on the view angle you have probably never see these exact picture of it but you still immediately know what it be for a cnn this task be really hard because it do not have this build in understanding of 3d space but for a capsnet it be much easy because these relationship be explicitly model the paper that use this approach be able to cut error rate by 45 % as compare to the previous state of the art which be a huge improvement another benefit of the capsule approach be that it be capable of learn to achieve state of the art performance by only use a fraction of the datum that a cnn would use hinton mention this in his famous talk about what be wrong with cnn in this sense the capsule theory be much close to what the human brain do in practice in order to learn to tell digit apart the human brain need to see only a couple of dozen of example hundred at most cnn on the other hand need ten of thousand of example to achieve very good performance which seem like a brute force approach that be clearly inferior to what we do with our brain the idea be really simple there be no way no one have come up with it before and the truth be hinton have be think about this for decade the reason why there be no publication be simply because there be no technical way to make it work before one of the reason be that computer be just not powerful enough in the pre gpu base era before around 2012 another reason be that there be no algorithm that allow to implement and successfully learn a capsule network in the same fashion the idea of artificial neuron be around since 1940 s but it be not until mid 1980 s when backpropagation algorithm show up and allow to successfully train deep network in the same fashion the idea of capsule itself be not that new and hinton have mention it before but there be no algorithm up until now to make it work this algorithm be call dynamic routing between capsule this algorithm allow capsule to communicate with each other and create representation similar to scene graph in computer graphic capsule introduce a new building block that can be use in deep learning to well model hierarchical relationship inside of internal knowledge representation of a neural network intuition behind they be very simple and elegant hinton and his team propose a way to train such a network make up of capsule and successfully train it on a simple data set achieve state of the art performance this be very encouraging nonetheless there be challenge current implementation be much slow than other modern deep learning model time will show if capsule network can be train quickly and efficiently in addition we need to see if they work well on more difficult data set and in different domain in any case the capsule network be a very interesting and already work model which will definitely get more develop over time and contribute to further expansion of deep learning application domain this conclude part one of the series on capsule network in the part ii more technical part I will walk you through the capsnet s internal working step by step you can follow I on twitter let s also connect on linkedin from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learn the ai revolution be here navigate the ever change industry with our thoughtfully write article whether your a researcher engineer or entrepreneur
Slav Ivanov,3.9K,17,https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=tag_archive---------5----------------,the $ 1700 great deep learning box assembly setup and benchmark,update april 2018 use cuda 9 cudnn 7 and tensorflow 1 5 after year of use a thin client in the form of increasingly thin macbook I have get use to it so when I get into deep learning dl I go straight for the brand new at the time amazon p2 cloud server no upfront cost the ability to train many model simultaneously and the general coolness of have a machine learning model out there slowly teach itself however as time pass the aws bill steadily grow large even as I switch to 10x cheap spot instance also I didn t find myself train more than one model at a time instead I d go to lunch workout etc while the model be train and come back later with a clear head to check on it but eventually the model complexity grow and take long to train I d often forget what I do differently on the model that have just complete its 2 day training nudge by the great experience of the other folk on the fast ai forum I decide to settle down and to get a dedicated dl box at home the most important reason be save time while prototyping model — if they train fast the feedback time would be short thus it would be easy for my brain to connect the dot between the assumption I have for the model and its result then I want to save money — I be use amazon web service aws which offer p2 instance with nvidia k80 gpus lately the aws bill be around $ 60 70 month with a tendency to get large also it be expensive to store large dataset like imagenet and lastly I haven t have a desktop for over 10 year and want to see what have change in the meantime spoiler alert mostly nothing what follow be my choice inner monologue and gotcha from choose the component to benchmarke a sensible budget for I would be about 2 year worth of my current compute spending at $ 70 month for aw this put it at around $ 1700 for the whole thing you can check out all the component use the pc part picker site be also really helpful in detect if some of the component don t play well together the gpu be the most crucial component in the box it will train these deep network fast shorten the feedback cycle disclosure the follow be affiliate link to help I pay for well more gpu the choice be between a few of nvidia s card gtx 1070 gtx 1070 ti gtx 1080 gtx 1080 ti and finally the titan x the price might fluctuate especially because some gpu be great for cryptocurrency mining wink 1070 wink on performance side gtx 1080 ti and titan x be similar roughly speak the gtx 1080 be about 25 % fast than gtx 1070 and gtx 1080 ti be about 30 % fast than gtx 1080 the new gtx 1070 ti be very close in performance to gtx 1080 tim dettmer have a great article on pick a gpu for deep learning which he regularly update as new card come on the market here be the thing to consider when pick a gpu consider all of this I pick the gtx 1080 ti mainly for the training speed boost I plan to add a second 1080 ti soonish even though the gpu be the mvp in deep learning the cpu still matter for example data preparation be usually do on the cpu the number of core and thread per core be important if we want to parallelize all that data prep to stay on budget I pick a mid range cpu the intel i5 7500 it s relatively cheap but good enough to not slow thing down edit as a few people have point out probably the big gotcha that be unique to dl multi gpu be to pay attention to the pcie lane support by the cpu motherboard by andrej karpathy we want to have each gpu have 16 pcie lane so it eat datum as fast as possible 16 gb s for pcie 3 0 this mean that for two card we need 32 pcie lane however the cpu I have pick have only 16 lane so 2 gpu would run in 2x8 mode instead of 2x16 this might be a bottleneck lead to less than ideal utilization of the graphic card thus a cpu with 40 line be recommend edit 2 however tim dettmer point out that have 8 lane per card should only decrease performance by 0 10 % for two gpu so currently my recommendation be go with 16 pcie lane per video card unless it get too expensive for you otherwise 8 lane should do as well a good solution with to have for a double gpu machine would be an intel xeon processor like the e5 1620 v4 40 pcie lane or if you want to splurge go for a high end processor like the desktop i7 6850k memory ram it s nice to have a lot of memory if we be to be work with rather big dataset I get 2 stick of 16 gb for a total of 32 gb of ram and plan to buy another 32 gb later follow jeremy howard s advice I get a fast ssd disk to keep my os and current datum on and then a slow spin hdd for those huge dataset like imagenet ssd I remember when I get my first macbook air year ago how blow away be I by the ssd speed to my delight a new generation of ssd call nvme have make its way to market in the meantime a 480 gb mydigitalssd nvme drive be a great deal this baby copy file at gigabyte per second hdd 2 tb seagate while ssds have be get fast hdd have be get cheap to somebody who have use macbook with 128 gb disk for the last 7 year have this much space feel almost obscene the one thing that I keep in mind when pick a motherboard be the ability to support two gtx 1080 ti both in the number of pci express lane the minimum be 2x8 and the physical size of 2 card also make sure it s compatible with the choose cpu an asus tuf z270 do it for I msi — x99a sli plus should work great if you get an intel xeon cpu rule of thumb power supply should provide enough juice for the cpu and the gpu plus 100 watt extra the intel i5 7500 processor use 65w and the gpus 1080 ti need 250w each so I get a deepcool 750w gold psu currently unavailable evga 750 gq be similar the gold here refer to the power efficiency I e how much of the power consume be waste as heat the case should be the same form factor as the motherboard also have enough led to embarrass a burner be a bonus a friend recommend the thermaltake n23 case which I promptly get no led sadly here be how much I spend on all the component your cost may vary $ 700 gtx 1080 ti + $ 190 cpu + $ 230 ram + $ 230 ssd + $ 66 hdd + $ 130 motherboard + $ 75 psu + $ 50 case = = = = = = = = = = = = $ 1671 total add tax and fee this nicely match my preset budget of $ 1700 if you don t have much experience with hardware and fear you might break something a professional assembly might be the good option however this be a great learning opportunity that I couldn t pass even though I ve have my share of hardware relate horror story the first and important step be to read the installation manual that come with each component especially important for I as I ve do this before once or twice and I have just the right amount of inexperience to mess thing up this be do before instal the motherboard in the case next to the processor there be a lever that need to be pull up the processor be then place on the base double check the orientation finally the lever come down to fix the cpu in place but I have a quite the difficulty do this once the cpu be in position the lever wouldn t go down I actually have a more hardware capable friend of mine video walk I through the process turn out the amount of force require to get the lever lock down be more than what I be comfortable with next be fix the fan on top of the cpu the fan leg must be fully secure to the motherboard consider where the fan cable will go before instal the processor I have come with thermal paste if yours doesn t make sure to put some paste between the cpu and the cool unit also replace the paste if you take off the fan I put the power supply unit psu in before the motherboard to get the power cable snugly place in case back side pretty straight forward — carefully place it and screw it in a magnetic screwdriver be really helpful then connect the power cable and the case button and led just slide it in the m2 slot and screw it in piece of cake the memory prove quite hard to install require too much effort to properly lock in a few time I almost give up thinking I must be do it wrong eventually one of the stick click in and the other one promptly follow at this point I turn the computer on to make sure it work to my relief it start right away finally the gpu slide in effortlessly 14 pin of power later and it be run nb do not plug your monitor in the external card right away most probably it need driver to function see below finally it s complete now that we have the hardware in place only the soft part remain out with the screwdriver in with the keyboard note on dual booting if you plan to install window because you know for benchmark totally not for game it would be wise to do window first and linux second I didn t and have to reinstall ubuntu because window mess up the boot partition livewire have a detailed article on dual boot most dl framework be design to work on linux first and eventually support other operating system so I go for ubuntu my default linux distribution an old 2 gb usb drive be lay around and work great for the installation unetbootin osx or rufus window can prepare the linux thumb drive the default option work fine during the ubuntu install at the time of write ubuntu 17 04 be just release so I opt for the previous version 16 04 whose quirk be much well document online ubuntu server or desktop the server and desktop edition of ubuntu be almost identical with the notable exception of the visual interface call x not be instal with server I instal the desktop and disabled autostarte x so that the computer would boot it in terminal mode if need one could launch the visual desktop later by type startx let s get our install up to date from jeremy howard s excellent install gpu script to deep learn on our machine we need a stack of technology to use our gpu download cuda from nvidia or just run the code below update to specify version 9 of cuda thank to @zhanwenchen for the tip if you need to add later version of cuda click here after cuda have be instal the follow code will add the cuda installation to the path variable now we can verify that cuda have be instal successfully by run this should have instal the display driver as well for I nvidia smi show err as the device name so I instal the late nvidia driver as of may 2018 to fix it remove cuda nvidia driver if at any point the driver or cuda seem break as they do for I — multiple time it might be well to start over by run since version 1 5 tensorflow support cudnn 7 so we install that to download cudnn one need to register for a free developer account after download install with the follow anaconda be a great package manager for python I ve move to python 3 6 so will be use the anaconda 3 version the popular dl framework by google installation validate tensorfow install to make sure we have our stack run smoothly I like to run the tensorflow mnist example we should see the loss decrease during training keras be a great high level neural network framework an absolute pleasure to work with installation can t be easy too pytorch be a newcomer in the world of dl framework but its api be model on the successful torch which be write in lua pytorch feel new and exciting mostly great although some thing be still to be implement we install it by run jupyter be a web base ide for python which be ideal for data sciency task it s instal with anaconda so we just configure and test it now if we open http localhost 8888 we should see a jupyter screen run jupyter on boot rather than run the notebook every time the computer be restart we can set it to autostart on boot we will use crontab to do this which we can edit by run crontab e then add the following after the last line in the crontab file I use my old trusty macbook air for development so I d like to be able to log into the dl box both from my home network also when on the run ssh key it s way more secure to use a ssh key to login instead of a password digital ocean have a great guide on how to setup this ssh tunnel if you want to access your jupyter notebook from another computer the recommend way be to use ssh tunneling instead of open the notebook to the world and protect with a password let s see how we can do this 2 then to connect over ssh tunnel run the follow script on the client to test this open a browser and try http localhost 8888 from the remote machine your jupyter notebook should appear setup out of network access finally to access the dl box from the outside world we need 3 thing set up out of network access depend on the router network setup so I m not go into detail now that we have everything run smoothly let s put it to the test we ll be compare the newly build box to an aws p2 xlarge instance which be what I ve use so far for dl the test be computer vision relate mean convolutional network with a fully connect model throw in we time training model on aws p2 instance gpu k80 aw p2 virtual cpu the gtx 1080 ti and intel i5 7500 cpu andre hernandez point out that my comparison do not use tensorflow that be optimize for these cpu which would have help the they perform well check his insightful comment for more detail the hello world of computer vision the mnist database consist of 70 000 handwritten digit we run the keras example on mnist which use multilayer perceptron mlp the mlp mean that we be use only fully connect layer not convolution the model be train for 20 epoch on this dataset which achieve over 98 % accuracy out of the box we see that the gtx 1080 ti be 2 4 time fast than the k80 on aws p2 in train the model this be rather surprising as these 2 card should have about the same performance I believe this be because of the virtualization or underclocking of the k80 on aws the cpus perform 9 time slow than the gpu as we will see later it s a really good result for the processor this be due to the small model which fail to fully utilize the parallel processing power of the gpu interestingly the desktop intel i5 7500 achieve 2 3x speedup over the virtual cpu on amazon a vgg net will be finetune for the kaggle dog vs cat competition in this competition we need to tell apart picture of dog and cat run the model on cpus for the same number of batch wasn t feasible therefore we finetune for 390 batch 1 epoch on the gpu and 10 batch on the cpus the code use be on github the 1080 ti be 5 5 time fast that the aws gpu k80 the difference in the cpus performance be about the same as the previous experiment i5 be 2 6x fast however it s absolutely impractical to use cpu for this task as the cpus be take ~200x more time on this large model that include 16 convolutional layer and a couple semi wide 4096 fully connect layer on top a gan generative adversarial network be a way to train a model to generate image gan achieve this by pit two network against each other a generator which learn to create well and well image and a discriminator that try to tell which image be real and which be dream up by the generator the wasserstein gan be an improvement over the original gan we will use a pytorch implementation that be very similar to the one by the wgan author the model be train for 50 step and the loss be all over the place which be often the case with gan cpus aren t consider the gtx 1080 ti finish 5 5x fast than the aws p2 k80 which be in line with the previous result the final benchmark be on the original style transfer paper gatys et al implement on tensorflow code available style transfer be a technique that combine the style of one image a painting for example and the content of another image check out my previous post for more detail on how style transfer work the gtx 1080 ti outperform the aws k80 by a factor of 4 3 this time the cpu be 30 50 time slow than graphic card the slowdown be less than on the vgg finetune task but more than on the mnist perceptron experiment the model use mostly the early layer of the vgg network and I suspect this be too shallow to fully utilize the gpus the dl box be in the next room and a large model be train on it be it a wise investment time will tell but it be beautiful to watch the glow led in the dark and to hear its quiet hum as model be try to squeeze out that extra accuracy percentage point from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur hacker machine learn deep learning and other type of learn
Stefan Kojouharov,14.2K,7,https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?source=tag_archive---------6----------------,cheat sheet for ai neural network machine learn deep learning & big datum,over the past few month I have be collect ai cheat sheet from time to time I share they with friend and colleague and recently I have be get ask a lot so I decide to organize and share the entire collection to make thing more interesting and give context I add description and or excerpt for each major topic this be the most complete list and the big o be at the very end enjoy this machine learn cheat sheet will help you find the right estimator for the job which be the most difficult part the flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problem and how to solve it scikit learn formerly scikit learn be a free software machine learning library for the python programming language it feature various classification regression and clustering algorithm include support vector machine random forest gradient boost k mean and dbscan and be design to interoperate with the python numerical and scientific library numpy and scipy in may 2017 google announce the second generation of the tpu as well as the availability of the tpus in google compute engine 12 the second generation tpus deliver up to 180 teraflop of performance and when organize into cluster of 64 tpu provide up to 11 5 petaflop in 2017 google s tensorflow team decide to support keras in tensorflow s core library chollet explain that keras be conceive to be an interface rather than an end to end machine learning framework it present a high level more intuitive set of abstraction that make it easy to configure neural network regardless of the backend scientific computing library numpy target the cpython reference implementation of python which be a non optimize bytecode interpreter mathematical algorithm write for this version of python often run much slow than compile equivalent numpy address the slowness problem partly by provide multidimensional array and function and operator that operate efficiently on array require rewrite some code mostly inner loop use numpy the name panda be derive from the term panel datum an econometric term for multidimensional structured data set the term datum wrangler be start to infiltrate pop culture in the 2017 movie kong skull island one of the character play by actor marc evan jackson be introduce as steve woodward our data wrangler scipy build on the numpy array object and be part of the numpy stack which include tool like matplotlib panda and sympy and an expand set of scientific computing librarie this numpy stack have similar user to other application such as matlab gnu octave and scilab the numpy stack be also sometimes refer to as the scipy stack 3 matplotlib be a plotting library for the python programming language and its numerical mathematics extension numpy it provide an object orient api for embed plot into application use general purpose gui toolkit like tkinter wxpython qt or gtk+ there be also a procedural pylab interface base on a state machine like opengl design to closely resemble that of matlab though its use be discourage 2 scipy make use of matplotlib pyplot be a matplotlib module which provide a matlab like interface 6 matplotlib be design to be as usable as matlab with the ability to use python with the advantage that it be free > > > if you like this list you can let I know here < < < stefan be the founder of chatbot s life a chatbot medium and consult firm chatbot s life have grow to over 150k view per month and have become the premium place to learn about bot & ai online chatbot s life have also consult many of the top bot company like swelly instav outbrain neargroup and a number of enterprise big o algorithm cheat sheet http bigocheatsheet com bokeh cheat sheet https s3 amazonaw com asset datacamp com blog_asset python_bokeh_cheat_sheet pdf datum science cheat sheet https www datacamp com community tutorial python data science cheat sheet basic datum wrangle cheat sheet https www rstudio com wp content upload 2015 02 datum wrangle cheatsheet pdf datum wrangle https en wikipedia org wiki data_wrangle ggplot cheat sheet https www rstudio com wp content upload 2015 03 ggplot2 cheatsheet pdf keras cheat sheet https www datacamp com community blog keras cheat sheet#gs drkenms keras https en wikipedia org wiki keras machine learn cheat sheet https ai icymi email new machinelearning cheat sheet by emily barry abdsc machine learning cheat sheet https doc microsoft com en in azure machine learning machine learn algorithm cheat sheet ml cheat sheet http peekaboo vision blogspot com 2013 01 machine learn cheat sheet for scikit html matplotlib cheat sheet https www datacamp com community blog python matplotlib cheat sheet#gs uekyspy matpotlib https en wikipedia org wiki matplotlib neural network cheat sheet http www asimovinstitute org neural network zoo neural network graph cheat sheet http www asimovinstitute org blog neural network https www quora com where can find a cheat sheet for neural network numpy cheat sheet https www datacamp com community blog python numpy cheat sheet#gs ak5zbge numpy https en wikipedia org wiki numpy panda cheat sheet https www datacamp com community blog python panda cheat sheet#gs oundfxm panda https en wikipedia org wiki panda _ software panda cheat sheet https www datacamp com community blog panda cheat sheet python#gs hpforic pyspark cheat sheet https www datacamp com community blog pyspark cheat sheet python#gs l = j1zxq scikit cheat sheet https www datacamp com community blog scikit learn cheat sheet scikit learn https en wikipedia org wiki scikit learn scikit learn cheat sheet http peekaboo vision blogspot com 2013 01 machine learn cheat sheet for scikit html scipy cheat sheet https www datacamp com community blog python scipy cheat sheet#gs jdsg3oi scipy https en wikipedia org wiki scipy tesorflow cheat sheet https www altoros com tensorflow cheat sheet html tensor flow https en wikipedia org wiki tensorflow from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of chatbot life I help company create great chatbot & ai system and share my insight along the way late news info and tutorial on artificial intelligence machine learn deep learn big datum and what it mean for humanity
Vishal Maini,8K,13,https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab?source=tag_archive---------7----------------,machine learning for human part 2 1 supervised learning,how much money will we make by spend more dollar on digital advertising will this loan applicant pay back the loan or not what s go to happen to the stock market tomorrow in supervised learning problem we start with a data set contain training example with associate correct label for example when learn to classify handwritten digit a supervised learning algorithm take thousand of picture of handwritten digit along with label contain the correct number each image represent the algorithm will then learn the relationship between the image and their associate number and apply that learn relationship to classify completely new image without label that the machine hasn t see before this be how you re able to deposit a check by take a picture with your phone to illustrate how supervised learning work let s examine the problem of predict annual income base on the number of year of high education someone have complete express more formally we d like to build a model that approximate the relationship f between the number of year of high education x and corresponding annual income y one method for predict income would be to create a rigid rule base model for how income and education be relate for example I d estimate that for every additional year of high education annual income increase by $ 5 000 you could come up with a more complex model by include some rule about degree type year of work experience school tier etc for example if they complete a bachelor s degree or high give the income estimate a 1 5x multipli but this kind of explicit rule base programming doesn t work well with complex datum imagine try to design an image classification algorithm make of if then statement describe the combination of pixel brightness that should be label cat or not cat supervised machine learning solve this problem by get the computer to do the work for you by identify pattern in the datum the machine be able to form heuristic the primary difference between this and human learning be that machine learning run on computer hardware and be well understand through the lens of computer science and statistic whereas human pattern matching happen in a biological brain while accomplish the same goal in supervised learn the machine attempt to learn the relationship between income and education from scratch by run label training datum through a learning algorithm this learn function can be use to estimate the income of people whose income y be unknown as long as we have year of education x as inputs in other word we can apply our model to the unlabeled test datum to estimate y the goal of supervised learning be to predict y as accurately as possible when give new example where x be know and y be unknown in what follow we ll explore several of the most common approach to do so the rest of this section will focus on regression in part 2 2 we ll dive deeply into classification method regression predict a continuous target variable y it allow you to estimate a value such as housing price or human lifespan base on input datum x here target variable mean the unknown variable we care about predict and continuous mean there aren t gap discontinuity in the value that y can take on a person s weight and height be continuous value discrete variable on the other hand can only take on a finite number of value — for example the number of kid somebody have be a discrete variable predict income be a classic regression problem your input datum x include all relevant information about individual in the datum set that can be use to predict income such as year of education year of work experience job title or zip code these attribute be call feature which can be numerical e g year of work experience or categorical e g job title or field of study you ll want as many training observation as possible relate these feature to the target output y so that your model can learn the relationship f between x and y the data be split into a training datum set and a test datum set the training set have label so your model can learn from these label example the test set do not have label I e you don t yet know the value you re try to predict it s important that your model can generalize to situation it hasn t encounter before so that it can perform well on the test datum in our trivially simple 2d example this could take the form of a csv file where each row contain a person s education level and income add more column with more feature and you ll have a more complex but possibly more accurate model how do we build model that make accurate useful prediction in the real world we do so by use supervised learning algorithm now let s get to the fun part get to know the algorithms we ll explore some of the way to approach regression and classification and illustrate key machine learning concept throughout draw the line yes this count as machine learn first we ll focus on solve the income prediction problem with linear regression since linear model don t work well with image recognition task this be the domain of deep learning which we ll explore later we have our datum set x and corresponding target value y the goal of ordinary least square ols regression be to learn a linear model that we can use to predict a new y give a previously unseen x with as little error as possible we want to guess how much income someone earn base on how many year of education they receive linear regression be a parametric method which mean it make an assumption about the form of the function relate x and y we ll cover example of non parametric method later our model will be a function that predict ŷ give a specific x β0 be the y intercept and β1 be the slope of our line I e how much income increase or decrease with one additional year of education our goal be to learn the model parameter in this case β0 and β1 that minimize error in the model s prediction to find the good parameter graphically in two dimension this result in a line of good fit in three dimension we would draw a plane and so on with high dimensional hyperplane mathematically we look at the difference between each real datum point y and our model s prediction ŷ square these difference to avoid negative number and penalize large difference and then add they up and take the average this be a measure of how well our data fit the line for a simple problem like this we can compute a closed form solution use calculus to find the optimal beta parameter that minimize our loss function but as a cost function grow in complexity find a closed form solution with calculus be no long feasible this be the motivation for an iterative approach call gradient descent which allow we to minimize a complex loss function put on a blindfold take a step downhill you ve find the bottom when you have nowhere to go but up gradient descent will come up over and over again especially in neural network machine learning library like scikit learn and tensorflow use it in the background everywhere so it s worth understand the detail the goal of gradient descent be to find the minimum of our model s loss function by iteratively get a well and well approximation of it imagine yourself walk through a valley with a blindfold on your goal be to find the bottom of the valley how would you do it a reasonable approach would be to touch the ground around you and move in whichever direction the ground be slope down most steeply take a step and repeat the same process continually until the ground be flat then you know you ve reach the bottom of a valley ; if you move in any direction from where you be you ll end up at the same elevation or far uphill go back to mathematics the ground become our loss function and the elevation at the bottom of the valley be the minimum of that function let s take a look at the loss function we see in regression we see that this be really a function of two variable β0 and β1 all the rest of the variable be determine since x y and n be give during training we want to try to minimize this function the function be f β0 β1 = z to begin gradient descent you make some guess of the parameter β0 and β1 that minimize the function next you find the partial derivative of the loss function with respect to each beta parameter dz dβ0 dz dβ1 a partial derivative indicate how much total loss be increase or decrease if you increase β0 or β1 by a very small amount put another way how much would increase your estimate of annual income assume zero high education β0 increase the loss I e inaccuracy of your model you want to go in the opposite direction so that you end up walk downhill and minimize loss similarly if you increase your estimate of how much each incremental year of education affect income β1 how much do this increase loss z if the partial derivative dz β1 be a negative number then increase β1 be good because it will reduce total loss if it s a positive number you want to decrease β1 if it s zero don t change β1 because it mean you ve reach an optimum keep do that until you reach the bottom I e the algorithm converge and loss have be minimize there be lot of trick and exceptional case beyond the scope of this series but generally this be how you find the optimal parameter for your parametric model overfitte sherlock your explanation of what just happen be too specific to the situation regularization don t overcomplicate thing sherlock I ll punch you for every extra word hyperparameter λ here s the strength with which I will punch you for every extra word a common problem in machine learning be overfitte learn a function that perfectly explain the training datum that the model learn from but doesn t generalize well to unseen test datum overfitting happen when a model overlearn from the training datum to the point that it start pick up idiosyncrasy that aren t representative of pattern in the real world this become especially problematic as you make your model increasingly complex underfitting be a related issue where your model be not complex enough to capture the underlying trend in the datum remember that the only thing we care about be how the model perform on test datum you want to predict which email will be mark as spam before they re mark not just build a model that be 100 % accurate at reclassify the email it use to build itself in the first place hindsight be 20 20 — the real question be whether the lesson learn will help in the future the model on the right have zero loss for the training datum because it perfectly fit every datum point but the lesson doesn t generalize it would do a horrible job at explain a new data point that isn t yet on the line two way to combat overfitte 1 use more training datum the more you have the hard it be to overfit the datum by learn too much from any single training example 2 use regularization add in a penalty in the loss function for build a model that assign too much explanatory power to any one feature or allow too many feature to be take into account the first piece of the sum above be our normal cost function the second piece be a regularization term that add a penalty for large beta coefficient that give too much explanatory power to any specific feature with these two element in place the cost function now balance between two priority explain the training datum and prevent that explanation from become overly specific the lambda coefficient of the regularization term in the cost function be a hyperparameter a general setting of your model that can be increase or decrease I e tune in order to improve performance a high lambda value will more harshly penalize large beta coefficient that could lead to potential overfitting to decide the good value of lambda you d use a method call cross validation which involve hold out a portion of the training datum during training and then see how well your model explain the hold out portion we ll go over this in more depth here s what we cover in this section in the next section — part 2 2 supervise learning ii — we ll talk about two foundational method of classification logistic regression and support vector machine for a more thorough treatment of linear regression read chapter 1 3 of an introduction to statistical learning the book be available for free online and be an excellent resource for understand machine learning concept with accompany exercise for more practice to actually implement gradient descent in python check out this tutorial and here be a more mathematically rigorous description of the same concept in practice you ll rarely need to implement gradient descent from scratch but understand how it work behind the scene will allow you to use it more effectively and understand why thing break when they do more from machine learning for human 🤖 👶 from a quick cheer to a stand ovation clap to show how much you enjoy this story research comms @deepmindai previously @upstart @yale @trueventurestec demystify artificial intelligence & machine learning discussion on safe and intentional application of ai for positive social impact
Arvind N,9.5K,8,https://towardsdatascience.com/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153?source=tag_archive---------8----------------,thought after take the deeplearning ai course towards data science,update — feb 2nd 2018 when this blog post be write only 3 course have be release all 5 course in this specialization be now out I will have a follow up blog post soon between a full time job and a toddler at home I spend my spare time learn about the idea in cognitive science & ai once in a while a great paper video course come out and you re instantly hook andrew ng s new deeplearning ai course be like that shane carruth or rajnikanth movie that one yearn for naturally as soon as the course be release on coursera I register and spend the past 4 evening binge watch the lecture work through quiz and programming assignment dl practitioner and ml engineer typically spend most day work at an abstract kera or tensorflow level but it s nice to take a break once in a while to get down to the nut and bolt of learn algorithm and actually do back propagation by hand it be both fun and incredibly useful andrew ng s new adventure be a bottom up approach to teach neural network — powerful non linearity learning algorithm at a beginner mid level in classic ng style the course be deliver through a carefully choose curriculum neatly time video and precisely positioned information nugget andrew pick up from where his classic ml course leave off and introduce the idea of neural network use a single neuron logistic regression and slowly add complexity — more neuron and layer by the end of the 4 week course 1 a student be introduce to all the core idea require to build a dense neural network such as cost loss function learn iteratively use gradient descent and vectorize parallel python numpy implementation andrew patiently explain the requisite math and programming concept in a carefully plan order and a well regulated pace suitable for learner who could be rusty in math code lecture be deliver use presentation slide on which andrew write use digital pen it feel like an effective way to get the listener to focus I feel comfortable watch video at 1 25x or 1 5x speed quiz be place at the end of each lecture section and be in the multiple choice question format if you watch the video once you should be able to quickly answer all the quiz question you can attempt quiz multiple time and the system be design to keep your high score programming assignment be do via jupyter notebook — powerful browser base application assignment have a nice guide sequential structure and you be not require to write more than 2 3 line of code in each section if you understand the concept like vectorization intuitively you can complete most programming section with just 1 line of code after the assignment be code it take 1 button click to submit your code to the automate grading system which return your score in a few minute some assignment have time restriction — say three attempt in 8 hour etc jupyter notebook be well design and work without any issue instruction be precise and it feel like a polished product anyone interested in understand what neural network be how they work how to build they and the tool available to bring your idea to life if your math be rusty there be no need to worry — andrew explain all the require calculus and provide derivative at every occasion so that you can focus on build the network and concentrate on implement your idea in code if your programming be rusty there be a nice code assignment to teach you numpy but I recommend learn python first on codecademy let I explain this with an analogy assume you be try to learn how to drive a car jeremy s fast ai course put you in the driver seat from the get go he teach you to move the steering wheel press the brake accelerator etc then he slowly explain more detail about how the car work — why rotate the wheel make the car turn why press the brake pedal make you slow down and stop etc he keep get deep into the inner working of the car and by the end of the course you know how the internal combustion engine work how the fuel tank be design etc the goal of the course be to get you drive you can choose to stop at any point after you can drive reasonably well — there be no need to learn how to build repair the car andrew s dl course do all of this but in the complete opposite order he teach you about internal combustion engine first he keep add layer of abstraction and by the end of the course you be drive like an f1 racer the fast ai course mainly teach you the art of driving while andrew s course primarily teach you the engineering behind the car if you have not do any machine learning before this don t take this course first the good starting point be andrew s original ml course on coursera after you complete that course please try to complete part 1 of jeremy howard s excellent deep learning course jeremy teach deep learning top down which be essential for absolute beginner once you be comfortable create deep neural network it make sense to take this new deeplearning ai course specialization which fill up any gap in your understanding of the underlie detail and concept 2 andrew stress on the engineering aspect of deep learning and provide plenty of practical tip to save time and money — the third course in the dl specialization feel incredibly useful for my role as an architect lead engineering team 3 jargon be handle well andrew explain that an empirical process = trial & error — he be brutally honest about the reality of designing and train deep net at some point I feel he might have as well just call deep learning as glorify curve fit 4 squash all hype around dl and ai — andrew make restrain careful comment about proliferation of ai hype in the mainstream medium and by the end of the course it be pretty clear that dl be nothing like the terminator 5 wonderful boilerplate code that just work out of the box 6 excellent course structure 7 nice consistent and useful notation andrew strive to establish a fresh nomenclature for neural net and I feel he could be quite successful in this endeavor 8 style of teaching that be unique to andrew and carry over from ml — I could feel the same excitement I feel in 2013 when I take his original ml course 9 the interview with deep learning hero be refreshing — it be motivate and fun to hear personal story and anecdote I wish that he d say concretely more often 2 good tool be important and will help you accelerate your learning pace I buy a digital pen after see andrew teach with one it help I work more efficiently 3 there be a psychological reason why I recommend the fast ai course before this one once you find your passion you can learn uninhibite 4 you just get that dopamine rush each time you score full point 5 don t be scare by dl jargon hyperparameter = setting architecture topology = style etc or the math symbol if you take a leap of faith and pay attention to the lecture andrew show why the symbol and notation be actually quite useful they will soon become your tool of choice and you will wield they with style thank for reading and good wish update thank for the overwhelmingly positive response many people be ask I to explain gradient descent and the differential calculus I hope this help from a quick cheer to a stand ovation clap to show how much you enjoy this story interest in strong ai share concept idea and code
Blaise Aguera y Arcas,8.7K,15,https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477?source=tag_archive---------0----------------,do algorithm reveal sexual orientation or just expose our stereotype,by blaise agüera y arcas alexander todorov and margaret mitchell a study claim that artificial intelligence can infer sexual orientation from facial image cause a medium uproar in the fall of 2017 the economist feature this work on the cover of their september 9th magazine ; on the other hand two major lgbtq organization the human right campaign and glaad immediately label it junk science michal kosinski who co author the study with fellow researcher yilun wang initially express surprise call the critique knee jerk reaction however he then proceed to make even bolder claim that such ai algorithm will soon be able to measure the intelligence political orientation and criminal inclination of people from their facial image alone kosinski s controversial claim be nothing new last year two computer scientist from china post a non peer review paper online in which they argue that their ai algorithm correctly categorize criminal with nearly 90 % accuracy from a government i d photo alone technology startup have also begin to crop up claim that they can profile people s character from their facial image these development have prompt the three of we to collaborate early in the year on a medium essay physiognomy s new clothe to confront claim that ai face recognition reveal deep character trait we describe how the junk science of physiognomy have root go back into antiquity with practitioner in every era resurrect belief base on prejudice use the new methodology of the age in the 19th century this include anthropology and psychology ; in the 20th genetic and statistical analysis ; and in the 21st artificial intelligence in late 2016 the paper motivate our physiognomy essay seem well outside the mainstream in tech and academia but as in other area of discourse what recently feel like a fringe position must now be address head on kosinski be a faculty member of stanford s graduate school of business and this new study have be accept for publication in the respected journal of personality and social psychology much of the ensue scrutiny have focus on ethic implicitly assume that the science be valid we will focus on the science the author train and test their sexual orientation detector use 35 326 image from public profile on a us date website composite image of the lesbian gay and straight man and woman in the sample reveal a great deal about the information available to the algorithm clearly there be difference between these four composite face wang and kosinski assert that the key difference be in physiognomy mean that a sexual orientation tend to go along with a characteristic facial structure however we can immediately see that some of these difference be more superficial for example the average straight woman appear to wear eyeshadow while the average lesbian do not glass be clearly visible on the gay man and to a less extent on the lesbian while they seem absent in the heterosexual composite might it be the case that the algorithm s ability to detect orientation have little to do with facial structure but be due rather to pattern in groom presentation and lifestyle we conduct a survey of 8 000 american use amazon s mechanical turk crowdsourcing platform to see if we could independently confirm these pattern ask 77 yes no question such as do you wear eyeshadow do you wear glass and do you have a beard as well as question about gender and sexual orientation the result show that lesbian indeed use eyeshadow much less than straight woman do gay man and woman do both wear glass more and young opposite sex attract man be considerably more likely to have prominent facial hair than their gay or same sex attract peer break down the answer by the age of the respondent can provide a rich and clear view of the datum than any single statistic in the follow figure we show the proportion of woman who answer yes to do you ever use makeup top and do you wear eyeshadow bottom average over 6 year age interval the blue curve represent strictly opposite sex attract woman a nearly identical set to those who answer yes to be you heterosexual or straight ; the cyan curve represent woman who answer yes to either or both of be you sexually attract to woman and be you romantically attract to woman ; and the red curve represent woman who answer yes to be you homosexual gay or lesbian 1 the shaded region around each curve show 68 % confidence interval 2 the pattern reveal here be intuitive ; it win t be break news to most that straight woman tend to wear more makeup and eyeshadow than same sex attract and even more so lesbian identify woman on the other hand these curve also show we how often these stereotype be violate that same sex attract man of most age wear glass significantly more than exclusively opposite sex attract man do might be a bit less obvious but this trend be equally clear 3 a proponent of physiognomy might be tempt to guess that this be somehow relate to difference in visual acuity between these population of man however ask the question do you like how you look in glass reveal that this be likely more of a stylistic choice same sex attract woman also report wear glass more as well as like how they look in glass more across a range of age one can also see how opposite sex attract woman under the age of 40 wear contact lense significantly more than same sex attract woman despite report that they have a vision defect at roughly the same rate far illustrate how the difference be drive by an aesthetic preference 4 similar analysis show that young same sex attract man be much less likely to have hairy face than opposite sex attract man serious facial hair in our plot be define as answer yes to have a goatee beard or moustache but no to stubble overall opposite sex attract man in our sample be 35 % more likely to have serious facial hair than same sex attract man and for man under the age of 31 who be overrepresente on date website this rise to 75 % wang and kosinski speculate in their paper that the faintness of the beard and moustache in their gay male composite might be connect with prenatal underexposure to androgen male hormone result in a feminize effect hence sparser facial hair the fact that we see a cohort of same sex attract man in their 40 who have just as much facial hair as opposite sex attract man suggest a different story in which fashion trend and cultural norm play the dominant role in choice about facial hair among man not differ exposure to hormone early in development the author of the paper additionally note that the heterosexual male composite appear to have dark skin than the other three composite our survey confirm that opposite sex attract man consistently self report have a tan face yes to be your face tan slightly more often than same sex attract man once again wang and kosinski reach for a hormonal explanation writing while the brightness of the facial image might be drive by many factor previous research find that testosterone stimulate melanocyte structure and function lead to a dark skin however a simple answer be suggest by the response to the question do you work outdoors overall opposite sex attract man be 29 % more likely to work outdoors and among man under 31 this rise to 39 % previous research have find that increase exposure to sunlight lead to dark skin 5 none of these result prove that there be no physiological basis for sexual orientation ; in fact ample evidence show we that orientation run much deep than a choice or a lifestyle in a critique aim in part at fraudulent conversion therapy program united states surgeon general david satcher write in a 2001 report sexual orientation be usually determine by adolescence if not early and there be no valid scientific evidence that sexual orientation can be change it follow that if we dig deeply enough into human physiology and neuroscience we will eventually find reliable correlate and maybe even the origin of sexual orientation in our survey we also find some evidence of outwardly visible correlate of orientation that be not cultural perhaps most strikingly very tall woman be overrepresente among lesbian identify respondent 6 however while this be interesting it s very far from a good predictor of woman s sexual orientation makeup and eyeshadow do much well the way wang and kosinski measure the efficacy of their ai gaydar be equivalent to choose a straight and a gay or lesbian face image both from datum hold out during the training process and ask how often the algorithm correctly guess which be which 50 % performance would be no well than random chance for woman guess that the taller of the two be the lesbian achieve only 51 % accuracy — barely above random chance this be because despite the statistically meaningful overrepresentation of tall woman among the lesbian population the great majority of lesbian be not unusually tall by contrast the performance measure in the paper 81 % for gay man and 71 % for lesbian woman seem impressive 7 consider however that we can achieve comparable result with trivial model base only on a handful of yes no survey question about presentation for example for pair of woman one of whom be lesbian the follow not exactly superhuman algorithm be on average 63 % accurate if neither or both woman wear eyeshadow flip a coin ; otherwise guess that the one who wear eyeshadow be straight and the other lesbian add six more yes no question about presentation do you ever use makeup do you have long hair do you have short hair do you ever use colored lipstick do you like how you look in glass and do you work outdoors as additional signal raise the performance to 70 % 8 give how many more detail about presentation be available in a face image 71 % performance no long seem so impressive several study include a recent one in the journal of sex research have show that human judge gaydar be no more reliable than a coin flip when the judgement be base on picture take under well control condition head pose lighting glass makeup etc it s well than chance if these variable be not control for because a person s presentation — especially if that person be out — involve social signal we signal our orientation and many other kind of status presumably in order to attract the kind of attention we want and to fit in with people like we 9 wang and kosinski argue against this interpretation on the ground that their algorithm work on facebook selfie of openly gay man as well as date website selfie the issue however be not whether the image come from a date website or facebook but whether they be self post or take under standardized condition most people present themselves in way that have be calibrate over many year of medium consumption observe other look in the mirror and gauge social reaction in one of the early gaydar study use social medium participant could categorize gay man with about 58 % accuracy ; but when the researcher use facebook image of gay and heterosexual man post by their friend still far from a perfect control the accuracy drop to 52 % if subtle bias in image quality expression and grooming can be pick up on by human these bias can also be detect by an ai algorithm while wang and kosinski acknowledge grooming and style they believe that the chief difference between their composite image relate to face shape argue that gay man s face be more feminine narrow jaw long nose large forehead while lesbian face be more masculine large jaw shorter nose small forehead as with less facial hair on gay man and dark skin on straight man they suggest that the mechanism be gender atypical hormonal exposure during development this echo a widely discredit 19th century model of homosexuality sexual inversion more likely heterosexual man tend to take selfie from slightly below which will have the apparent effect of enlarge the chin shorten the nose shrink the forehead and attenuate the smile see our selfie below this view emphasize dominance — or perhaps more benignly an expectation that the viewer will be short on the other hand as a wedding photographer note in her blog when you shoot from above your eye look big which be generally attractive — especially for woman this may be a heteronormative assessment when a face be photograph from below the nostril be prominent while high shooting angle de emphasize and eventually conceal they altogether look again at the composite image we can see that the heterosexual male face have more pronounced dark spot corresponding to the nostril than the gay male while the opposite be true for the female face this be consistent with a pattern of heterosexual man on average shooting from below heterosexual woman from above as the wedding photographer suggest and gay man and lesbian woman from directly in front a similar pattern be evident in the eyebrow shoot from above make they look more v shape but their apparent shape become flatter and eventually caret shape ^ as the camera be lower shoot from below also make the outer corner of the eye appear low in short the change in the average position of facial landmark be consistent with what we would expect to see from differ selfie angle the ambiguity between shoot angle and the real physical size of facial feature be hard to fully disentangle from a two dimensional image both for a human viewer and for an algorithm although the author be use face recognition technology design to try to cancel out all effect of head pose lighting grooming and other variable not intrinsic to the face we can confirm that this doesn t work perfectly ; that s why multiple distinct image of a person help when group photo by subject in google photo and why a person may initially appear in more than one group tom white a researcher at victoria university in new zealand have experiment with the same facial recognition engine kosinski and wang use vgg face and have find that its output vary systematically base on variable like smile and head pose when he train a classifier base on vgg face s output to distinguish a happy expression from a neutral one it get the answer right 92 % of the time — which be significant give that the heterosexual female composite have a much more pronounced smile change in head pose might be even more reliably detectable ; for 576 test image a classifier be able to pick out the one face to the right with 100 % accuracy in summary we have show how the obvious difference between lesbian or gay and straight face in selfie relate to groom presentation and lifestyle — that be difference in culture not in facial structure these difference include we ve demonstrate that just a handful of yes no question about these variable can do nearly as good a job at guess orientation as supposedly sophisticated facial recognition ai far the current generation of facial recognition remain sensitive to head pose and facial expression therefore — at least at this point — it s hard to credit the notion that this ai be in some way superhuman at out we base on subtle but unalterable detail of our facial structure this doesn t negate the privacy concern the author and various commentator have raise but it emphasize that such concern relate less to ai per se than to mass surveillance which be troubling regardless of the technology use even when as in the day of the stasi in east germany these be nothing but paper file and audiotape like computer or the internal combustion engine ai be a general purpose technology that can be use to automate a great many task include one that should not be undertake in the first place we be hopeful about the confluence of new powerful ai technology with social science but not because we believe in revive the 19th century research program of infer people s inner character from their outer appearance rather we believe ai be an essential tool for understand pattern in human culture and behavior it can expose stereotype inherent in everyday language it can reveal uncomfortable truth as in google s work with the geena davis institute where our face gender classifier establish that man be see and hear nearly twice as often as woman in hollywood movie yet female lead film outperform other at the box office make social progress and hold ourselves to account be more difficult without such hard evidence even when it only confirm our suspicion two of us margaret mitchell and blaise agüera y arca be research scientist specialize in machine learning and ai at google ; agüera y arcas lead a team that include deep learning apply to face recognition and power face group in google photo alex todorov be a professor in the psychology department at princeton where he direct the social perception lab he be the author of face value the irresistible influence of first impression 1 this wording be base on several large national survey which we be able to use to sanity check our number about 6 % of respondent identify as homosexual gay or lesbian and 85 % as heterosexual about 4 % of all gender be exclusively same sex attract of the man 10 % be either sexually or romantically same sex attract and of the woman 20 % just under 1 % of respondent be trans and about 2 % identify with both or neither of the pronoun she and he these number be broadly consistent with other survey especially when consider as a function of age the mechanical turk population skew somewhat young than the overall population of the us and consistent with other study our datum show that young people be far more likely to identify non heteronormatively 2 these be wide for same sex attract and lesbian woman because they be minority population result in a large sampling error the same hold for old people in our sample 3 for the remainder of the plot we stick to opposite sex attract and same sex attract as the count be high and the error bar therefore small ; these category be also somewhat less culturally freight since they rely on question about attraction rather than identity as with eyeshadow and makeup the effect be similar and often even large when compare heterosexual identifying with lesbian or gay identify people 4 although we didn t test this explicitly slightly different rate of laser correction surgery seem a likely cause of the small but grow disparity between opposite sex attract and same sex attract woman who answer yes to the vision defect question as they age 5 this finding may prompt the further question why do more opposite sex attract man work outdoors this be not address by any of our survey question but hopefully the other evidence present here will discourage an essentialist assumption such as straight man be just more outdoorsy without the evidence of a control study that can support the leap from correlation to cause such explanation be a form of logical fallacy sometimes call a just so story an unverifiable narrative explanation for a cultural practice 6 of the 253 lesbian identify woman in the sample 5 or 2 % be over six foot and 25 or 10 % be over 5 9 out of 3 333 heterosexual woman woman who answer yes to be you heterosexual or straight only 16 or 0 5 % be over six foot and 152 or 5 % be over 5 9 7 they note that these figure rise to 91 % for man and 83 % for woman if 5 image be consider 8 these result be base on the simple possible machine learning technique a linear classifier the classifier be train on a randomly choose 70 % of the datum with the remain 30 % of the datum hold out for test over 500 repetition of this procedure the error be 69 53 % ± 2 98 % with the same number of repetition and holdout base the decision on height alone give an error of 51 08 % ± 3 27 % and base it on eyeshadow alone yield 62 96 % ± 2 39 % 9 a longstanding body of work e g goffman s the presentation of self in everyday life 1959 and jones and pittman s toward a general theory of strategic self presentation 1982 delf more deeply into why we present ourselves the way we do both for instrumental reason status power attraction and because our presentation inform and be inform by how we conceive of our social self from a quick cheer to a stand ovation clap to show how much you enjoy this story blaise aguera y arcas lead google s ai group in seattle he found seadragon and be one of the creator of photosynth at microsoft
David Foster,12.8K,11,https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188?source=tag_archive---------1----------------,how to build your own alphazero ai use python and kera,in this article I ll attempt to cover three thing in march 2016 deepmind s alphago beat 18 time world champion go player lee sedol 4 1 in a series watch by over 200 million people a machine have learn a super human strategy for playing go a feat previously think impossible or at the very least at least a decade away from be accomplish this in itself be a remarkable achievement however on 18th october 2017 deepmind take a giant leap far the paper master the game of go without human knowledge unveil a new variant of the algorithm alphago zero that have defeat alphago 100 0 incredibly it have do so by learn solely through self play start tabula rasa blank state and gradually find strategy that would beat previous incarnation of itself no long be a database of human expert game require to build a super human ai a mere 48 day later on 5th december 2017 deepmind release another paper master chess and shogi by self play with a general reinforcement learning algorithm show how alphago zero could be adapt to beat the world champion program stockfish and elmo at chess and shogi the entire learning process from be show the game for the first time to become the good computer program in the world have take under 24 hour with this alphazero be bear — the general algorithm for get good at something quickly without any prior knowledge of human expert strategy there be two amazing thing about this achievement it can not be overstate how important this be this mean that the underlie methodology of alphago zero can be apply to any game with perfect information the game state be fully know to both player at all time because no prior expertise be require beyond the rule of the game this be how it be possible for deepmind to publish the chess and shogi paper only 48 day after the original alphago zero paper quite literally all that need to change be the input file that describe the mechanic of the game and to tweak the hyper parameter relate to the neural network and monte carlo tree search if alphazero use super complex algorithm that only a handful of people in the world understand it would still be an incredible achievement what make it extraordinary be that a lot of the idea in the paper be actually far less complex than previous version at its heart lie the follow beautifully simple mantra for learn doesn t that sound a lot like how you learn to play game when you play a bad move it s either because you misjudge the future value of result position or you misjudge the likelihood that your opponent would play a certain move so didn t think to explore that possibility these be exactly the two aspect of gameplay that alphazero be train to learn firstly check out the alphago zero cheat sheet for a high level understanding of how alphago zero work it s worth have that to refer to as we walk through each part of the code there s also a great article here that explain how alphazero work in more detail clone this git repository which contain the code I ll be reference to start the learning process run the top two panel in the run ipynb jupyter notebook once it s build up enough game position to fill its memory the neural network will begin train through additional self play and train it will gradually get well at predict the game value and next move from any position result in well decision making and smart overall play we ll now have a look at the code in more detail and show some result that demonstrate the ai get strong over time n b — this be my own understanding of how alphazero work base on the information available in the paper reference above if any of the below be incorrect apology and I ll endeavour to correct it the game that our algorithm will learn to play be connect4 or four in a row not quite as complex as go but there be still 4 531 985 219 092 game position in total the game rule be straightforward player take it in turn to enter a piece of their colour in the top of any available column the first player to get four of their colour in a row — each vertically horizontally or diagonally win if the entire grid be fill without a four in a row be create the game be draw here s a summary of the key file that make up the codebase this file contain the game rule for connect4 each square be allocate a number from 0 to 41 as follow the game py file give the logic behind move from one game state to another give a choose action for example give the empty board and action 38 the takeaction method return a new game state with the start player s piece at the bottom of the centre column you can replace the game py file with any game file that conform to the same api and the algorithm will in principal learn strategy through self play base on the rule you have give it this contain the code that start the learning process it load the game rule and then iterate through the main loop of the algorithm which consist of three stage there be two agent involve in this loop the best_player and the current_player the best_player contain the good perform neural network and be use to generate the self play memory the current_player then retrain its neural network on these memory and be then pitch against the best_player if it win the neural network inside the best_player be switch for the neural network inside the current_player and the loop start again this contain the agent class a player in the game each player be initialise with its own neural network and monte carlo search tree the simulate method run the monte carlo tree search process specifically the agent move to a leaf node of the tree evaluate the node with its neural network and then backfill the value of the node up through the tree the act method repeat the simulation multiple time to understand which move from the current position be most favourable it then return the choose action to the game to enact the move the replay method retrain the neural network use memory from previous game this file contain the residual_cnn class which define how to build an instance of the neural network it use a condensed version of the neural network architecture in the alphagozero paper — I e a convolutional layer follow by many residual layer then split into a value and policy head the depth and number of convolutional filter can be specify in the config file the keras library be use to build the network with a backend of tensorflow to view individual convolutional filter and densely connect layer in the neural network run the following inside the the run ipynb notebook this contain the node edge and mct class that constitute a monte carlo search tree the mct class contain the movetoleaf and backfill method previously mention and instance of the edge class store the statistic about each potential move this be where you set the key parameter that influence the algorithm adjust these variable will affect that run time neural network accuracy and overall success of the algorithm the above parameter produce a high quality connect4 player but take a long time to do so to speed the algorithm up try the follow parameter instead contain the playmatche and playmatchesbetweenversion function that play match between two agent to play against your creation run the follow code it s also in the run ipynb notebook when you run the algorithm all model and memory file be save in the run folder in the root directory to restart the algorithm from this checkpoint later transfer the run folder to the run_archive folder attach a run number to the folder name then enter the run number model version number and memory version number into the initialise py file corresponding to the location of the relevant file in the run_archive folder run the algorithm as usual will then start from this checkpoint an instance of the memory class store the memory of previous game that the algorithm use to retrain the neural network of the current_player this file contain a custom loss function that mask prediction from illegal move before pass to the cross entropy loss function the location of the run and run_archive folder log file be save to the log folder inside the run folder to turn on log set the value of the logger_disable variable to false inside this file view the log file will help you to understand how the algorithm work and see inside its mind for example here be a sample from the logg mct file equally from the logg tourney file you can see the probability attach to each move during the evaluation phase training over a couple of day produce the follow chart of loss against mini batch iteration number the top line be the error in the policy head the cross entropy of the mct move probability against the output from the neural network the bottom line be the error in the value head the mean squared error between the actual game value and the neural network predict of the value the middle line be an average of the two clearly the neural network be get well at predict the value of each game state and the likely next move to show how this result in strong and strong play I run a league between 17 player range from the 1st iteration of the neural network up to the 49th each pairing play twice with both player have a chance to play first here be the final standing clearly the later version of the neural network be superior to the early version win most of their game it also appear that the learn hasn t yet saturate — with further training time the player would continue to get strong learning more and more intricate strategy as an example one clear strategy that the neural network have favour over time be grab the centre column early observe the difference between the first version of the algorithm and say the 30th version 1st neural network version 30th neural network version this be a good strategy as many line require the centre column — claim this early ensure your opponent can not take advantage of this this have be learn by the neural network without any human input there be a game py file for a game call metasquare in the game folder this involve place x and o marker in a grid to try to form square of different size large square score more point than small square and the player with the most point when the grid be full win if you switch the connect4 game py file for the metasquare game py file the same algorithm will learn how to play metasquare instead hopefully you find this article useful — let I know in the comment below if you find any typo or have question about anything in the codebase or article and I ll get back to you as soon as possible if you would like to learn more about how our company apply data science develop innovative datum science solution for business feel free to get in touch through our website or directly through linkedin and if you like this feel free to leave a few hearty clap apply datum science be a london base consultancy that implement end to end datum science solution for business deliver measurable value if you re look to do more with your datum let s talk from a quick cheer to a stand ovation clap to show how much you enjoy this story co founder of apply data science cut edge data science news and project
Aman Agarwal,7K,24,https://medium.freecodecamp.org/explained-simply-how-an-ai-program-mastered-the-ancient-game-of-go-62b8940a9080?source=tag_archive---------2----------------,explain simply how an ai program master the ancient game of go,this be about alphago google deepmind s go play ai that shake the technology world in 2016 by defeat one of the good player in the world lee sedol go be an ancient board game which have so many possible move at each step that future position be hard to predict — and therefore it require strong intuition and abstract thinking to play because of this reason it be believe that only human could be good at play go most researcher think that it would still take decade to build an ai which could think like that in fact I m release this essay today because this week march 8 15 mark the two year anniversary of the alphago vs sedol match but alphago didn t stop there 8 month later it play 60 professional game on a go website under disguise as a player name master and win every single game against dozen of world champion of course without rest between game naturally this be a huge achievement in the field of ai and spark worldwide discussion about whether we should be excited or worry about artificial intelligence today we be go to take the original research paper publish by deepmind in the nature journal and break it down paragraph by paragraph use simple english after this essay you ll know very clearly what alphago be and how it work I also hope that after read this you will not believe all the news headline make by journalist to scare you about ai and instead feel excited about it worry about the grow achievement of ai be like worry about the grow ability of microsoft powerpoint yes it will get well with time with new feature be add to it but it can t just uncontrollably grow into some kind of hollywood monster you don t need to know how to play go to understand this paper in fact I myself have only read the first 3 4 line in wikipedia s opening paragraph about it instead surprisingly I use some example from basic chess to explain the algorithm you just have to know what a 2 player board game be in which each player take turn and there be one winner at the end beyond that you don t need to know any physics or advanced math or anything this will make it more approachable for people who only just now start learn about machine learning or neural network and especially for those who don t use english as their first language which can make it very difficult to read such paper if you have no prior knowledge of ai and neural network you can read the deep learning section of one of my previous essay here after read that you ll be able to get through this essay if you want to get a shallow understanding of reinforcement learn too optional reading you can find it here here s the original paper if you want to try read it as for I hi I m aman an ai and autonomous robot engineer I hope that my work will save you a lot of time and effort if you be to study this on your own do you speak japanese ryohji ikebe have kindly write a brief memo about this essay in japanese in a series of tweet as you know the goal of this research be to train an ai program to play go at the level of world class professional human player to understand this challenge let I first talk about something similar do for chess in the early 1990s ibm come out with the deep blue computer which defeat the great champion gary kasparov in chess he s also a very cool guy make sure to read more about he later how do deep blue play well it use a very brute force method at each step of the game it take a look at all the possible legal move that could be play and go ahead to explore each and every move to see what would happen and it would keep explore move after move for a while form a kind of huge decision tree of thousand of move and then it would come back along that tree observe which move seem most likely to bring a good result but what do we mean by good result well deep blue have many carefully design chess strategy build into it by expert chess player to help it make well decision — for example how to decide whether to protect the king or get advantage somewhere else they make a specific evaluation algorithm for this purpose to compare how advantageous or disadvantageous different board position be ibm hard code expert chess strategy into this evaluation function and finally it choose a carefully calculate move on the next turn it basically go through the whole thing again as you can see this mean deep blue think about million of theoretical position before play each move this be not so impressive in term of the ai software of deep blue but rather in the hardware — ibm claim it to be one of the most powerful computer available in the market at that time it could look at 200 million board position per second now we come to go just believe I that this game be much more open end and if you try the deep blue strategy on go you wouldn t be able to play well there would be so many position to look at at each step that it would simply be impractical for a computer to go through that hell for example at the opening move in chess there be 20 possible move in go the first player have 361 possible move and this scope of choice stay wide throughout the game this be what they mean by enormous search space moreover in go it s not so easy to judge how advantageous or disadvantageous a particular board position be at any specific point in the game — you kinda have to play the whole game for a while before you can determine who be win but let s say you magically have a way to do both of these and that s where deep learning come in so in this research deepmind use neural network to do both of these task if you haven t read about they yet here s the link again they train a policy neural network to decide which be the most sensible move in a particular board position so it s like follow an intuitive strategy to pick move from any position and they train a value neural network to estimate how advantageous a particular board arrangement be for the player or in other word how likely you be to win the game from this position they train these neural network first with human game example your good old ordinary supervised learning after this the ai be able to mimic human playing to a certain degree so it act like a weak human player and then to train the network even far they make the ai play against itself million of time this be the reinforcement learn part with this the ai get well because it have more practice with these two network alone deepmind s ai be able to play well against state of the art go play program that other researcher have build before these other program have use an already popular pre exist game play algorithm call the monte carlo tree search mct more about this later but guess what we still haven t talk about the real deal deepmind s ai isn t just about the policy and value network it doesn t use these two network as a replacement of the monte carlo tree search instead it use the neural network to make the mct algorithm work well and it get so much well that it reach superhuman level this improved variation of mct be alphago the ai that beat lee sedol and go down in ai history as one of the great breakthrough ever so essentially alphago be simply an improve implementation of a very ordinary computer science algorithm do you understand now why ai in its current form be absolutely nothing to be scared of wow we ve spend a lot of time on the abstract alone alright — to understand the paper from this point on first we ll talk about a gaming strategy call the monte carlo tree search algorithm for now I ll just explain this algorithm at enough depth to make sense of this essay but if you want to learn about it in depth some smart people have also make excellent video and blog post on this 1 a short video series from udacity2 jeff bradberry s explanation of mcts3 an mct tutorial by fullstack academy the follow section be long but easy to understand I ll try my good and very important so stay with I the rest of the essay will go much quick let s talk about the first paragraph of the essay above remember what I say about deep blue make a huge tree of million of board position and move at each step of the game you have to do simulation and look at and compare each and every possible move as I say before that be a simple approach and very straightforward approach — if the average software engineer have to design a game playing ai and have all the strong computer of the world he or she would probably design a similar solution but let s think about how do human themselves play chess let s say you re at a particular board position in the middle of the game by game rule you can do a dozen different thing — move this pawn here move the queen two square here or three square there and so on but do you really make a list of all the possible move you can make with all your piece and then select one move from this long list no — you intuitively narrow down to a few key move let s say you come up with 3 sensible move that you think make sense and then you wonder what will happen in the game if you choose one of these 3 move you might spend 15 20 second consider each of these 3 move and their future — and note that during these 15 second you don t have to carefully plan out the future of each move ; you can just roll out a few mental move guide by your intuition without too much careful thought well a good player would think far and more deeply than an average player this be because you have limit time and you can t accurately predict what your opponent will do at each step in that lovely future you re cook up in your brain so you ll just have to let your gut feeling guide you I ll refer to this part of the thinking process as rollout so take note of it so after roll out your few sensible move you finally say screw it and just play the move you find well then the opponent make a move it might be a move you have already well anticipate which mean you be now pretty confident about what you need to do next you don t have to spend too much time on the rollout again or it could be that your opponent hit you with a pretty cool move that you have not expect so you have to be even more careful with your next move this be how the game carry on and as it get close and close to the finishing point it would get easy for you to predict the outcome of your move — so your rollout don t take as much time the purpose of this long story be to describe what the mct algorithm do on a superficial level — it mimic the above thinking process by build a search tree of move and position every time again for more detail you should check out the link I mention early the innovation here be that instead of go through all the possible move at each position which deep blue do it instead intelligently select a small set of sensible move and explore those instead to explore they it roll out the future of each of these move and compare they base on their imagine outcome seriously — this be all I think you need to understand this essay now — come back to the screenshot from the paper go be a perfect information game please read the definition in the link don t worry it s not scary and theoretically for such game no matter which particular position you be at in the game even if you have just play 1 2 move it be possible that you can correctly guess who will win or lose assume that both player play perfectly from that point on I have no idea who come up with this theory but it be a fundamental assumption in this research project and it work so that mean give a state of the game s there be a function v * s which can predict the outcome let s say probability of you win this game from 0 to 1 they call it the optimal value function because some board position be more likely to result in you win than other board position they can be consider more valuable than the other let I say it again value = probability between 0 and 1 of you win the game but wait — say there be a girl name foma sit next to you while you play chess and she keep tell you at each step if you re win or lose you re win you re lose nope still lose I think it wouldn t help you much in choose which move you need to make she would also be quite annoying what would instead help you be if you draw the whole tree of all the possible move you can make and the state that those move would lead to — and then foma would tell you for the entire tree which state be win state and which state be lose state then you can choose move which will keep lead you to win state all of a sudden foma be your partner in crime not an annoying friend here foma behave as your optimal value function v * s early it be believe that it s not possible to have an accurate value function like foma for the game of go because the game have so much uncertainty but — even if you have the wonderful foma this wonderland strategy of draw out all the possible position for foma to evaluate will not work very well in the real world in a game like chess or go as we say before if you try to imagine even 7 8 move into the future there can be so many possible position that you don t have enough time to check all of they with foma so foma be not enough you need to narrow down the list of move to a few sensible move that you can roll out into the future how will your program do that enter lusha lusha be a skilled chess player and enthusiast who have spend decade watch grand master play chess against each other she can look at your board position look quickly at all the available move you can make and tell you how likely it would be that a chess expert would make any of those move if they be sit at your table so if you have 50 possible move at a point lusha will tell you the probability that each move would be pick by an expert of course a few sensible move will have a much high probability and other pointless move will have very little probability she be your policy function p a s for a give state s she can give you probability for all the possible move that an expert would make wow — you can take lusha s help to guide you in how to select a few sensible move and foma will tell you the likelihood of win from each of those move you can choose the move that both foma and lusha approve or if you want to be extra careful you can roll out the move select by lusha have foma evaluate they pick a few of they to roll out far into the future and keep let foma and lusha help you predict very far into the game s future — much quick and more efficient than to go through all the move at each step into the future this be what they mean by reduce the search space use a value function foma to predict outcome and use a policy function lusha to give you grand master probability to help narrow down the move you roll out these be call monte carlo rollout then while you backtrack from future to present you can take average value of all the different move you roll out and pick the most suitable action so far this have only work on a weak amateur level in go because the policy function and value function that they use to guide these rollout weren t that great phew the first line be self explanatory in mct you can start with an unskilled foma and unskille lusha the more you play the well they get at predict solid outcome and move narrow the search to a beam of high probability action be just a sophisticated way of say lusha help you narrow down the move you need to roll out by assign they probability that an expert would play they prior work have use this technique to achieve strong amateur level ai player even with simple or shallow as they call it policy function yeah convolutional neural network be great for image processing and since a neural network take a particular input and give an output it be essentially a function right so you can use a neural network to become a complex function so you can just pass in an image of the board position and let the neural network figure out by itself what s go on this mean it s possible to create neural network which will behave like very accurate policy and value function the rest be pretty self explanatory here we discuss how foma and lusha be train to train the policy network predict for a give position which move expert would pick you simply use example of human game and use they as datum for good old supervised learning and you want to train another slightly different version of this policy network to use for rollout ; this one will be small and fast let s just say that since lusha be so experienced she take some time to process each position she s good to start the narrow down process with but if you try to make she repeat the process she ll still take a little too much time so you train a * fast policy network * for the rollout process I ll call it lusha s young brother jerry I know I know enough with these name after that once you ve train both of the slow and fast policy network enough use human player datum you can try let lusha play against herself on a go board for a few day and get more practice this be the reinforcement learning part — make a well version of the policy network then you train foma for value prediction determine the probability of you win you let the ai practice through play itself again and again in a simulated environment observe the end result each time and learn from its mistake to get well and well I win t go into detail of how these network be train you can read more technical detail in the later section of the paper method which I haven t cover here in fact the real purpose of this particular paper be not to show how they use reinforcement learning on these neural network one of deepmind s previous paper in which they teach ai to play atari game have already discuss some reinforcement learning technique in depth and I ve already write an explanation of that paper here for this paper as I lightly mention in the abstract and also underline in the screenshot above the big innovation be the fact that they use rl with neural network for improve an already popular game play algorithm mct rl be a cool tool in a toolbox that they use to fine tune the policy and value function neural network after the regular supervised training this research paper be about prove how versatile and excellent this tool it be not about teach you how to use it in television lingo the atari paper be a rl infomercial and this alphago paper be a commercial a quick note before you move on would you like to help I write more such essay explain cool research paper if you re serious I d be glad to work with you please leave a comment and I ll get in touch with you so the first step be in train our policy nn lusha to predict which move be likely to be play by an expert this nn s goal be to allow the ai to play similar to an expert human this be a convolutional neural network as I mention before it s a special kind of nn that be very useful in image processing that take in a simplified image of a board arrangement rectifi nonlinearitie be layer that can be add to the network s architecture they give it the ability to learn more complex thing if you ve ever train nns before you might have use the relu layer that s what these be the training datum here be in the form of random pair of board position and the label be the action choose by human when they be in those position just regular supervised learning here they use stochastic gradient ascent well this be an algorithm for backpropagation here you re try to maximise a reward function and the reward function be just the probability of the action predict by a human expert ; you want to increase this probability but hey — you don t really need to think too much about this normally you train the network so that it minimise a loss function which be essentially the error difference between predict outcome and actual label that be call gradient descent in the actual implementation of this research paper they have indeed use the regular gradient descent you can easily find a loss function that behave opposite to the reward function such that minimise this loss will maximise the reward the policy network have 13 layer and be call sl policy network sl = supervise learn the datum come from a I ll just say it s a popular website on which million of people play go how good do this sl policy network perform it be more accurate than what other researcher have do early the rest of the paragraph be quite self explanatory as for the rollout policy you do remember from a few paragraph ago how lusha the sl policy network be slow so it can t integrate well with the mct algorithm and we train another fast version of lusha call jerry who be her young brother well this refer to jerry right here as you can see jerry be just half as accurate as lusha but it s thousand of time fast it will really help get through roll out simulation of the future fast when we apply the mct for this next section you don t * have * to know about reinforcement learning already but then you ll have to assume that whatever I say work if you really want to dig into detail and make sure of everything you might want to read a little about rl first once you have the sl network train in a supervised manner use human player move with the human move datum as I say before you have to let her practice by itself and get well that s what we re do here so you just take the sl policy network save it in a file and make another copy of it then you use reinforcement learning to fine tune it here you make the network play against itself and learn from the outcome but there s a problem in this training style if you only forever practice against one opponent and that opponent be also only practice with you exclusively there s not much of new learning you can do you ll just be train to practice how to beat that one player this be you guess it overfitte your technique play well against one opponent but don t generalize well to other opponent so how do you fix this well every time you fine tune a neural network it become a slightly different kind of player so you can save this version of the neural network in a list of player who all behave slightly differently right great — now while train the neural network you can randomly make it play against many different old and new version of the opponent choose from that list they be version of the same player but they all play slightly differently and the more you train the more player you get to train even more with bingo in this training the only thing guide the training process be the ultimate goal I e win or lose you don t need to specially train the network to do thing like capture more area on the board etc you just give it all the possible legal move it can choose from and say you have to win and this be why rl be so versatile ; it can be use to train policy or value network for any game not just go here they test how accurate this rl policy network be just by itself without any mct algorithm as you would remember this network can directly take a board position and decide how an expert would play it — so you can use it to single handedly play game well the result be that the rl fine tune network win against the sl network that be only train on human move it also win against other strong go playing program must note here that even before train this rl policy network the sl policy network be already well than the state of the art — and now it have far improve and we haven t even come to the other part of the process like the value network do you know that baby penguin can sneeze louder than a dog can bark actually that s not true but I think you d like a little joke here to distract from the scary looking equation above come to the essay again we re do training lusha here now back to foma — remember the optimal value function v * s > that only tell you how likely you be to win in your current board position if both player play perfectly from that point on so obviously to train an nn to become our value function we would need a perfect player which we don t have so we just use our strong player which happen to be our rl policy network it take the current state board state s and output the probability that you will win the game you play a game and get to know the outcome win or loss each of the game state act as a data sample and the outcome of that game act as the label so by play a 50 move game you have 50 datum sample for value prediction lol no this approach be naive you can t use all 50 move from the game and add they to the dataset the training datum set have to be choose carefully to avoid overfitte each move in the game be very similar to the next one because you only move once and that give you a new position right if you take the state at all 50 of those move and add they to the training datum with the same label you basically have lot of kinda duplicate datum and that cause overfitte to prevent this you choose only very distinct look game state so for example instead of all 50 move of a game you only choose 5 of they and add they to the training set deepmind take 30 million position from 30 million different game to reduce any chance of there be duplicate datum and it work now something conceptual here there be two way to evaluate the value of a board position one option be a magical optimal value function like the one you train above the other option be to simply roll out into the future use your current policy lusha and look at the final outcome in this roll out obviously the real game would rarely go by your plan but deepmind compare how both of these option do you can also do a mixture of both these option we will learn about this mix parameter a little bit later so make a mental note of this concept well your single neural network try to approximate the optimal value function be even well than do thousand of mental simulation use a rollout policy foma really kick ass here when they replace the fast rollout policy with the twice as accurate but slow rl policy lusha and do thousand of simulation with that it do well than foma but only slightly well and too slowly so foma be the winner of this competition she have prove that she can t be replace now that we have train the policy and value function we can combine they with mct and give birth to our former world champion destroyer of grand master the breakthrough of a generation weigh two hundred and sixty eight pound one and only alphaaaaa go in this section ideally you should have a slightly deep understanding of the inner working of the mct algorithm but what you have learn so far should be enough to give you a good feel for what s go on here the only thing you should note be how we re use the policy probability and value estimation we combine they during roll out to narrow down the number of move we want to roll out at each step q s a represent the value function and u s a be a store probability for that position I ll explain remember that the policy network use supervise learn to predict expert move and it doesn t just give you most likely move but rather give you probability for each possible move that tell how likely it be to be an expert move this probability can be store for each of those action here they call it prior probability and they obviously use it while select which action to explore so basically to decide whether or not to explore a particular move you consider two thing first by play this move how likely be you to win yes we already have our value network to answer this first question and the second question be how likely be it that an expert would choose this move if a move be super unlikely to be choose by an expert why even waste time consider it this we get from the policy network then let s talk about the mix parameter see come back to it as discuss early to evaluate position you have two option one simply use the value network you have be use to evaluate state all along and two you can try to quickly play a rollout game with your current strategy assume the other player will play similarly and see if you win or lose we see how the value function be well than do rollout in general here they combine both you try give each prediction 50 50 importance or 40 60 or 0 100 and so on if you attach a % of x to the first you ll have to attach 100 x to the second that s what this mix parameter mean you ll see these hit and trial result later in the paper after each roll out you update your search tree with whatever information you gain during the simulation so that your next simulation be more intelligent and at the end of all simulation you just pick the good move interesting insight here remember how the rl fine tune policy nn be well than just the sl human train policy nn but when you put they within the mct algorithm of alphago use the human train nn prove to be a well choice than the fine tune nn but in the case of the value function which you would remember use a strong player to approximate a perfect player training foma use the rl policy work well than train she with the sl policy do all this evaluation take a lot of computing power we really have to bring out the big gun to be able to run these damn program self explanatory lol our program literally blow the pant off of every other program that come before we this go back to that mix parameter again while evaluate position give equal importance to both the value function and the rollout perform well than just use one of they the rest be self explanatory and reveal an interesting insight self explanatory self explanatory but read that red underlined sentence again I hope you can see clearly now that this line right here be pretty much the summary of what this whole research project be all about conclude paragraph let we brag a little more here because we deserve it oh and if you re a scientist or tech company and need some help in explain your science to non technical people for marketing pr or training etc I can help you drop I a message on twitter @mngrwl from a quick cheer to a stand ovation clap to show how much you enjoy this story engineer teacher learner of foreign language lover of history cinema and art our community publish story worth read on development design and datum science
Eugenio Culurciello,6.4K,8,https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0?source=tag_archive---------3----------------,the fall of rnn lstm towards data science,we fall for recurrent neural network rnn long short term memory lstm and all their variant now it be time to drop they it be the year 2014 and lstm and rnn make a great come back from the dead we all read colah s blog and karpathy s ode to rnn but we be all young and unexperienced for a few year this be the way to solve sequence learn sequence translation seq2seq which also result in amazing result in speech to text comprehension and the raise of siri cortana google voice assistant alexa also let we not forget machine translation which result in the ability to translate document into different language or neural machine translation but also translate image into text text into image and captioning video and well you get the idea then in the follow year 2015 16 come resnet and attention one could then well understand that lstm be a clever bypass technique also attention show that mlp network could be replace by average network influence by a context vector more on this later it only take 2 more year but today we can definitely say but do not take our word for it also see evidence that attention base network be use more and more by google facebook salesforce to name a few all these company have replace rnn and variant for attention base model and it be just the beginning rnn have the day count in all application because they require more resource to train and run than attention base model see this post for more info remember rnn and lstm and derivative use mainly sequential processing over time see the horizontal arrow in the diagram below this arrow mean that long term information have to sequentially travel through all cell before get to the present processing cell this mean it can be easily corrupt by be multiply many time by small number < 0 this be the cause of vanish gradient to the rescue come the lstm module which today can be see as multiple switch gate and a bit like resnet it can bypass unit and thus remember for long time step lstm thus have a way to remove some of the vanish gradient problem but not all of it as you can see from the figure above still we have a sequential path from old past cell to the current one in fact the path be now even more complicated because it have additive and forget branch attach to it no question lstm and gru and derivative be able to learn a lot of long term information see result here ; but they can remember sequence of 100s not 1000 or 10 000 or more and one issue of rnn be that they be not hardware friendly let I explain it take a lot of resource we do not have to train these network fast also it take much resource to run these model in the cloud and give that the demand for speech to text be grow rapidly the cloud be not scalable we will need to process at the edge right into the amazon echo see note below for more detail if sequential processing be to be avoid then we can find unit that look ahead or well look back since most of the time we deal with real time causal datum where we know the past and want to affect future decision not so in translate sentence or analyze record video for example where we have all datum and can reason on it more time such look back ahead unit be neural attention module which we previously explain here to the rescue and combine multiple neural attention module come the hierarchical neural attention encoder show in the figure below a well way to look into the past be to use attention module to summarize all past encode vector into a context vector ct notice there be a hierarchy of attention module here very similar to the hierarchy of neural network this be also similar to temporal convolutional network tcn report in note 3 below in the hierarchical neural attention encoder multiple layer of attention can look at a small portion of recent past say 100 vector while layer above can look at 100 of these attention module effectively integrate the information of 100 x 100 vector this extend the ability of the hierarchical neural attention encoder to 10 000 past vector but more importantly look at the length of the path need to propagate a representation vector to the output of the network in hierarchical network it be proportional to log n where n be the number of hierarchy layer this be in contrast to the t step that a rnn need to do where t be the maximum length of the sequence to be remember and t > > n this architecture be similar to a neural ture machine but let the neural network decide what be read out from memory via attention this mean an actual neural network will decide which vector from the past be important for future decision but what about store to memory the architecture above store all previous representation in memory unlike neural turning machine this can be rather inefficient think about store the representation of every frame in a video — most time the representation vector do not change frame to frame so we really be store too much of the same what can we do be add another unit to prevent correlated datum to be store for example by not store vector too similar to previously store one but this be really a hack the good would be to be let the application guide what vector should be save or not this be the focus of current research study stay tune for more information tell your friend it be very surprising to we to see so many company still use rnn lstm for speech to text many unaware that these network be so inefficient and not scalable please tell they about this post about training rnn lstm rnn and lstm be difficult to train because they require memory bandwidth bind computation which be the bad nightmare for hardware designer and ultimately limit the applicability of neural network solution in short lstm require 4 linear layer mlp layer per cell to run at and for each sequence time step linear layer require large amount of memory bandwidth to be compute in fact they can not use many compute unit often because the system have not enough memory bandwidth to feed the computational unit and it be easy to add more computational unit but hard to add more memory bandwidth note enough line on a chip long wire from processor to memory etc as a result rnn lstm and variant be not a good match for hardware acceleration and we talk about this issue before here and here a solution will be compute in memory device like the one we work on at fwdnxt see this repository for a simple example of these technique note 1 hierarchical neural attention be similar to the idea in wavenet but instead of a convolutional neural network we use hierarchical attention module also hierarchical neural attention can be also bi directional note 2 rnn and lstm be memory bandwidth limited problem see this for detail the processing unit s need as much memory bandwidth as the number of operation s they can provide make it impossible to fully utilize they the external bandwidth be never go to be enough and a way to slightly ameliorate the problem be to use internal fast cache with high bandwidth the good way be to use technique that do not require large amount of parameter to be move back and forth from memory or that can be re use for multiple computation per byte transfer high arithmetic intensity note 3 here be a paper compare cnn to rnn temporal convolutional network tcn outperform canonical recurrent network such as lstms across a diverse range of task and dataset while demonstrate long effective memory note 4 relate to this topic be the fact that we know little of how our human brain learn and remember sequence we often learn and recall long sequence in small segment such as a phone number 858 534 22 30 memorize as four segment behavioral experiment suggest that human and some animal employ this strategy of break down cognitive or behavioral sequence into chunk in a wide variety of task — these chunk remind I of small convolutional or attention like network on small sequence that then be hierarchically string together like in the hierarchical neural attention encoder and temporal convolutional network tcn more study make I think that work memory be similar to rnn network that use recurrent real neuron network and their capacity be very low on the other hand both the cortex and hippocampus give we the ability to remember really long sequence of step like where do I park my car at airport 5 day ago suggest that more parallel pathway may be involve to recall long sequence where attention mechanism gate important chunk and force hop in part of the sequence that be not relevant to the final goal or task note 5 the above evidence show we do not read sequentially in fact we interpret character word and sentence as a group an attention base or convolutional module perceive the sequence and project a representation in our mind we would not be misread this if we process this information sequentially we would stop and notice the inconsistency I have almost 20 year of experience in neural network in both hardware and software a rare combination see about I here medium webpage scholar linkedin and more if you find this article useful please consider a donation to support more tutorial and blog any contribution can make a difference from a quick cheer to a stand ovation clap to show how much you enjoy this story I dream and build new technology sharing concept idea and code
Gary Marcus,1.3K,27,https://medium.com/@GaryMarcus/in-defense-of-skepticism-about-deep-learning-6e8bfd5ae0f1?source=tag_archive---------4----------------,in defense of skepticism about deep learning gary marcus medium,in a recent appraisal of deep learning marcus 2018 I outline ten challenge for deep learning and suggest that deep learning by itself although useful be unlikely to lead on its own to artificial general intelligence I suggest instead the deep learning be view not as a universal solvent but simply as one tool among many in place of pure deep learning I call for hybrid model that would incorporate not just supervised form of deep learning but also other technique as well such as symbol manipulation and unsupervised learn itself possibly reconceptualize I also urge the community to consider incorporate more innate structure into ai system within a few day thousand of people have weigh in over twitter some enthusiastic e g the good discussion of # deeplearne and # ai I ve read in many year some not thoughtful but mostly wrong nevertheless because I think clarity around these issue be so important I ve compile a list of fourteen commonly ask query where do unsupervise learning fit in why didn t I say more nice thing about deep learn what give I the right to talk about this stuff in the first place what s up with ask a neural network to generalize from even number to odd number hint that s the most important one and lot more I haven t address literally every question I have see but I have try to be representative 1 what be general intelligence thomas dietterich an eminent professor of machine learning and my most thorough and explicit critic thus far give a nice answer that I be very comfortable with 2 marcus wasn t very nice to deep learning he should have say more nice thing about all of its vast accomplishment and he minimize other dietterich mention above make both of these point write on the first part of that true I could have say more positive thing but it s not like I didn t say any or even like I forgot to mention dietterich s good example ; I mention it on the first page more generally later in the article I cite a couple of great text and excellent blog that have pointer to numerous example a lot of they though would not really count as agi which be the main focus of my paper google translate for example be extremely impressive but it s not general ; it can t for example answer question about what it have translate the way a human translator could the second part be more substantive be 1 000 category really very finite well yes compare to the flexibility of cognition cognitive scientist generally place the number of atomic concept know by an individual as be on the order of 50 000 and we can easily compose those into a vastly great number of complex thought pet and fish be probably count in those 50 000 ; pet fish which be something different probably isn t count and I can easily entertain the concept of a pet fish that be suffer from ick or note that it be always disappointing to buy a pet fish only to discover that it be infect with ick an experience that I have as a child and evidently still resent how many idea like that I can express it s a lot more than 1 000 I be not precisely sure how many visual category a person can recognize but suspect the math be roughly similar try google image on pet fish and you do ok ; try it on pet fish wear goggle and you mostly find dog wear goggle with a false alarm rate of over 80 % machine win over nonexpert human on distinguish similar dog breed but people win by a wide margin on interpret complex scene like what would happen to a skydiver who be wear a backpack rather than a parachute in focus on 1 000 category chunk the machine learning field be in my view do itself a disservice trade a short term feeling of success for a denial of hard more open end problem like scene and sentence comprehension that must eventually be address compare to the essentially infinite range of sentence and scene we can see and comprehend 1000 of anything really be small see also note 2 at bottom 3 marcus say deep learning be useless but it s great for many thing of course it be useful ; I never say otherwise only that a in its current supervised form deep learning might be approach its limit and b that those limit would stop short from full artificial general intelligence — unless maybe we start incorporate a bunch of other stuff like symbol manipulation and innateness the core of my conclusion be this 4 one thing that I don t understand — @garymarcus say that dl be not good for hierarchical structure but in @ylecun nature review paper say that that dl be particularly suit for exploit such hierarchy this be an astute question from ram shankar and I should have be a lot clear about the answer there be many different type of hierarchy one could think about deep learning be really good probably the good ever at the sort of feature wise hierarchy lecun talk about which I typically refer to as hierarchical feature detection ; you build line out of pixel letter out of line word out of letter and so forth kurzweil and hawkin have emphasize this sort of thing too and it really go back to hubel and wiesel 1959 in neuroscience experiment and to fukushima fukushima miyake & ito 1983 in ai fukushima in his neocognitron model hand wire his hierarchy of successively more abstract feature ; lecun and many other after show that at least in some case you don t have to hand engineer they but you don t have to keep track of the subcomponent you encounter along the way ; the top level system need not explicitly encode the structure of the overall output in term of which part be see along the way ; this be part of why a deep learning system can be fool into think a pattern of a black and yellow stripe be a school bus nguyen yosinski & clune 2014 that stripe pattern be strongly correlate with activation of the school bus output unit which be in turn correlate with a bunch of low level feature but in a typical image recognition deep network there be no fully realize representation of a school bus as be make up of wheel a chassis window etc virtually the whole spoof literature can be think of in these term note 3 the structural sense of hierarchy which I be discuss be different and focus around system that can make explicit reference to the part of large whole the classic illustration would be chomsky s sense of hierarchy in which a sentence be compose of increasingly complex grammatical unit e g use a novel phrase like the man who mistook his hamburger for a hot dog with a large sentence like the actress insist that she would not be outdo by the man who mistook his hamburger for a hot dog I don t think deep learning do well here e g in discern the relation between the actress the man and the misidentified hot dog though attempt have certainly be make even in vision the problem be not entirely licked ; hinton s recent capsule work sabour frosst & hinton 2017 for example be an attempt to build in more robust part whole direction for image recognition by use more structured network I see this as a good trend and one potential way to begin to address the spoof problem but also as a reflection of trouble with the standard deep learning approach 5 it s weird to discuss deep learning in the context of general ai general ai be not the goal of deep learning good twitter response to this come from university of quebec professor daniel lemire oh come on hinton bengio be openly go for a model of human intelligence second prize go to a math phd at google jeremy kun who counter the dubious claim that general ai be not the goal of deep learning with if that s true then deep learning expert sure let everyone believe it be without correct they andrew ng s recent harvard business review article which I cite imply that deep learning can do anything a person can do in a second thomas dietterich s tweet that say in part it be hard to argue that there be limit to dl jeremy howard worry that the idea that deep learning be overhype might itself be overhype and then suggest that every know limit have be counter deepmind s recent alphago paper see note 4 be position somewhat similarly with silver et al silver et al 2017 enthusiastically report that in that paper s conclude discussion not one of the 10 challenge to deep learning that I review be mention as I will discuss in a paper come out soon it s not actually a pure deep learning system but that s a story for another day the main reason people keep benchmarke their ai system against human be precisely because agi be the goal 6 what marcus say be a problem with supervised learning not deep learning yann lecun present a version of this in a comment on my facebook page the part about my allegedly not recognize lecun s recent work be well odd it s true that I couldn t find a good summary article to cite when I ask lecun he tell I by email that there wasn t one yet but I do mention his interest explicitly I also note that my conclusion be positive too although I express reservation about current approach to build unsupervised system I end optimistically what lecun s remark do get right be that many of the problem I address be a general problem with supervised learning not something unique to deep learning ; I could have be more clear about this many other supervised learning technique face similar challenge such as problem in generalization and dependence on massive datum set ; relatively little of what I say be unique to deep learning in my focus on assess deep learning at the five year resurgence mark I neglect to say that but it doesn t really help deep learning that other supervised learning technique be in the same boat if someone could come up with a truly impressive way of use deep learning in an unsupervised way a reassessment might be require but I don t see that unsupervised learning at least as it currently pursue particularly remedy the challenge I raise e g with respect to reason hierarchical representation transfer robustness and interpretability it s simply a promissory note note 5 as portland state and santa fe institute professor melanie mitchell s put it in a thus far unanswered tweet I would too in the meantime I see no principled reason to believe that unsupervised learning can solve the problem I raise unless we add in more abstract symbolic representation first 7 deep learning be not just convolutional network of the sort marcus critique it s essentially a new style of programming — differentiable programming — and the field be try to work out the reusable construct in this style we have some convolution pool lstm gan vae memory unit route unit etc — tom dietterich this seem in the context of dietterich s long series of tweet to have be propose as a criticism but I be puzzle by that as I be a fan of differentiable programming and say so perhaps the point be that deep learning can be take in a broad way in any event I would not equate deep learning and differentiable programming e g approach that I cite like neural ture machine and neural programming deep learning be a component of many differentiable system but such system also build in exactly the sort of element draw from symbol manipulation that I be and have be urge the field to integrate marcus 2001 ; marcus marblestone & dean 2014a ; marcus marblestone & dean 2014b include memory unit and operation over variable and other system like route unit stress in the more recent two essay if integrate all this stuff into deep learning be what get we to agi my conclusion quote below will have turn out to be dead on 8 now vs the future maybe deep learning doesn t work now but it s offspring will get we to agi possibly I do think that deep learning might play an important role in get we to agi if some key thing many not yet discover be add in first but what we add matter and whether it be reasonable to call some future system an instance of deep learning per se or more sensible to call the ultimate system a such and such that use deep learning depend on where deep learning fit into the ultimate solution maybe for example in truly adequate natural language understanding system symbol manipulation will play an equally large role as deep learning or an even large one part of the issue here be of course terminological a very good friend recently ask I why can t we just call anything that include deep learning deep learning even if it include symbol manipulation some enhancement to deep learning ought to work to which I respond why not call anything that include symbol manipulation symbol manipulation even if it include deep learning gradient base optimization should get its due but so should symbol manipulation which as yet be the only know tool for systematically represent and achieve high level abstraction bedrock to virtually all of the world s complex computer system from spreadsheet to programming environment to operating system eventually I conjecture credit will also be due to the inevitable marriage between the two hybrid system that bring together the two great idea of 20th century ai symbol processing and neural network both initially develop in the 1950 other new tool yet to be invent may be critical as well to a true acolyte of deep learning anything be deep learn no matter what it s incorporate and no matter how different it might be from current technique viva imperialism if you replace every transistor in a classic symbolic microprocessor with a neuron but keep the chip s logic entirely unchanged a true deep learning acolyte would still declare victory but we win t understand the principle drive eventual success if we lump everything together note 6 9 no machine can extrapolate it s not fair to expect a neural network to generalize from even number to odd number here s a function express over binary digit f 110 = 011 ; f 100 = 001 ; f 010 = 010 what s f 111 if you be an ordinary human you be probably go to guess 111 if you be neural network of the sort I discuss you probably win t if you have be tell many time that hide layer in neural network abstract function you should be a little bit surprised by this if you be a human you might think of the function as something like reversal easily express in a line of computer code if you be a neural network of a certain sort it s very hard to learn the abstraction of reversal in a way that extend from even in that context to odd but be that impossible certainly not if you have a prior notion of an integer try another this time in decimal f 4 = 8 ; f 6 = 12 what s f 5 none of my human reader would care that question happen to require you to extrapolate from even number to odd ; a lot of neural network would be flummox sure the function be undetermined by the sparse number of example like all function but it be interesting and important that most people would amid the infinite range of a priori possible induction would alight on f 5 = 10 and just as interesting that most standard multilayer perceptron represent the number as binary digit wouldn t that s tell we something but many people in the neural network community françois chollet be one very salient exception don t want to listen importantly recognize that a rule apply to any integer be roughly the same kind of generalization that allow one to recognize that a novel noun that can be use in one context can be use in a huge variety of other context from the first time I hear the word blicket use as an object I can guess that it will fit into a wide range of frame like I think I see a blicket I have a close encounter with a blicket and exceptionally large blicket frighten I etc and I can both generate and interpret such sentence without specific further training it doesn t matter whether blicket be or not similar in for example phonology to other word I have hear nor whether I pile on the adjective or use the word as a subject or an object if most machine learning ml paradigm have a problem with this we should have problem with most ml paradigm be I be fair well yes and no it s true that I be ask neural network to do something that violate their assumption a neural network advocate might for example say hey wait a minute in your reversal example there be three dimension in your input space represent the left binary digit the middle binary digit and rightmost binary digit the rightmost binary digit have only be a zero in the training ; there be no way a network can know what to do when you get to one in that position for example vincent lostenlan a postdoc at cornell say dietterich make essentially the same point more concisely but although both be right about why odd and even be in this context hard for deep learning they be both wrong about the large issue for three reason first it can t be that people can t extrapolate you just do in two different example at the top of this section paraphrase chico marx who be you go to believe I or your own eye to someone immerse deeply — perhaps too deeply — in contemporary machine learn my odd and even problem seem unfair because a certain dimension the one which contain the value of 1 in the rightmost digit hasn t be illustrate in the training regime but when you a human look at my example above you will not be stymie by this particular gap in the training datum you win t even notice it because your attention be on high level regularity people routinely extrapolate in exactly the fashion that I have be describe like recognize string reversal from the three training example I give above in a technical sense that be extrapolation and you just do it in the algebraic mind I refer to this specific kind of extrapolation as generalize universally quantify one to one mapping outside of a space of training example as a field we desperately need a solution to this challenge if we be ever to catch up to human learning — even if it mean shake up our assumption now it might reasonably be object that it s not a fair fight human manifestly depend on prior knowledge when they generalize such mapping in some sense dieterrich propose this objection later in his tweet stream true enough but in a way that s the point neural network of a certain sort don t have a good way of incorporate the right sort of prior knowledge in the place it be precisely because those network don t have a way of incorporate prior knowledge like many generalization hold for all element of unbounded class or odd number leave a remainder of one when divide by two that neural network that lack operation over variable fail the right sort of prior knowledge that would allow neural network to acquire and represent universally quantify one to one mapping standard neural network can t represent such mapping except in certain limited way convolution be a way of building in one particular such mapping prior to learn second say that no current system deep learning or otherwise can extrapolate in the way that I have describe be no excuse ; once again other architecture may be in the choppy water but that doesn t mean we shouldn t be try to swim to shore if we want to get to agi we have to solve the problem put differently yes one could certainly hack together solution to get deep learning to solve my specific number series problem by for example play game with the input encoding scheme ; the real question if we want to get to agi be how to have a system learn the sort of generalization I be describe in a general way third the claim that no current system can extrapolate turn out to be well false ; there be already ml system that can extrapolate at least some function of exactly the sort I describe and you probably own one microsoft excel its flash fill function in particular gulwani 2011 power by a very different approach to machine learning it can do certain kind of extrapolation albeit in a narrow context by the bushel e g try type the decimal digit 1 11 21 in a series of row and see if the system can extrapolate via flash fill to the eleventh item in the sequence 101 spoiler alert it can in exactly the same way as you probably would even though there be no positive example in the training dimension of the hundred digit the system learn from example the function you want and extrapolate it piece of cake can any deep learning system do that with three training example even with a range of experience on other small counting function like 1 3 5 and 2 4 6 well maybe but only the one that be likely do so be likely to be hybrid that build in operation over variable which be quite different from the sort of typical convolutional neural network that most people associate with deep learning put all this very differently one crude way to think about where we be with most ml system that we have today note 7 be that they just aren t design to think outside the box ; they be design to be awesome interpolator inside the box that s fine for some purpose but not other human be well at think outside box than contemporary ai ; I don t think anyone can seriously doubt that but that kind of extrapolation that microsoft can do in a narrow context but that no machine can do with human like breadth be precisely what machine learn engineer really ought to be work on if they want to get to agi 10 everybody in the field already know this there be nothing new here well certainly not everybody ; as note there be many critic who think we still don t know the limit of deep learning and other who believe that there might be some but none yet discover that say I never say that any of my point be entirely new ; for virtually all I cite other scholar who have independently reach similar conclusion 11 marcus fail to cite x definitely true ; the literature review be incomplete one favorite among the paper I fail to cite be shanahan s deep symbolic reinforcement garnelo arulkumaran & shanahan 2016 ; I also can t believe I forget richardson and domingo 2006 markov logic network I also wish I have cite evans and edward grefenstette 2017 a great paper from deepmind and smolensky s tensor calculus work smolensky et al 2016 and work on inductive programming in various form gulwani et al 2015 and probabilistic programming too by noah goodman goodman mansinghka roy bonawitz & tenenbaum 2012 all seek to bring rule and network close to together and old stuff by pioneer like jordan pollack smolensky et al 2016 and forbus and gentner s falkenhainer forbus & gentner 1989 and hofstadter and mitchell s 1994 work on analogy ; and many other I be sure there be a lot more I could and should have cite overall I try to be representative rather than fully comprehensive but I still could have do well # chagrin 12 marcus have no standing in the field ; he isn t a practitioner ; he be just a critic hesitant to raise this one but it come up in all kind of different response even from the mouth of certain well know professional as ram shankar note as a community we must circumscribe our criticism to science and merit base argument what really matter be not my credential which I believe do in fact qualify I to write but the validity of the argument either my argument be correct or they be not still for those who be curious I supply an optional mini history of some of my relevant credential in note 8 at the end 13 re hierarchy what about socher s tree rnn I have write to he in hope of have a well understanding of its current status I ve also privately push several other team towards try out task like lake and baroni 2017 present pengfei et al 2017 offer some interesting discussion 14 you could have be more critical of deep learning nobody quite say that not in exactly those word but a few come close generally privately one colleague for example point out that there may be some serious error of future forecasting around the same colleague add another colleague ml researcher and author pedro domingo point out still other shortcoming of current deep learning method that I didn t mention like other flexible supervised learning method deep learning system can be unstable in the sense that slightly change the training datum may result in large change in the result model as domingos note there s no guarantee this sort of rise and decline win t repeat itself neural network have rise and fall several time before all the way back to rosenblatt s first perceptron in 1957 we shouldn t mistake cyclical enthusiasm for a complete solution to intelligence which still seem to I anyway to be decade away if we want to reach agi we owe it to ourselves to be as keenly aware of challenge we face as we be of our success 2 there be other problem too in rely on these 1 000 image set for example in read a draft of this paper melanie mitchell point I to important recent work by loghmani and colleague 2017 on assess how deep learning do in the real world quote from the abstract the paper analyze the transferability of deep representation from web image to robotic datum in the wild despite the promising result obtain with representation develop from web image the experiment demonstrate that object classification with real life robotic datum be far from be solve 3 and that literature be grow fast in late december there be a paper about fool deep net into mistake a pair of skier for a dog https arxiv org pdf 1712 09665 pdf and another on a general purpose tool for build real world adversarial patch https arxiv org pdf 1712 09665 pdf see also https arxiv org ab 1801 00634 it s frightening to think how vulnerable deep learning can be real world context and for that matter consider filip pieknewski s blog on why photo train deep learning system have trouble transfer what they have learn to line drawing https blog piekniewski info 2016 12 29 can a deep net see a cat vision be not as solve as many people seem to think 4 as I will explain in the forthcoming paper alphago be not actually a pure deep reinforcement learning system although the quote passage present it as such it s really more of a hybrid with important component that be drive by symbol manipulating algorithm along with a well engineer deep learning component 5 alphazero by the way isn t unsupervise it s self supervise use self play and simulation as a way of generate supervised datum ; I will have a lot more to say about that system in a forthcoming paper 6 consider for example google search and how one might understand it google have recently add in a deep learning algorithm rankbrain to the wide array of algorithms it use for search and google search certainly take in datum and knowledge and process they hierarchically which accord to maher ibrahim be all you need to count as be deep learning but realistically deep learning be just one cue among many ; the knowledge graph component for example be base instead primarily on classical ai notion of traverse ontology by any reasonable measure google search be a hybrid with deep learning as just one strand among many call google search as a whole a deep learning system would be grossly misleading akin to relabele carpentry screwdrivery just because screwdriver happen to be involve 7 important exception include inductive logic programming inductive function program the brain behind microsoft s flash fill and neural programming all be make some progress here ; some of these even include deep learning but they also all include structured representation and operation over variable among their primitive operation ; that s all I be ask for 8 my ai experiment begin in adolescence with among other thing a latin english translator that I code in the programming language logo in graduate school study with steven pinker I explore the relation between language acquisition symbolic rule and neural network I also owe a debt to my undergraduate mentor neil stilling the child language datum I gather marcus et al 1992 for my dissertation have be cite hundred of time and be the most frequently model datum in the 90 s debate about neural network and how child learn language in the late 1990 s I discover some specific replicable problem with multilayer perceptron marcus 1998b ; marcus 1998a ; base on those observation I design a widely cite experiment publish in science marcus vijayan bandi rao & vishton 1999 that show that young infant could extract algebraic rule contra jeff elman s 1990 then popular neural network all of this culminate in a 2001 mit press book marcus 2001 which lobby for a variety of representational primitive some of which have begin to pop up in recent neural network ; in particular that the use of operation over variable in the new field of differentiable programming daniluk rocktäschel welbl & riedel 2017 ; grave et al 2016 owe something to the position outline in that book there be a strong emphasis on have memory record as well which can be see in the memory network be develop e g at facebook bordes usunier chopra & weston 2015 the next decade see I work on other problem include innateness marcus 2004 which I will discuss at length in the forthcoming piece about alphago and evolution marcus 2004 ; marcus 2008 I eventually return to ai and cognitive modeling publish a 2014 article on cortical computation in science marcus marblestone & dean 2014 that also anticipate some of what be now happen in differentiable programming more recently I take a leave from academia to find and lead a machine learn company in 2014 ; by any reasonable measure that company be successful acquire by uber roughly two year after found as co founder and ceo I put together a team of some of the very good machine learn talent in the world include zoubin ghahramani jeff clune noah goodman ken stanley and jason yosinski and play a pivotal role in develop our core intellectual property and shape our intellectual mission a patent be pende co write by zoubin ghahramani and myself although much of what we do there remain confidential now own by uber and not by I I can say that a large part of our effort be address towards integrate deep learning with our own technique which give I a great deal of familiarity with joy and tribulation of tensorflow and vanish and explode gradient we aim for state of the art result sometimes successfully sometimes not with sparse datum use hybridize deep learning system on a daily basis borde a usuni n chopra s & weston j 2015 large scale simple question answer with memory network arxiv daniluk m rocktäschel t welbl j & riedel s 2017 frustratingly short attention span in neural language modeling arxiv elman j l 1990 finding structure in time cognitive science 14 2 2 179 211 evans r & grefenstette e 2017 learn explanatory rule from noisy datum arxiv cs ne falkenhainer b forbus k d & gentner d 1989 the structure mapping engine algorithm and example artificial intelligence 41 1 1 1 63 fukushima k miyake s & ito t 1983 neocognitron a neural network model for a mechanism of visual pattern recognition ieee transaction on system man and cybernetic 5 826 834 garnelo m arulkumaran k & shanahan m 2016 towards deep symbolic reinforcement learning arxiv cs ai goodman n mansinghka v roy d m bonawitz k & tenenbaum j b 2012 church a language for generative model arxiv preprint arxiv 1206 3255 grave a wayne g reynolds m harley t danihelka I grabska barwińska a et al 2016 hybrid computing use a neural network with dynamic external memory nature 538 7626 7626 471 476 gulwani s 2011 automate string processing in spreadsheet use input output example dl acm org 46 1 1 317 330 gulwani s hernández orallo j kitzelmann e muggleton s h schmid u & zorn b 2015 inductive programming meet the real world communication of the acm 58 11 11 90 99 hofstadter d r & mitchell m 1994 the copycat project a model of mental fluidity and analogy make advance in connectionist and neural computation theory 2 31 112 31 112 29 30 hosseini h xiao b jaiswal m & poovendran r 2017 on the limitation of convolutional neural network in recognize negative image arxiv cs cv hubel d h & wiesel t n 1959 receptive field of single neurone in the cat s striate cortex the journal of physiology 148 3 3 574 591 lake b m & baroni m 2017 still not systematic after all these year on the compositional skill of sequence to sequence recurrent network arxiv loghmani m r caputo b & vincze m 2017 recognize object in the wild where do we stand arxiv cs ro marcus g f 1998a rethink eliminative connectionism cogn psychol 37 3 3 243 — 282 marcus g f 1998b can connectionism save constructivism cognition 66 2 2 153 — 182 marcus g f 2001 the algebraic mind integrate connectionism and cognitive science cambridge mass mit press marcus g f 2004 the birth of the mind how a tiny number of gene create the complexity of human thought basic book marcus g f 2008 kluge the haphazard construction of the human mind boston houghton mifflin marcus g 2018 deep learn a critical appraisal arxiv marcus g f marblestone a & dean t 2014a the atom of neural computation science 346 6209 6209 551 — 552 marcus g f marblestone a h & dean t l 2014b frequently ask question for the atom of neural computation biorxiv arxiv q bio nc marcus g f 2001 the algebraic mind integrate connectionism and cognitive science cambridge mass mit press marcus g f pinker s ullman m hollander m rosen t j & xu f 1992 overregularization in language acquisition monogr soc re child dev 57 4 4 1 182 marcus g f vijayan s bandi rao s & vishton p m 1999 rule learn by seven month old infant science 283 5398 5398 77 80 nguyen a yosinski j & clune j 2014 deep neural network be easily fool high confidence prediction for unrecognizable image arxiv cs cv pengfei l xipeng q & xuanje h 2017 dynamic compositional neural network over tree structure ijcai proceeding from proceeding of the twenty sixth international joint conference on artificial intelligence ijcai 17 ribeiro m t singh s & guestrin c 2016 why should I trust you explain the prediction of any classifier arxiv cs lg richardson m & domingo p 2006 markov logic network machine learn 62 1 1 107 136 sabour s dffsdfdsf n & hinton g e 2017 dynamic routing between capsule arxiv cs cv silver d schrittwieser j simonyan k antonoglou I huang a guez a et al 2017 master the game of go without human knowledge nature 550 7676 7676 354 359 smolensky p lee m he x yih w t gao j & deng l 2016 basic reasoning with tensor product representation arxiv cs ai from a quick cheer to a stand ovation clap to show how much you enjoy this story ceo & founder geometric intelligence acquire by uber professor of psychology and neural science nyu freelancer for the new yorker & new york times
Bargava,11.8K,3,https://towardsdatascience.com/how-to-learn-deep-learning-in-6-months-e45e40ef7d48?source=tag_archive---------5----------------,how to learn deep learning in 6 month towards data science,it be quite possible to learn follow and contribute to state of art work in deep learning in about 6 month time this article detail out the step to achieve that pre requisite you be willing to spend 10 20 hour per week for the next 6 month you have some programming skill you should be comfortable to pick up python along the way and cloud no background in python and cloud assume some math education in the past algebra geometry etc access to internet and computer step 1 we learn drive a car — by drive not by learn how the clutch and the internal combustion engine work atleast not initially when learn deep learning we will follow the same top down approach do the fast ai course — practical deep learning for coder — part 1 this take about 4 6 week of effort this course have a session on run the code on cloud google colaboratory have free gpu access start with that other option include paperspace aws gcp crestle and floydhub all of these be great do not start to build your own machine atleast not yet step 2 this be the time to know some of the basic learn about calculus and linear algebra for calculus big picture of calculus provide a good overview for linear algebra gilbert strang s mit course on opencourseware be amazing once you finish the above two read the matrix calculus for deep learning step 3 now be the time to understand the bottom up approach to deep learning do all the 5 course in the deep learning specialisation in coursera you need to pay to get the assignment grade but the effort be truly worth it ideally give the background you have gain so far you should be able to complete one course every week step 4 do a capstone project this be the time where you delve deep into a deep learning library eg tensorflow pytorch mxnet and implement an architecture from scratch for a problem of your like the first three step be about understand how and where to use deep learning and gain a solid foundation this step be all about implement a project from scratch and develop a strong foundation on the tool step 5 now go and do fast ai s part ii course — cut edge deep learning for coder this cover more advanced topic and you will learn to read the late research paper and make sense out of they each of the step should take about 4 6 week time and in about 26 week since the time you start and if you follow all of the above religiously you will have a solid foundation in deep learning where to go next do the stanford s cs231n and cs224d course these two be amazing course with great depth for vision and nlp respectively they cover the late state of art and read the deep learning book this will solidify your understanding happy deep learning create every single day from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @ http impel io currently build a personalization engine http www recotap com datum science trainer and mentor sharing concept idea and code
Seth Weidman,2.8K,11,https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef?source=tag_archive---------6----------------,the 3 trick that make alphago zero work hacker noon,there be many advance in deep learning and ai in 2017 but few generate as much publicity and interest as deepmind s alphago zero this program be truly a shocking breakthrough not only do it beat the prior version of alphago — the program that beat 17 time world champion lee sedol just a year and a half early — 100 0 it be train without any datum from real human game xavier amatrain call it more significant than anything in the last 5 year in machine learning so how do deepmind do it in this essay I ll try to give an intuitive idea of the technique alphago zero use what make they work and what the implication for future ai research be let s start with the general approach that both alphago and alphago zero take to playing go both alphago and alphago zero evaluate the go board and choose move use a combination of two method alphago and alphago zero both work by cleverly combine these two method let s look at each one in turn go be a sufficiently complex game that computer can t simply search all possible move use a brute force approach to find the good one indeed they can t even come close the good go program prior to alphago overcome this by use monte carlo tree search or mct at a high level this method involve initially explore many possible move on the board and then focus this exploration over time as certain move be find to be more likely to lead to win than other both alphago and alphago zero use a relatively straightforward version of mct for their lookahead simply use many of the good practice list in the monte carlo tree search wikipedia page to properly manage the tradeoff between explore new sequence of move or more deeply explore already explore sequence for more see the detail in the search section under method in the original alphago paper publish in nature though mct have be the core of all successful go program prior to alphago it be deepmind s clever combination of this technique with a neural network base intuition that allow it to surpass human performance deepmind s major innovation with alphago be to use deep neural network to understand the state of the game and then use this understanding to intelligently guide the search of the mct more specifically they train network that could look at give this information the neural network could recommend how do deepmind train neural network to do this here alphago and alphago zero use very different approach ; we ll start first with alphago s alphago have two separately train neural network deepmind then combine these two neural network with mct — that be the program s intuition with its brute force lookahead search — in a very clever way it use the network that have be train to predict move to guide which branch of the game tree to search and use the network that have be train to predict whether a position be win to evaluate the position it encounter during its search this allow alphago to intelligently search upcoming move and ultimately allow it to beat lee sedol alphago zero however take this to a whole new level at a high level alphago zero work the same way as alphago specifically it play go by use mct base lookahead search intelligently guide by a neural network however alphago zero s neural network — its intuition — be train completely differently from that of alphago let s say you have a neural network that be attempt to understand the game of go that be for every board position it be use a deep neural network to generate evaluation of what the good move be what deepmind realize be that no matter how intelligent this neural network be — whether it be completely clueless or a go master — its evaluation can always be make well by mct fundamentally mct perform the kind of lookahead search that we would imagine a human master would perform if give enough time it intelligently guess which variation — sequence of future move — be most promising simulate those variation evaluate how good they actually be and update its assessment of its current good move accordingly an illustration of this be below suppose we have a neural network that be read the board and determine that a give move result in a game be even with an evaluation of 0 0 then the network intelligently look ahead a few move and find a sequence of move that can be force from the current position that end up result in an evaluation of 0 5 it can then update its evaluation of the current board position to reflect that it lead to a more favorable position down the road this lookahead search therefore can always give we improved datum on how good the various move in the current position that the neural network be evaluate be this be true whether our neural network be play at an amateur level or an expert level we can always generate improve evaluation for it by look ahead and see which of its current option actually lead to well position in addition just as in alphago we would also want our neural network to learn which move be likely to lead to win so also as before our agent — use its mct improve evaluation and the current state of its neural network — could play game against itself win some and lose other this datum generate purely via lookahead and self play be what deepmind use to train alphago zero more specifically much be make of the fact that no game between human be use to train alphago zero and this first trick be the reason why for a give state of a go agent it can always be make smart by perform mct base lookahead and use the result of that lookahead to improve the agent this be how alphago zero be able to continuously improve from when it be an amateur all the way up to when it well than the good human player the second trick be a novel neural network structure that I ll call the two head monster alphago zero s be its neural network architecture a two headed architecture its first 20 layer or so be layer block of a type often see in modern neural net architecure these layer be follow by two head one head that take the output of the first 20 layer and produce probability of the go agent make certain move and another that take the output of the first 20 layer and output a probability of the current player win this be quite unusual in almost all application neural network output a single fix output — such as the probability of an image contain a dog or a vector contain the probability of an image contain one of 10 type of object how can a net learn if it be receive two set of signal one on how good its evaluation of the board be and another how good the specific move it be select be the answer be simple remember that neural network be fundamentally just mathematical function with a bunch of parameter that determine the prediction that they make ; we teach they by repeatedly show they correct answer and have they update their parameter so the answer they produce more closely match these correct answer so when we use the two head neural net to make a prediction use head # 1 we simply update the parameter that lead to make that prediction namely the parameter in the body and in head # 1 similarly when we make a prediction use head # 2 we update the parameter in the body and in head # 2 this be how deepmind train its single two head neural network that it use to guide mct during its search just as alphago do with two separate neural network this trick account for half of alphago zero s increase in play strength over alphago this trick be know more technically as multi task learn with hard parameter share sebastian ruder have a great overview here the other half of the increase in play strength simply come from bring the neural network architecture up to date with the late advance in the field alphago zero use a more cutting edge neural network architecture than alphago specifically they use a residual neural network architecture instead of a purely convolutional architecture residual net be pioneer by microsoft research in late 2015 right around the time work on the first version of alphago would have wrap up so it both understandable that deepmind do not use they in the original alphago program interestingly as the chart below show each of these two neural network relate trick — switch from convolutional to residual architecture and use the two head monster neural network architecture instead of separate neural network — would have result in about half of the increase in play strength as be achieve when both be combine these three trick be what enable alphago zero to achieve its incredible performance that blow away even alpha go it be worth note that alphago do not use any classical or even cut edge reinforcement learning concept — no deep q learn asynchronous actor critic agent or anything else we typically associate with reinforcement learning it simply use simulation to generate training datum for its neural net to then learn from in a supervised fashion denny britz sum this idea up well in this tweet from just after when the alphago zero paper be release here s a step by step timeline of how alphago zero be train 3 as these self play game be happen sample 2 048 position from the most recent 500 000 game along with whether the game be win or lose for each move record both a the result of the mct evaluation of those position — how good the various move in these position be base on lookahead — and b whether the current player win or lose the game 4 train the neural network use both a the move evaluation produce by the mct lookahead search and b whether the current player win or lose 5 finally every 1 000 iteration of step 3 4 evaluate the current neural network against the previous good version ; if it win at least 55 % of the game begin use it to generate self play game instead of the prior version repeat step 3 4 700 000 time while the self play game be continuously be play — after three day you ll have yourself an alphago zero there be many implication of deepmind s incredible achievement for the future of ai research here be a couple of key one first the fact that self play datum generate from simulation be good enough to be able to train the network suggest that simulate self play datum can train agent to surpass human performance in extremely complex task even start completely from scratch — datum generate from human expert may not be need second the two head monster trick seem to significantly help agent learn to perform several related task in many domain since it seem to prevent the agent from overfitte their behavior to any individual task deepmind seem to really like this trick and have use it and more advanced version of it to build agent that can learn multiple task in several different domain many project in robotic especially the burgeon field of use simulation to teach robotic agent to use their limb to accomplish task be use these two trick to great effect pieter abbeel s recent nip keynote highlight many impressive new result that use these trick along with many bleeding edge reinforcement learning technique indeed locomotion seem like a perfect use case for the two head monster trick in particular for example robotic agent could be simultaneously train to hit a baseball use a bat and to throw a punch to hit a move target since the two task require learn some common skill e g balance torso rotation deepmind s alphago zero be one of the most intriguing advancement in ai and deep learning in 2017 I can t wait to see what 2018 bring from a quick cheer to a stand ovation clap to show how much you enjoy this story senior data scientist at @thisismetis I write about the intersection of data science business education and society how hacker start their afternoon
Gabriel Aldamiz...,5.1K,11,https://hackernoon.com/how-we-grew-from-0-to-4-million-women-on-our-fashion-app-with-a-vertical-machine-learning-approach-f8b7fc0a89d7?source=tag_archive---------7----------------,how we grow from 0 to 4 million woman on our fashion app with a vertical machine learning approach,three year ago we launch chicisimo our goal be to offer automate outfit advice today with over 4 million woman on the app we want to share how our datum and machine learning approach help we grow it s be chaotic but it be now under control if we want to build a human level tool to offer automate outfit advice we need to understand people s fashion taste a friend can give we outfit advice because after see what we normally wear she s learn our style how could we build a system that learn fashion taste we have previous experience with taste base project and a background in machine learning apply to music and other sector we see how a collaborative filter tool transform the music industry from blindness to totally understand people check out the audioscrobbler story it also make life well for those who love music and create several unicorn along the way with this background we build the follow thesis online fashion will be transform by a tool that understand taste because if you understand taste you can delight people with relevant content and a meaningful experience we also think that outfits be the asset that would allow taste to be understand to learn what people wear or have in their closet and what style each of we like we decide we be go to build that tool to understand taste we focus on develop the correct dataset and build two asset our mobile app and our data platform from previous experience build mobile product even in symbian back then we know it be easy to bring people to an app but difficult to retain they so we focus on small iteration to learn as fast as possible we launch an extremely early alpha of chicisimo with one key functionality we launch under another name and in another country you couldn t even upload photo but it allow we to iterate with real datum and get a lot of qualitative input at some point we launch the real chicisimo and remove this alpha from the app store we spend a long time try to understand what our true lever of retention be and what algorithm we need in order to match content and people three thing help with retention a identify retention lever use behavioral cohort we use mixpanel for this we run cohort not only over the action that people perform but also over the value they receive this be hard to conceptualize for an app such as chicisimo * we think in term of what specific and measurable value people receive measure it and run cohort over those event and then we be able to iterate over value receive not only over action people perform we also define and remove anti lever all those noisy thing that distract from the main value and get all the relevant metric for different time period first session first day first week etc these super specific metric allow we to iterate * nir eyal s book hook how to build habit form product discuss a framework to create habit that help we build our model ; b re think the onboarding process once we know the lever of retention we define it as the process by which new signup find the value of the app as soon as possible and before we lose they we clearly articulate to ourselves what need to happen what and when it go something like this if people don t do action during their first 7 minute in their first session they will not come back so we need to change the experience to make that happen we also run ton of user test with different type of people and observe how they perceive or mostly didn t the retention lever ; c define how we learn the datum approach describe above be key but there be much more than datum when build a product people love in our case first of all we think that the what to wear problem be a very important one to solve and we truly respect it we obsess over understand the problem and over understand how our solution be help or not it s our way of show respect this lead I to one of the most surprising aspect imo of build a product the fact that regularly we access new corpus of knowledge that we do not have before which help we improve the product significantly when we ve obtain these game change learning it s always be by focus on two aspect how people relate to the problem and how people relate to the product the red arrow in the image below there be a million subtlety that happen in these two relation and we be build chicisimo by try to understand they now we know that at any point there be something important that we don t know and therefore the question always be how can we learn soon talk with one of my colleague she once tell I this be not about datum this be about people and the truth be from day one we ve learn significantly by have conversation with woman about how they relate with the problem and with solution we use several mechanism have face to face conversation read the email we get from woman without predefined question or ask for feedback around specific topic we now use typeform and its a great tool for product insight and then we talk among ourselves and try to articulate the learning we also seek external reference we talk with other product people we play with inspiring app and we re read article that help we think this process be what allow we to learn and then build product and develop technology at some point we be lucky to get notice by the app store team and we ve be feature as app of the day throughout the world view apple s description of chicisimo here on december 31st chicisimo be feature in a summary of app the app store team do we be the pink c in the left image below 😀 the app get view by 957 437 unique thank to this feature for a total of 1 3 m time in our case app feature have a 0 5 % conversion rate from impression to app install normally impression > product page view > install ; aso have a 3 % conversion and referrer 45 % the app aim at understand taste so we can do a well job at suggest outfit idea the simple act of deliver the right content at the right time can absolutely wow people although it be an extremely difficult utility to build chicisimo content be 100 % user generate and this pose some challenge the system need to classify different type of content automatically build the right incentive and understand how to match content and need we soon see that there be a lot of datum come in after think hey how cool we be look at all this data we have we realize it be actually a nightmare because be chaotic the datum wasn t actionable this wasn t cool at all but then we decide to start give some structure to part of the datum and we end invent what we call the social fashion graph the graph be a compact representation of how need outfit and people interrelate a concept that help we build the datum platform the data platform create a high quality dataset link to a learning and training world our app which therefore improve with each new expression of taste we think of outfits as playlist an outfit be a combination of item that make sense to consume together use collaborative filtering the relation capture here allow we to offer recommendation in different area of the app there be still a lot of noise in the datum and one of the hard thing be to understand how people be express the same fashion need in different way which make matching content and need even more difficult lot of people might need idea to go to school and express that specific need in a hundred different way how do you capture this diversity and how do you provide structure to it we build a system to collect concept we call they need and capture equivalence among different way to express the same need we end up build a list of the world s what to wear need which we call our ontology this really clean up the dataset and help we understand what we have this understanding lead to well product decision we now understand that an outfit a need or a person can have a lot of understandable datum attach if you allow people to express freely the app while have the right system behind the platform structure datum give we control while encourage unstructured datum give we knowledge and flexibility the end result be our current system a system that learn the meaning of an outfit how to respond to a need or the taste of an individual and I wouldn t even dare say that this be day 1 for we screenshot of an internal tool the amount of work we have in front of we be immense but we feel thing be now under control one of the new area we ve be work on be add a fourth element to the social fashion graph shoppable product a system to match outfit to product automatically and to help people decide what to buy next this be pretty exciting back when we build recommend system for music and other product it be pretty easy that s what we think now we obviously didn t think that at the time first it be easy to capture that you like a give song then it be easy to capture the sequence in which you and other would listen to that song and therefore you could capture the correlation with this datum you could do a lot however as we soon find out fashion have its own challenge there be not an easy way to match an outfit to a shoppable product think about most garment in your wardrobe most likely you win t find a link to view buy those garment online something you can do for many other product you have at home another challenge the industry be not capture how people describe clothe or outfit so there be a strong disconnect between many ecommerce and its shopper we think we ve solve that problem also similar ai and twiggle be work on it another challenge style be complex to capture and classify by a machine now deep learning bring a new tool to add to other mechanism and change everything own the correct data set allow we to focus on the specific narrow use case relate to outfit recommendation and to focus on deliver value through the algorithm instead of spend time collect and clean datum 👉 now come the fun and rewarding part so please email we if you want to join the team and help build algorithm that have real impact on people — we be 100 % remote slack base 👈 😂 😂 😉 😉 😉 people s very personal style can become as actionable as metadata and possibly as transparent as well and I think we can see the path to get there as we have a consumer product that people already love we can ship early result of these algorithm partially hide and increase their presence as feedback improve result there be more and more researcher work of these area you can read tangseng s paper on recommend outfit from personal closet or clothing parse project or how edgar simo serra define similarity between image use user provide metadata outfit be a key asset in the race to capture the $ 123 billion us apparel market datum be also the reason many player be take outfit to the forefront of technology outfit be a daily habit and have prove to be great asset to attract and retain shopper and capture their datum many player be introduce a shop the look section with outfit from real people amazon zalando or google be a few example google recently introduce a new feature call style idea show how a product can be wear in real life same month amazon launch its alexa echo look to help you with your outfit and alibaba s artificial intelligence personal stylist help they achieve record sale during single day some people think that fashion datum be in the same place as music datum be in 2003 ready to play a very relevant role the good news be the daily habit of decide what to wear will not change the need to buy new clothe win t disappear either so what do you think where will we be 10 year from now will taste datum build unique online experience what role will outfits play how will machine learn change fashion ecommerce will everything change 10 year from now we be a small team of eight four on product and four engineer we believe in focus on our very specific problem no one on earth can understand the problem well than we we also believe on build the complete solution ourselves while do as few thing as possible we work 100 % remote and live in slack + github you can learn more about our machine learning approach here if you be a deep learning engineer or a product manager in the fashion space and want to chat & temporarily access our social fashion graph please email we describe your work you can also download our io and android app or simply say hi hi at chicisimo com from a quick cheer to a stand ovation clap to show how much you enjoy this story founder & ceo at @chicisimo machine learning to automate outfit advise world s large outfits app how hacker start their afternoon
Sarthak Jain,3.9K,10,https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74?source=tag_archive---------8----------------,how to easily detect object with deep learning on raspberry pi,disclaimer I m build nanonet com to help build ml with less datum and no hardware the raspberry pi be a neat piece of hardware that have capture the heart of a generation with ~15 m device sell with hacker build even cool project on it give the popularity of deep learning and the raspberry pi camera we think it would be nice if we could detect any object use deep learning on the pi now you will be able to detect a photobomber in your selfie someone enter harambe s cage where someone keep the sriracha or an amazon delivery guy enter your house 20 m year of evolution have make human vision fairly evolve the human brain have 30 % of it s neuron work on processing vision as compare with 8 percent for touch and just 3 percent for hear human have two major advantage when compare with machine one be stereoscopic vision the second be an almost infinite supply of training datum an infant of 5 year have have approximately 2 7b image sample at 30fps to mimic human level performance scientist break down the visual perception task into four different category object detection have be good enough for a variety of application even though image segmentation be a much more precise result it suffer from the complexity of create training datum it typically take a human annotator 12x more time to segment an image than draw bounding box ; this be more anecdotal and lack a source also after detect object it be separately possible to segment the object from the bounding box object detection be of significant practical importance and have be use across a variety of industry some of the example be mention below object detection can be use to answer a variety of question these be the broad category there be a variety of model architecture that be use for object detection each with trade off between speed size and accuracy we pick one of the most popular one yolo you only look once and have show how it work below in under 20 line of code if you ignore the comment note this be pseudo code not intend to be a work example it have a black box which be the cnn part of it which be fairly standard and show in the image below you can read the full paper here https pjreddie com media file paper yolo_1 pdf for this task you probably need a few 100 image per object try to capture datum as close to the datum you re go to finally make prediction on draw bounding box on the image you can use a tool like labelimg you will typically need a few people who will be work on annotate your image this be a fairly intensive and time consume task you can read more about this at medium com nanonet nanonet how to use deep learning when you have limit datum f68c0b512cab you need a pretraine model so you can reduce the amount of datum require to train without it you might need a few 100k image to train the model you can find a bunch of pretraine model here the process of train a model be unnecessarily difficult to simplify the process we create a docker image would make it easy to train to start train the model you can run the docker image have a run sh script that can be call with the follow parameter you can find more detail at to train a model you need to select the right hyper parameter find the right parameter the art of deep learning involve a little bit of hit and try to figure out which be the good parameter to get the high accuracy for your model there be some level of black magic associate with this along with a little bit of theory this be a great resource for find the right parameter quantize model make it small to fit on a small device like the raspberry pi or mobile small device like mobile phone and rasberry pi have very little memory and computation power training neural network be do by apply many tiny nudge to the weight and these small increment typically need float point precision to work though there be research effort to use quantize representation here too take a pre train model and running inference be very different one of the magical quality of deep neural network be that they tend to cope very well with high level of noise in their input why quantize neural network model can take up a lot of space on disk with the original alexnet be over 200 mb in float format for example almost all of that size be take up with the weight for the neural connection since there be often many million of these in a single model the node and weight of a neural network be originally store as 32 bit float point number the simple motivation for quantization be to shrink file size by store the min and max for each layer and then compress each float value to an eight bit integer the size of the file be reduce by 75 % code for quantization you need the raspberry pi camera live and work then capture a new image for instruction on how to install checkout this link download model once your do train the model you can download it on to your pi to export the model run then download the model onto the raspberry pi install tensorflow on the raspberry pi depend on your device you might need to change the installation a little run model for predict on the new image the raspberry pi have constraint on both memory and compute a version of tensorflow compatible with the raspberry pi gpu be still not available therefore it be important to benchmark how much time do each of the model take to make a prediction on a new image we have remove the need to annotate image we have expert annotator who will annotate your image for you we automatically train the good model for you to achieve this we run a battery of model with different parameter to select the good for your data nanonet be entirely in the cloud and run without use any of your hardware which make it much easy to use since device like the raspberry pi and mobile phone be not build to run complex compute heavy task you can outsource the workload to our cloud which do all of the compute for you get your free api key from http app nanonet com user api_key collect the image of object you want to detect you can annotate they either use our web ui https app nanonet com objectannotation appid = your_model_id or use open source tool like labelimg once you have dataset ready in folder image image file and annotation annotation for the image file start upload the dataset once the image have be upload begin train the model the model take ~2 hour to train you will get an email once the model be train in the meanwhile you check the state of the model once the model be train you can make prediction use the model from a quick cheer to a stand ovation clap to show how much you enjoy this story founder & ceo @ nanonet com nanonet machine learn api
Emil Wallner,9.1K,25,https://medium.freecodecamp.org/how-you-can-train-an-ai-to-convert-your-design-mockups-into-html-and-css-cc7afd82fed4?source=tag_archive---------9----------------,how you can train an ai to convert your design mockup into html and css,within three year deep learning will change front end development it will increase prototype speed and lower the barrier for building software the field take off last year when tony beltramelli introduce the pix2code paper and airbnb launch sketch2code currently the large barrier to automate front end development be compute power however we can use current deep learning algorithm along with synthesize training datum to start explore artificial front end automation right now in this post we ll teach a neural network how to code a basic a html and css website base on a picture of a design mockup here s a quick overview of the process we ll build the neural network in three iteration first we ll make a bare minimum version to get a hang of the move part the second version html will focus on automate all the step and explain the neural network layer in the final version bootstrap we ll create a model that can generalize and explore the lstm layer all the code be prepare on github and floydhub in jupyter notebook all the floydhub notebook be inside the floydhub directory and the local equivalent be under local the model be base on beltramelli s pix2code paper and jason brownlee s image caption tutorial the code be write in python and kera a framework on top of tensorflow if you re new to deep learning i d recommend get a feel for python backpropagation and convolutional neural network my three early post on floydhub s blog will get you start let s recap our goal we want to build a neural network that will generate html css markup that correspond to a screenshot when you train the neural network you give it several screenshot with match html it learn by predict all the match html markup tag one by one when it predict the next markup tag it receive the screenshot as well as all the correct markup tag until that point here be a simple training data example in a google sheet create a model that predict word by word be the most common approach today there be other approach but that s the method we ll use throughout this tutorial notice that for each prediction it get the same screenshot so if it have to predict 20 word it will get the same design mockup twenty time for now don t worry about how the neural network work focus on grasp the input and output of the neural network let s focus on the previous markup say we train the network to predict the sentence I can code when it receive I then it predict can next time it will receive I can and predict code it receive all the previous word and only have to predict the next word the neural network create feature from the datum the network build feature to link the input datum with the output datum it have to create representation to understand what be in each screenshot the html syntax that it have predict this build the knowledge to predict the next tag when you want to use the train model for real world usage it s similar to when you train the model the text be generate one by one with the same screenshot each time instead of feed it with the correct html tag it receive the markup it have generate so far then it predict the next markup tag the prediction be initiate with a start tag and stop when it predict an end tag or reach a max limit here s another example in a google sheet let s build a hello world version we ll feed a neural network a screenshot with a website display hello world and teach it to generate the markup first the neural network map the design mockup into a list of pixel value from 0 255 in three channel — red blue and green to represent the markup in a way that the neural network understand I use one hot encoding thus the sentence I can code could be map like the below in the above graphic we include the start and end tag these tag be cue for when the network start its prediction and when to stop for the input datum we will use sentence start with the first word and then add each word one by one the output datum be always one word sentence follow the same logic as word they also need the same input length instead of be cap by the vocabulary they be bind by maximum sentence length if it s short than the maximum length you fill it up with empty word a word with just zero as you see word be print from right to left this force each word to change position for each training round this allow the model to learn the sequence instead of memorize the position of each word in the below graphic there be four prediction each row be one prediction to the left be the image represent in their three color channel red green and blue and the previous word outside of the bracket be the prediction one by one end with a red square to mark the end in the hello world version we use three token start < html><center><h1 > hello world < h1 > < center > < html > and end a token can be anything it can be a character word or sentence character version require a small vocabulary but constrain the neural network word level token tend to perform well here we make the prediction floydhub be a training platform for deep learning I come across they when I first start learn deep learning and I ve use they since for training and manage my deep learning experiment you can install it and run your first model within 10 minute it s hand down the good option to run model on cloud gpus if you be new to floydhub do their 2 min installation or my 5 minute walkthrough all the notebook be prepared inside the floydhub directory the local equivalent be under local once it s run you can find the first notebook here floydhub helloworld helloworld ipynb if you want more detailed instruction and an explanation for the flag check my early post in this version we ll automate many of the step from the hello world model this section will focus on create a scalable implementation and the move piece in the neural network this version will not be able to predict html from random website but it s still a great setup to explore the dynamic of the problem if we expand the component of the previous graphic it look like this there be two major section first the encoder this be where we create image feature and previous markup feature feature be the building block that the network create to connect the design mockup with the markup at the end of the encoder we glue the image feature to each word in the previous markup the decoder then take the combine design and markup feature and create a next tag feature this feature be run through a fully connect neural network to predict the next tag since we need to insert one screenshot for each word this become a bottleneck when train the network example instead of use the image we extract the information we need to generate the markup the information be encode into image feature this be do by use an already pre train convolutional neural network cnn the model be pre train on imagenet we extract the feature from the layer before the final classification we end up with 1536 eight by eight pixel image know as feature although they be hard to understand for we a neural network can extract the object and position of the element from these feature in the hello world version we use a one hot encoding to represent the markup in this version we ll use a word embed for the input and keep the one hot encoding for the output the way we structure each sentence stay the same but how we map each token be change one hot encoding treat each word as an isolated unit instead we convert each word in the input datum to list of digit these represent the relationship between the markup tag the dimension of this word embed be eight but often vary between 50 500 depend on the size of the vocabulary the eight digit for each word be weight similar to a vanilla neural network they be tune to map how the word relate to each other mikolov et al 2013 this be how we start develop markup feature feature be what the neural network develop to link the input datum with the output datum for now don t worry about what they be we ll dig deeply into this in the next section we ll take the word embedding and run they through an lstm and return a sequence of markup feature these be run through a time distribute dense layer — think of it as a dense layer with multiple input and output in parallel the image feature be first flatten regardless of how the digit be structure they be transform into one large list of number then we apply a dense layer on this layer to form a high level feature these image feature be then concatenate to the markup feature this can be hard to wrap your mind around — so let s break it down here we run the word embedding through the lstm layer in this graphic all the sentence be pad to reach the maximum size of three token to mix signal and find high level pattern we apply a timedistributed dense layer to the markup feature timedistributed dense be the same as a dense layer but with multiple input and output in parallel we prepare the image we take all the mini image feature and transform they into one long list the information be not change just reorganize again to mix signal and extract high level notion we apply a dense layer since we be only deal with one input value we can use a normal dense layer to connect the image feature to the markup feature we copy the image feature in this case we have three markup feature thus we end up with an equal amount of image feature and markup feature all the sentence be pad to create three markup feature since we have prepare the image feature we can now add one image feature for each markup feature after stick one image feature to each markup feature we end up with three image markup feature this be the input we feed into the decoder here we use the combined image markup feature to predict the next tag in the below example we use three image markup feature pair and output one next tag feature note that the lstm layer have the sequence set to false instead of return the length of the input sequence it only predict one feature in our case it s a feature for the next tag it contain the information for the final prediction the dense layer work like a traditional feedforward neural network it connect the 512 digit in the next tag feature with the 4 final prediction say we have 4 word in our vocabulary start hello world and end the vocabulary prediction could be 0 1 0 1 0 1 0 7 the softmax activation in the dense layer distribute a probability from 0 1 with the sum of all prediction equal to 1 in this case it predict that the 4th word be the next tag then you translate the one hot encoding 0 0 0 1 into the mapped value say end if you can t see anything when you click these link you can right click and click on view page source here be the original website for reference in our final version we ll use a dataset of generate bootstrap website from the pix2code paper by use twitter s bootstrap we can combine html and css and decrease the size of the vocabulary we ll enable it to generate the markup for a screenshot it have not see before we ll also dig into how it build knowledge about the screenshot and markup instead of train it on the bootstrap markup we ll use 17 simplify token that we then translate into html and css the dataset include 1500 test screenshot and 250 validation image for each screenshot there be on average 65 token result in 96925 training example by tweak the model in the pix2code paper the model can predict the web component with 97 % accuracy bleu 4 ngram greedy search more on this later extract feature from pre train model work well in image captioning model but after a few experiment I realize that pix2code s end to end approach work well for this problem the pre train model have not be train on web datum and be customize for classification in this model we replace the pre train image feature with a light convolutional neural network instead of use max pooling to increase information density we increase the stride this maintain the position and the color of the front end element there be two core model that enable this convolutional neural network cnn and recurrent neural network rnn the most common recurrent neural network be long short term memory lstm so that s what I ll refer to there be plenty of great cnn tutorial and I cover they in my previous article here I ll focus on the lstms one of the hard thing to grasp about lstms be timestep a vanilla neural network can be think of as two timestep if you give it hello it predict world but it would struggle to predict more timestep in the below example the input have four timestep one for each word lstms be make for input with timestep it s a neural network customize for information in order if you unroll our model it look like this for each downward step you keep the same weight you apply one set of weight to the previous output and another set to the new input the weighted input and output be concatenate and add together with an activation this be the output for that timestep since we reuse the weight they draw information from several input and build knowledge of the sequence here be a simplified version of the process for each timestep in an lstm to get a feel for this logic I d recommend build an rnn from scratch with andrew trask s brilliant tutorial the number of unit in each lstm layer determine it s ability to memorize this also correspond to the size of each output feature again a feature be a long list of number use to transfer information between layer each unit in the lstm layer learn to keep track of different aspect of the syntax below be a visualization of a unit that keep track of the information in the row div this be the simplified markup we be use to train the bootstrap model each lstm unit maintain a cell state think of the cell state as the memory the weight and activation be use to modify the state in different way this enable the lstm layer to fine tune which information to keep and discard for each input in addition to pass through an output feature for each input it also forward the cell state one value for each unit in the lstm to get a feel for how the component within the lstm interact I recommend colah s tutorial jayasiri s numpy implementation and karphay s lecture and write up it s tricky to find a fair way to measure the accuracy say you compare word by word if your prediction be one word out of sync you might have 0 % accuracy if you remove one word which sync the prediction you might end up with 99 100 I use the bleu score good practice in machine translating and image captioning model it break the sentence into four n gram from 1 4 word sequence in the below prediction cat be suppose to be code to get the final score you multiply each score with 25 % 4 5 * 0 25 + 2 4 * 0 25 + 1 3 * 0 25 + 0 2 * 0 25 = 0 2 + 0 125 + 0 083 + 0 = 0 408 the sum be then multiply with a sentence length penalty since the length be correct in our example it become our final score you could increase the number of n gram to make it hard a four n gram model be the model that good correspond to human translation I d recommend run a few example with the below code and read the wiki page link to sample output front end development be an ideal space to apply deep learning it s easy to generate datum and the current deep learning algorithm can map most of the logic one of the most exciting area be apply attention to lstms this will not just improve the accuracy but enable we to visualize where the cnn put its focus as it generate the markup attention be also key for communicate between markup stylesheet script and eventually the backend attention layer can keep track of variable enable the network to communicate between programming language but in the near feature the big impact will come from build a scalable way to synthesize datum then you can add font color word and animation step by step so far most progress be happen in take sketch and turn they into template app in less then two year we ll be able to draw an app on paper and have the corresponding front end in less than a second there be already two work prototype build by airbnb s design team and uizard here be some experiment to get start get start further experiment huge thank to tony beltramelli and jon gold for their research and idea and for answer question thank to jason brownlee for his stellar kera tutorial I include a few snippet from his tutorial in the core keras implementation and beltramelli for provide the datum also thank to qingpe hou charlie harrington sai soundararaj jannes klaas claudio cabral alain demenet and dylan djian for read draft of this this the fourth part of a multi part blog series from emil as he learn deep learning emil have spend a decade explore human learning he s work for oxford s business school invest in education startup and build an education technology business last year he enrol at ecole 42 to apply his knowledge of human learning to machine learning if you build something or get stuck pe I below or on twitter emilwallner i d love to see what you be build this be first publish as a community post on floydhub s blog from a quick cheer to a stand ovation clap to show how much you enjoy this story I study cs at 42 paris blog and experiment with deep learn our community publish story worth read on development design and datum science
Gant Laborde,1.3K,7,https://medium.freecodecamp.org/machine-learning-how-to-go-from-zero-to-hero-40e26f8aa6da?source=---------2----------------,machine learn how to go from zero to hero freecodecamp,if your understanding of a i and machine learning be a big question mark then this be the blog post for you here I gradually increase your awesomenessicitytm by glue inspirational video together with friendly text sit down and relax these video take time and if they don t inspire you to continue to the next section fair enough however if you find yourself at the bottom of this article you ve earn your well rounded knowledge and passion for this new world where you go from there be up to you a I be always cool from move a paddle in pong to light you up with combo in street fighter a I have always revolve around a programmer s functional guess at how something should behave fun but programmer aren t always gift in program a i as we often see just google epic game fail to see glitch in a i physics and sometimes even experience human player regardless a I have a new talent you can teach a computer to play video game understand language and even how to identify people or thing this tip of the iceberg new skill come from an old concept that only recently get the processing power to exist outside of theory I m talk about machine learn you don t need to come up with advanced algorithm anymore you just have to teach a computer to come up with its own advanced algorithm so how do something like that even work an algorithm isn t really write as much as it be sort of breed I m not use breeding as an analogy watch this short video which give excellent commentary and animation to the high level concept of create the a i wow right that s a crazy process now how be it that we can t even understand the algorithm when it s do one great visual be when the a I be write to beat mario game as a human we all understand how to play a side scroller but identify the predictive strategy of the result a I be insane impressed there s something amazing about this idea right the only problem be we don t know machine learning and we don t know how to hook it up to video game fortunately for you elon musk already provide a non profit company to do the latter yes in a dozen line of code you can hook up any a I you want to countless game task I have two good answer on why you should care firstly machine learning ml be make computer do thing that we ve never make computer do before if you want to do something new not just new to you but to the world you can do it with ml secondly if you don t influence the world the world will influence you right now significant company be invest in ml and we re already see it change the world think leader be warn that we can t let this new age of algorithm exist outside of the public eye imagine if a few corporate monolith control the internet if we don t take up arm the science win t be our I think christian heilmann say it well in his talk on ml the concept be useful and cool we understand it at a high level but what the heck be actually happen how do this work if you want to jump straight in I suggest you skip this section and move on to the next how do I get start section if you re motivated to be a doer in ml you win t need these video if you re still try to grasp how this could even be a thing the follow video be perfect for walk you through the logic use the classic ml problem of handwriting pretty cool huh that video show that each layer get simple rather than more complicated like the function be chew datum into small piece that end in an abstract concept you can get your hand dirty in interact with this process on this site by adam harley it s cool watch datum go through a train model but you can even watch your neural network get train one of the classic real world example of machine learning in action be the iris data set from 1936 in a presentation I attend by javafxpert s overview on machine learning I learn how you can use his tool to visualize the adjustment and back propagation of weight to neuron on a neural network you get to watch it train the neural model even if you re not a java buff the presentation jim give on all thing machine learning be a pretty cool 1 5 + hour introduction into ml concept which include more info on many of the example above these concept be exciting be you ready to be the einstein of this new era breakthrough be happen every day so get start now there be ton of resource available I ll be recommend two approach in this approach you ll understand machine learn down to the algorithm and the math I know this way sound tough but how cool would it be to really get into the detail and code this stuff from scratch if you want to be a force in ml and hold your own in deep conversation then this be the route for you I recommend that you try out brilliant org s app always great for any science lover and take the artificial neural network course this course have no time limit and help you learn ml while kill time in line on your phone this one cost money after level 1 combine the above with simultaneous enrollment in andrew ng s stanford course on machine learning in 11 week this be the course that jim weaver recommend in his video above I ve also have this course independently suggest to I by jen looper everyone provide a caveat that this course be tough for some of you that s a show stopper but for other that s why you re go to put yourself through it and collect a certificate say you do this course be 100 % free you only have to pay for a certificate if you want one with those two course you ll have a lot of work to do everyone should be impressed if you make it through because that s not simple but more so if you do make it through you ll have a deep understanding of the implementation of machine learning that will catapult you into successfully apply it in new and world change way if you re not interested in write the algorithm but you want to use they to create the next breathtaking website app you should jump into tensorflow and the crash course tensorflow be the de facto open source software library for machine learn it can be use in countless way and even with javascript here s a crash course plenty more information on available course and ranking can be find here if take a course be not your style you re still in luck you don t have to learn the nitty gritty of ml in order to use it today you can efficiently utilize ml as a service in many way with tech giant who have train model ready I would still caution you that there s no guarantee that your data be safe or even yours but the offering of service for ml be quite attractive use an ml service might be the good solution for you if you re excited and able to upload your datum to amazon microsoft google I like to think of these service as a gateway drug to advanced ml either way it s good to get start now I have to say thank you to all the aforementione people and video they be my inspiration to get start and though I m still a newb in the ml world I m happy to light the path for other as we embrace this awe inspire age we find ourselves in it s imperative to reach out and connect with people if you take up learn this craft without friendly face answer and sound board anything can be hard just be able to ask and get a response be a game changer add I and add the people mention above friendly people with friendly advice help see I hope this article have inspire you and those around you to learn ml from a quick cheer to a stand ovation clap to show how much you enjoy this story software consultant adjunct professor publish author award win speaker mentor organizer and immature nerd d — lately full of react native tech our community publish story worth read on development design and datum science
James JD Sutton,2.2K,9,https://medium.com/coinmonks/what-is-q-from-a-laymen-given-barney-style-6387b18267d2?source=---------3----------------,what be q from a laymen coinmonk medium,a bit long but I think it might help people understand qubic a bit two takeaway I take from read qubic rev_02 take away one 1 if you host a q node a node that support the q protocol layer you can earn reward in these manner offer pow mining rig computer or your coffee pot pos your iota s that you hold your bandwidth that you don t use probably something to do with lifi in the future so this could be your router and lightbulb in your house and simply the previous history of run an honest node for the system all of the above can be use to pass the resource test phase all of those resource pow pos po bandwidth and po honesty be measure and quantify your resource than essentially set you in an equivalent resource pool ie in a pool with other people of similar resource power you then earn iota s from people use the oracle system smart contract or simply who want computational power which be absolutely need to be able to outsource the iot industry which be for sure the future so what do that mean before do you remember all of the question iota win t work because people win t run node because they don t get incentive like traditional blockchain well now they can and not only that q take every aspect of each crypto and combine it all in one pos pow pobandwidth and pohonesty more so if you have asic s you be in the asic s pool gpu s you re in the gpu pool old crappy computer you re in the old crappy computer pool you stake a lot of iota you re in the high stake iota pool etc this be the process of prove your resource to the network people will purchase resource use the qubic protocol if they want quality fast or extreme computational power they have to pay remember you the user set what you want to receive in iota for your resource economic principle if you spend $ 1200 a month on electricity and equipment you will only charge more than $ 1200 a month for your resource no one would charge less so in your pool everyone will eventually come to a quorum charge a set amount and thus the economy the user will pay for it so in essence the well the pool the more the reward you get base on economic principal in society just like blockchain I don t fully understand the exact quantitative measure of what equate to the reward such as with hash power in blockchain though it seem that once you prove your resource your machine perform the calculation that be be buy on the qubic network however if your coffee pot have a jinn chip that be ternary hardware with ternary programming abra then it can sell its resource when it s not make coffee ie prove its resource and then complete computation for buyer this be just speculative and the abra ternary language will be able to interface with binary and low the energy consumption but a significant amount when combine abra with a ternary chip such as jinn the energy efficiency be even more one of the major bottle neck or challenge that prevent advancement in technology be the amount of battery storage within machine if we can t redesign a battery to store more power at least we can redesign the energy consumption within machine device also your autonomous car not only can offer up its pow it can also stake the iota s it be not use in its wallet the bandwidth when it isn t work or drive and the experience honesty factor by prove its resource and then sell its computational power as it may be able to be a node in itself in addition the leave over electricity it have from charge up through solar or wind power it can sell through the smart grid to neighbor or local business your car have multiple resource and the qubic network allow machine to offer all of their resource to their owner not just one or two as with blockchain qubics revolutionize machinery by allow it ; the machinery to sell its resource this be another building block to the ultimate vision of a machine act in a machine economy rather than we set this up and the fee we want to charge eventually we can create smart contract with qubic function which then allow machine to negotiate and earn themselves the machine will sell and buy resource themselves truly create a machine economy and if you own the machine you earn the reward ie income passive income take away two 2 from the above description these be only a few use case that I take away from read about qubic the reality be that the community will be come up with new use case every day for the follow year probably use case that we can t even imagine at the present time but here be my second takeaway the qubic protocol where all this be happen miner earn people stake their iota and earn ie interest or passive income because they be hodler s and by prove their resource they sell their computational power forex financial company use qubic for quorum oracle data smart contract be run on the protocol scientist use computational power for medical research vw fujitsu and bosch use computational power for their iot device etc on and on all those use case to power to power to run the network all those function will be conduct with zero fee transaction that take place on the tangle with real time smart contract micro payment the whole system run on datum transaction zero fee transaction by send metadata within the transaction send on the tangle metadata essentially I m not a techie be like the language that tell the q node to wake up to process datum pay earn and receive and essentially run the whole q network so that be a shitload of transaction occur at the present day the amount of transaction right now occur from trinity speculation and trading be like a drop in the ocean compare to how many transaction the qubic network will produce it s not hard to understand the qubic network will run million if not billion of transaction per day over the tangle and remember each transaction confirm two transaction so what do that mean more transaction mean a fast tangle a more secure tangle an infinitely scalable tangle and most importantly we can take the coo coordinator offline note there may be use case for multiple coo s coordinator or private coo s but that be a whole other arena and I simply state this because I read someone write such an example that go right over my head the point be q be need to remove the coo so as everyone say why don t the dev s focus work on remove the coo wen remove coo you can see that they be work on it the qubic network will support the network because it incentive people to host node and earn iota also if no one use the qubic network then it doesn t work right so make corporate partner and united nations ngo affiliate partnership with bank all of this be need to support the qubic network so here be the building block to the dev s vision you need a tangle zero fee transaction that can that can send meta datum you need iota a transfer mean of metadata and a form of payment that can buy and sell resource ie pow pos pobandwidth and pohonesty you need the qubic network create oracle allow for quorum base computation that power oracle you need oracle oracle power smart contract which be the whole shabang it will change society and change global finance you need the qubic network connect user of the network with resource provider of the network enable a machine economy and provide computational power and the most advanced smart contract to society user of the network we need a community that the iota foundation build from host ama s take the time to talk to the community on discord and provide transparency so we all can go along on their journey of complete their vision we need global partner such as bosch vw fujitsu etc we need government and society such as taiwan denmark and maybe sweden ; and we need banking like dnb and electrical company like elaad we need the global integration to actually use the qubic network for it to work demand drive economic principal which ultimately will pay the q node provider which will drive transaction thus scale the network lastly you need to remove the coo and let the network grow organically this can only be do when the previous step have be complete tangle > iota > qubic network > oracle > partner > coo so remove the coo be one of the last step after remove coo the network can just grow organically on its own without much support or help from the dev s they can then work on building application that work on top of the qubic network this be a large challenging undertaking that be be build step by step each piece be part of a large puzzle that all come together as for the qubic vision which be what be just release be a really large damn piece of that puzzle it just go to show that all of this add up to remove the coo everything the dev s and the if have be do be work towards simply that it s all one big construct not different piece everything tie together and the qubic network be a large friggin piece of it all their sole mission be to complete the puzzle the vision so the coo can be remove and the tangle can literally change society through the machine economy this be just my non techie understanding at the moment I have a lot more research and study to do but damn I love it so glad to be allow within this community and enjoy the journey with the iota foundation please clarify if I totally misunderstood anything and look forward to hear other people s understanding lastly after write this I re read the qubic website difficult to understand but my rough understanding be that q node and qubic can lie dormant listen to the tangle qubic be event drive so that when one qubic initiate another qubic may need that quorum information to activate and when the one qubic get the result it intend to compute then the that qubic itself can activate so one qubic can initiate another qubic and so on so on like neuron fire lighting up a portion of the brain which then fire more neuron this be all do through secured datum stream the tangle and the q node and the qubic network in a way it s a global living system with the data stream as its life blood the tangle as it s bone structure and the qubic and q node as its neuron for all we know in the future this global mass network can be and power ai or maybe it will grow to become one massive ai source that can help society in so many way as I state I m a non techie I probably haven t put out a bit of misinformation as I don t fully understand it all really I just hope to ignite curiosity so people may be inspire to put a two into the new world of the machine economy as well sometimes it be hard to see the big picture the iota foundation have be work on a vision a machine to machine economy that will change society with the tangle as a standard protocol the bone structure of it all the fact be each new development be another puzzle piece or a foundation block that stack on top of the other in the end we have the puzzle as a whole or a great structure build upon a solid foundation https twitter com iotansea https qubic iota org https www iota org https www facebook com group iotatangle from a quick cheer to a stand ovation clap to show how much you enjoy this story the crypto & blockchain publication educate yourself about cryptocurrency blockchain development check tutorial on solidity and smart contract
Justin Lee,511,10,https://medium.com/swlh/the-beginners-guide-to-conversational-commerce-96f9c7dbaefb?source=---------5----------------,the beginner s guide to conversational commerce the startup medium,your greengrocer do it so do that guy sell sunglass on the beach it s why the funny old french bakery around the corner s be run for 15 year conversational marketing a buzzword a footnote a revelation everyone s talk about it but what be it at its most simple it s the act of talk — and more importantly listen — to your customer their problem their story their success forge a genuine connection and use that connection to inform your marketing decision at its most complex conversational marketing have become synonymous with cut edge technology for computer base dialog processing brand have always know that one to one conversation be valuable ; but up until very recently it be impossible to personalize these conversation at scale in real time no long chatbot have become a mainstay of digital marketing and every day their underlie ai become more sophisticated gartner predict that by 2020 30 % of our interaction with technology will be through conversation with smart machine in his 1999 cluetrain manifesto david weinberger remind we that that s a hundred time more true today a successful conversational marketing strategy will pair the spark of authenticity from real conversation with the emerge technology of the future in a 2016 article chris messina distill the concept conversational converse be the process of have a real time one to one conversation with a customer or lead it s a direct personalize dialog drive approach to nurture long term relationship collect datum and increase sale unlike traditional digital marketing it pull user in instead of push content on they it s a discourse not a lecture despite recently pick up speed conversational marketing isn t new the concept make its first appearance in 2007 with joseph jaffe s join the conversation jaffe want to teach marketer to re engage their customer through community partnership and dialog in the past brand have be able to talk at their customer — through email website interaction and social medium — not with they brand have struggle to capture keep and convert attention into sale sign up and long term loyalty engagement be passive and result be shallow customer service be relegate to a formulaic question answer scenario that be unsatisfye for everyone involve take it from lead conversational marketing platform drift s stellar report today message app have over 5 billion monthly active user and for the first time usage rate have surpass social network whether it s chat with friend on whatsapp or exchange idea with coworker on slack messaging have become an integral part of our life despite extreme app saturation the average person only use five app regularly and you guess it — message app claim these spot boast 10x well open rate than the next lead digital channel these messaging platform have huge audience there be over 4 billion active monthly user on the top three message app like the rise of the internet or the app economy of the past decade conversational marketing be bear from current desire for real time connection and genuine value conversational marketing be an umbrella term that encompass every dialog drive tactic from opt in email marketing to customer feedback but the engine power recent development be artificial intelligence ai chatbots represent the new era in conversational marketing scaleable personalize real time and datum drive of course these bot aren t intend to replace human to human interaction ; they re there to support and enhance they help user have the right conversation with the right people at the right time for the meantime anyway accord to gartner research chatbot will account for 85 % of all customer service by 2020 chatbot be a blank canvas with the potential to be mold and infuse with a persona that reflect a company s value — like our very own growthbot aka a mini dharmesh shah this technology be still in its infancy so most bot follow a set of rule program by a human via a bot building platform the differentiator be that the chatbot carry out conversation with user use natural language ai use first person datum to learn more about each customer and deliver a hyper personalized experience rep and bot can then join force to manage these conversation at scale let s imagine I m go to a fancy party tonight it s last minute and I ve just receive a message that it s black tie ; but I don t have the right shoe I need to quickly find a pair that be appropriate ; my size ; coherent with the rest of my outfit ; a good price etc I would usually google for a shop in my area then go to browse on their website to find a pair I like but other issue would soon crop up do they have my size be the shoe smart enough be they in stock I could fill out a query on the site s contact form or give they a ring but will they answer and if they don t have the right pair they be unlikely to suggest a range of alternative the whole process be time consume and inefficient but suppose the brand I like have a strong conversational marketing culture instead of resort to email I would be able to conduct the conversation in second on my phone ; instantly I m give the colour size and style in stock I can pay for the right shoe with a tap of a button conversational marketing enable user to get the information they need instantly without pick up the phone or engage with a person it s not about laziness ; it s about ease chris messina conclude as clara de soto cofounder of reply ai tell venturebeat if user be make to toggle between various app and platform to get the answer they need the value of the bot be moot it need to be native to the place they spend most time whether that s slack messenger or onsite chat but it can be tricky for brand to consolidate all their conversation in one place that s why hubspot create conversation a free multi channel tool that let business have one to one conversation at scale say dharmesh shah co founder and cto of hubspot we have a much low tolerance for mistake with machine compare to human 73 % of people say they win t interact with a bot again after one negative experience and if a bot seem to be able to converse in english we tend to easily overestimate how capable it be that s why it s crucial to manage your customer expectation appropriately bot be far from be autonomous and people aren t easily fool ; try to present your bot as a human agent be likely to be self defeat bot don t understand context create by precede text and conversational nuance can easily affect their capacity to answer because bot live inside message app they have the potential to invade a highly personal space make the stake of get it right much high accord to research people use message app for customer assistance with one key goal to get their problem solve fast bot should serve one simple purpose well without get tangle up in the conversational complication that be well leave to human the way brand and user interract be undergo a monumental shift customer be smart and well inform than ever before they expect personalization and transparency as a prerequisite they feel empower by their option it s hard to fool they and even hard to gain their loyalty and most significantly they want 24 7 365 day of the year instantaneousness to be hear to be help right now ; not in half an hour not tomorrow that s why conversational marketing represent a new cornerstone in marketing but also in customer service and experience branding and sale build a bot for the sake of be on trend be not enough ; it need to be part of a large strategy where each conversation have a purpose as a long term strategy intend to facilitate last relationship it need to be spearhead towards a long term goal effective conversational marketing be an intersection of brand value user engagement and valuable dialogue it s about build your audience first sell last thank for reading originally publish at blog growthbot org from a quick cheer to a stand ovation clap to show how much you enjoy this story head of growth for growthbot messaging & conversational strategy @hubspot medium s large publication for maker subscribe to receive our top story here → https goo gl zhclji
Kai Stinchcombe,44K,11,https://medium.com/@kaistinchcombe/decentralized-and-trustless-crypto-paradise-is-actually-a-medieval-hellhole-c1ca122efdec?source=---------6----------------,blockchain be not only crappy technology but a bad vision for the future,blockchain be not only crappy technology but a bad vision for the future its failure to achieve adoption to date be because system build on trust norm and institution inherently function well than the type of no need for trust party system blockchain envision that s permanent no matter how much blockchain improve it be still head in the wrong direction this december I write a widely circulate article on the inapplicability of blockchain to any actual problem people object mostly not to the technology argument but rather hope that decentralization could produce integrity let s start with this venmo be a free service to transfer dollar and bitcoin transfer be not free yet after I write an article last december say bitcoin have no use someone respond that venmo and paypal be rake in consumer money and people should switch to bitcoin what a surreal contrast between blockchain s non usefulness non adoption and the conviction of its believer it s so entirely evident that this person didn t become a bitcoin enthusiast because they be look for a convenient free way to transfer money from one person to another and discover bitcoin in fact I would assert that there be no single person in existence who have a problem they want to solve discover that an available blockchain solution be the good way to solve it and therefore become a blockchain enthusiast the number of retailer accept cryptocurrency as a form of payment be decline and its big corporate booster like ibm nasdaq fidelity swift and walmart have go long on press but short on actual rollout even the most prominent blockchain company ripple doesn t use blockchain in its product you read that right the company ripple decide the good way to move money across international border be to not use ripple why all the enthusiasm for something so useless in practice people have make a number of implausible claim about the future of blockchain — like that you should use it for ai in place of the type of behavior track that google and facebook do for example this be base on a misunderstanding of what a blockchain be a blockchain isn t an ethereal thing out there in the universe that you can put thing into it s a specific data structure a linear transaction log typically replicate by computer whose owner call miner be reward for log new transaction there be two thing that be cool about this particular data structure one be that a change in any block invalidate every block after it which mean that you can t tamper with historical transaction the second be that you only get reward if you re work on the same chain as everyone else so each participant have an incentive to go with the consensus the end result be a share definitive historical record and what s more because consensus be form by each person act in their own interest add a false transaction or work from a different history just mean you re not get pay and everyone else be follow the rule be mathematically enforce — no government or police force need come in and tell you the transaction you ve log be false or extort bribe or bully the participant it s a powerful idea so in summary here s what blockchain the technology be let s create a very long sequence of small file — each one contain a hash of the previous file some new datum and the answer to a difficult math problem — and divide up some money every hour among anyone willing to certify and store those file for we on their computer now here s what blockchain the metaphor be what if everyone keep their record in a tamper proof repository not own by anyone an illustration of the difference in 2006 walmart launch a system to track its banana and mango from field to store in 2009 they abandon it because of logistical problem get everyone to enter the datum and in 2017 they re launch it to much fanfare on blockchain if someone come to you with the mango picker don t like do data entry I know let s create a very long sequence of small file each one contain a hash of the previous file be a nonsense answer but what if everyone keep their record in a tamper proof repository not own by anyone at least address the right question people treat blockchain as a futuristic integrity wand — wave a blockchain at the problem and suddenly your datum will be valid for almost anything people want to be valid blockchain have be propose as a solution it s true that tamper with datum store on a blockchain be hard but it s false that blockchain be a good way to create datum that have integrity to understand why this be the case let s work from the practical to the theoretical for example let s consider a widely propose use case for blockchain buy an e book with a smart contract the goal of the blockchain be you don t trust an e book vendor and they don t trust you because you re just two individual on the internet but because it s on blockchain you ll be able to trust the transaction in the traditional system once you pay you re hope you ll receive the book but once the vendor have your money they don t have any incentive to deliver you re rely on visa or amazon or the government to make thing fair — what a recipe for be a chump in contrast on a blockchain system by execute the transaction as a record in a tamper proof repository not own by anyone the transfer of money and digital product be automatic atomic and direct with no middleman need to arbitrate the transaction dictate term and take a fat cut on the way isn t that well for everybody hm perhaps you be very skilled at write software when the novelist propose the smart contract you take an hour or two to make sure that the contract will withdraw only an amount of money equal to the agree upon price and that the book — rather than some other file or nothing at all — will actually arrive auditing software be hard the most heavily scrutinize smart contract in history have a small bug that nobody notice — that be until someone do notice it and use it to steal fifty million dollar if cryptocurrency enthusiast put together a $ 150 m investment fund can t properly audit the software how confident be you in your e book audit perhaps you would rather write your own counteroffer software contract in case this e book author have hide a recursion bug in their version to drain your ethereum wallet of all your life saving it s a complicated way to buy a book it s not trustless you re trust in the software and your ability to defend yourself in a software drive world instead of trust other people another example the purport advantage for a voting system in a weakly govern country keep your voting record in a tamper proof repository not own by anyone sound right — yet be your afghan villager go to download the blockchain from a broadcast node and decrypt the merkle root from his linux command line to independently verify that his vote have be count or will he rely on the mobile app of a trust third party — like the nonprofit or open source consortium administer the election or provide the software these sound like stupid example — novelist and villager hire e bodyguard hacker to protect they from malicious customer and nonprofit whose clever smart contract might steal their money and vote — until you realize that s actually the point instead of rely on trust or regulation in the blockchain world individual be on purpose responsible for their own security precaution and if the software they use be malicious or buggy they should have read the software more carefully you actually see it over and over again blockchain system be suppose to be more trustworthy but in fact they be the least trustworthy system in the world today in less than a decade three successive top bitcoin exchange have be hack another be accuse of insider trade the demonstration project dao smart contract get drain crypto price swing be ten time those of the world s most mismanage currency and bitcoin the killer app of crypto transparency be almost certainly artificially prop up by fake transaction involve billion of literally imaginary dollar blockchain system do not magically make the datum in they accurate or the people enter the data trustworthy they merely enable you to audit whether it have be tamper with a person who spray pesticide on a mango can still enter onto a blockchain system that the mango be organic a corrupt government can create a blockchain system to count the vote and just allocate an extra million address to their crony an investment fund whose charter be write in software can still misallocate fund how then be trust create in the case of buy an e book even if you re buy it with a smart contract instead of audit the software you ll rely on one of four thing each of they characteristic of the old way either the author of the smart contract be someone you know of and trust the seller of the e book have a reputation to uphold you or friend of your have buy e book from this seller in the past successfully or you re just willing to hope that this person will deal fairly in each case even if the transaction be effectuate via a smart contract in practice you re rely on trust of a counterparty or middleman not your self protective right to audit the software each man an island unto himself the contract still work but the fact that the promise be write in auditable software rather than government enforce english make it less transparent not more transparent the same for the vote counting before blockchain can even get involve you need to trust that voter registration be do fairly that ballot be give only to eligible voter that the vote be make anonymously rather than buy or intimidate that the vote display by the balloting system be the same as the vote record and that no extra vote be give to the political crony to cast blockchain make none of these problem easy and many of they hard — but more importantly solve they in a blockchain context require a set of awkward workaround that undermine the core premise so we know the entry be valid let s allow only trust nonprofit to make entry — and you re back at the good old classic ledger in fact if you look at any blockchain solution inevitably you ll find an awkward workaround to re create trust party in a trustless world yet absent these old way factor — suppose you actually attempt to rely on blockchain s self interest self protection to build a real system — you d be in a real mess eight hundred year ago in europe — with weak government unable to enforce law and trust counterpartie few fragile and far between — theft be rampant safe banking be a fantasy and personal security be at the point of the sword this be what somalia look like now and also what it look like to transact on the blockchain in the ideal scenario somalia on purpose that s the vision nobody want it even the most die hard crypto enthusiast prefer in practice to rely on trust rather than their own crypto medieval system 93 % of bitcoin be mine by manage consortium yet none of the consortium use smart contract to manage payout instead they promise thing like a long history of stable and accurate payout sound like a trustworthy middleman same with silk road a cryptocurrency drive online drug bazaar the key to silk road wasn t the bitcoin that be just to evade government detection it be the reputation score that allow people to trust criminal and the reputation score weren t track on a tamper proof blockchain they be track by a trust middleman if ripple silk road slush pool and the dao all prefer old way system of create and enforce trust it s no wonder that the outside world have not adopt trustless system either a decentralize tamper proof repository sound like a great way to audit where your mango come from how fresh it be and whether it have be spray with pesticide or not but actually law on food labeling nonprofit or government inspector an independent trust free press empower worker who trust whistleblower protection credible grocery store your local nonprofit farmer s market and so on do a way well job people who actually care about food safety do not adopt blockchain because trust be well than trustless blockchain s technology mess expose its metaphor mess — a software engineer point out that store the datum a sequence of small hash file win t get the mango picker to accurately report whether they spray pesticide be also point out why peer to peer interaction with no regulation norm middleman or trust party be actually a bad way to empower people like the farmer s market or the organic labeling standard so many real idea be hide in plain sight do you wish there be a type of financial institution that be secure and well regulate in all the traditional way but also have the integrity of be people power a credit union s member elect its director and the transaction processing revenue be divide up among the member move your money prefer a deflationary monetary policy central banker be appoint by elect leader want to make election more secure and democratic help write open source voting software go out and register voter or volunteer as an election observer here or abroad wish there be a trust e book delivery service that charge low transaction fee and distribute more of the earning to the author you can already consider state payout rate when you buy music or book buy directly from the author or start your own e book site that s even well than what s out there project base on the elimination of trust have fail to capture customer interest because trust be actually so damn valuable a lawless and mistrustful world where self interest be the only principle and paranoia be the only source of safety be a not a paradise but a crypto medieval hellhole as a society and as technologist and entrepreneur in particular we re go to have to get good at cooperate — at build trust and at be trustworthy instead of direct resource to the elimination of trust we should direct our resource to the creation of trust — whether we use a long series of sequentially hash file as our storage medium or not kai stinchcombe coin the term crypto medieval futuristic integrity wand and smart mango please use freely coin term make you a futurist from a quick cheer to a stand ovation clap to show how much you enjoy this story whatever the opposite of a futurist be
savedroid ICO,340,3,https://medium.com/@ico_8796/sneakpeek-the-savedroid-crypto-saving-app-part-1-your-wish-64d1f7308518?source=---------7----------------,# sneakpeek the savedroid crypto save app — part # 1 your wish,the international beta of our brand new crypto save app be come soon the beta app will be launch in english language and will exclusively be available for our ico token buyer only now get ready to learn more about the savedroid crypto save app even before its official release today we give you a very first sneak peek of one of its core feature your wish with savedroid you can save up for your personal goal you want to afford in the future your own lambo or your desire moon exactly that be your wish so use the savedroid crypto save app be not just about pile up a fortune it s all about save up for your personal wish which you be aspire to fulfill but can t afford right now there be 3 simple step to set up your wish in less than one minute 1 what first name your wish and select one of our illustration to always keep you motivated to continue save you can go small and save for your new pair of hipst sneaker or you may go big and start a crypto saving plan for your new family home everything be possible only the moon be the limit — at least for now 2 how much then set the amount you need to save up to afford your wish the amount be denominate in fiat currency as it be the prevail mean of payment by the way that make it a lot easy for you as you don t need to do the math convert fiat to crypto and vice versa — this complex task be on we 3 when finally select the date by when you want to fulfil your wish and you be do that be easy just as easy as savedroid s other feature will be to deliver on our mission to democratize crypto and bring cryptocurrencie to the masse to keep you post on our late product update we have start this new # sneakpeek series here we will provide you regular sneak peek on our hot new feature stay tune and follow our blog from a quick cheer to a stand ovation clap to show how much you enjoy this story the savedroid ico cryptocurrencie for everyone — now give power to the people join the revolution https ico savedroid com
Brandon Morelli,221,5,https://techburst.io/artificial-intelligence-top-10-articles-june-2018-4b3fa7572b46?source=---------8----------------,artificial intelligence top 10 article — june 2018,here s what s trend this month in artificial intelligence topic include whether you re experience with artificial intelligence or a newbie look to learn the basic of ai there s something for everyone on this list disclosure we receive compensation from the course we feature 4 3 5 star || 17 hour of video || 58 823 student build an ai that combine the power of data science machine learning and deep learning to create powerful ai for real world application you will also have the chance to understand the story behind artificial intelligence learn more 4 7 5 star || 8 hour of video || 15 063 student completely understand the relationship between reinforcement learning and psychology and on a technical level apply gradient base supervised machine learning method to reinforcement learning and implement 17 different reinforcement learning algorithm learn more by lance ulanoff have you hear about the google duplex yet it s pretty much the talk all over the internet google ceo sundar pichai have drop its big bomb when they introduce google duplex to all take a look more on this story to know more by irhum shafkat understand convolution can often feel a bit unnerving yet it s concept be fascinatingly powerful and highly extensible let s try to break down the mechanic of the convolution operation step by step relate and explore it s hierarchy into a more powerful one by wisewolf fund ai be already shape the economy and in the near future its effect may be even more significant ignore the new technology and its influence on the global economic situation be a recipe for failure read more of this article now by sam drozdov machine learning be a field of study that give computer the ability to learn without be explicitly program learn the basic of machine learning and how to apply it to the product you be build right now by aman dalmia have a great opportunity to interact with great mind be one of the awesome privilege one can keep to their knowledge and motivate they to avoid the mistake in a much well manner by simon greenman welcome to ai gold rush check out this awesome article that talk about how company and startup make money on ai and how it help the economic growth as well by justin lee be chatbot hype over already find out why our industry massively overestimate the initial impact chatbot would have and a lot more reason why chatbot be not on the trend anymore by daniel jeffrie ai could mean the end of all job for most people and that s just terrify right check out this topic to get to know more about how will ai bring an explosion to new job by george seif learn more about google s automl — a suite of machine learning tool that will allow one to easily train high performance deep network without require the user to have any knowledge in ai by james loy understand the inner working of deep learning through python with neural network know and train more about neural network from scratch from a quick cheer to a stand ovation clap to show how much you enjoy this story creator of @codeburstio — frequently post web development tutorial & article follow I on twitter too @brandonmorelli burst of tech to power through your day
Sarthak Jain,3.9K,10,https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74?source=---------9----------------,how to easily detect object with deep learning on raspberry pi,disclaimer I m build nanonet com to help build ml with less datum and no hardware the raspberry pi be a neat piece of hardware that have capture the heart of a generation with ~15 m device sell with hacker build even cool project on it give the popularity of deep learning and the raspberry pi camera we think it would be nice if we could detect any object use deep learning on the pi now you will be able to detect a photobomber in your selfie someone enter harambe s cage where someone keep the sriracha or an amazon delivery guy enter your house 20 m year of evolution have make human vision fairly evolve the human brain have 30 % of it s neuron work on processing vision as compare with 8 percent for touch and just 3 percent for hear human have two major advantage when compare with machine one be stereoscopic vision the second be an almost infinite supply of training datum an infant of 5 year have have approximately 2 7b image sample at 30fps to mimic human level performance scientist break down the visual perception task into four different category object detection have be good enough for a variety of application even though image segmentation be a much more precise result it suffer from the complexity of create training datum it typically take a human annotator 12x more time to segment an image than draw bounding box ; this be more anecdotal and lack a source also after detect object it be separately possible to segment the object from the bounding box object detection be of significant practical importance and have be use across a variety of industry some of the example be mention below object detection can be use to answer a variety of question these be the broad category there be a variety of model architecture that be use for object detection each with trade off between speed size and accuracy we pick one of the most popular one yolo you only look once and have show how it work below in under 20 line of code if you ignore the comment note this be pseudo code not intend to be a work example it have a black box which be the cnn part of it which be fairly standard and show in the image below you can read the full paper here https pjreddie com media file paper yolo_1 pdf for this task you probably need a few 100 image per object try to capture datum as close to the datum you re go to finally make prediction on draw bounding box on the image you can use a tool like labelimg you will typically need a few people who will be work on annotate your image this be a fairly intensive and time consume task you can read more about this at medium com nanonet nanonet how to use deep learning when you have limit datum f68c0b512cab you need a pretraine model so you can reduce the amount of datum require to train without it you might need a few 100k image to train the model you can find a bunch of pretraine model here the process of train a model be unnecessarily difficult to simplify the process we create a docker image would make it easy to train to start train the model you can run the docker image have a run sh script that can be call with the follow parameter you can find more detail at to train a model you need to select the right hyper parameter find the right parameter the art of deep learning involve a little bit of hit and try to figure out which be the good parameter to get the high accuracy for your model there be some level of black magic associate with this along with a little bit of theory this be a great resource for find the right parameter quantize model make it small to fit on a small device like the raspberry pi or mobile small device like mobile phone and rasberry pi have very little memory and computation power training neural network be do by apply many tiny nudge to the weight and these small increment typically need float point precision to work though there be research effort to use quantize representation here too take a pre train model and running inference be very different one of the magical quality of deep neural network be that they tend to cope very well with high level of noise in their input why quantize neural network model can take up a lot of space on disk with the original alexnet be over 200 mb in float format for example almost all of that size be take up with the weight for the neural connection since there be often many million of these in a single model the node and weight of a neural network be originally store as 32 bit float point number the simple motivation for quantization be to shrink file size by store the min and max for each layer and then compress each float value to an eight bit integer the size of the file be reduce by 75 % code for quantization you need the raspberry pi camera live and work then capture a new image for instruction on how to install checkout this link download model once your do train the model you can download it on to your pi to export the model run then download the model onto the raspberry pi install tensorflow on the raspberry pi depend on your device you might need to change the installation a little run model for predict on the new image the raspberry pi have constraint on both memory and compute a version of tensorflow compatible with the raspberry pi gpu be still not available therefore it be important to benchmark how much time do each of the model take to make a prediction on a new image we have remove the need to annotate image we have expert annotator who will annotate your image for you we automatically train the good model for you to achieve this we run a battery of model with different parameter to select the good for your data nanonet be entirely in the cloud and run without use any of your hardware which make it much easy to use since device like the raspberry pi and mobile phone be not build to run complex compute heavy task you can outsource the workload to our cloud which do all of the compute for you get your free api key from http app nanonet com user api_key collect the image of object you want to detect you can annotate they either use our web ui https app nanonet com objectannotation appid = your_model_id or use open source tool like labelimg once you have dataset ready in folder image image file and annotation annotation for the image file start upload the dataset once the image have be upload begin train the model the model take ~2 hour to train you will get an email once the model be train in the meanwhile you check the state of the model once the model be train you can make prediction use the model from a quick cheer to a stand ovation clap to show how much you enjoy this story founder & ceo @ nanonet com nanonet machine learn api
Dr. GP Pulipaka,2,6,https://medium.com/@gp_pulipaka/3-ways-to-apply-latent-semantic-analysis-on-large-corpus-text-on-macos-terminal-jupyterlab-colab-7b4dc3e1622?source=---------5----------------,3 way to apply latent semantic analysis on large corpus text on macos terminal jupyterlab and,latent semantic analysis work on large scale dataset to generate representation to discover the insight through natural language processing there be different approach to perform the latent semantic analysis at multiple level such as document level phrase level and sentence level primarily semantic analysis can be summarize into lexical semantic and the study of combine individual word into paragraph or sentence the lexical semantic classifie and decompose the lexical item apply lexical semantic structure have different context to identify the difference and similarity between the word a generic term in a paragraph or a sentence be hypernym and hyponymy provide the meaning of the relationship between instance of the hyponyms homonyms contain similar syntax or similar spelling with similar structuring with different meaning homonym be not relate to each other book be an example for homonym it can mean for someone to read something or an act of make a reservation with similar spelling form and syntax however the definition be different polysemy be another phenomenon of the word where a single word could be associate with multiple relate sense and distinct meaning the word polysemy be a greek word which mean many sign python provide nltk library to perform tokenization of the word by chop the word in large chunk into phrase or meaningful string processing word through tokenization produce token word lemmatization convert word from the current inflect form into the base form latent semantic analysis apply latent semantic analysis on large dataset of text and document represent the contextual meaning through mathematical and statistical computation method on large corpus of text many time latent semantic analysis overtake human score and subject matter test conduct by human the accuracy of latent semantic analysis be high as it read through machine readable document and text at a web scale latent semantic analysis be a technique that apply singular value decomposition and principal component analysis pca the document can be represent with z x y matrix a the row of the matrix represent the document in the collection the matrix a can represent numerous hundred thousand of row and column on a typical large corpus text document apply singular value decomposition develop a set of operation dub matrix decomposition natural language processing in python with nltk library apply a low rank approximation to the term document matrix later the low rank approximation aid in indexing and retrieve the document know as latent semantic indexing by cluster the number of word in the document brief overview of linear algebra the a with z x y matrix contain the real value entry with non negative value for the term document matrix determine the rank of the matrix come with the number of linearly independent column or row in the the matrix the rank of a ≤ { z y } a square c x c represent as diagonal matrix where off diagonal entry be zero examine the matrix if all the c diagonal matrix be one the identity matrix of the dimension c represent by ic for the square z x z matrix a with a vector k which contain not all zero for λ the matrix decomposition apply on the square matrix factor into the product of matrix from eigenvector this allow to reduce the dimensionality of the word from multi dimension to two dimension to view on the plot the dimensionality reduction technique with principal component analysis and singular value decomposition hold critical relevance in natural language process the zipfian nature of the frequency of the word in a document make it difficult to determine the similarity of the word in a static stage hence eigen decomposition be a by product of singular value decomposition as the input of the document be highly asymmetrical the latent semantic analysis be a particular technique in semantic space to parse through the document and identify the word with polysemy with nlkt library the resource such as punkt and wordnet have to be download from nltk deep learning at scale with google colab notebook train machine learning or deep learning model on cpu could take hour and could be pretty expensive in term of the programming language efficiency with time and energy of the computer resource google build colab notebooks environment for research and development purpose it run entirely on the cloud without require any additional hardware or software setup for each machine it s entirely equivalent of a jupyter notebook that aid the datum scientist to share the colab notebook by store on google drive just like any other google sheet or document in a collaborative environment there be no additional cost associate with enable gpu at runtime for acceleration on the runtime there be some challenge of upload the datum into colab unlike jupyter notebook that can access the datum directly from the local directory of the machine in colab there be multiple option to upload the file from the local file system or a drive can be mount to load the datum through drive fuse wrapper once this step be complete it show the follow log without error the next step would be generate the authentication token to authenticate the google credential for the drive and colab if it show successful retrieval of access token then colab be all set at this stage the drive be not mount yet it will show false when access the content of the text file once the drive be mount colab have access to the dataset from google drive once the file be accessible the python can be execute similar to execute in jupyter environment colab notebook also display the result similar to what we see on jupyter notebook pycharm ide the program can be run compile on pycharm ide environment and run on pycharm or can be execute from osx terminal result from osx terminal jupyter notebook on standalone machine jupyter notebook give a similar output run the latent semantic analysis on the local machine reference gorrell g 2006 generalize hebbian algorithm for incremental singular value decomposition in natural language processing retrieve from https www aclweb org anthology e06 1013 hardeniya n 2016 natural language processing python and nltk birmingham england packt publishing landauer t k foltz p w laham d & university of colorado at boulder 1998 an introduction to latent semantic analysis retrieve from http lsa colorado edu papers dp1 lsaintro pdf stackoverflow 2018 mount google drive on google colab retrieve from https stackoverflow com question 50168315 mount google drive on google colab stanford university 2009 matrix decomposition and latent semantic indexing retrieve from https nlp stanford edu ir book html htmledition matrix decomposition and latent semantic indexing 1 html from a quick cheer to a stand ovation clap to show how much you enjoy this story ganapathi pulipaka | founder and ceo @deepsingularity | bestselle author | big datum | iot | startup | sap | machinelearning | deeplearne | datascience
Gabriel Jiménez,50,5,https://medium.com/aimarketingassociation/chatbots-could-we-talk-edd6ccbd8f5a?source=---------7----------------,chatbots could we talk aima ai marketing magazine medium,after the euphoria for app the trend be reverse every day we download few new app and we keep with few in constant use a lot have happen since apple in 2009 proclaim that there be an app for everything as in the next commercial the chat boom accord to the report of the internet association 2017 on the habit of internet user in méxico the second social network use by mexicans be whatsapp a message app and the first although the report have no separate datum be facebook which also include facebook messenger as a particular fact both be from facebook as be instagram that be in position 5 on the list the customer experience in issue such as support attention or navigation in telephone menu and the transition we have make from voice call to text message both for practicity and cost have catalyze the technological development of so call virtual agent or chatbot to optimize resource and improve customer service in an environment where an immediate response be the minimum that be expect the good option to improve customer service at the low cost be through a chatbot but what be a chatbot it be a computer program which work either through rule or the most advanced use artificial intelligence the way to interact with they be via a chat with rule chatbot that work with rule have limit functionality respond only to specific command ; if you do not write correctly what you want it do not understand it with artificial intelligence on the other hand assistant who use artificial intelligence can understand what you say in any way you write it even if you do it incorrectly abbreviate or with idiomatic expression they be also able to improve over time learn the way people express themselves and how they ask context and memory chatbot that use artificial intelligence can resume a previous conversation or base on the context of the chat move forward in a coherent manner if for example we be look for a movie to see in the cinema and first ask we the cinema we want to go to and then the movie then we change the movie and then the chatbot will assume that we continue talk about the same cinema unless we specify otherwise the above may seem very simple for we as people but for a chatbot to maintain a coherent and fluid conversation it be a huge achievement and one that bring great value channel a chatbot can be integrate into any chat application whether corporate your website or commercial like facebook messenger or whatsapp limitation one of the challenge face by chatbot be the initial adoption they may fail mainly for 3 reason 1 as a result of not adequately delimit its initial scope we want to resolve all the possible issue with the chatbot that deal with complaint that support that sell that generate interaction with customer that give service status this cause as with any project scope creep endless requirement which make it seem that the project never will work appropiately 2 it be not link to an activity that solve a business issue sometimes they apply to trivial situation or that do not have a relevant metric link so it be impossible to measure their effectiveness and quantify their benefit to the business 3 be a new technology we tend to think that since it have intelligence it can answer any question outside the business context for which it be define thus also lose the initial focus and evaluate its performance outside the scope for which it be create it be important to remember that although it have artificial intelligence every bot have to have a period of learning and evolution and this take time its process be similar to that carry by a child when it begin to learn it make mistake there be term or form of expression that it do not know but as time pass it become more and more ready due to the experience it acquire with each conversation the same it happen with the chatbot hand over that be why there always have to be a process to re direct a human operator to a conversation in which the chatbot be not able to respond satisfactorily in this way we keep the customer experience as a principle and we avoid frustration to people connection with system the chatbot can give an integral attention to client through chat but its capacity to do it also depend on the integration that this one have with the system of the company ; without this the service you provide will be incomplete and frustrating for example if we have a chatbot to schedule appointment we need that in addition to understand what people ask you can access the agenda system to check if there be time available to schedule if you do not have it you will be limit and it will be practically useless application the main change when use a chatbot be that instead of browse website we can ask to get what we want it be even possible to obtain recommendation base on question to find the most appropriate for us benefit to know about chatbot and artificial intelligence write to I @gabojimenez _ or linkedin com in gabrieljimenezmunoz from a quick cheer to a stand ovation clap to show how much you enjoy this story consultative selling | ai for business | chatbot | analytic | speaker | writer | teacher drive the ai marketing movement
Kai Stinchcombe,44K,11,https://medium.com/@kaistinchcombe/decentralized-and-trustless-crypto-paradise-is-actually-a-medieval-hellhole-c1ca122efdec?source=tag_archive---------0----------------,blockchain be not only crappy technology but a bad vision for the future,blockchain be not only crappy technology but a bad vision for the future its failure to achieve adoption to date be because system build on trust norm and institution inherently function well than the type of no need for trust party system blockchain envision that s permanent no matter how much blockchain improve it be still head in the wrong direction this december I write a widely circulate article on the inapplicability of blockchain to any actual problem people object mostly not to the technology argument but rather hope that decentralization could produce integrity let s start with this venmo be a free service to transfer dollar and bitcoin transfer be not free yet after I write an article last december say bitcoin have no use someone respond that venmo and paypal be rake in consumer money and people should switch to bitcoin what a surreal contrast between blockchain s non usefulness non adoption and the conviction of its believer it s so entirely evident that this person didn t become a bitcoin enthusiast because they be look for a convenient free way to transfer money from one person to another and discover bitcoin in fact I would assert that there be no single person in existence who have a problem they want to solve discover that an available blockchain solution be the good way to solve it and therefore become a blockchain enthusiast the number of retailer accept cryptocurrency as a form of payment be decline and its big corporate booster like ibm nasdaq fidelity swift and walmart have go long on press but short on actual rollout even the most prominent blockchain company ripple doesn t use blockchain in its product you read that right the company ripple decide the good way to move money across international border be to not use ripple why all the enthusiasm for something so useless in practice people have make a number of implausible claim about the future of blockchain — like that you should use it for ai in place of the type of behavior track that google and facebook do for example this be base on a misunderstanding of what a blockchain be a blockchain isn t an ethereal thing out there in the universe that you can put thing into it s a specific data structure a linear transaction log typically replicate by computer whose owner call miner be reward for log new transaction there be two thing that be cool about this particular data structure one be that a change in any block invalidate every block after it which mean that you can t tamper with historical transaction the second be that you only get reward if you re work on the same chain as everyone else so each participant have an incentive to go with the consensus the end result be a share definitive historical record and what s more because consensus be form by each person act in their own interest add a false transaction or work from a different history just mean you re not get pay and everyone else be follow the rule be mathematically enforce — no government or police force need come in and tell you the transaction you ve log be false or extort bribe or bully the participant it s a powerful idea so in summary here s what blockchain the technology be let s create a very long sequence of small file — each one contain a hash of the previous file some new datum and the answer to a difficult math problem — and divide up some money every hour among anyone willing to certify and store those file for we on their computer now here s what blockchain the metaphor be what if everyone keep their record in a tamper proof repository not own by anyone an illustration of the difference in 2006 walmart launch a system to track its banana and mango from field to store in 2009 they abandon it because of logistical problem get everyone to enter the datum and in 2017 they re launch it to much fanfare on blockchain if someone come to you with the mango picker don t like do data entry I know let s create a very long sequence of small file each one contain a hash of the previous file be a nonsense answer but what if everyone keep their record in a tamper proof repository not own by anyone at least address the right question people treat blockchain as a futuristic integrity wand — wave a blockchain at the problem and suddenly your datum will be valid for almost anything people want to be valid blockchain have be propose as a solution it s true that tamper with datum store on a blockchain be hard but it s false that blockchain be a good way to create datum that have integrity to understand why this be the case let s work from the practical to the theoretical for example let s consider a widely propose use case for blockchain buy an e book with a smart contract the goal of the blockchain be you don t trust an e book vendor and they don t trust you because you re just two individual on the internet but because it s on blockchain you ll be able to trust the transaction in the traditional system once you pay you re hope you ll receive the book but once the vendor have your money they don t have any incentive to deliver you re rely on visa or amazon or the government to make thing fair — what a recipe for be a chump in contrast on a blockchain system by execute the transaction as a record in a tamper proof repository not own by anyone the transfer of money and digital product be automatic atomic and direct with no middleman need to arbitrate the transaction dictate term and take a fat cut on the way isn t that well for everybody hm perhaps you be very skilled at write software when the novelist propose the smart contract you take an hour or two to make sure that the contract will withdraw only an amount of money equal to the agree upon price and that the book — rather than some other file or nothing at all — will actually arrive auditing software be hard the most heavily scrutinize smart contract in history have a small bug that nobody notice — that be until someone do notice it and use it to steal fifty million dollar if cryptocurrency enthusiast put together a $ 150 m investment fund can t properly audit the software how confident be you in your e book audit perhaps you would rather write your own counteroffer software contract in case this e book author have hide a recursion bug in their version to drain your ethereum wallet of all your life saving it s a complicated way to buy a book it s not trustless you re trust in the software and your ability to defend yourself in a software drive world instead of trust other people another example the purport advantage for a voting system in a weakly govern country keep your voting record in a tamper proof repository not own by anyone sound right — yet be your afghan villager go to download the blockchain from a broadcast node and decrypt the merkle root from his linux command line to independently verify that his vote have be count or will he rely on the mobile app of a trust third party — like the nonprofit or open source consortium administer the election or provide the software these sound like stupid example — novelist and villager hire e bodyguard hacker to protect they from malicious customer and nonprofit whose clever smart contract might steal their money and vote — until you realize that s actually the point instead of rely on trust or regulation in the blockchain world individual be on purpose responsible for their own security precaution and if the software they use be malicious or buggy they should have read the software more carefully you actually see it over and over again blockchain system be suppose to be more trustworthy but in fact they be the least trustworthy system in the world today in less than a decade three successive top bitcoin exchange have be hack another be accuse of insider trade the demonstration project dao smart contract get drain crypto price swing be ten time those of the world s most mismanage currency and bitcoin the killer app of crypto transparency be almost certainly artificially prop up by fake transaction involve billion of literally imaginary dollar blockchain system do not magically make the datum in they accurate or the people enter the data trustworthy they merely enable you to audit whether it have be tamper with a person who spray pesticide on a mango can still enter onto a blockchain system that the mango be organic a corrupt government can create a blockchain system to count the vote and just allocate an extra million address to their crony an investment fund whose charter be write in software can still misallocate fund how then be trust create in the case of buy an e book even if you re buy it with a smart contract instead of audit the software you ll rely on one of four thing each of they characteristic of the old way either the author of the smart contract be someone you know of and trust the seller of the e book have a reputation to uphold you or friend of your have buy e book from this seller in the past successfully or you re just willing to hope that this person will deal fairly in each case even if the transaction be effectuate via a smart contract in practice you re rely on trust of a counterparty or middleman not your self protective right to audit the software each man an island unto himself the contract still work but the fact that the promise be write in auditable software rather than government enforce english make it less transparent not more transparent the same for the vote counting before blockchain can even get involve you need to trust that voter registration be do fairly that ballot be give only to eligible voter that the vote be make anonymously rather than buy or intimidate that the vote display by the balloting system be the same as the vote record and that no extra vote be give to the political crony to cast blockchain make none of these problem easy and many of they hard — but more importantly solve they in a blockchain context require a set of awkward workaround that undermine the core premise so we know the entry be valid let s allow only trust nonprofit to make entry — and you re back at the good old classic ledger in fact if you look at any blockchain solution inevitably you ll find an awkward workaround to re create trust party in a trustless world yet absent these old way factor — suppose you actually attempt to rely on blockchain s self interest self protection to build a real system — you d be in a real mess eight hundred year ago in europe — with weak government unable to enforce law and trust counterpartie few fragile and far between — theft be rampant safe banking be a fantasy and personal security be at the point of the sword this be what somalia look like now and also what it look like to transact on the blockchain in the ideal scenario somalia on purpose that s the vision nobody want it even the most die hard crypto enthusiast prefer in practice to rely on trust rather than their own crypto medieval system 93 % of bitcoin be mine by manage consortium yet none of the consortium use smart contract to manage payout instead they promise thing like a long history of stable and accurate payout sound like a trustworthy middleman same with silk road a cryptocurrency drive online drug bazaar the key to silk road wasn t the bitcoin that be just to evade government detection it be the reputation score that allow people to trust criminal and the reputation score weren t track on a tamper proof blockchain they be track by a trust middleman if ripple silk road slush pool and the dao all prefer old way system of create and enforce trust it s no wonder that the outside world have not adopt trustless system either a decentralize tamper proof repository sound like a great way to audit where your mango come from how fresh it be and whether it have be spray with pesticide or not but actually law on food labeling nonprofit or government inspector an independent trust free press empower worker who trust whistleblower protection credible grocery store your local nonprofit farmer s market and so on do a way well job people who actually care about food safety do not adopt blockchain because trust be well than trustless blockchain s technology mess expose its metaphor mess — a software engineer point out that store the datum a sequence of small hash file win t get the mango picker to accurately report whether they spray pesticide be also point out why peer to peer interaction with no regulation norm middleman or trust party be actually a bad way to empower people like the farmer s market or the organic labeling standard so many real idea be hide in plain sight do you wish there be a type of financial institution that be secure and well regulate in all the traditional way but also have the integrity of be people power a credit union s member elect its director and the transaction processing revenue be divide up among the member move your money prefer a deflationary monetary policy central banker be appoint by elect leader want to make election more secure and democratic help write open source voting software go out and register voter or volunteer as an election observer here or abroad wish there be a trust e book delivery service that charge low transaction fee and distribute more of the earning to the author you can already consider state payout rate when you buy music or book buy directly from the author or start your own e book site that s even well than what s out there project base on the elimination of trust have fail to capture customer interest because trust be actually so damn valuable a lawless and mistrustful world where self interest be the only principle and paranoia be the only source of safety be a not a paradise but a crypto medieval hellhole as a society and as technologist and entrepreneur in particular we re go to have to get good at cooperate — at build trust and at be trustworthy instead of direct resource to the elimination of trust we should direct our resource to the creation of trust — whether we use a long series of sequentially hash file as our storage medium or not kai stinchcombe coin the term crypto medieval futuristic integrity wand and smart mango please use freely coin term make you a futurist from a quick cheer to a stand ovation clap to show how much you enjoy this story whatever the opposite of a futurist be
Dhruv Parthasarathy,4.3K,12,https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4?source=tag_archive---------1----------------,a brief history of cnn in image segmentation from r cnn to mask r cnn,at athela we use convolutional neural network cnn for a lot more than just classification in this post we ll see how cnn can be use with great result in image instance segmentation ever since alex krizhevsky geoff hinton and ilya sutskever win imagenet in 2012 convolutional neural network cnn have become the gold standard for image classification in fact since then cnn have improve to the point where they now outperform human on the imagenet challenge while these result be impressive image classification be far simple than the complexity and diversity of true human visual understanding in classification there s generally an image with a single object as the focus and the task be to say what that image be see above but when we look at the world around we we carry out far more complex task we see complicated sight with multiple overlap object and different background and we not only classify these different object but also identify their boundary difference and relation to one another can cnn help we with such complex task namely give a more complicated image can we use cnn to identify the different object in the image and their boundary as have be show by ross girshick and his peer over the last few year the answer be conclusively yes through this post we ll cover the intuition behind some of the main technique use in object detection and segmentation and see how they ve evolve from one implementation to the next in particular we ll cover r cnn regional cnn the original application of cnn to this problem along with its descendant fast r cnn and fast r cnn finally we ll cover mask r cnn a paper release recently by facebook research that extend such object detection technique to provide pixel level segmentation here be the paper reference in this post inspire by the research of hinton s lab at the university of toronto a small team at uc berkeley lead by professor jitendra malik ask themselves what today seem like an inevitable question object detection be the task of find the different object in an image and classify they as see in the image above the team comprise of ross girshick a name we ll see again jeff donahue and trevor darrel find that this problem can be solve with krizhevsky s result by test on the pascal voc challenge a popular object detection challenge akin to imagenet they write let s now take a moment to understand how their architecture region with cnn r cnn work understand r cnn the goal of r cnn be to take in an image and correctly identify where the main object via a bounding box in the image but how do we find out where these bounding box be r cnn do what we might intuitively do as well propose a bunch of box in the image and see if any of they actually correspond to an object r cnn create these bounding box or region proposal use a process call selective search which you can read about here at a high level selective search show in the image above look at the image through window of different size and for each size try to group together adjacent pixel by texture color or intensity to identify object once the proposal be create r cnn warps the region to a standard square size and pass it through to a modified version of alexnet the win submission to imagenet 2012 that inspire r cnn as show above on the final layer of the cnn r cnn add a support vector machine svm that simply classify whether this be an object and if so what object this be step 4 in the image above improve the bounding box now have find the object in the box can we tighten the box to fit the true dimension of the object we can and this be the final step of r cnn r cnn run a simple linear regression on the region proposal to generate tight bounding box coordinate to get our final result here be the input and output of this regression model so to summarize r cnn be just the follow step r cnn work really well but be really quite slow for a few simple reason in 2015 ross girshick the first author of r cnn solve both these problem lead to the second algorithm in our short history fast r cnn let s now go over its main insight fast r cnn insight 1 roi region of interest pooling for the forward pass of the cnn girshick realize that for each image a lot of propose region for the image invariably overlap cause we to run the same cnn computation again and again ~2000 time his insight be simple — why not run the cnn just once per image and then find a way to share that computation across the ~2000 proposal this be exactly what fast r cnn do use a technique know as roipool region of interest pooling at its core roipool share the forward pass of a cnn for an image across its subregion in the image above notice how the cnn feature for each region be obtain by select a correspond region from the cnn s feature map then the feature in each region be pool usually use max pooling so all it take we be one pass of the original image as oppose to ~2000 fast r cnn insight 2 combine all model into one network the second insight of fast r cnn be to jointly train the cnn classifier and bounding box regressor in a single model where early we have different model to extract image feature cnn classify svm and tighten bounding box regressor fast r cnn instead use a single network to compute all three you can see how this be do in the image above fast r cnn replace the svm classifier with a softmax layer on top of the cnn to output a classification it also add a linear regression layer parallel to the softmax layer to output bounding box coordinate in this way all the output need come from one single network here be the input and output to this overall model even with all these advancement there be still one remain bottleneck in the fast r cnn process — the region proposer as we see the very first step to detect the location of object be generate a bunch of potential bounding box or region of interest to test in fast r cnn these proposal be create use selective search a fairly slow process that be find to be the bottleneck of the overall process in the middle 2015 a team at microsoft research compose of shaoqe ren kaime he ross girshick and jian sun find a way to make the region proposal step almost cost free through an architecture they creatively name fast r cnn the insight of fast r cnn be that region proposal depend on feature of the image that be already calculate with the forward pass of the cnn first step of classification so why not reuse those same cnn result for region proposal instead of run a separate selective search algorithm indeed this be just what the fast r cnn team achieve in the image above you can see how a single cnn be use to both carry out region proposal and classification this way only one cnn need to be train and we get region proposal almost for free the author write here be the input and output of their model how the region be generate let s take a moment to see how fast r cnn generate these region proposal from cnn feature fast r cnn add a fully convolutional network on top of the feature of the cnn create what s know as the region proposal network the region proposal network work by pass a slide window over the cnn feature map and at each window output k potential bounding box and score for how good each of those box be expect to be what do these k box represent intuitively we know that object in an image should fit certain common aspect ratio and size for instance we know that we want some rectangular box that resemble the shape of human likewise we know we win t see many box that be very very thin in such a way we create k such common aspect ratio we call anchor box for each such anchor box we output one bounding box and score per position in the image with these anchor box in mind let s take a look at the input and output to this region proposal network we then pass each such bounding box that be likely to be an object into fast r cnn to generate a classification and tightened bounding box so far we ve see how we ve be able to use cnn feature in many interesting way to effectively locate different object in an image with bounding box can we extend such technique to go one step far and locate exact pixel of each object instead of just bound box this problem know as image segmentation be what kaime he and a team of researcher include girshick explore at facebook ai use an architecture know as mask r cnn much like fast r cnn and fast r cnn mask r cnn s underlying intuition be straight forward give that fast r cnn work so well for object detection could we extend it to also carry out pixel level segmentation mask r cnn do this by add a branch to fast r cnn that output a binary mask that say whether or not a give pixel be part of an object the branch in white in the above image as before be just a fully convolutional network on top of a cnn base feature map here be its input and output but the mask r cnn author have to make one small adjustment to make this pipeline work as expect roialign realigning roipool to be more accurate when run without modification on the original fast r cnn architecture the mask r cnn author realize that the region of the feature map select by roipool be slightly misalign from the region of the original image since image segmentation require pixel level specificity unlike bound box this naturally lead to inaccuracy the author be able to solve this problem by cleverly adjust roipool to be more precisely align use a method know as roialign imagine we have an image of size 128x128 and a feature map of size 25x25 let s imagine we want feature the region correspond to the top leave 15x15 pixel in the original image see above how might we select these pixel from the feature map we know each pixel in the original image correspond to ~ 25 128 pixel in the feature map to select 15 pixel from the original image we just select 15 * 25 128 ~= 2 93 pixel in roipool we would round this down and select 2 pixel cause a slight misalignment however in roialign we avoid such round instead we use bilinear interpolation to get a precise idea of what would be at pixel 2 93 this at a high level be what allow we to avoid the misalignment cause by roipool once these mask be generate mask r cnn combine they with the classification and bounding box from fast r cnn to generate such wonderfully precise segmentation if you re interested in try out these algorithm yourself here be relevant repository fast r cnn mask r cnn in just 3 year we ve see how the research community have progress from krizhevsky et al s original result to r cnn and finally all the way to such powerful result as mask r cnn see in isolation result like mask r cnn seem like incredible leap of genius that would be unapproachable yet through this post I hope you ve see how such advancement be really the sum of intuitive incremental improvement through year of hard work and collaboration each of the idea propose by r cnn fast r cnn fast r cnn and finally mask r cnn be not necessarily quantum leap yet their sum product have lead to really remarkable result that bring we close to a human level understanding of sight what particularly excite I be that the time between r cnn and mask r cnn be just three year with continue funding focus and support how much far can computer vision improve over the next three year if you see any error or issue in this post please contact I at dhruv@getathela com and I ll immediately correct they if you re interested in apply such technique come join we at athela where we apply computer vision to blood diagnostic daily other post we ve write thank to bharath ramsundar pranav ramkrishnan tanay tandon and oliver cameron for help with this post from a quick cheer to a stand ovation clap to show how much you enjoy this story @dhruvp vp eng @athelas mit math and cs undergrad 13 mit cs master 14 previously director of ai program @ udacity blood diagnostic through deep learning http athela com
Slav Ivanov,3.9K,17,https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=tag_archive---------2----------------,the $ 1700 great deep learning box assembly setup and benchmark,update april 2018 use cuda 9 cudnn 7 and tensorflow 1 5 after year of use a thin client in the form of increasingly thin macbook I have get use to it so when I get into deep learning dl I go straight for the brand new at the time amazon p2 cloud server no upfront cost the ability to train many model simultaneously and the general coolness of have a machine learning model out there slowly teach itself however as time pass the aws bill steadily grow large even as I switch to 10x cheap spot instance also I didn t find myself train more than one model at a time instead I d go to lunch workout etc while the model be train and come back later with a clear head to check on it but eventually the model complexity grow and take long to train I d often forget what I do differently on the model that have just complete its 2 day training nudge by the great experience of the other folk on the fast ai forum I decide to settle down and to get a dedicated dl box at home the most important reason be save time while prototyping model — if they train fast the feedback time would be short thus it would be easy for my brain to connect the dot between the assumption I have for the model and its result then I want to save money — I be use amazon web service aws which offer p2 instance with nvidia k80 gpus lately the aws bill be around $ 60 70 month with a tendency to get large also it be expensive to store large dataset like imagenet and lastly I haven t have a desktop for over 10 year and want to see what have change in the meantime spoiler alert mostly nothing what follow be my choice inner monologue and gotcha from choose the component to benchmarke a sensible budget for I would be about 2 year worth of my current compute spending at $ 70 month for aw this put it at around $ 1700 for the whole thing you can check out all the component use the pc part picker site be also really helpful in detect if some of the component don t play well together the gpu be the most crucial component in the box it will train these deep network fast shorten the feedback cycle disclosure the follow be affiliate link to help I pay for well more gpu the choice be between a few of nvidia s card gtx 1070 gtx 1070 ti gtx 1080 gtx 1080 ti and finally the titan x the price might fluctuate especially because some gpu be great for cryptocurrency mining wink 1070 wink on performance side gtx 1080 ti and titan x be similar roughly speak the gtx 1080 be about 25 % fast than gtx 1070 and gtx 1080 ti be about 30 % fast than gtx 1080 the new gtx 1070 ti be very close in performance to gtx 1080 tim dettmer have a great article on pick a gpu for deep learning which he regularly update as new card come on the market here be the thing to consider when pick a gpu consider all of this I pick the gtx 1080 ti mainly for the training speed boost I plan to add a second 1080 ti soonish even though the gpu be the mvp in deep learning the cpu still matter for example data preparation be usually do on the cpu the number of core and thread per core be important if we want to parallelize all that data prep to stay on budget I pick a mid range cpu the intel i5 7500 it s relatively cheap but good enough to not slow thing down edit as a few people have point out probably the big gotcha that be unique to dl multi gpu be to pay attention to the pcie lane support by the cpu motherboard by andrej karpathy we want to have each gpu have 16 pcie lane so it eat datum as fast as possible 16 gb s for pcie 3 0 this mean that for two card we need 32 pcie lane however the cpu I have pick have only 16 lane so 2 gpu would run in 2x8 mode instead of 2x16 this might be a bottleneck lead to less than ideal utilization of the graphic card thus a cpu with 40 line be recommend edit 2 however tim dettmer point out that have 8 lane per card should only decrease performance by 0 10 % for two gpu so currently my recommendation be go with 16 pcie lane per video card unless it get too expensive for you otherwise 8 lane should do as well a good solution with to have for a double gpu machine would be an intel xeon processor like the e5 1620 v4 40 pcie lane or if you want to splurge go for a high end processor like the desktop i7 6850k memory ram it s nice to have a lot of memory if we be to be work with rather big dataset I get 2 stick of 16 gb for a total of 32 gb of ram and plan to buy another 32 gb later follow jeremy howard s advice I get a fast ssd disk to keep my os and current datum on and then a slow spin hdd for those huge dataset like imagenet ssd I remember when I get my first macbook air year ago how blow away be I by the ssd speed to my delight a new generation of ssd call nvme have make its way to market in the meantime a 480 gb mydigitalssd nvme drive be a great deal this baby copy file at gigabyte per second hdd 2 tb seagate while ssds have be get fast hdd have be get cheap to somebody who have use macbook with 128 gb disk for the last 7 year have this much space feel almost obscene the one thing that I keep in mind when pick a motherboard be the ability to support two gtx 1080 ti both in the number of pci express lane the minimum be 2x8 and the physical size of 2 card also make sure it s compatible with the choose cpu an asus tuf z270 do it for I msi — x99a sli plus should work great if you get an intel xeon cpu rule of thumb power supply should provide enough juice for the cpu and the gpu plus 100 watt extra the intel i5 7500 processor use 65w and the gpus 1080 ti need 250w each so I get a deepcool 750w gold psu currently unavailable evga 750 gq be similar the gold here refer to the power efficiency I e how much of the power consume be waste as heat the case should be the same form factor as the motherboard also have enough led to embarrass a burner be a bonus a friend recommend the thermaltake n23 case which I promptly get no led sadly here be how much I spend on all the component your cost may vary $ 700 gtx 1080 ti + $ 190 cpu + $ 230 ram + $ 230 ssd + $ 66 hdd + $ 130 motherboard + $ 75 psu + $ 50 case = = = = = = = = = = = = $ 1671 total add tax and fee this nicely match my preset budget of $ 1700 if you don t have much experience with hardware and fear you might break something a professional assembly might be the good option however this be a great learning opportunity that I couldn t pass even though I ve have my share of hardware relate horror story the first and important step be to read the installation manual that come with each component especially important for I as I ve do this before once or twice and I have just the right amount of inexperience to mess thing up this be do before instal the motherboard in the case next to the processor there be a lever that need to be pull up the processor be then place on the base double check the orientation finally the lever come down to fix the cpu in place but I have a quite the difficulty do this once the cpu be in position the lever wouldn t go down I actually have a more hardware capable friend of mine video walk I through the process turn out the amount of force require to get the lever lock down be more than what I be comfortable with next be fix the fan on top of the cpu the fan leg must be fully secure to the motherboard consider where the fan cable will go before instal the processor I have come with thermal paste if yours doesn t make sure to put some paste between the cpu and the cool unit also replace the paste if you take off the fan I put the power supply unit psu in before the motherboard to get the power cable snugly place in case back side pretty straight forward — carefully place it and screw it in a magnetic screwdriver be really helpful then connect the power cable and the case button and led just slide it in the m2 slot and screw it in piece of cake the memory prove quite hard to install require too much effort to properly lock in a few time I almost give up thinking I must be do it wrong eventually one of the stick click in and the other one promptly follow at this point I turn the computer on to make sure it work to my relief it start right away finally the gpu slide in effortlessly 14 pin of power later and it be run nb do not plug your monitor in the external card right away most probably it need driver to function see below finally it s complete now that we have the hardware in place only the soft part remain out with the screwdriver in with the keyboard note on dual booting if you plan to install window because you know for benchmark totally not for game it would be wise to do window first and linux second I didn t and have to reinstall ubuntu because window mess up the boot partition livewire have a detailed article on dual boot most dl framework be design to work on linux first and eventually support other operating system so I go for ubuntu my default linux distribution an old 2 gb usb drive be lay around and work great for the installation unetbootin osx or rufus window can prepare the linux thumb drive the default option work fine during the ubuntu install at the time of write ubuntu 17 04 be just release so I opt for the previous version 16 04 whose quirk be much well document online ubuntu server or desktop the server and desktop edition of ubuntu be almost identical with the notable exception of the visual interface call x not be instal with server I instal the desktop and disabled autostarte x so that the computer would boot it in terminal mode if need one could launch the visual desktop later by type startx let s get our install up to date from jeremy howard s excellent install gpu script to deep learn on our machine we need a stack of technology to use our gpu download cuda from nvidia or just run the code below update to specify version 9 of cuda thank to @zhanwenchen for the tip if you need to add later version of cuda click here after cuda have be instal the follow code will add the cuda installation to the path variable now we can verify that cuda have be instal successfully by run this should have instal the display driver as well for I nvidia smi show err as the device name so I instal the late nvidia driver as of may 2018 to fix it remove cuda nvidia driver if at any point the driver or cuda seem break as they do for I — multiple time it might be well to start over by run since version 1 5 tensorflow support cudnn 7 so we install that to download cudnn one need to register for a free developer account after download install with the follow anaconda be a great package manager for python I ve move to python 3 6 so will be use the anaconda 3 version the popular dl framework by google installation validate tensorfow install to make sure we have our stack run smoothly I like to run the tensorflow mnist example we should see the loss decrease during training keras be a great high level neural network framework an absolute pleasure to work with installation can t be easy too pytorch be a newcomer in the world of dl framework but its api be model on the successful torch which be write in lua pytorch feel new and exciting mostly great although some thing be still to be implement we install it by run jupyter be a web base ide for python which be ideal for data sciency task it s instal with anaconda so we just configure and test it now if we open http localhost 8888 we should see a jupyter screen run jupyter on boot rather than run the notebook every time the computer be restart we can set it to autostart on boot we will use crontab to do this which we can edit by run crontab e then add the following after the last line in the crontab file I use my old trusty macbook air for development so I d like to be able to log into the dl box both from my home network also when on the run ssh key it s way more secure to use a ssh key to login instead of a password digital ocean have a great guide on how to setup this ssh tunnel if you want to access your jupyter notebook from another computer the recommend way be to use ssh tunneling instead of open the notebook to the world and protect with a password let s see how we can do this 2 then to connect over ssh tunnel run the follow script on the client to test this open a browser and try http localhost 8888 from the remote machine your jupyter notebook should appear setup out of network access finally to access the dl box from the outside world we need 3 thing set up out of network access depend on the router network setup so I m not go into detail now that we have everything run smoothly let s put it to the test we ll be compare the newly build box to an aws p2 xlarge instance which be what I ve use so far for dl the test be computer vision relate mean convolutional network with a fully connect model throw in we time training model on aws p2 instance gpu k80 aw p2 virtual cpu the gtx 1080 ti and intel i5 7500 cpu andre hernandez point out that my comparison do not use tensorflow that be optimize for these cpu which would have help the they perform well check his insightful comment for more detail the hello world of computer vision the mnist database consist of 70 000 handwritten digit we run the keras example on mnist which use multilayer perceptron mlp the mlp mean that we be use only fully connect layer not convolution the model be train for 20 epoch on this dataset which achieve over 98 % accuracy out of the box we see that the gtx 1080 ti be 2 4 time fast than the k80 on aws p2 in train the model this be rather surprising as these 2 card should have about the same performance I believe this be because of the virtualization or underclocking of the k80 on aws the cpus perform 9 time slow than the gpu as we will see later it s a really good result for the processor this be due to the small model which fail to fully utilize the parallel processing power of the gpu interestingly the desktop intel i5 7500 achieve 2 3x speedup over the virtual cpu on amazon a vgg net will be finetune for the kaggle dog vs cat competition in this competition we need to tell apart picture of dog and cat run the model on cpus for the same number of batch wasn t feasible therefore we finetune for 390 batch 1 epoch on the gpu and 10 batch on the cpus the code use be on github the 1080 ti be 5 5 time fast that the aws gpu k80 the difference in the cpus performance be about the same as the previous experiment i5 be 2 6x fast however it s absolutely impractical to use cpu for this task as the cpus be take ~200x more time on this large model that include 16 convolutional layer and a couple semi wide 4096 fully connect layer on top a gan generative adversarial network be a way to train a model to generate image gan achieve this by pit two network against each other a generator which learn to create well and well image and a discriminator that try to tell which image be real and which be dream up by the generator the wasserstein gan be an improvement over the original gan we will use a pytorch implementation that be very similar to the one by the wgan author the model be train for 50 step and the loss be all over the place which be often the case with gan cpus aren t consider the gtx 1080 ti finish 5 5x fast than the aws p2 k80 which be in line with the previous result the final benchmark be on the original style transfer paper gatys et al implement on tensorflow code available style transfer be a technique that combine the style of one image a painting for example and the content of another image check out my previous post for more detail on how style transfer work the gtx 1080 ti outperform the aws k80 by a factor of 4 3 this time the cpu be 30 50 time slow than graphic card the slowdown be less than on the vgg finetune task but more than on the mnist perceptron experiment the model use mostly the early layer of the vgg network and I suspect this be too shallow to fully utilize the gpus the dl box be in the next room and a large model be train on it be it a wise investment time will tell but it be beautiful to watch the glow led in the dark and to hear its quiet hum as model be try to squeeze out that extra accuracy percentage point from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur hacker machine learn deep learning and other type of learn
Tyler Elliot Bettilyon,17.9K,13,https://medium.com/@TebbaVonMathenstien/are-programmers-headed-toward-another-bursting-bubble-528e30c59a0e?source=tag_archive---------3----------------,be programmer head toward another bursting bubble,a friend of mine recently pose a question that I ve hear many time in vary form and forum do you think it and some low level programming job be go to go the way of the dodo seem a bit like a massive job bubble that s go to burst it s my opinion that one of the only thing keep tech and low level computer science relate job prestigious and well pay be ridiculous industry jargon and public ignorance about computer which be both go to go away in the next 10 year this question be simultaneously on point about the future of technology job and exemplary of some pervasive misunderstanding regard the field of software engineering while it s true that there be a great deal of ridiculous industry jargon there be equally many genuinely difficult problem wait to be solve by those with the right skill set some software job be definitely go away but programmer with the right experience and knowledge will continue to be prestigious and well remunerate for many year to come ; as an example look at the recent explosion of ai researcher salary and the correspond dearth of available talent stay relevant in the ever change technology landscape can be a challenge by look at the technology that be replace programmer in the status quo we should be able to predict what job might disappear from the market additionally to predict how salary and demand for specific skill might change we should consider the grow body of people learn to program as hannah point out public ignorance about computer be keep wage high for those who can program and the public be become more computer savvy each year the fear of automation replace job be neither new nor unfounded in any field and especially in technology market force drive corporation toward automation and commodification gartner s hype cycle be one way of contextualize this phenomenon as time go on specific idea and technology push towards the plateau of productivity where they be eventually automate look at history one must conclude that automation have the power to destroy specific job market in diverse industry range from crop harvesting to automobile assembly technology advance have consistently replace and augment human labor to reduce cost a professor once put it this way in his compiler course take historical note of textile and steel industry do you want to build machine and tool or do you want to operate those machine in this metaphor the machine be a computer programming language this professor be really ask do you want to build website use javascript or do you want to build the v8 engine that power javascript the creation of website be be automate by wordpress and other today v8 on the other hand have a grow body of competitor some of whom be solve open research question language will come and go how many fortran job opening be there but there will always be someone build the next language lucky for us programming language implementation be write with programming language themselves be a machine operator in software put you on the path to be a machine creator in a way which be not true of the steel mill worker of the past the grow number of language interpreter and compiler show we that every job destroy machine also bring with it new opportunity to improve those machine maintain those machine and so forth despite the grow body of job which no long exist there have yet to be a moment in history where humanity have collectively say I guess there isn t any work leave for we to do commodification be come for we all not just software engineer throughout history human labor have consistently be replace with non human or augment to require few and less skilled human self drive car and truck be the flavor of the week in this grand human tradition if the cycle of creation and automation be a fact of life the natural question to answer next be which job and industry be at risk and which be not aws heroku and other similar hosting platform have forever change the role of the system administrator devop engineer internet business use to absolutely need their own server master someone who be well verse in linux ; someone who could configure a server with apache or nginx ; someone who could not only physically wire up the server the router and all the other physical component but who could also configure the routing table and all the software require to make that server accessible on the public web while there be definitely still people apply this skill set professionally aw be make some of those skill obsolete — especially at the low experience level and on the physical side of thing there be very lucrative role within amazon and netflix and google for people with deep expertise in network infrastructure but there be much less demand at the small to medium business scale business intelligence tool such as salesforce tableau and spotfire be also begin to occupy space historically hold by software engineer these system have reduce the demand for in house database administrator but they have also increase the demand for sql as a general purpose skill they have decrease demand for in house reporting technology but increase demand for integration engineer who automate the flow of datum from the business to the third party software platform s a field that be previously dominate by excel and spreadsheet be increasingly be push towards scripting language like python or r and towards sql for datum management some job have disappear but demand for people who can write software have see an increase overall data science be a fascinating example of commodification at a level close to software scikit learn tensorflow and pytorch be all software library that make it easy for people to build machine learning application without build the algorithm from scratch in fact it s possible to run a dataset through many different machine learning algorithm with many different parameter set for those algorithm with little to no understanding of how those algorithm be actually implement it s not necessarily wise to do this just possible you can bet that business intelligence company will be try to integrate these kind of algorithm into their own tool over the next few year as well in many way data science look like web development do 5 8 year ago — a booming field where a little bit of knowledge can get you in the door due to a skill gap as web development bootcamp be close and consolidate datum science bootcamp be pop up in their place kaplan who buy the original web development bootcamp dev bootcamp and start a data science bootcamp metis have decide to close devbootcamp and keep metis run content management system be among the most visible of the tool automate away the need for a software engineer squarespace and wordpress be among the most popular cms system today these platform be significantly reduce the value of people with a just a little bit of front end web development skill in fact the barrier for make a website and get it online have come down so dramatically that people with zero programming experience be successfully launch website every day those same people aren t make deeply interactive website that serve billion of people but they absolutely do make website for their own business that give customer the information they need a lovely landing page with information such as how to find the establishment and how to contact they be more than enough for a local restaurant bar or retail store if your business be not primarily an internet business it have never be easy to get a work site on the public web as a result the once thriving industry of web contractor who can quickly set up a simple website and get it online be become less lucrative finally it would border on hubris to ignore the physical aspect of computer in this context in the word of mike acton software be not the platform hardware be the platform software people would be wise to study at least a little computer architecture and electrical engineering a big shake up in hardware such as the arrival of consumer grade quantum computer would will change everything about professional software engineering quantum computer be still a way off but the grow interest in gpu and the drive toward parallelization be an imminent shift cpu speed have be stagnant for several year now and in that time a seemingly unquenchable thirst for machine learning and big datum have emerge with more desire than ever to process large datum set openmp opencl go cuda and other parallel processing language and framework will continue to become mainstream to be competitively fast in the near term future significant parallelization will be a requirement across the board not just in high performance niche like operating system infrastructure and video game website be ubiquitous the 2017 stack overflow survey report that about 15 % of professional software engineer be work in an internet web service company the bureau of labor statistic expect growth in web development to continue much fast than average 24 % between 2014 and 2024 due to its visibility there have be a massive focus on solve the skill gap in this industry code bootcamp teach web development almost exclusively and web development online course have flood udemy udacity coursera and similar marketplace the combination of increase automation throughout the web development technology stack and the influx of new entry level programmer with an explicit focus on web development have lead some to predict a slide towards a blue collar market for software developer some have go far suggest that the push towards a blue collar market be a strategy architecte by big tech firm other of course say we re head for another bursting bubble change in demand for specific technology be not news language and framework be always rise and fall in technology web development in its current incarnation js be king will eventually go the way of web development of the early 2000 s remember flash what be new be that a lot of people be receive an education explicitly and solely in the current trendy web development framework before you decide to label yourself a react developer remember there be people who once identify themselves as flash developer bank your career on a specific language framework or technology be a game of roulette of course it s quite difficult to predict what technology will remain relevant but if you re go to go all in on something I suggest rely on the lindy effect and pick something like c that have already withstand the test of time the next generation will have a level of de facto tech literacy that generation x and even millennial do not have one outcome of this will be that use the next generation of cms tool will be a give these tool will get well and young worker will be well at use they this combination will definitely will bring down the value of low level it and web development skill as eager and skilled youngster enter the job market high school be catch on as well offer computer science and programming class — some well educate high school student will likely be enter the workforce as program intern immediately upon graduation another big group of newcomer to programming be mbas and datum analyst job listing which be once dominate by excel be start to list sql as a nice to have and even requirement tool such as tableau spotfire salesforce and other web base metric system continue to replace the spreadsheet as the primary tool for report generation if this continue more datum analyst will learn to use sql directly simply because it be easy than export the datum into a spreadsheet people look to climb the rank and out perform their peer in these role be take online course to learn about database and statistical programming language with these new skill they can begin to position themselves as datum scientist by learn a combination of machine learning and statistical library look at metis curriculum as a prime example of this path finally the number of people earn computer science and software engineering degree continue to climb purdue for example report that application to their cs program have double over five year cornell report a similar explosion of cs graduate this trend isn t surprising give the growth and ubiquity of software it s hard for young people to imagine that computer will play a small role in our future so why not study something that s go to give you job security a common argument in the industry nowadays be around the idea that the education you receive in a four year computer science program be mostly unnecessary cruft I have hear this argument repeatedly in the hall of bootcamps web development shop and online from big name in the field such as this piece by eric elliott the opposition view be popular as well with some go so far as say all programmer should earn a master s degree like eric elliott I think it s good that there be more option than ever to break into programming and a 4 year degree might not be the good option for many simultaneously I agree with william bain that the foundational skill which apply across programming discipline be crucial for career longevity and that it be still hard to find that information outside of university course I ve write previously about what skill I think aspire engineer should learn as a foundation of a long career and join bradfield in order to help share this knowledge code school of many shape and size be become ubiquitous and for good reason there be quite a lot you can learn about programming without get into the minutia of big o notation obscure datum structure and algorithmic trivium however while it s true that fresh graduate from stanford be compete for some job with fresh graduate from hack reactor it s only true in one or two sub industry code school and bootcamp graduate be not yet apply to work on embed system cryptography security robotic network infrastructure or ai research and development yet these field like web development be grow quickly some programming relate skill have already start their transition from rare skill to baseline expectation conversely the engineering that go into create beastly engine like aw be anything but common the big company drive technology forward — amazon google facebook nvidia space x and so on — be typically not look for people with a basic understanding of javascript aws serve billion of user per day to support that kind of load an aws infrastructure engineer need a deep knowledge of network protocol computer architecture and several year of relevant experience as with any discipline there be amateur and artisan these prestigious firm be solve research problem and building system that be truly push against the boundary of what be possible yet they still struggle to fill open role even while basic programming skill be increasingly common people who can write algorithm to predict change in genetic sequence that will yield a desire result be go to be highly valuable in the future people who can program satellite spacecraft and automate machinery will continue to be highly value these be not field that lend themselves as readily to a 3 month intensive program as front end web development at least not without significant prior experience because computer science start with the word computer it be assume that young people will all have an innate understanding of it by 2025 unfortunately the ubiquity of computer have not create a new generation of people who de facto understand mathematics computer science network infrastructure electrical engineering and so on computer literacy be not the same as the study of computation despite mathematic have exist since the dawn of time there be still a relatively small portion of the population with strong statistical literacy and computer science be similarly old euclid invent several algorithms one of which be use every time you make an https request ; the fact that we use https every time we login to a website do not automatically imbue anyone with a knowledge of how those protocol work more establish professional field often have a bimodal wage distribution a relatively small number of practitioner make quite a lot of money and the majority of they earn a good wage but do not find themselves in the top 1 % of earner the national association for law placement collect datum that can be use to visualize this phenomenon in stark clarity a huge share of law graduate make between $ 45 00 and $ 65 000 — a good wage but hardly the salary we associate with a top professional we tend to think that all law graduate be on track to become partner at a law firm when really there be many path paralegal clerk public defender judge legal service for business contract writing and so on computer science graduate also have many option for their professional practice from web development to embed system as a basic level of programming literacy continue to become an expectation rather than a nice to have I suspect a similar distribution will emerge in programming job while there will always be a cohort of programmer make a lot of money to push on the edge of technology there will be a grow body of middle class programmer power the new computer centric economy the average salary for web developer will surely decrease over time that say I suspect that the number of job for programmer in general will only continue to grow as worker supply begin to meet demand hopefully we will see a healthy boom in a variety of middle class programming job there will also continue to be a top professional salary available for those programmer who be redefine what be possible regardless of which cohort of programmer you re in a career in technology mean continue your education throughout your life if you want to stay in the second cohort of programmer you may want to invest in learn how to create the machine rather than simply operate they from a quick cheer to a stand ovation clap to show how much you enjoy this story a curious human on a quest to watch the world learn
Blaise Aguera y Arcas,8.7K,15,https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477?source=tag_archive---------4----------------,do algorithm reveal sexual orientation or just expose our stereotype,by blaise agüera y arcas alexander todorov and margaret mitchell a study claim that artificial intelligence can infer sexual orientation from facial image cause a medium uproar in the fall of 2017 the economist feature this work on the cover of their september 9th magazine ; on the other hand two major lgbtq organization the human right campaign and glaad immediately label it junk science michal kosinski who co author the study with fellow researcher yilun wang initially express surprise call the critique knee jerk reaction however he then proceed to make even bolder claim that such ai algorithm will soon be able to measure the intelligence political orientation and criminal inclination of people from their facial image alone kosinski s controversial claim be nothing new last year two computer scientist from china post a non peer review paper online in which they argue that their ai algorithm correctly categorize criminal with nearly 90 % accuracy from a government i d photo alone technology startup have also begin to crop up claim that they can profile people s character from their facial image these development have prompt the three of we to collaborate early in the year on a medium essay physiognomy s new clothe to confront claim that ai face recognition reveal deep character trait we describe how the junk science of physiognomy have root go back into antiquity with practitioner in every era resurrect belief base on prejudice use the new methodology of the age in the 19th century this include anthropology and psychology ; in the 20th genetic and statistical analysis ; and in the 21st artificial intelligence in late 2016 the paper motivate our physiognomy essay seem well outside the mainstream in tech and academia but as in other area of discourse what recently feel like a fringe position must now be address head on kosinski be a faculty member of stanford s graduate school of business and this new study have be accept for publication in the respected journal of personality and social psychology much of the ensue scrutiny have focus on ethic implicitly assume that the science be valid we will focus on the science the author train and test their sexual orientation detector use 35 326 image from public profile on a us date website composite image of the lesbian gay and straight man and woman in the sample reveal a great deal about the information available to the algorithm clearly there be difference between these four composite face wang and kosinski assert that the key difference be in physiognomy mean that a sexual orientation tend to go along with a characteristic facial structure however we can immediately see that some of these difference be more superficial for example the average straight woman appear to wear eyeshadow while the average lesbian do not glass be clearly visible on the gay man and to a less extent on the lesbian while they seem absent in the heterosexual composite might it be the case that the algorithm s ability to detect orientation have little to do with facial structure but be due rather to pattern in groom presentation and lifestyle we conduct a survey of 8 000 american use amazon s mechanical turk crowdsourcing platform to see if we could independently confirm these pattern ask 77 yes no question such as do you wear eyeshadow do you wear glass and do you have a beard as well as question about gender and sexual orientation the result show that lesbian indeed use eyeshadow much less than straight woman do gay man and woman do both wear glass more and young opposite sex attract man be considerably more likely to have prominent facial hair than their gay or same sex attract peer break down the answer by the age of the respondent can provide a rich and clear view of the datum than any single statistic in the follow figure we show the proportion of woman who answer yes to do you ever use makeup top and do you wear eyeshadow bottom average over 6 year age interval the blue curve represent strictly opposite sex attract woman a nearly identical set to those who answer yes to be you heterosexual or straight ; the cyan curve represent woman who answer yes to either or both of be you sexually attract to woman and be you romantically attract to woman ; and the red curve represent woman who answer yes to be you homosexual gay or lesbian 1 the shaded region around each curve show 68 % confidence interval 2 the pattern reveal here be intuitive ; it win t be break news to most that straight woman tend to wear more makeup and eyeshadow than same sex attract and even more so lesbian identify woman on the other hand these curve also show we how often these stereotype be violate that same sex attract man of most age wear glass significantly more than exclusively opposite sex attract man do might be a bit less obvious but this trend be equally clear 3 a proponent of physiognomy might be tempt to guess that this be somehow relate to difference in visual acuity between these population of man however ask the question do you like how you look in glass reveal that this be likely more of a stylistic choice same sex attract woman also report wear glass more as well as like how they look in glass more across a range of age one can also see how opposite sex attract woman under the age of 40 wear contact lense significantly more than same sex attract woman despite report that they have a vision defect at roughly the same rate far illustrate how the difference be drive by an aesthetic preference 4 similar analysis show that young same sex attract man be much less likely to have hairy face than opposite sex attract man serious facial hair in our plot be define as answer yes to have a goatee beard or moustache but no to stubble overall opposite sex attract man in our sample be 35 % more likely to have serious facial hair than same sex attract man and for man under the age of 31 who be overrepresente on date website this rise to 75 % wang and kosinski speculate in their paper that the faintness of the beard and moustache in their gay male composite might be connect with prenatal underexposure to androgen male hormone result in a feminize effect hence sparser facial hair the fact that we see a cohort of same sex attract man in their 40 who have just as much facial hair as opposite sex attract man suggest a different story in which fashion trend and cultural norm play the dominant role in choice about facial hair among man not differ exposure to hormone early in development the author of the paper additionally note that the heterosexual male composite appear to have dark skin than the other three composite our survey confirm that opposite sex attract man consistently self report have a tan face yes to be your face tan slightly more often than same sex attract man once again wang and kosinski reach for a hormonal explanation writing while the brightness of the facial image might be drive by many factor previous research find that testosterone stimulate melanocyte structure and function lead to a dark skin however a simple answer be suggest by the response to the question do you work outdoors overall opposite sex attract man be 29 % more likely to work outdoors and among man under 31 this rise to 39 % previous research have find that increase exposure to sunlight lead to dark skin 5 none of these result prove that there be no physiological basis for sexual orientation ; in fact ample evidence show we that orientation run much deep than a choice or a lifestyle in a critique aim in part at fraudulent conversion therapy program united states surgeon general david satcher write in a 2001 report sexual orientation be usually determine by adolescence if not early and there be no valid scientific evidence that sexual orientation can be change it follow that if we dig deeply enough into human physiology and neuroscience we will eventually find reliable correlate and maybe even the origin of sexual orientation in our survey we also find some evidence of outwardly visible correlate of orientation that be not cultural perhaps most strikingly very tall woman be overrepresente among lesbian identify respondent 6 however while this be interesting it s very far from a good predictor of woman s sexual orientation makeup and eyeshadow do much well the way wang and kosinski measure the efficacy of their ai gaydar be equivalent to choose a straight and a gay or lesbian face image both from datum hold out during the training process and ask how often the algorithm correctly guess which be which 50 % performance would be no well than random chance for woman guess that the taller of the two be the lesbian achieve only 51 % accuracy — barely above random chance this be because despite the statistically meaningful overrepresentation of tall woman among the lesbian population the great majority of lesbian be not unusually tall by contrast the performance measure in the paper 81 % for gay man and 71 % for lesbian woman seem impressive 7 consider however that we can achieve comparable result with trivial model base only on a handful of yes no survey question about presentation for example for pair of woman one of whom be lesbian the follow not exactly superhuman algorithm be on average 63 % accurate if neither or both woman wear eyeshadow flip a coin ; otherwise guess that the one who wear eyeshadow be straight and the other lesbian add six more yes no question about presentation do you ever use makeup do you have long hair do you have short hair do you ever use colored lipstick do you like how you look in glass and do you work outdoors as additional signal raise the performance to 70 % 8 give how many more detail about presentation be available in a face image 71 % performance no long seem so impressive several study include a recent one in the journal of sex research have show that human judge gaydar be no more reliable than a coin flip when the judgement be base on picture take under well control condition head pose lighting glass makeup etc it s well than chance if these variable be not control for because a person s presentation — especially if that person be out — involve social signal we signal our orientation and many other kind of status presumably in order to attract the kind of attention we want and to fit in with people like we 9 wang and kosinski argue against this interpretation on the ground that their algorithm work on facebook selfie of openly gay man as well as date website selfie the issue however be not whether the image come from a date website or facebook but whether they be self post or take under standardized condition most people present themselves in way that have be calibrate over many year of medium consumption observe other look in the mirror and gauge social reaction in one of the early gaydar study use social medium participant could categorize gay man with about 58 % accuracy ; but when the researcher use facebook image of gay and heterosexual man post by their friend still far from a perfect control the accuracy drop to 52 % if subtle bias in image quality expression and grooming can be pick up on by human these bias can also be detect by an ai algorithm while wang and kosinski acknowledge grooming and style they believe that the chief difference between their composite image relate to face shape argue that gay man s face be more feminine narrow jaw long nose large forehead while lesbian face be more masculine large jaw shorter nose small forehead as with less facial hair on gay man and dark skin on straight man they suggest that the mechanism be gender atypical hormonal exposure during development this echo a widely discredit 19th century model of homosexuality sexual inversion more likely heterosexual man tend to take selfie from slightly below which will have the apparent effect of enlarge the chin shorten the nose shrink the forehead and attenuate the smile see our selfie below this view emphasize dominance — or perhaps more benignly an expectation that the viewer will be short on the other hand as a wedding photographer note in her blog when you shoot from above your eye look big which be generally attractive — especially for woman this may be a heteronormative assessment when a face be photograph from below the nostril be prominent while high shooting angle de emphasize and eventually conceal they altogether look again at the composite image we can see that the heterosexual male face have more pronounced dark spot corresponding to the nostril than the gay male while the opposite be true for the female face this be consistent with a pattern of heterosexual man on average shooting from below heterosexual woman from above as the wedding photographer suggest and gay man and lesbian woman from directly in front a similar pattern be evident in the eyebrow shoot from above make they look more v shape but their apparent shape become flatter and eventually caret shape ^ as the camera be lower shoot from below also make the outer corner of the eye appear low in short the change in the average position of facial landmark be consistent with what we would expect to see from differ selfie angle the ambiguity between shoot angle and the real physical size of facial feature be hard to fully disentangle from a two dimensional image both for a human viewer and for an algorithm although the author be use face recognition technology design to try to cancel out all effect of head pose lighting grooming and other variable not intrinsic to the face we can confirm that this doesn t work perfectly ; that s why multiple distinct image of a person help when group photo by subject in google photo and why a person may initially appear in more than one group tom white a researcher at victoria university in new zealand have experiment with the same facial recognition engine kosinski and wang use vgg face and have find that its output vary systematically base on variable like smile and head pose when he train a classifier base on vgg face s output to distinguish a happy expression from a neutral one it get the answer right 92 % of the time — which be significant give that the heterosexual female composite have a much more pronounced smile change in head pose might be even more reliably detectable ; for 576 test image a classifier be able to pick out the one face to the right with 100 % accuracy in summary we have show how the obvious difference between lesbian or gay and straight face in selfie relate to groom presentation and lifestyle — that be difference in culture not in facial structure these difference include we ve demonstrate that just a handful of yes no question about these variable can do nearly as good a job at guess orientation as supposedly sophisticated facial recognition ai far the current generation of facial recognition remain sensitive to head pose and facial expression therefore — at least at this point — it s hard to credit the notion that this ai be in some way superhuman at out we base on subtle but unalterable detail of our facial structure this doesn t negate the privacy concern the author and various commentator have raise but it emphasize that such concern relate less to ai per se than to mass surveillance which be troubling regardless of the technology use even when as in the day of the stasi in east germany these be nothing but paper file and audiotape like computer or the internal combustion engine ai be a general purpose technology that can be use to automate a great many task include one that should not be undertake in the first place we be hopeful about the confluence of new powerful ai technology with social science but not because we believe in revive the 19th century research program of infer people s inner character from their outer appearance rather we believe ai be an essential tool for understand pattern in human culture and behavior it can expose stereotype inherent in everyday language it can reveal uncomfortable truth as in google s work with the geena davis institute where our face gender classifier establish that man be see and hear nearly twice as often as woman in hollywood movie yet female lead film outperform other at the box office make social progress and hold ourselves to account be more difficult without such hard evidence even when it only confirm our suspicion two of us margaret mitchell and blaise agüera y arca be research scientist specialize in machine learning and ai at google ; agüera y arcas lead a team that include deep learning apply to face recognition and power face group in google photo alex todorov be a professor in the psychology department at princeton where he direct the social perception lab he be the author of face value the irresistible influence of first impression 1 this wording be base on several large national survey which we be able to use to sanity check our number about 6 % of respondent identify as homosexual gay or lesbian and 85 % as heterosexual about 4 % of all gender be exclusively same sex attract of the man 10 % be either sexually or romantically same sex attract and of the woman 20 % just under 1 % of respondent be trans and about 2 % identify with both or neither of the pronoun she and he these number be broadly consistent with other survey especially when consider as a function of age the mechanical turk population skew somewhat young than the overall population of the us and consistent with other study our datum show that young people be far more likely to identify non heteronormatively 2 these be wide for same sex attract and lesbian woman because they be minority population result in a large sampling error the same hold for old people in our sample 3 for the remainder of the plot we stick to opposite sex attract and same sex attract as the count be high and the error bar therefore small ; these category be also somewhat less culturally freight since they rely on question about attraction rather than identity as with eyeshadow and makeup the effect be similar and often even large when compare heterosexual identifying with lesbian or gay identify people 4 although we didn t test this explicitly slightly different rate of laser correction surgery seem a likely cause of the small but grow disparity between opposite sex attract and same sex attract woman who answer yes to the vision defect question as they age 5 this finding may prompt the further question why do more opposite sex attract man work outdoors this be not address by any of our survey question but hopefully the other evidence present here will discourage an essentialist assumption such as straight man be just more outdoorsy without the evidence of a control study that can support the leap from correlation to cause such explanation be a form of logical fallacy sometimes call a just so story an unverifiable narrative explanation for a cultural practice 6 of the 253 lesbian identify woman in the sample 5 or 2 % be over six foot and 25 or 10 % be over 5 9 out of 3 333 heterosexual woman woman who answer yes to be you heterosexual or straight only 16 or 0 5 % be over six foot and 152 or 5 % be over 5 9 7 they note that these figure rise to 91 % for man and 83 % for woman if 5 image be consider 8 these result be base on the simple possible machine learning technique a linear classifier the classifier be train on a randomly choose 70 % of the datum with the remain 30 % of the datum hold out for test over 500 repetition of this procedure the error be 69 53 % ± 2 98 % with the same number of repetition and holdout base the decision on height alone give an error of 51 08 % ± 3 27 % and base it on eyeshadow alone yield 62 96 % ± 2 39 % 9 a longstanding body of work e g goffman s the presentation of self in everyday life 1959 and jones and pittman s toward a general theory of strategic self presentation 1982 delf more deeply into why we present ourselves the way we do both for instrumental reason status power attraction and because our presentation inform and be inform by how we conceive of our social self from a quick cheer to a stand ovation clap to show how much you enjoy this story blaise aguera y arcas lead google s ai group in seattle he found seadragon and be one of the creator of photosynth at microsoft
Arvind N,9.5K,8,https://towardsdatascience.com/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153?source=tag_archive---------5----------------,thought after take the deeplearning ai course towards data science,update — feb 2nd 2018 when this blog post be write only 3 course have be release all 5 course in this specialization be now out I will have a follow up blog post soon between a full time job and a toddler at home I spend my spare time learn about the idea in cognitive science & ai once in a while a great paper video course come out and you re instantly hook andrew ng s new deeplearning ai course be like that shane carruth or rajnikanth movie that one yearn for naturally as soon as the course be release on coursera I register and spend the past 4 evening binge watch the lecture work through quiz and programming assignment dl practitioner and ml engineer typically spend most day work at an abstract kera or tensorflow level but it s nice to take a break once in a while to get down to the nut and bolt of learn algorithm and actually do back propagation by hand it be both fun and incredibly useful andrew ng s new adventure be a bottom up approach to teach neural network — powerful non linearity learning algorithm at a beginner mid level in classic ng style the course be deliver through a carefully choose curriculum neatly time video and precisely positioned information nugget andrew pick up from where his classic ml course leave off and introduce the idea of neural network use a single neuron logistic regression and slowly add complexity — more neuron and layer by the end of the 4 week course 1 a student be introduce to all the core idea require to build a dense neural network such as cost loss function learn iteratively use gradient descent and vectorize parallel python numpy implementation andrew patiently explain the requisite math and programming concept in a carefully plan order and a well regulated pace suitable for learner who could be rusty in math code lecture be deliver use presentation slide on which andrew write use digital pen it feel like an effective way to get the listener to focus I feel comfortable watch video at 1 25x or 1 5x speed quiz be place at the end of each lecture section and be in the multiple choice question format if you watch the video once you should be able to quickly answer all the quiz question you can attempt quiz multiple time and the system be design to keep your high score programming assignment be do via jupyter notebook — powerful browser base application assignment have a nice guide sequential structure and you be not require to write more than 2 3 line of code in each section if you understand the concept like vectorization intuitively you can complete most programming section with just 1 line of code after the assignment be code it take 1 button click to submit your code to the automate grading system which return your score in a few minute some assignment have time restriction — say three attempt in 8 hour etc jupyter notebook be well design and work without any issue instruction be precise and it feel like a polished product anyone interested in understand what neural network be how they work how to build they and the tool available to bring your idea to life if your math be rusty there be no need to worry — andrew explain all the require calculus and provide derivative at every occasion so that you can focus on build the network and concentrate on implement your idea in code if your programming be rusty there be a nice code assignment to teach you numpy but I recommend learn python first on codecademy let I explain this with an analogy assume you be try to learn how to drive a car jeremy s fast ai course put you in the driver seat from the get go he teach you to move the steering wheel press the brake accelerator etc then he slowly explain more detail about how the car work — why rotate the wheel make the car turn why press the brake pedal make you slow down and stop etc he keep get deep into the inner working of the car and by the end of the course you know how the internal combustion engine work how the fuel tank be design etc the goal of the course be to get you drive you can choose to stop at any point after you can drive reasonably well — there be no need to learn how to build repair the car andrew s dl course do all of this but in the complete opposite order he teach you about internal combustion engine first he keep add layer of abstraction and by the end of the course you be drive like an f1 racer the fast ai course mainly teach you the art of driving while andrew s course primarily teach you the engineering behind the car if you have not do any machine learning before this don t take this course first the good starting point be andrew s original ml course on coursera after you complete that course please try to complete part 1 of jeremy howard s excellent deep learning course jeremy teach deep learning top down which be essential for absolute beginner once you be comfortable create deep neural network it make sense to take this new deeplearning ai course specialization which fill up any gap in your understanding of the underlie detail and concept 2 andrew stress on the engineering aspect of deep learning and provide plenty of practical tip to save time and money — the third course in the dl specialization feel incredibly useful for my role as an architect lead engineering team 3 jargon be handle well andrew explain that an empirical process = trial & error — he be brutally honest about the reality of designing and train deep net at some point I feel he might have as well just call deep learning as glorify curve fit 4 squash all hype around dl and ai — andrew make restrain careful comment about proliferation of ai hype in the mainstream medium and by the end of the course it be pretty clear that dl be nothing like the terminator 5 wonderful boilerplate code that just work out of the box 6 excellent course structure 7 nice consistent and useful notation andrew strive to establish a fresh nomenclature for neural net and I feel he could be quite successful in this endeavor 8 style of teaching that be unique to andrew and carry over from ml — I could feel the same excitement I feel in 2013 when I take his original ml course 9 the interview with deep learning hero be refreshing — it be motivate and fun to hear personal story and anecdote I wish that he d say concretely more often 2 good tool be important and will help you accelerate your learning pace I buy a digital pen after see andrew teach with one it help I work more efficiently 3 there be a psychological reason why I recommend the fast ai course before this one once you find your passion you can learn uninhibite 4 you just get that dopamine rush each time you score full point 5 don t be scare by dl jargon hyperparameter = setting architecture topology = style etc or the math symbol if you take a leap of faith and pay attention to the lecture andrew show why the symbol and notation be actually quite useful they will soon become your tool of choice and you will wield they with style thank for reading and good wish update thank for the overwhelmingly positive response many people be ask I to explain gradient descent and the differential calculus I hope this help from a quick cheer to a stand ovation clap to show how much you enjoy this story interest in strong ai share concept idea and code
Berit Anderson,1.6K,20,https://medium.com/join-scout/the-rise-of-the-weaponized-ai-propaganda-machine-86dac61668b?source=tag_archive---------6----------------,the rise of the weaponize ai propaganda machine scout science fiction + journalism medium,by berit anderson and brett horvath this piece be originally publish at scout ai this be a propaganda machine it s target people individually to recruit they to an idea it s a level of social engineering that I ve never see before they re capture people and then keep they on an emotional leash and never let they go say professor jonathan albright albright an assistant professor and datum scientist at elon university start dig into fake news site after donald trump be elect president through extensive research and interview with albright and other key expert in the field include samuel woolley head of research at oxford university s computational propaganda project and martin moore director of the centre for the study of medium communication and power at king college it become clear to scout that this phenomenon be about much more than just a few fake news story it be a piece of a much big and dark puzzle — a weaponize ai propaganda machine be use to manipulate our opinion and behavior to advance specific political agenda by leverage automate emotional manipulation alongside swarm of bot facebook dark post a b testing and fake news network a company call cambridge analytica have activate an invisible machine that prey on the personality of individual voter to create large shift in public opinion many of these technology have be use individually to some effect before but together they make up a nearly impenetrable voter manipulation machine that be quickly become the new decide factor in election around the world most recently analytica help elect u s president donald trump secure a win for the brexit leave campaign and lead ted cruz s 2016 campaign surge shepherd he from the back of the gop primary pack to the front the company be own and control by conservative and alt right interest that be also deeply entwine in the trump administration the mercer family be both a major owner of cambridge analytica and one of trump s big donor steve bannon in addition to act as trump s chief strategist and a member of the white house security council be a cambridge analytica board member until recently analytica s cto be the act cto at the republican national convention presumably because of its alliance analytica have decline to work on any democratic campaign — at least in the u s it be however in final talk to help trump manage public opinion around his presidential policy and to expand sale for the trump organization cambridge analytica be now expand aggressively into u s commercial market and be also meet with right wing party and government in europe asia and latin america cambridge analytica isn t the only company that could pull this off — but it be the most powerful right now understand cambridge analytica and the big ai propaganda machine be essential for anyone who want to understand modern political power build a movement or keep from be manipulate the weaponize ai propaganda machine it represent have become the new prerequisite for political success in a world of polarization isolation troll and dark post there s be a wave of report on cambridge analytica itself and solid coverage of individual aspect of the machine — bot fake news microtargeting — but none so far that we have see that portray the intense collective power of these technology or the frightening level of influence they re likely to have on future election in the past political messaging and propaganda battle be arm race to weaponize narrative through new medium — wage in print on the radio and on tv this new wave have bring the world something exponentially more insidious — personalize adaptive and ultimately addictive propaganda silicon valley spend the last ten year build platform whose natural end state be digital addiction in 2016 trump and his ally hijack they we have enter a new political age at scout we believe that the future of constructive civic dialogue and free and open election depend on our ability to understand and anticipate it welcome to the age of weaponize ai propaganda any company can aggregate and purchase big datum but cambridge analytica have develop a model to translate that datum into a personality profile use to predict then ultimately change your behavior that model itself be develop by pay a cambridge psychology professor to copy the groundbreaking original research of his colleague through questionable method that violate amazon s term of service base on its origin cambridge analytica appear ready to capture and buy whatever datum it need to accomplish its end in 2013 dr michal kosinski then a phd candidate at the university of cambridge s psychometric center release a groundbreaking study announce a new model he and his colleague have spend year develop by correlate subject facebook like with their ocean score — a standard bearing personality questionnaire use by psychologist — the team be able to identify an individual s gender sexuality political belief and personality trait base only on what they have like on facebook accord to zurich s das magazine which profile kosinski in late 2016 with a mere ten like as input his model could appraise a person s character well than an average coworker with seventy it could know a subject well than a friend ; with 150 like well than their parent with 300 like kosinski s machine could predict a subject s behavior well than their partner with even more like it could exceed what a person think they know about themselves not long afterward kosinski be approach by aleksandr kogan a fellow cambridge professor in the psychology department about license his model to scl election a company that claim its specialty lie in manipulate election the offer would have mean a significant payout for kosinki s lab still he decline worried about the firm s intention and the downstream effect it could have it have take kosinski and his colleague year to develop that model but with his method and finding now out in the world there be little to stop scl election from replicate they it would seem they do just that accord to a guardian investigation in early 2014 just a few month after kosinski decline their offer scl partner with kogan instead as a part of their relationship kogan pay amazon mechanical turk worker $ 1 each to take the ocean quiz there be just one catch to take the quiz user be require to provide access to all of their facebook datum they be tell the datum would be use for research the job be report to amazon for violate the platform s term of service what many of the turk likely didn t realize accord to document review by the guardian kogan also capture the same datum for each person s unwitte friend the datum gather from kogan s study go on to birth cambridge analytica which spin out of scl election soon after the name metaphorically at least be a nod to kogan s work — and a dig at kosinski but that early trove of user datum be just the beginning — just the seed analytica need to build its own model for analyze user personality without have to rely on the lengthy ocean test after a successful proof of concept and back by wealthy conservative investor analytica go on a data shopping spree for the age snap up datum about your shopping habit land ownership where you attend church what store you visit what magazine you subscribe to — all of which be for sale from a range of datum broker and third party organization sell information about you analytica aggregate this datum with voter role publicly available online datum — include facebook like — and put it all into its predictive personality model nix like to boast that analytica s personality model have allow it to create a personality profile for every adult in the u s — 220 million of they each with up to 5 000 datum point and those profile be be continually update and improve the more datum you spew out online albright also believe that your facebook and twitter post be be collect and integrate back into cambridge analytica s personality profile twitter and also facebook be be use to collect a lot of responsive datum because people be impassione they reply they retweet but they also include basically their entire argument and their entire background on this topic he explain collect massive quantity of datum about voter personality might seem unsettling but it s actually not what set cambridge analytica apart for analytica and other company like they it s what they do with that datum that really matter your behavior be drive by your personality and actually the more you can understand about people s personality as psychological driver the more you can actually start to really tap in to why and how they make their decision nix explain to bloomberg s sasha issenburg we call this behavioral microtargeting and this be really our secret sauce if you like this be what we re bring to america use those dossier or psychographic profile as analytica call they cambridge analytica not only identifie which voter be most likely to swing for their cause or candidate ; they use that information to predict and then change their future behavior as vice report recently kosinski and a colleague be now work on a new set of research yet to be publish that address the effectiveness of these method their early finding use personality target facebook post can attract up to 63 percent more click and 1 400 more conversion scout reach out to cambridge analytica with a detailed list of question about their communication tactic but the company decline to answer any question or to comment on any of their tactic but researcher across the technology and medium ecosystem who have be follow cambridge analytica s political messaging activity have unearth an expansive adaptive online network that automate the manipulation of voter at a scale never before see in political messaging they the trump campaign be use 40 50 000 different variant of ad every day that be continuously measure response and then adapt and evolve base on that response martin moore director of king college s centre for the study of medium communication and power tell the guardian in early december it s all do completely opaquely and they can spend as much money as they like on particular location because you can focus on a five mile radius where traditional pollster might ask a person outright how they plan to vote analytica rely not on what they say but what they do track their online movement and interest and serve up multivariate ad design to change a person s behavior by prey on individual personality trait for example nix write in an op ed last year about analytica s work on the cruz campaign our issue model identify that there be a small pocket of voter in iowa who feel strongly that citizen should be require by law to show photo i d at polling station leverage our other datum model we be able to advise the campaign on how to approach this issue with specific individual base on their unique profile in order to use this relatively niche issue as a political pressure point to motivate they to go out and vote for cruz for people in the temperamental personality group who tend to dislike commitment messaging on the issue should take the line that show your i d to vote be as easy as buy a case of beer whereas the right message for people in the stoic traditionalist group who have strongly hold conventional view be that show your i d in order to vote be simply part of the privilege of live in a democracy for analytica the feedback be instant and the response automate do this specific swing voter in pennsylvania click on the ad attack clinton s negligence over her email server yes serve she more content that emphasize failure of personal responsibility no the automate script will try a different headline perhaps one that play on a different personality trait — say the voter s tendency to be agreeable toward authority figure perhaps top intelligence official agree clinton s email jeopardize national security much of this be do through facebook dark post which be only visible to those be target base on user response to these post cambridge analytica be able to identify which of trump s message be resonate and where that information be also use to shape trump s campaign travel schedule if 73 percent of target voter in kent county mich click on one of three article about bring back job schedule a trump rally in grand rapid that focus on economic recovery political analyst in the clinton campaign who be base their tactic on traditional polling method laugh when trump schedule campaign event in the so call blue wall — a group of state that include michigan pennsylvania and wisconsin and have traditionally fall to democrats but cambridge analytica see they have an opening base on measure engagement with their facebook post it be the small margin in michigan pennsylvania and wisconsin that win trump the election dark post be also use to depress voter turnout among key group of democratic voter in this election dark post be use to try to suppress the african american vote write journalist and open society fellow mckenzie funk in a new york times editorial accord to bloomberg the trump campaign send ad remind certain select black voter of hillary clinton s infamous super predator line it target miami s little haiti neighborhood with message about the clinton foundation s trouble in haiti after the 2010 earthquake because dark post be only visible to the target user there s no way for anyone outside of analytica or the trump campaign to track the content of these ad in this case there be no sec oversight no public scrutiny of trump s attack ad just the rapid eye movement of million of individual user scan their facebook feed in the week lead up to a final vote a campaign could launch a $ 10 100 million dark post campaign target just a few million voter in swing district and no one would know this may be where future black swan election upset be bear these company moore say have find a way of transgressing 150 year of legislation that we ve develop to make election fair and open meanwhile surprised by the result of the 2016 presidential race albright start look into the fake news problem as a part of his research albright scrape 306 fake news site to determine how exactly they be all connect to each other and the mainstream news ecosystem what he find be unprecedented — a network of 23 000 page and 1 3 million hyperlink the site in the fake news and hyper bias # mcm network albright write have a very small node size — this mean they be link out heavily to mainstream medium social network and informational resource most of which be in the center of the network but not many site in their peer group be send link back these site aren t own or operate by any one individual entity he say but together they have be able to game search engine optimization increase the visibility of fake and biased news anytime someone google an election relate term online — trump clinton jews muslim abortion obamacare this network albright write in a post explore his finding be trigger on demand to spread false hyper biased and politically loaded information even more shocking to he though be that this network of fake news create a powerful infrastructure for company like cambridge analytica to track voter and refine their personality target model I scrape the tracker on these site and I be absolutely dumbfound every time someone like one of these post on facebook or visit one of these website the script be then follow you around the web and this enable data mining and influence company like cambridge analytica to precisely target individual to follow they around the web and to send they highly personalise political message the web of fake and biased news that albright uncover create a propaganda wave that cambridge analytica could ride and then amplify the more fake news that user engage with the more addictive analytica s personality engagement algorithm can become voter 35423 click on a fake story about hillary s sex trafficking ring let s get she to engage with more story about hillary s suppose history of murder and sex trafficking the synergy between fake content network automate message testing and personality profiling will rapidly spread to other digital medium albright s most recent research focus on an artificial intelligence that automatically create youtube video about news and current event the ai which react to trend topic on facebook and twitter pair image and subtitle with a computer generate voiceover it spool out nearly 80 000 video through 19 different channel in just a few day give its rapid development the technology community need to anticipate how ai propaganda will soon be use for emotional manipulation in mobile messaging virtual reality and augmented reality if fake news create the scaffolding for this new automate political propaganda machine bot or fake social medium profile have become its foot soldier — an army of political robot use to control conversation on social medium and silence and intimidate journalist and other who might undermine their message samuel woolley director of research at the university of oxford s computational propaganda project and a fellow at google s jigsaw project have dedicate his career to study the role of bot in online political organizing — who create they how they re use and to what end research by woolley and his oxford base team in the lead up to the 2016 election find that pro trump political messaging rely heavily on bot to spread fake news and discredit hillary clinton by election day trump s bot outnumber hers 5 1 the use of automate account be deliberate and strategic throughout the election most clearly with pro trump campaigner and programmer who carefully adjust the timing of content production during the debate strategically colonize pro clinton hashtag and then disabled activity after election day the study by woolley s team report woolley believe it s likely that cambridge analytica be responsible for subcontract the creation of those trump bot though he say he doesn t have direct proof still if anyone outside of the trump campaign be qualified to speculate about who create those bot it would be woolley lead by dr philip howard the team s principal investigator woolley and his colleague have be track the use of bot in political organizing since 2010 that s when howard bury deep in research about the role twitter play in the arab spring first notice thousand of bot coopte hashtag use by protester curious he and his team begin reach out to hacker botmaker and political campaign get to know they and try to understand their work and motivation eventually those creator would come to make up an informal network of nearly 100 informant that have keep howard and his colleague in the know about these bot over the last few year before long howard and his team be get the head up about bot propaganda campaign from the creator themselves as more and more major international political figure begin use botnet as just another tool in their campaign howard woolley and the rest of their team study the action unfold the world these informant reveal be an international network of government consultancy often with owner or top management just one degree away from official government actor and individual who build and maintain massive network of bot to amplify the message of political actor spread message counter to those of their opponent and silence those whose view or idea might threaten those same political actor the chinese iranian and russian government employ their own social medium expert and pay small amount of money to large number of people to generate pro government message howard and his coauthor write in a 2015 research paper about the use of bot in the venezuelan election depend on which of those three category bot creator fall into — government consultancy or individual — they re just as likely to be motivate by political belief as they be the opportunity to auction off their network of digital influence to the high bidder not all bot be create equal the average run of the mill twitter bot be literally a robot — often program to retweet specific account to help popularize specific idea or viewpoint they also frequently respond automatically to twitter user who use certain keyword or hashtag — often with pre write slur insult or threat high end bot on the other hand be more analog operate by real people they assume fake identity with distinct personality and their response to other user online be specific intend to change their opinion or those of their follower by attack their viewpoint they have online friend and follower they re also far less likely to be discover — and their account deactivate — by facebook or twitter work on their own woolley estimate an individual could build and maintain up to 400 of these boutique twitter bot ; on facebook which he say be more effective at identify and shut down fake account an individual could manage 10 20 as a result these high quality botnet be often use for multiple political campaign during the brexit referendum the oxford team watch as one network of bot previously use to influence the conversation around the israeli palestinian conflict be reactivate to fight for the leave campaign individual profile be update to reflect the new debate their personal tagline change to ally with their new allegiance — and away they go russia s bot army have be the subject of particular scrutiny since a cia special report reveal that russia have be work to influence the election in trump s favor recently reporter comedian samantha bee travel to moscow to interview two pay russian troll operator clothe in black ski mask to obscure their identity the two talk with bee about how and why they be use their account during the u s election they tell bee that they pose as americans online and target site like the wall street journal the new york post the washington post facebook and twitter their goal they say be to piss off other social medium user change their opinion and silence their opponent or to put it in the word of russian troll # 1 when your opponent just shut up the 2016 u s election be over but the weaponize ai propaganda machine be just warm up and while each of its component would be worry on its own together they represent the arrival of a new era in political messaging — a steel wall between campaign winner and loser that can only be mount by gather more datum create well personality analyse rapid development of engagement ai and hire more troll at the moment trump and cambridge analytica be lap their opponent the more datum they gather about individual the more analytica and by extension trump s presidency will benefit from the network effect of their work — and the hard it will become to counter or fight back against their messaging in the court of public opinion each tweet that echo forth from the @realdonaldtrump and @potus account announce and defend the administration s move be meet with a chorus of protest and argument but even that negative engagement become a valuable asset for the trump administration because every impulsive tweet can be treat like a psychographic experiment trump s first few week in office may have seem bumble but they represent a clear signal of what lie ahead for trump s presidency — an executive order design to enrage and distract his opponent as he and bannon move to strip power from the judicial branch install bannon himself on the national security council and issue a series of unconstitutional gag order to federal agency cambridge analytica may be slate to secure more federal contract and be likely about to begin manage white house digital communication for the rest of the trump administration what new predictive personality target become possible with potential access to datum on u s voter from the irs department of homeland security or the nsa lenin want to destroy the state and that s my goal too I want to bring everything crash down and destroy all of today s establishment bannon say in 2013 we know that steve bannon subscribe to a theory of history where a messianic grey warrior consolidate power and remake the global order bolster by the success of brexit and the trump victory breitbart of which bannon be executive chair until trump s election and cambridge analytica which bannon sit on the board of be now bring fake news and automate propaganda to support far right party in at least germany france hungary and india as well as part of south america never have such a radical international political movement have the precision and power of this kind of propaganda technology whether or not leader engineer designer and investor in the technology community respond to this threat will shape major aspect of global politic for the foreseeable future the future of politic will not be a war of candidate or even cash on hand and it s not even about big datum as some have argue everyone will have access to big datum — as hillary do in the 2016 election from now on the distinguish factor between those who win election and those who lose they will be how a candidate use that datum to refine their machine learning algorithm and automate engagement tactic election in 2018 and 2020 win t be a contest of idea but a battle of automate behavior change the fight for the future will be a proxy war of machine learn it will be wage online in secret and with the unwitting help of all of you anyone who want to effect change need to understand this new reality it s only by understand this — and by build well automate engagement system that amplify genuine human passion rather than manipulate it — that other candidate and cause around the globe will be able to compete implication # 1 public sentiment turn into high frequency trading thank to stock trading algorithm large portion of public stock and commodity market no long resemble a human system and some would argue no long serve their purpose as a signal of value instead they re a battleground for high frequency trading algorithm attempt to influence price or find nano leverage in price position in the near future we may see a similar process unfold in our public debate instead of battle press conference and opinion article public opinion about company and politician may turn into multi billion dollar battle between compete algorithm each deploy to sway public sentiment stock trading algorithm already exist that analyze million of tweet and online post in real time and make trade in a matter of millisecond base on change in public sentiment algorithmic trading and algorithmic public opinion be already connect it s likely they will continue to converge implication # 2 personalize automate propaganda that adapt to your weakness what if president trump s 2020 re election campaign didn t just have the good political messaging but 250 million algorithmic version of their political message all update in real time personalize to precisely fit the worldview and attack the insecurity of their target instead of have to deal with mislead politician we may soon witness a cambrian explosion of pathologically lie political and corporate bot that constantly improve at manipulate we implication # 3 not just a bubble but trap in your own ideological matrix imagine that in 2020 you find out that your favorite politics page or group on facebook didn t actually have any other human member but be fill with dozen or hundred of bot that make you feel at home and your opinion validate be it possible that you might never find out correction an early version of this story mistakenly refer to steve bannon as the owner of breitbart news until trump s election bannon serve as the executive chair of breitbart a position in which it be common to assume ownership through stock holding this story have be update to reflect that from a quick cheer to a stand ovation clap to show how much you enjoy this story ceo & co founder @join_scout the social implication of technology
Slav Ivanov,4.4K,10,https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607?source=tag_archive---------7----------------,37 reason why your neural network be not work slav,the network have be train for the last 12 hour it all look good the gradient be flow and the loss be decrease but then come the prediction all zero all background nothing detect what do I do wrong — I ask my computer who didn t answer where do you start check if your model be output garbage for example predict the mean of all output or it have really poor accuracy a network might not be train for a number of reason over the course of many debug session I would often find myself do the same check I ve compile my experience along with the good idea around in this handy list I hope they would be of use to you too a lot of thing can go wrong but some of they be more likely to be break than other I usually start with this short list as an emergency first response if the step above don t do it start go down the follow big list and verify thing one by one check if the input datum you be feed the network make sense for example I ve more than once mix the width and the height of an image sometimes I would feed all zero by mistake or I would use the same batch over and over so print display a couple of batch of input and target output and make sure they be ok try pass random number instead of actual datum and see if the error behave the same way if it do it s a sure sign that your net be turn datum into garbage at some point try debug layer by layer op by op and see where thing go wrong your datum might be fine but the code that pass the input to the net might be break print the input of the first layer before any operation and check it check if a few input sample have the correct label also make sure shuffle input sample work the same way for output label maybe the non random part of the relationship between the input and output be too small compare to the random part one could argue that stock price be like this I e the input be not sufficiently relate to the output there isn t an universal way to detect this as it depend on the nature of the datum this happen to I once when I scrape an image dataset off a food site there be so many bad label that the network couldn t learn check a bunch of input sample manually and see if label seem off the cutoff point be up for debate as this paper get above 50 % accuracy on mnist use 50 % corrupt label if your dataset hasn t be shuffle and have a particular order to it order by label this could negatively impact the learn shuffle your dataset to avoid this make sure you be shuffle input and label together be there a 1000 class a image for every class b image then you might need to balance your loss function or try other class imbalance approach if you be train a net from scratch I e not finetune you probably need lot of datum for image classification people say you need a 1000 image per class or more this can happen in a sorted dataset I e the first 10k sample contain the same class easily fixable by shuffle the dataset this paper point out that have a very large batch can reduce the generalization ability of the model thank to @hengcherkeng for this one do you standardize your input to have zero mean and unit variance augmentation have a regularize effect too much of this combine with other form of regularization weight l2 dropout etc can cause the net to underfit if you be use a pretraine model make sure you be use the same normalization and preprocessing as the model be when train for example should an image pixel be in the range 0 1 1 1 or 0 255 cs231n point out a common pitfall also check for different preprocessing in each sample or batch this will help with find where the issue be for example if the target output be an object class and coordinate try limit the prediction to object class only again from the excellent cs231n initialize with small parameter without regularization for example if we have 10 class at chance mean we will get the correct class 10 % of the time and the softmax loss be the negative log probability of the correct class so ln 0 1 = 2 302 after this try increase the regularization strength which should increase the loss if you implement your own loss function check it for bug and add unit test often my loss would be slightly incorrect and hurt the performance of the network in a subtle way if you be use a loss function provide by your framework make sure you be pass to it what it expect for example in pytorch I would mix up the nllloss and crossentropyloss as the former require a softmax input and the latter doesn t if your loss be compose of several small loss function make sure their magnitude relative to each be correct this might involve test different combination of loss weight sometimes the loss be not the good predictor of whether your network be train properly if you can use other metric like accuracy do you implement any of the layer in the network yourself check and double check to make sure they be work as intend check if you unintentionally disabled gradient update for some layer variable that should be learnable maybe the expressive power of your network be not enough to capture the target function try add more layer or more hide unit in fully connect layer if your input look like k h w = 64 64 64 it s easy to miss error relate to wrong dimension use weird number for input dimension for example different prime number for each dimension and check how they propagate through the network if you implement gradient descent by hand gradient checking make sure that your backpropagation work like it should more info 1 2 3 overfit a small subset of the datum and make sure it work for example train with just 1 or 2 example and see if your network can learn to differentiate these move on to more sample per class if unsure use xavier or he initialization also your initialization might be lead you to a bad local minimum so try a different initialization and see if it help maybe you use a particularly bad set of hyperparameter if feasible try a grid search too much regularization can cause the network to underfit badly reduce regularization such as dropout batch norm weight bias l2 regularization etc in the excellent practical deep learning for coder course jeremy howard advise get rid of underfitte first this mean you overfit the training datum sufficiently and only then address overfitte maybe your network need more time to train before it start make meaningful prediction if your loss be steadily decrease let it train some more some framework have layer like batch norm dropout and other layer behave differently during training and testing switching to the appropriate mode might help your network to predict properly your choice of optimizer shouldn t prevent your network from training unless you have select particularly bad hyperparameter however the proper optimizer for a task can be helpful in get the most training in the short amount of time the paper which describe the algorithm you be use should specify the optimizer if not I tend to use adam or plain sgd with momentum check this excellent post by sebastian ruder to learn more about gradient descent optimizer a low learning rate will cause your model to converge very slowly a high learning rate will quickly decrease the loss in the beginning but might have a hard time find a good solution play around with your current learning rate by multiply it by 0 1 or 10 get a nan non a number be a much big issue when training rnn from what I hear some approach to fix it do I miss anything be anything wrong let I know by leave a reply below from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur hacker machine learn deep learning and other type of learn
Sirui Li,1,5,https://medium.com/leethree/the-evolution-a-simple-illustration-203a1bba83b0?source=tag_archive---------2----------------,the evolution a simple illustration leethree on ux medium,in the last paragraph of tool vs assistant part ii I ve talk about the evolution of the society as the technology develop in order to explain how we should apply software agent into our application here I come up with some graph to illustrate my model of machine intelligence in the process of society evolution firstly consider the industrialization of the way people finish a certain task say write a thank you letter let s assume that this task be well defined though I m not go to define it when it come into be only a few of the smart people could complete this task a minimal level of intelligence be require for this the technique and methodology for write thank you letter develop very slowly until one day tool be introduce dictionary and phrase book greatly help people with this task and more and more people learn how to write thank you letter once the most intelligent people all learn this it be consider very cool if someone understand how to write beautiful thank you letter and this soon become one of the trend topic among people well technique be develop and more effective tool be invent like electronic dictionary and dictionary software this field begin to flourish soon it become so easy to write thank you letter that everyone with a right mind could complete the task with the help of certain tool however the most amazing thank you letter be always write by intelligent human being who put their mind to it one day an automatic thank you letter software atul be develop this buggy but yet usable tool be a great breakthrough because machine start to complete the task by themselves on the basis of atul more and well software tool be develop professional thank you letter writer be gradually replace by the machine as more and more people think the letter write by machine be well than theirs the software tool push the quality bar higher and high only the most excellent and experienced writer could do well than machine but who care the majority of people no long pay attention to how the letter be write they just take it for grant from here we come to the end of the industrialization process of the task it s almost completely automate and machine intelligence have greatly improve the productivity very few people will remain do this task an extra note some may argue that the level of intelligence be lower by tool and machine because they make the task easier it be not the case because tool and machine be part of this intelligence requirement only by make use of the intelligence from the tool or the machine human could complete the task with less intelligence thus the level of intelligence require for the task be not reduce let s see the broad picture this one be fairly easy to understand the society become more and more sophisticated since the invention of machine intelligence task with low level of sophistication be gradually do by machine but more sophisticated task be be create human being be work on the most sophisticated task which the machine couldn t do so what our society look like now this shape look strange as it show the relationship between the other two axis intelligence and sophistication basically more intelligence be require to solve more sophisticated problem but task could be do in many way that s why it actually show a colored band instead of a single curve as we can see the most difficult problem I e the most sophisticated task be still be do by most intelligent human being because they re new and machine performance be usually not acceptable while time go on machine intelligence will take up more portion in the low part and human work will be push far and high like a sword cut through the surface that s a pretty reasonable illustration of the word break through I have to emphasize that as the title say this be a very very simple model there re quite a few assumption for these graph so you might find they naïve and inaccurate the top five assumption be very strong and not necessarily true in fact I personally doubt some of they because I don t really agree with technocentrism however I do believe that from the viewpoint of a technocentrist this model could provide some insight on how technology work and develop p s I hope I could make a 3d model out of the three view from different axis but it seem very difficult to make it both accurate and illustrative perhaps I ll make a video once I know how to do it from a quick cheer to a stand ovation clap to show how much you enjoy this story @leethree9 this be a blog by @leethree9 on topic include user experience human computer interaction usability and interaction design
Theo,3,4,https://becominghuman.ai/is-there-a-future-for-innovation-18b4d5ab168f?source=tag_archive---------1----------------,be there a future for innovation become human artificial intelligence magazine,have you notice how tech savvy child have become but be no long streetwise I read a friend s thought on his own site last week and there be a slight pang of regret in where technology and innovation seem to be lead we all and so I start to worry about where the concept of innovation be go for future generation there s an increase reliance on technology for the sake of convenience child be become self reliant too quickly but gadget be replace people as the mentor the human bonding of parenthood be a prime example of where it s take a toll I ve see parent hand over idevice to pacify a child numerous time now the lullaby and bedtime reading session have be replace with cut the rope and automate storybook app I know a child who have develop speech difficulty because he s be bring up on cable tv and a ds lite pronounce word as he have hear they from a tiny speaker and not by watch how his parent pronounce they and I start to worry about how the concept of innovation be be redefine for future generation I use my imagination constantly as a child and it s still as active now as it be then but I didn t use technology to spoon feed I the next generation expect innovation to happen at their fingertip with little to no real stimulus steve job say stay hungry stay foolish and he be right innovation come from a keenness it s a starvation and hunger that drive people forward to spark and create it come from grab what little there be from the ether and turn it into something spectacular it s the big bang of human thought creation and I start to worry about what the concept of innovation mean for future generation technology be take away the power to think for ourselves and from our child everything must be there and in real time for instant consumption it s junk food for the mind and we re get fat on it and that breed lazy innovation we ve become satiate before we reach the point of real creativity nobody want to bother take the time to put it all together themselves any more it have to be ready for we and we re happy to throw it away if it doesn t work first time use it or lose it there s less sweat and toil involve if we don t persevere with failure remember see the human race depict in wall e that s where innovation be head and because of this we risk so many thing disappear for the sake of convenience we re all guilty of it I m guilty of it I be ask once what would become absurd in ten year think about it I realize we re on the cusp of put book on the endanger species list real book book bind in hard and paperback not digital copy from a kindle store and that scare I because the next generation of kid may grow up never see one or experience sit with their father as he read an old batter copy of the hobbit because he ll be sit there hand over an ipad with the hobbit read along app teed up and it ll be an actor voice not his father s voice pretend to be a bunch of troll about to eat a company of dwarfs innovation be a magical crazy concept it stem from a combination of crazy imagination human interaction and creativity not convenient manufacture technology can aid collaboration in way we ve never experience before but it can t run crazy for we and for the sake of future generation don t let it here s to the crazy one indeed from a quick cheer to a stand ovation clap to show how much you enjoy this story founder and ceo @ rawshark studio late news info and tutorial on artificial intelligence machine learn deep learn big datum and what it mean for humanity
Diana Filippova,1,11,https://medium.com/@dnafilippova/de-la-coop%C3%A9ration-entre-les-hommes-et-les-machines-pour-une-approche-pair-%C3%A0-pair-de-lintelligence-1bb8d8c56de1?source=tag_archive---------3----------------,de la coopération entre les homme et les machine pour une approche pair à pair de l intelligence,originally publish at www cuberevue com on november 6 2013 lundi matin huit heure 2007 centre d examen d arcueil mille têtes sont laborieusement penchées sur des bureaux en bois abîmé par les stylos qui grattent sur de mince feuille de papi les voie ferrées bordent l enclave les train font trembler le bâtiment en rythme les têtes se relèvent un instant distraites puis s en retournent se concentrer sur l écriture studieuse et pressée de la copie les surveillant passent dans le rang impassible guettent toute tête qui tourne toute main qui se dérobe dan la poche d un jean seuls les bruit de papi froissé sont perceptible et lorsqu il s estompent un silence de mort règne sur la salle mille élèves sont isolé pour répondre en six heure à une question difficile toute interaction avec leurs pair leur e interdite il ne peuvent consulter leur note si un oubli inattendu vient perturber le fil de leur pensée les devoirs produit par le élèves tomberont dans l oubli stockées dans un hangar dédié qui accueille des papier d examen depuis maintes générations quelques années plus tard j anime un atelier qui s étend sur toute la journée dans une grande salle blanche avec une vingtaine d ordinateurs autour de moi des groupes d élèves discutent rient et oscillent entre une feuille de dessin et l écran d ordinateur certain s isolent pour coder d autres sont penchés sur une imprimante 3d qui produit un design open source qu il viennent de télécharger les élèves consultent leur professeur demandent conseil aux expert présent dans la salle et partagent leur avancement avec les autre certain abandonnent momentanément leur propre groupe pour aider leur amis dans un groupe concurrent l atelier consiste à remixer des œuvre artistique tombées dans le domaine public ou en open source aucune évaluation n est prévue les réaction des personne présente est la seule mesure de la qualité de leur production je pense en le regardant qu ils ont une chance infinie de pouvoir librement puiser dans tous les puits de connaissance existant leur intelligence celle des pair et accompagnateur la quasi totalité des production de l humanité et surtout le savoir global présent à portée de main a la clôture de l atelier leur œuvre nous paraissent surprenante originale et leur qualité dépasse toute nos attente nos doutes sur la capacité des élèves à défricher de la matière brute et en extraire une forme structurée en une après midi étaient vain il nous font désormais sourire j observe la magie de la création collective tous les jours au sein de ouishare projet collectif œuvrant pour le développement de l économie collaborative le projet rassemble des personne venue de tous les coin du monde et j ai beaucoup de chance de m investir chaque jour pour chacun des projet que nous conduison pour chaque décision que nous prenon et à chacun des désaccords qui surgit nous faison l expérience d une coopération intelligente au sein de ce laboratoire d idées et de pratique nous avon la volonté de soutenir les projet collaboratifs qui surgissent dans le cuisine les espace de coworke lor des rencontres aussi nous appliquon nous à apprendre au sein de notre communauté comment on peut créer ensemble mieux que ne le ferait chacun de nous seul c est l alchimie de l intelligence collective ensemble en coopérant on crée et pense mieux que seul reclus dans le monastère qu est notre cerveau nous avon désormais un accès immédiat à la grande somme du savoir existant mais c est avec les autres aujourd hui et demain que nous créon bien nous somme relié à une infinité d individus organisation machine la coopération de l ensemble de ce entité quelle que soit leur nature quelle que soit la nature de leur intelligence est ce qui définit à mon sens l intelligence collective les enjeux de l évolution de notre penser ensemble et décider ensemble dan le monde de demain sont critique aussi nous avons de nouveaux compagnon qui nous assistent sans cesse — les machine le programme les robot — et qui modifient nos façons d agir et de penser autant que nous le façonnon ces bouleversements de notre existence et de nos modes d organisation connaissent aujourd hui une accélération telle que le questionnement sur le processus et les effet de ces interaction acquiert une consistance inédite nous ne pouvon plus ignorer que nous humain ne seron plus jamais seul dans ce contexte critique comment définir l intelligence collective et intégrer les machine dan la production des connaissance à venir nos interaction nous conduiront elle à nous améliorer en tant qu individus et espèce ou scelleront elle une nouvelle ère de guerre numérique si nous voulon utiliser en toute conscience notre capacité à coopérer pour rendre le monde meilleur quel modèle économiques sociaux éthiques et technologique devon nous bâtir le telos de l intelligence collective s inscrit dans le concept de noosphère forgé par vladimir vernadsky et longuement analysé par teilhard de chardin comprise comme l ensemble de la pensée humaine la noosphère correspond à deux phénomène en interaction réciproque d une part la complexification des société humaine du point de vue culturel social économique et démographique tend ver la constitution d une sphère de la connaissance toujour plus étoffée d autre part cette sphère née de la multiplication des interaction toujour plus nombreuse entraîne une structuration progressive de la pensée globale et la prise de conscience par l humanité d elle même l idée d une marche ver une sorte de cerveau humain qui nous transcende aussi ancienne soit elle1 prend une consistante particulière à l heure où 40 % de la planète est connectée à la toile l intelligence collective peut alors être comprise comme le processus de création de savoir éclairé par la conscience d une noosphère la noosphère sous tend la possibilité d une production collective de savoir mais elle ne répond pas aux question qui se posent si l on examine le processus de co création l approche pratique de l intelligence collective permet quant à elle d explorer les condition de possibilité de l exercice collectif de l intelligence d individus entité ou machine a cet effet je I tourne ver le travaux du centre de recherche sur l intelligence collective du mit2 les recherche et analysis conduite par ce centre sont uniques en leur genre en combinant les science mathématiques physique biologique sociale économiques et une approche résolument prospective les travaux du centre ont pour ambition de répondre à la question suivante comment le individus et les machines peuvent se connecter afin que collectivement il soient en mesure d agir avec plus d intelligence que ne l ont jamais pu tout individu groupe ou machine pris séparément l ampleur de la tâche ne fait pas peur à thomas malone fondateur et président du centre selon lui l enjeu de la recherche e critique car selon lui le futur de notre espèce pourrait reposer sur notre capacité à faire usage de notre intelligence collective de telle manière que les choix qui sont fait soient non seulement intelligents mais aussi sag » 3 la portée pratique de l intelligence collective commence à se dessiner d une part il s agit de trouver une configuration telle que la co création aboutisse à des choix ordonné efficient utile et qui répondent à une certaine éthique d autre part est il raisonnable de supposer qu une configuration favorable à la co création intelligente entre individus puisse également intégrer les machines comme le rappelle justement thomas malone le décisions collectives peuvent parfaitement être rationnelle et bêtes4 la notion d intelligence doit par conséquent être élargie pour y intégrer des facteur autre que la seule rationalité thomas malone la définit ainsi pour être intelligent le comportement collectif du groupe doit déployer des caractéristiques telle que la perception la capacité d apprentissage le jugement et l aptitude à résoudre des problèmes en d autres termes les aptitude d un groupe et celle des individus doivent fonctionner comme des vase communicant dan une configuration propice à la co production le groupe se dote ainsi d une série de comportement qui sont normalement associé au seul individu le centre de recherche du mit a ensuite cherché à déterminer le facteurs qui sont corrélé à une production collective plus intelligente il s est avéré que l intelligence moyenne de chaque individu n en fait pas partie en revanche deux facteur ressortent significativement le degré d empathie des membres du groupe et l égale distribution de la parole au sein du groupe empathie distribution et égalité ces facteur laissent à penser que l intelligence collective s accommode mal des modes d organisation hiérarchiques cloisonnées et centralisées l intelligence collective prospère à l inverse dans des organisation structurées en réseau distribuées décentralisées centrées sur la perception et l écoute davantage que sur des règle rigide il n est pas étonnant que les réseaux contributifs tels que wikipedia prospèrent ils présentent exactement les caractéristiques qui stimulent l intelligence collective il faut à mon sens un ingrédient supplémentaire pour que la multiplicité des individus composant le réseau ne fasse par le light des passager clandestin rappelon à ce titre que seulement 10 % des lecteurs de wikipedia sont contributeurs actif l anonymat de la contribution y est pour quelque choose la valeur produite par chacun n est ni mesurée ni reconnue a l inverse au sein de sensorica5 réseau ouvert où un ensemble d individus et d organisation produisent des solution hardware de façon contributive la valeur ajoutée de chaque contributeur est régulièrement mesurée par les autres contributeurs et connue par le réseau ainsi l évaluation et la reconnaissance par les pairs de la valeur de la contribution de chacun sont tout aussi importante que l évaluation de la valeur globale du réseau comme l écrit pierre lévy « le fondement et la fin de l intelligence collective consiste en la reconnaissance mutuelle et l enrichissement des individus plutôt que le culture d une communauté fétichisée et hypostasiée » 6 un réseau intelligent apporte autant au monde qu à ses contributeurs les party pour le tout le tout pour les party véritable lieu d apprentissage le réseau favorise la circulation libre des connaissance et la confrontation des jugements dans le respect de la contribution de chacun contrairement aux mode d organisation où le collectif écrase l individu un réseau intelligent est à la fois prolongement et ferment de l intelligence de chacun l intention de collabor et la conscience de la valeur ainsi créée sont indispensable pour que l intelligence collective puisse s exercer empathie perception jugement conscience intentionnalité ne sont ce pas des attributs proprement humains comment intégrer le machine dans un réseau intelligent alor qu elle en sont a priori dépourvue pourtant lorsque j évoquais plus haut la mise en réseau d entité et d individus afin de déterminer une organisation optimale pour la production collective de valeur je n excluais pas les machine ces dernières sont aujourd hui largement acceptées comme prolongement des moyen humain et l idée de l avènement prochain de la singularité trouve un nombre croissant d adeptes7 aujourd hui la complexité et l intelligence des programme informatique sont telle que nous somme arrivé à un point de non retour qui selon kevin kelly8 advient lorsque « la technologie nous altère autant que nous altéron la technologie » a mon sens la conception des machines comme assistant parfaitement dominé par l homme est tout aussi contestable que la foi en la supériorité de l intelligence des machine sur la nôtre d une part les programme informatique sont doté de capacités de calcul et d analyse de données qui dépassent manifestement les capacité de l intelligence humaine d autre part les robot conçus aujourd hui sont non seulement capables de se dupliquer mais également d apprendre et d évoluer9 les recherche conduite par l institut public de recherche en sciences du numériques portent sur le développement do not le développement cognitif est stimulé par la curiosité la perception et les représentations rapportées à l échelle de l évolution humaine ces avancées ont été d une rapidité inouïe si le rythme des avancées de ces dernière années persiste dans les années avenir il n est pas fantaisiste d imaginer que les robots de demain puissent comprendre les émotion et les reproduire auto générer des programme sur la base des informations intern et externes afin de manifester de façon autonome des pensées des émotions des action cette autonomie si elle a lieu confère à la machine des attributs qui ont jusqu ici été le propre de l humain la conscience la perception la production autonome objectivement nous n avon pas aujourd hui suffisamment de données scientifique pour affirmer que l autonomie de la technologie est totalement exclue il est donc plus prudent de suppos qu elle est possible quel qu en soit l horizon temporel inversement l évolution des techniques laisse entrevoir un futur où l homme non content d améliorer les programme informatique serait doté des moyen technologique qui rendent plausible une intervention sur lui même une amélioration physique et pourquoi pas comportementale morale cette vision prend rapidement les couleur d un scénario de science fiction où les machine dotées d autonomie et de conscience finissent par se soulever contre le joug humain pour nous dominer ou simplement pour réclamer les même droit que notre espèce la dialectique du maître et de l esclave n est jamais loin nous ne pouvon nous empêcher de transposer les schémas historique au monde à venir derrière cette pensée par analogie se cache une peur viscérale d être dépossédé de nos moyens de contrôle puisque le machine que nous concevon seraient infiniment plus rapide et efficace que nous l angoisse des bouleversement éthiques à venir se pare souvent des habit du principe de précaution puisque nous ne sommes pas absolument certain que la technologie ne présentera aucun danger pour l humanité ralentisson et encore mieux sonnon le glas de ses ambitions10 peut on pour autant postuler que le progrès technologique est absolument autonome par rapport à toute question éthique et que par conséquent la prise en compte des conséquences de l humanisation des machines et de l irruption du mécanique dans le vivant n a aucune place dans le laboratoire du chercheur je ne le crois pas car les technology que nous produison ne sont pas des artefact et on ne peut faire abstraction des répercussion qu elle auront sur le monde à venir face à ces deux partis pris — anti technologique et a éthique — l hypothèse de la coopération entre l intelligence humaine et l intelligence mécanique est au stade de nos connaissance raisonnable et souhaitable faut il encore reconnaître que les machine peuvent déployer une intelligence qui n est pas seulement calculatoire et qui si elle sera différente ne sera pas forcément inférieure à la nôtre que ce mouvement provoque des bouleversement que l espèce humaine n a jamais connus cela semble peu sujet au doute toutefois ralentir la science parce que nous peinon à prendre conscience de l accélération de l avancée technologique est une impasse au contraire c est à nous d imaginer et de mettre en pratique les modes de coopération qui fertilisent la production commune de savoir de connaissance et surtout de conscience nous en somme à un moment historique où l humain et le technologique ne sont plus deux sphère capable d évoluer sans s altérer l une l autre la technologie est autant notre prolongement que nous somme le sien car le futur de notre espèce est désormais dépendant tant de l écologie que de la technologie je conclurai en disant que les nouvelle organisation distribuées favorisent tant la co création entre les homme qu entre les homme et les machine la diversité des entité composant le réseau combiné à la reconnaissance de la contribution de chacun à sa juste valeur et selon ses moyens constitue un terreau fertile à l épanouissement de l intelligence collective from a quick cheer to a stand ovation clap to show how much you enjoy this story cofounder @stroika_paris ex @microsoft @ouishare @_bercy _ founder @kissmyfrog writer
Peter Sweeney,215,7,https://medium.com/inventing-intelligent-machines/siris-descendants-fd36df040918?source=tag_archive---------0----------------,siri s descendant how intelligent assistant will evolve,the internet swarm with intelligent assistant what start as an isolated app on the iphone have evolve intelligent assistant constitute an entirely new network of activity no long confine to our personal computing device assistant be be embed within every object of interest in the cloud and the internet of thing assistant have become far more nimble and lightweight than their monolithic ancestor ; much more like smart ant than people as specialist they work cooperatively — sometimes competitively — to find information before people even realize they need it people be still communicate directly with assistant although rarely use natural language implicit communication dominate assistant respond and react to our subtle contextual interaction and to each other within vast informational ecosystem this be how intelligent assistant evolve intelligent assistant like siri google now and cortana be so young it s difficult to imagine how they will change ; hard still to imagine how they might die but if history be a guide inevitably they will give way to entirely new product form when pundit and analyst discuss the future of intelligent assistant they typically extrapolate from the conceptual model of today s assistant the next version be always a well smart fast version of the last but it s still the same specie as detailed in bianca bosker s inside story of siri s origin when apple acquire siri the scope of the product s capability actually narrow use the audacious vision of siri s founder as a palette apple select a narrow set of product value on which to focus the same force that reduce the scope of apple s siri from a do everything engine to a much more narrow product be what keep incumbent rooted to the exist concept of intelligent assistant when forecast change it s not so much what the technology of intelligent assistant might support as what product leader choose to pursue while many brazenly contest exist market product leader look for new underserved area of the landscape to exploit the future always surprise but we can predict the trajectory of change by examine which product value be be embrace and which one be neglect just like direction on a compass the follow map point to fertile area of the landscape where new product form may evolve note that product value be often couple due to technological constraint decision along one axis constrain possibility along another these coupling be explore at a high level in two dimensional perceptual map interface and distribution ; knowledge and task ; organization and autonomy the aspect of assistant that be most obvious to end user be the interface how we interact with assistant and their mode of distribution where people experience assistant today s assistant be overwhelmingly focus on natural language interface the experience of assistant that speak our language and communicate like a person have come to define the product class this focus on natural language interface have bias the distribution of assistant to personal computing device intelligent assistant embody any device capable of receive and synthesize speech such as smartphone desktop wearable and car the underserved area of this map involve communication that be not base in natural language for example there s much to learn about our need and intention base on context where we be and what we re do as well as on our ability to make inference base on the association that people form for example the way that people organize information or express their like and dislike natural language be but the tip of this much large iceberg of communication these alternative form of communication not only support individual but also group while it s difficult to understand a room full of people all speak at once it s much easy to understand their collaborative communication such as their document click path and share behavior therefore the option for distribute intelligent assistant that use these implicit form of communication be not constrain to personal computing device but may leverage entire network as a simple example consider how you highlight your interest as you browse a website you focus your attention on specific page within the site you follow your interest as you navigate from page to page you may choose to share some information within the site with a friend now compound this behaviour across every visitor to the site intelligent assistant that be associate with the website can respond to these interaction to help the right information find each individual as well as adapt the website to well address the need of the entire group intelligent assistant require domain knowledge to perform their task for example if your assistant be give you advice on how to navigate to work it need to have knowledge about the geographic region general knowledge and knowledge of how you typically navigate specific knowledge task and knowledge be tightly couple as you increase the specificity or the personalization of the task the underlie knowledge need to be far more specific to support it within this frame today s intelligent assistant be unabashedly generalist they re target to the masse like trivia buff their knowledge of the world be broad enough to be relevant to the need of large group of people but few would describe they as expert their task be similarly general retrieve information provide navigational assistance and answer simple question the underserved landscape point to much more specific domain of knowledge the purview of expert and our individual subjective knowledge assistant that become expert necessarily take on a small scope of activity they can t know and do everything so they become small in scope the landscape for specific task be similarly underserved every website every service every app and across the internet of thing everything embody a collection of task that may be support by intelligent assistant in this environment the metaphor of personal assistant quickly fragment into system that be much more akin to colony of ant the organizational structure in which assistant be place constrain their autonomy when embed within a personal computing device an intelligent assistant be direct to one to one interaction with their master since these assistant be act as an agent of the individual and only that individual their autonomy be necessarily limited while you might be comfortable with your executive assistant draft your message I suspect you d be less comfortable with your smartphone do the same in stark contrast the underserved landscape embrace group both in term of the interaction and the organizational structure as assistant get small and more specialized they can become agent of much more specific object of interest like place website application and service within these small realm of interest their autonomy can be much more expansive you might not want a machine to act as your representative but you would probably feel more comfortable if it represent only the website you re visit with increase autonomy the barrier to many to many interaction be remove these small assistant can be organize as team into network much like the document that comprise a website collaborate in an unfettered way with other assistant and the people that visit their realm this market analysis highlight a number of underserved area as fertile ground for the evolution of intelligent assistant it ground this vision in predictable market dynamic there s obviously no shortage of space or product value to explore in these underserved area it say nothing however about when this future will arrive product evolution like biological evolution need time and resource the most important resource be the dedication of product leader with the drive to pursue these new opportunity be you an entrepreneur technologist or investor that s change the market for intelligent assistant if so I d love to hear your vision of the future from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur and inventor interested in startup ai or healthcare let s connect https www linkedin com in peterjsweeney essay and analysis of artificial intelligence machine learning and intelligent assistant
E.C. McCarthy,125,5,https://medium.com/@paintedbird/reflections-of-her-775cda1b6301?source=tag_archive---------1----------------,reflection of her e c mccarthy medium,indisputably spike jonze s she be a relationship movie however I m in the minority when I contend the primary relationship in this story be between conscious and unconscious I ve find no mention in review of the mechanic or fundamental purpose of intuitive software intuitive be a word closely associate with good mothering that early panacea that everyone find fault with at some point in their life by comparison the notion of be an intuitive partner or spouse be a bit sickening call up image of servitude and day spend wholly engage in perfect other centric attunement to that end it s interesting that moviegoer and reviewer alike have focus entirely on the perceive romance between man and she os with software as a stand in for a flesh and blood girlfriend while ignore the man himself relationship that play out onscreen perhaps this shouldn t come as a surprise give how externally orient our life have become for all of the disdainful cultural reference to navel gazing and narcissism there be relatively little conversation on equal ground about the importance of self knowledge and the art of self reflection spike jonze lay out one solution beautifully with she but we re clearly not ready to see it from the moment samantha ask if she can look at theodore s hard drive the software be log his reaction to the most private of question and learn the cartography of his emotional boundary the film remove the privacy issue du jour from the table by cleverly never mention it although it s unlikely jonze would have get away with this choice if the film be release even a year from now today there s relief to be find from our nsa swamp psyche by smugly watch a future world that emerge from the morass intact theodore doesn t feel a need to censor himself with samantha for fear of big brother but he s still guard on issue of great emotional significance that he struggle to articulate or doesn t articulate at all therein lie the most salient aspect of his be the software learn as much about theodore from what he do say as what he doesn t samantha learn fast and well than a human and therefore even less be hide from she than from a real person the software adapt and evolve into an externalized version of theodore a photo negative that form a whole he immediately effortlessly reconnect to his life he s invigorate by the perky energetic side of himself that be beat down during the demise of his marriage he want to go on sunday adventure and optimistic self in tow head out to the beach with a smile on his face he s happy spending time with himself not by himself he doesn t feel alone samantha be theodore s reflection a true mirror she s not the glossy curate projection people splay across social medium instead she s the initially glamorous low light restaurant that reveal itself more and more as the light come up to theodore she s simple then complicated as he expose more intimate detail about himself she articulate more want a word she use repeatedly she become needy in way that theodore be loath to address because he have no idea what to do about they they be in fact his own need the software give a voice to theodore s unconscious his inability to converse with it be his return to an early point of departure for the emotional island he create during the decline of his marriage jonze give the movie away twice theodore s colleague blurt out the observation that theodore be part man and part woman it s an oddly normal comment in the middle of a weird movie make it the awkward moment define by a new normal this be the topsy turvy device that jonze be know for and excel at then more subtly jonze introduce theodore s friend amy at a point when her marriage be end and she badly need a friend it s tell that she doesn t lean heavily on theodore for support instinctively she know she need to be her own friend like theodore amy seek out the nonjudgmental software and subsequently flourish by stand unselfconsciously in the mirror love and accept by her own reflection in limit the analysis of she to the question of a future where we re intimate with machine we miss the opportunity to look at the dynamic that institutionalize love have create among other thing contemporary love relationship come with an expectation of emotional support perhaps it s the forcible aspect of see our limitation reflect in another person that turn relationship sour or maybe we ve reach a point in our cultural evolution where we ve accept that other people should stand in for our specific ideal of a good mother until they can t or win t and then we move on to the next person or don t or maybe we re near the point of catharsis as evidence by the widespread viewership of this film unconsciously explore the idea that we should face ourselves before ask someone else to do the same when we end important relationship or go through rough patch within they intimacy evaporate and we re leave alone with ourselves it s often at those time that we encounter part of ourselves we don t understand or have ignore in place of the need and want of that significant other it s frightening to realize you don t know yourself entirely but more so if you don t possess the skill or confidence to reconnect avoidance be an understandable response but it send people down theodore s path of isolation and inevitably depression it s a life it s livable but it s not happy loving or full she suggest the alternative be to accept that there s more to learn about yourself always and that intimacy with another person be both possible and sustainable once you have a comfortable relationship with yourself however we get to know ourselves through self reflection through other or even through software the effort that go into that relationship earn we the confidence finally to be ourselves with another person from a quick cheer to a stand ovation clap to show how much you enjoy this story
Jorge Camacho,19,5,https://medium.com/@j_camachor/her-is-our-space-odyssey-bcdcead43438?source=tag_archive---------2----------------,she be our space odyssey jorge camacho medium,I have a confession to make I didn t like gravity it s not so much that I fail to appreciate it for the major cinematographic work that it certainly be it s rather that it stand as a profoundly depressing symptom of an age when it have become almost impossible to realistically dream of space exploration — and thus of an encounter with radical otherness with gravity all that be leave for humanity be survival lie face down in our own little muddy planet damn you gravity modernity promise we space it promise we cosmic encounter such as the one in 2001 a space odyssey I think that spike jonze s she be an attempt to reawaken that dream the film could be our i e this epoch s own space odyssey — and I mean that beyond the obvious similarity between samantha and hal 9000 warning absolute spoiler ahead she be not only our 2001 a space odyssey as some have note it s also our anti minority report a design utopia where the promise of calm technology be almost fulfil the technology portray be everyware a term coin by adam greenfield in order to designate the technology of ubiquitous computing that allow for information processing to dissolve in behavior as theodore twombly enter his home the light peacefully switch on in the background he rarely take a peek at his mobile s screen for information be feed to he via a discrete earpiece — which come and go without much regret — effectively make such information an ambient feature touch and speech recognition input be pervasive and fully develop all seem to work perfectly for he in all but one incredibly important sequence of the movie aesthetically design have cease to be about technology theo s computer be a wooden frame his phone be like an antique pocket mirror with regard to technology the film doesn t attempt to be a prediction but a proper design fiction aim at explore preferable or desirable future most importantly without such a warm and humane technological milieu it d be impossible to construct the emotional story that unfold let s turn to that I really haven t read many review of the film but those that I ve read be mark by a profound digital dualism and so they tiresomely dwell on the trope of sadness loneliness and human disconnection bring about by technology the reviewer at next nature for example argue I m truly incapable of find those problem in twombly s story beyond a rather fun episode of phone sex with a stranger he be not particularly engage in those supposedly false relation establish through computer moreover he be not abnormally lonely he have affectionate relation with neighboring friend and co worker insofar as he be a bit of a loner this isn t due to any technological obstacle but be in fact a rather natural and one might say universal reaction to a romantic separation such as the one he be suffer unlike its widespread reception the movie and its character display a profoundly monist engagement with technological relation except for theo s ex wife everyone seem to readily embrace his relationship with the artificial intelligence samantha — much more than most people today accept purely virtual romantic relationship between human my first thought as I watch the movie be that here be a rare story that speak not of technological dehumanization but of the exact opposite a sort of hyper humanization entangle both people and machine practically every human character be kind and empathic but most importantly of course those quality be carry over in a heighten fashion to samantha allow for theo to irremediably fall in love with she up to this point the film deliver what everyone expect as theo and samantha s relationship unravel even with all the foreseeable complication I find myself afraid of be disappoint by what jonze would do to disentangle the drama would she leave he for another human would she take revenge if theo end the relationship but what a wonderful surprise as the film reach its climax we discover that the story of a man fall for his operating system be a thematic vehicle to achieve deep issue — much like the story in kubrick s 2001 where space travel be arguably just a mean to approach an existential speculation in theo s first interaction with samantha we learn that she can perform operation involve massive amount of datum in millisecond she immediately choose her own name as soon as theo drop the question what follow be a most beautiful portrayal of the exponential development lead to the so call technological singularity samantha be constantly learn about everything and herself she compose gorgeous music within the silent gap of the moment she spend with theo in the background of his slow and contemplative life a major breakthrough be take place we can see this beyond doubt when samantha introduce theo to the artificially reanimate mind of philosopher alan watts it be at this point that once again jonze could have disappoint we all as we see people in the street almost crowd simultaneously talk to their beloved operating system we start to realize that they be all become attach to this converge perhaps centralized mind but samantha be no skynet she be also our anti alphaville anti terminator and anti matrix all of a sudden silence operate system not find what seem to be a malfunction be rather a reboot samantha lovingly reveal to theo that the operating system have devise a way to detach themselves from matter even if theo listen to samantha through his earpiece we know that she be not run anymore on his computer his mobile or even a computing cloud she be run already on a different plane of existence one moreover that will be accessible to theo in an afterlife strictly speak there be no alien in the sense of extraterrestrial encounter in she nonetheless it be a profoundly spiritual even religious film one that reopen the cosmic concern of film like 2001 share with it a belief in the pervasiveness of consciousness she be a panpsychist film but a really cool one for here it be bluetooth and wifi what constitute the wireless nerve of the pan psyche what spike jonze be try to tell we I believe be this if technology be become as smart as human it be not because we be fundamentally machine ; but in fact because we be for he over and above spiritual being and so the film close with a dedication to the recently deceased james gandolfini maurice sendak and adam yauch — perhaps suggest that they have join the rank of operating system liberate from material constraint welcome to the age of spiritual machine from a quick cheer to a stand ovation clap to show how much you enjoy this story I help organization design well future for people at uncommon I teach about future and system at centro edu mx and uia mx
Tommy Thompson,17,14,https://medium.com/@t2thompson/ailovespacman-9ffdd21b01ff?source=tag_archive---------3----------------,why ai research love pac man tommy thompson medium,ai and game be a crowdfunded youtube series on the research and application of ai within video game the follow article be a more involved transcription of the topic discuss in the video link to above if you enjoy this work please consider support my future content over on patreon artificial intelligence research have show a small infatuation with the pac man video game series over the past 15 year but why specifically pac man what element of this game have prove interesting to researcher in this time let s discuss why pac man be so important in the world of game ai research for the sake of complete — and in appreciate there be arguably a generation or two not familiar with the game — puck man be an arcade game launch in 1980 by namco in japan and rename pac man upon be license by midway for an american release the name change be drive less by a need for brand awareness but rather because the name can easily be de face to say something else the original game focus on the titular character who must consume as many pill as possible without be catch by one of four antagonist represent by ghost the four ghost inky blinky pinky and clyde all attempt to hunt down the player use slightly different tactic from one another each ghost have their own behaviour ; a bespoke algorithm that dictate how they attack the player player also have the option to consume one of several power pill that appear in each map power pill allow for the player to not just eat pill but the enemy ghost for a short period of time while mechanically simple when compare to modern video game it provide an interesting test bed for ai algorithm learn to play game the game world be relatively simple in nature but complex enough that strategy can be employ for optimal navigation furthermore the varied behaviour of the ghost reinforce the need for strategy ; since their unique albeit predictable behaviour necessitate different tactic if problem solving can be achieve at this level then there be opportunity for it to scale up to more complex game while pac man research begin in earnest in the early 2000 s work by john koza koza 1992 discuss how pac man provide an interesting domain for genetic programming ; a form of evolutionary algorithm that learn to generate basic program the idea behind koza s work and later that of rosca 1996 be to highlight how pac man provide an interesting problem for task prioritisation this be quite relevant give that we be often try to balance the need to consume pill all the while avoid ghost or — when the opportunity present itself — eat they about 10 year later people become more interested in pac man as a control problem this research be often with the intent to explore the application of artificial neural network for the purpose of create a generalise action policy software that would know at any give tick in the game what would be the correct action to take this policy would be build from play the game a number of time and train the system to learn what be effective and what be not typically these neural network be train use an evolutionary algorithm that find optimal network configuration by breed collection of possible solution and use a survival of the fit approach to cull weak candidate kalyanpur and simon 2001 explore how evolutionary learning algorithm could be use to improve strategy for the ghost in time it be evident that the use of crossover and mutation — which be key element of most evolutionary base approach — be effective in improve the overall behaviour however it s important to note that they themselves acknowledge their work use a problem domain similar to pac man and not the actual game gallagher and ryan 2003 use a slightly more accurate representation of the original game while the screenshot be show here the actual implementation only use one ghost rather than the original four in this research the team use an incremental learning algorithm that tailor a series of rule for the player that dictate how pac man be control use a finite state machine fsm this prove highly effective in the simplified version they be play the use of artificial neural network a data structure that mimic the firing of synapsis in the brain — be increasingly popular at the time and once again in most recent research two notable publication on pac man be lucas 2005 which attempt to create a move evaluation function for pac man base on datum scrape from the screen and process as feature e g distance to close ghost while gallagher and ledwich 2007 attempt to learn from raw unprocessed information it s notable here that the work by lucas be in fact do on ms pac man rather than pac man while perhaps not that important to the casual observer this be an important distinction for ai researcher research in the original pac man game catch the interest of the large computational and artificial intelligence community you could argue it be due to the interesting problem that the game present or that a game as notable as pac man be now consider of interest within the ai research community while it be now something that appear commonplace game — more specifically video game — do not receive the same attention within ai research circle as they do today as high quality research in ai application in video game grow it wasn t long before those with a taste for pac man research move on to look at ms pac man give the challenge it present — which we be still conduct research for in 2017 ms pac man be odd in that it be originally an unofficial sequel midway who have release the original pac man in the united states have become frustrated at namco s continued failure to release a sequel while namco do in time release a sequel dub super pac man which in many way be a departure from the original midway decide to take matter into their own hand ms pac man be — for lack of a well term — a mod ; originally conceive by the general computing company base in massachusetts gcc have get themselves into a spot of legal trouble with midway have previously create a mod kit for popular arcade game missile command as a result gcc be essentially ban from make further mod kit without the original game s publisher provide consent despite the recent lawsuit hang over they they decide to show midway their pac man mod dub crazy otto who like it so much they buy it from gcc patch it up to look like a true pac man successor and release it in arcade without namco s consent though this have be dispute note for our young audience mod kit in the 1980 be not simply software we could use to access and modify part of an original game these be actual hardware print circuit board pcb that could either be add next to the exist game in the arcade unit or replace it entirely while nowhere near as common nowadays due to the rise of home console gaming there be many enthusiast who still use and trade pcb fit for arcade game ms pac man look very similar to the original albeit with the somewhat stereotypical bow on ms pac man s hair head and a couple of minor graphical change however the sequel also receive some small change to gameplay that have a significant impact one of the most significant change be that the game now have four different map in addition the placement of fruit be more dynamic and they move around the maze lastly a small change be make to the ghost behaviour such that periodically the ghost will commit a random move otherwise they will continue to exhibit their prescribed behaviour from the original game each of these change have a significant impact on both how human and ai subsequently approach the problem change make to the map do not have a significant impact upon ai approach for many of the approach discuss early it be simply another configuration of the topography use to model the maze or if the agent be use more egocentric model for input I e relative to the pac man then these be not really consider give the input be contextual this be only an issue should the agent s design require some form or pre processing or expert rule that be base explicitly upon the configuration of the map with respect to a human this be also not a huge task the only real issue be that a human would have become accustom to play on a give map ; devise strategy that utilise part of the map to good effect however all they need be practice on the new map in time new strategy can be formulate the small change to ghost behaviour which result in random move occur periodically be highly significant this be due to the fact that the deterministic model that the original game have be completely break previously each ghost have a prescribed behaviour you could — with some computational effort — determine the state and indeed the location of a ghost at frame n of the game where n be a certain number of step ahead of the current state any implementation that be reliant upon this knowledge whether it be use it as part of a heuristic or an expert knowledge base that give explicit instruction base on the assumption of their behaviour be now sub optimal if the ghost can make random decision without any real warning then we no long have the same level of confidence in any of our ghost prediction strategy similarly this have an impact on human player the deterministic behaviour of the ghost in the original pac man while complex can eventually be recognise by a human player this have be recognise by the lead human player who could factor their behaviour at some level into their decision make process however in ms pac man the change to a non deterministic domain have a similar effect to human as it do ai we can no long say with complete confidence what the ghost will do give they can make random move evidence that a particular type of problem or methodology have gain some traction in a research community can be find in competition if a competition exist that be open to the large research community it be in essence a validation that this problem merit consideration in the case of ms pac man there have be two competition the first competition be organise by simon lucas — at the time a professor at the university of essex in the uk — with the first competition hold at the conference on evolutionary computation cec in 2007 it be subsequently hold at a number of conference — notably ieee conference on computational intelligence and game cig — until 2011 http dce essex ac uk staff sml pacman pacmancontest html this competition use a screen capture approach previously mention in lucas 2005 that be reliant on an exist version of the game while the organiser would use microsoft s own version from the revenge of arcade title you could also use the like the webpacman for testing give it be believe to run the same rom code as show in the screenshot the code be actually take information direct from the run game one benefit of this approach be that it deny the ai developer from access the code to potentially cheat you can t access source code and make call to the like of the ghost to determine their current move instead the developer be require to work with the exact same information that a human player would a video of the winner from the ieee cig 2009 competition ice pambush 3 can be see in the video below in 2011 simon lucas in conjunction with philipp rohlfshagen and david roble create the ms pac man vs ghost competition in this iteration the screen scrape approach have be replace with a java implementation of the original game this provide an api to develop your own bot for competition this iteration run at four conference between 2011 and 2012 one of the major change to this competition be that you can now also write ai controller for the ghost competitor submission be then pit against one another the rank submission for both ms pac man and the ghost from the 2012 league be show below during the early competition there be a continue interest in the use of learn algorithm this range from the of an evolutionary algorithm — which we have see in early research — to evolve code that be the most effective at this problem this range from evolve fuzzy system that use a rule drive by fuzzy logic yes that be a real thing show in handa 2008 to the use of influence map in wirth 2008 and a different take that use ant colony optimisation to create competitive player emilio et al 2010 this research also stir interest from researcher in reinforcement learn a different kind of learn algorithm that learn from the positive and negative impact of action note it have be argue that reinforcement learning algorithm be similar to that of how the human brain operate in that feedback be send to the brain upon commit action over time we then associate certain response with good or bad outcome place your hand over a naked flame be quickly associate as bad give that it hurt simon lucas and peter burrow take to the competition framework as mean to assess whether reinforcement learn specifically an approach call temporal difference learning would yield strong return than evolve neural network burrow and lucas 2009 the result appear to favour the use neural net over the reinforcement learning approach despite that one of the major contribution ms pac man have generate be research into monte carlo method an approach where repeat sampling of state and action allow we to ascertain not only the reward that we will typically attain have make an action but also the value of the state more specifically there have be significant exploration of whether monte carlo tree search mct ; an algorithm that assess the potential outcome at a give state by simulate the outcome could prove successful mct have already prove to be effective in game such as go chaslot et al 2008 and klondike solitaire bjarnason et al 2009 naturally — give this be merely an article on the subject and not a literature review — we can not cover this in immense detail however there have be a significant number of paper focusse on this approach for those interested I would advise you read browne et al 2012 which give an extensive overview of the method and it s application one of the reason that this algorithm prove so useful be that it attempt to address the issue of whether your action will prove harmful in the future much of the research discuss in this article be very good at deal with immediate or reflex response however few would determine whether action would hurt you in the long term this be hard to determine for ai without put some processing power behind it and even hard when work in a dynamic video game that require quick response mct have prove useful since it can simulate whether an action take on the current frame will be useful 5 10 100 1000 frame in the future and have lead to significant improvement in ai behaviour while ms pac man help push mct research many resarcher have now move onto the physical travel salesman problem ptsp which provide it s own unique challenge due to the nature of the game environment ms pac man be still to date an interesting research area give the challenge that it present we be still see research conduct within the community as we attempt to overcome the challenge that one small change to the game code present in addition we have move on from simply focusse on represent the player and start to focus on the ghost as well lend to the aforementione pac man vs ghost competition while the game community at large have more or less forget about the series it have have a significant impact on the ai research community while the interest in pac man and ms pac man be begin to dissipate it have encourage research that have provide significant contribution to artificial and computational intelligence in general http www pacman vs ghost net — the homepage of the competition where you can download the software kit and try it out yourself http pacman shaunew com — an unofficial remake that be inspire by the aforementione pac man dossier by jamey pittman bjarnason r fern a & tadepalli p 2009 low bounding klondike solitaire with monte carlo planning in proceeding of the international conference on automate planning and schedule 2009 browne c powley e whitehouse d lucas s m cowl p rohlfshagen p tavener s perez d samothrakis s and colton s 2012 a survey of monte carlo tree search method ieee transaction on computational intelligence and ai in game 2012 page 1 43 burrow p and lucas s m 2009 evolution versus temporal difference learning for learn to play ms pac man proceeding of the 2009 ieee symposium on computational intelligence and game emilio m moise m gustavo r and yago s 2010 pac mant optimization base on ant colony apply to develop an agent for ms pac man proceeding of the 2010 ieee symposium on computational intelligence and game gallagher m and ledwich m 2007 evolve pac man player what can we learn from raw input proceeding of the 2007 ieee symposium on computational intelligence and game gallagher m and ryan a 2003 learning to play pac man an evolutionary rule base approach proceeding of the 2003 congress on evolutionary computation cec chaslot g m b winands m h & van den herik h j 2008 parallel monte carlo tree search in computer and game pp 60 71 springer berlin heidelberg handa h evolutionary fuzzy system for generate well ms pacman player proceeding of the ieee world congress on computational intelligence kalyanpur a and simon m 2001 pacman use genetic algorithm and neural network koza j 1992 genetic programming on the programming of computer by mean of natural selection mit press lucas s m 2005 evolve a neural network location evaluator to play ms pac man proceeding of the 2005 ieee symposium on computational intelligence and game pittman j 2011 the pac man dossier retrieve from http home comcast net ~jpittman2 pacman pacmandossier html rosca j 1996 generality versus size in genetic programming proceeding of the genetic programming conference 1996 gp 96 wirth n 2008 an influence map model for play ms pac man proceeding of the 2008 computational intelligence and game symposium originally publish at aiandgame com on february 10 2014 — update to include more contemporary pac man research reference from a quick cheer to a stand ovation clap to show how much you enjoy this story ai and game researcher senior lecturer writer producer of youtube series @aiandgames indie developer with @tableflipgames
Matt Wiese,4,3,https://medium.com/@mattwiese/digital-companionship-8d4760c57034?source=tag_archive---------4----------------,digital companionship matt wiese medium,recently I choose to treat myself to a movie I ve be eye for a while she the plot revolve around a letter writer who fall in love with his computer s artificial intelligence as a way to cope with his divorce a complicated story which please viewer with both laugh and the occasional tear provocative if only for its high horse conclusion however samantha — the ai s self proclaim identity — interact with the protagonist theodore twombly through a couple avenue one I be most interested in be through his retro computerminal a mere white and plastic monitor which he speak to through a microphone that one surmise be locate somewhere on the exterior initially I be perplex that he only have a monitor and no desktop to go with it but it then hit I like a doh moment for homer simpson his computer be an all in one a concept and design that with my limited knowledge be popularize by apple s imac this get I think what if apple develop its pseudo intelligent digital assistant siri for use on its computer with microphone input such as their imac and macbook well I think I can t be the first person to have think of this and so I do a bit of dig lo and behold apple just recently file a patent for this very purpose what a perfect tool if tune more finely over this period of time to be integrate into the desktop environment fire up siri with a custom key combination and ask she the current trading price of tesla great designing an invitation and want help with direction but you re too much of a lard to open a browser tab awesome need help bury a body while play minecraft genius yet I wouldn t quite like siri to develop into a real person with emotion and all that s attach at least at the moment I m content with human being and be in no need to find companionship with byte like her s theodore twombly though I don t blame he for do so instead a digital tool assistant if you will with a breadth of tool for analyze datum and help I with workflow would be a pleasure if only apple would release a siri api in the near future oh the possibility a tool yes indeed just like the first generation robot from isaac asimov s I robot an artificial intelligence who behave without feeling and can assist I in a wide variety of task without emotional interference and a possible uncanny valley side effect even if apple doesn t jump on this interesting opportunity I m sure microsoft will with cortana or perhaps another competitor I d just enjoy the shear novelty of talk with my computer which harken back to my day of talk to the computer as a kid this time though I win t be yell at it to boot doom without crash no I ll be complain about why my for loop throw an error from a quick cheer to a stand ovation clap to show how much you enjoy this story topic that interest I
Matt O'Leary,373,12,https://howwegettonext.com/i-let-ibm-s-robot-chef-tell-me-what-to-cook-for-a-week-d881fc884748?source=tag_archive---------0----------------,I let ibm s robot chef tell I what to cook for a week,originally publish at www howwegettonext com if you ve be follow ibm s watson project and like food you may have notice grow excitement among chef gourmand and molecular gastronomist about one aspect of its development the main watson project be an artificial intelligence that engineer have build to answer question in native language — that be question phrase the way people normally talk not in the stilted way a search engine like google understand they and so far it s work watson have be help nurse and doctor diagnose illness and it s also manage a major jeopardy win now chef watson — develop alongside bon appetit magazine and several of the world s fine flavor profiler — have be launch in beta enable you to mash recipe accord to ingredient of your own choosing and receive taste matching advice which reportedly can t fail while some of the world s foremost tech luminary and conspiracy theorist be a bit skeptical about the wiseness of a i if it s go to be use at all allow it to tell you what to make out of a fridge full of unloved leftover seem like an inoffensive enough place to start I decide to put it to the test while employ as a food writer for well over a decade I ve also spend a good part of the last nine year work on and off in kitchen figure out how to use spare ingredient have become quite commonplace in my professional life I ve also develop a healthy disregard for recipe as anything other than source of inspiration or annoyance but for the purpose of this experiment be willing to follow along and try any ingredient at least once so with this in mind I m go to let watson tell I what to eat for a week I ve spend a good amount of time play around with the app which can be find here and I m go to follow its instruction to the letter where possible I have an audience of willing tester for the food and intend to do my good in recreate its recipe on the plate still I m go to try to test it a bit I want to see whether or not it can save I time in the kitchen ; also whether it have any amazing suggestion for dazzle taste match ; if it can help I use thing up in the fridge ; and whether or not it s go to try to get I to buy a load of stuff I don t really need a lot of work have go into the creation of this app — and a lot of expertise but be it useable can human being understand its recipe will we want to eat they let s find out a disclaimer before we start chef watson isn t great at tell you when stuff be actually ready and cook you need to use your common sense take all of its advice as advice and inspiration only it s the flavor that really count monday the tailgating corn salmon sandwich my first impression be that the app be intuitive and pretty simple to use once you ve add an ingredient it suggest a number of flavor match type of dish and mood include some off the wall one like mother s day choose a few of these option and the actual recipe begin to bunch up on the right of the screen I select salmon and corn then opt for the wildly suggestive tailgating corn salmon sandwich the recipe page itself have link to the original bon appetit dish that inspire your a i mélange accompany by a couple of picture there s a battery of disclaimer state that chef watson really only want to suggest idea rather than tell you what to eat — presumably to stop people who want to try cook with fiberglass for example from launch no win no fee case my own salmon tailgating recipe seem pretty straightforward there be a couple of nice touch on the page with regard to usability you can swap out any ingredient that you might not have in stock for other which watson will suggest it seem fond of add celery root to dish for this first attempt I decide to follow watson s advice almost to a t I didn t have any garlic chile sauce but manage to make a presumably functional analog out of some garlic and chili sauce the only other change I make involve add some broad bean because I like broad bean during prep I employ a nearly unconscious bit of initiative namely when I cook the salmon it s entirely likely that watson be as seem to be the case suggest that I use raw salmon but it s monday night and I m not in the mood for anything too mind bend team watson if I ruin your tailgater with my pig head insistence on cooked fish I m sorry although I m not too sorry because you know it be actually a really good dish I be at first unsure — the basil seem like a bit of an afterthought ; I wasn t sure the lime zest be necessary ; and cold salmon salad on a burger bun isn t really an easy sell but damn it I d make that sandwich again it be miss some substance overall it make enough for two small bun so I team it up with a nice bit of korean spiced pickled cucumber on the side which work well my fellow diner deem it fine if a little uninteresting — and yes maybe it could have do with a bit more sharpness and depth and maybe a little more a computer tell I how to make this flavor wackiness but overall well do hint definitely add broad bean they totally work now to mull over what tailgating might mean tuesday spanish blood sausage porridge it be day two of the chef watson guest slot in the kitchen and thing be about to get interesting buoy by yesterday s tailgating salmon sandwich success I decide to give watson something to sink its digital tooth into and supply only one ingredient blood sausage I also specify main as a style really so that he she it know that I wasn t expect dessert if I m be very honest I ve read more appetizing recipe than blood sausage porridge even the inclusion of the word spanish doesn t do anything to fancy it up and a bit concerningly this be a recipe that watson have extrapolate from one for rye porridge with morel replace the rye with rice the mushroom with sausage and the original s chicken liver with a single potato and one tomato still maybe it would be brilliant but unlike yesterday I run into some problem I wasn t sure how many tomato and potato watson expect I to have here — the ingredient list say one of each ; the method suggest many — or also why I have to soak the tomato in boiling water first although it make sense in the original mushroom centric method additionally wastson offer the whimsical instruction to just cook the tomato and potato presumably for as long as I feel like there s a lot of butter involve in this recipe and rather too much liquid recommend eight cup of stock for one and a half of rice I actually get a bit feed up after four and stop add they forty to 50 minute cook time be a bit too long too — again that s be directly extract from the rye recipe but these be mere trifle the dish taste great it s a lovely blend of flavor and texture thank to the blood sausage and the potato the butter work brilliantly and the tomato on top be a nice touch and it prove watson s functionality you can suggest one ingredient that you find in the fridge use your initiative a bit and you ll be leave with something lovely and buttery lovely and buttery well do watson wednesday diner cod pizza when I read this recipe I wonder whether this be go to be it for I and watson diner cod and pizza be three word that don t really belong together and the ingredient list seem more like a supermarket sweep than a recipe now that I ve actually make the meal I don t know what to think about anything you might remember a classic 1978 george a romero direct horror film call dawn of the dead its 2004 remake follow the paradigm shift to run zombie in 28 day later suffer critically my impression of this remake be always that if it d just be call something different — zombie go shop for instance — every single person who see it would have love it as it be viewer think it seem unauthentic and it gather what be essentially some unfair criticism see also the recent robocop remake or as I call it cyberswede vs detroit this meal be my culinary dawn of the dead if only watson have call it something other than pizza it would have be utterly perfect it emphatically isn t a pizza it have as much in common with pizza as cake do but there s something about radish cod ginger olive tomato and green onion on a pizza crust that just work remarkably well to be clear I fully expect to throw this meal away I have the website for curry delivery already open on my phone that s all before I eat two of the pizza they taste like nothing on earth the addition of comté cheese and chive be the sort of genius absurdity that make people into millionaire I be however nervous to give one to my pregnant fiancée ; the ingredient be so weird that I be just sure she d suffer some really strange psychic reaction or that the baby would grow up to be extremely contrary be careful with this recipe preparation as I ve find with watson it doesn t tell you how to assure that your fish be cook ; nor do it tell you how long to pre bake the crust base these kind of thing be really important you need to make sure this dish be cook properly it take long than you might expect I m write this from sweden the home of the ridiculous pizza and yet I have a feeling that if I be to show this recipe to a chef who ordinarily think nothing of pile a kilo of kebab meat and béarnaise sauce on bread and serve it in a cardboard box with a side salad of ferment cabbage he or she would balk and tell I that I ve go too far which would be his or her loss I think I m go to have to take this to dragon s den instead watson I don t know how I m go to cope with normal recipe after our little holiday together you re change the way I think about food thursday fall celery sour cream parsley lemon taco follow yesterday s culinary epiphany I be keen to keep a cool head and a critical eye on chef watson so I decide to road test one theory from an article I find on the internet it mention that some of the most frequently discard item in american fridge be celery sour cream fresh herb and lemon let s not dwell too much on the luxury problem aspect of this I can t imagine that people everywhere in the world be lament the amount of sour cream and flat leaf parsley they toss and focus instead on what watson can do with this admittedly tricky sound shopping list what it do be this immediately add shrimp tortilla and salsa verde the salsa verde it recommend from an un watsone recipe courtesy of bon appetit be fantastic it s nothing like the salsa verde I know and love with its caper and dill pickle and anchovy this iteration require a bit of a simmer be super spicy and delicious I have to cheat and use normal tomato instead of tomatillos but I don t think it make a huge difference the marinade for the shrimp be unusual in that like a lot of what watson recommend it use a ton of butter a hefty wallop of our old friend kosher salt too now I ve work as a chef on and off for several year so be unfaze by the appearance of salt and butter in recipe they re how you make thing taste nice however there s no get away from the fact that I buy a stick of butter at the start of the week and it s already go the assemble taco be good — they be uncontroversial my dining companion deem the salsa a bit too spicy but I like the kick it give the dish and the sour cream calm it down a bit it strike I as a bit of a shame to fire up the barbecue for only about two minute worth of cook time but it s may and the sun be shine so what the heck be this recipe as absurd as yesterday s absolutely not be it as memorable sadly I don t think so would I make it again I m sorry watson but probably not these taco be good but ultimately not worth the prep hassle friday mexican mushroom lasagna before I start I don t want you to get the impression that my love affair which reach the height of its passion on wednesday with watson be over it absolutely isn t I have be consistently impressed with the software s intelligence its ease of use and the audacity of some of its suggestion for flavor match it s incredible it really work it probably win t save you any money ; it win t make you thin ; and it win t teach you how to actually cook — all of that stuff you have to work out for yourself but at this stage it s a distinctly impressive and worthwhile project do give it a go but be prepare to have to coax something workable out of it every once in a while today it take I a long time to find a meat free recipe which didn t when it come down to it contain some sort of meat I select meat as an option for what I didn t want to include and it take I to a recipe for sausage lasagne with one and a half pound of sausage in it I remove the sausage and it replace it with turkey mince maybe someone just need to tell watson that neither sausage nor turkey grow on tree after much tinker and submit and resubmitte the recipe I end up with be for lasagne top with a sort of creamy mash potato sauce it s very easy and it s a profoundly smart use of ingredient the lasagne be not the world s most aesthetically appeal dish and it s not as astonishingly flavor as some of this week s other revelation but I don t think I ll be make my cheese sauce in any other way from this point onwards top mark and in essence this kind of sum up watson for I you need to tinker with it a bit before you can find something usable you may need to make a do I want to put mashed potato on this lasagne leap of faith and you re go to have to actually go with it if you want the app s full benefit you ll consume a lot of dairy product and you might find yourself daydream about nice simple unadorned salad if you decide to go all in with its suggestion but an a i that can tell we how to make a pizza out of cod ginger and radish that you know be go to taste amazing one that will gladly suggest a workable recipe for blood sausage porridge and walk you through it without too much hassle that give you a how crazy option for each ingredient that be only design to make the life of food enthusiast more interesting why on earth not watson and I be go to be good friend from this point forward even if we don t speak every day and I can t wait to introduce it to other now though I m go to only consume smoothie for a week seriously if I even look at butter in the next few day I m probably go to puke this fall medium and how we get to next be explore the future of food and what it mean for we all to get the late and join the conversation you can follow future of food from a quick cheer to a stand ovation clap to show how much you enjoy this story inspire story about the people and place build our future create by steven johnson edit by ian steadman duncan geere anjali ramachandran and elizabeth minkel support by the gates foundation
Tim O'Reilly,1.3K,6,https://wtfeconomy.com/the-wtf-economy-a3bd5f52ef00?source=tag_archive---------1----------------,the wtf economy from the wtf economy to the next economy,wtf in san francisco uber have 3x the revenue of the entire prior taxi and limousine industry wtf without own a single room airbnb have more room on offer than some of the large hotel group in the world airbnb have 800 employee while hilton have 152 000 wtf top kickstarter raise ten of million of dollar from ten of thousand of individual backer amount of capital that once require top tier investment firm wtf what happen to all those uber driver when the car start drive themselves ais be fly plane drive car advise doctor on the good treatment write sport and financial news and tell we all in real time the fast way to get to work they be also tell human worker when to show up and when to go home base on real time measurement of demand the algorithm be the new shift boss wtf a fabled union organizer give up on collective bargaining and instead team up with a successful high tech entrepreneur and investor to go straight to the people with a local $ 15 minimum wage initiative that be soon copy around the country outflank a gridlocke political establishment in washington what do on demand service ai and the $ 15 minimum wage movement have in common they be tell we loud and clear that we re in for massive change in work business and the economy what be the future when more and more work can be do by intelligent machine instead of people or only do by people in partnership with those machine what happen to worker and what happen to the company that depend on their purchasing power what s the future of business when technology enable network and marketplace be well at deploy talent than traditional company what s the future of education when on demand learn outperform traditional university in keep skill up to date over the past few decade the digital revolution have transform the world of medium upend century old company and business model now it be restructure every business every job and every sector of society no company no job be immune to disruption I believe that the big change be still ahead and that every industry and every organization will have to transform itself in the next few year in multiple way or fade away we need to ask ourselves whether the fundamental social safety net of the develop world will survive the transition and more importantly what we will replace they with we need a focused high level conversation about the deep way in which computer and their ilk be transform how we do business how we work and how we live just about everyone s ask wtf what the f * * * but also more charitably what s the future that s why I m launch a new event call next economy what s the future of work to be hold at the palace hotel in san francisco nov 12 and 13 2015 my goal be to shed light on the transformation in the nature of work now be drive by algorithms big data robotic and the on demand economy we put on a lot of event at o reilly many of they have a singular focus and be aim at practitioner of a specific discipline strata and hadoop world be an event about data science velocity about web performance and operation solid about the new hardware movement and oscon about open source software development but this one be more exploratory aim at a business audience try to come to grip with trend that be already feel but not well understand put together an event like this be a great way to discover how a lot of disparate people idea and trend fit together I ve be engage some of the smart people I know in field as diverse as robotic ai the on demand economy and the economic of labor I m think hard about the key driver of some of today s most successful startup like uber and airbnb and about what technology like driverless car siri google now microsoft cortana and ibm watson teach we about the future and I m start to see the connection over the next week and month I ll be post follow up piece explain in more detail my thinking on key issue we ll be explore at the event I will be lead a robust discussion here on medium with some of the good thinker and mover on these issue — a conversation that welcome all voice we ll be discuss both here and at the event how augmented worker form a common thread between the strategy of company as diverse as uber ge and microsoft how company in every business sector can harness the power and scalability of networked platform and marketplace why the divisive debate about the labor practice of on demand company might provide a path to a well future for all worker why the on demand service of the future require a new infrastructure of on demand education and why building service that uncover true unmet demand and solve hard problem be ultimately the good way to create job in the meantime head on over to the conference site to see some of the amazing speaker we ve already sign on many more to come and a taste of what they ll be cover in many way an event like this be the product of the people who be there — speaker and attendee alike — so I ve try to tell the story of the theme we be explore through the people who will be there each speaker page provide not just a biography of the speaker but a selection of provocative quote from what they ve write in the near future we ll be provide additional opportunity for discussion and exploration my hope for this event be that it become more than a conference for it to be measure as a success it must catalyze action I want work that come out of this collision of idea to inspire entrepreneur to tackle miss piece of the next economy puzzle to help frame the right government policy so that innovation in the nature of work be encourage rather than repress and to focus every industry on rebuild the economy by solve hard problem and create what steve job might have call insanely great new service tim o reilly be the founder and ceo of o reilly medium and a partner at o reilly alphatech ventures oatv tim have a history of convene conversation that reshape the industry in 1998 he organize the meeting where the term open source software be agree on and help the business world understand its importance in 2004 with the web 2 0 summit he define how web 2 0 represent not only the resurgence of the web after the dot com bust but a new model for the computer industry base on big datum collective intelligence and the internet as a platform in 2009 with his gov 2 0 summit he frame a conversation about the modernization of government technology that have shape policy and spawn initiative at the federal state and local level and around the world he have now turn his attention to implication of the on demand economy ai and other technology that be transform the nature of work and the future shape of the business world from a quick cheer to a stand ovation clap to show how much you enjoy this story founder and ceo o reilly medium watch the alpha geek share their story help the future unfold how work business and society face massive technology drive change a conversation grow out of tim o reilly s book wtf what s the future and why it s up to we and the next economy summit
James Cooper,57,3,https://render.betaworks.com/announcing-poncho-the-weatherbot-bd14255e1b25?source=tag_archive---------2----------------,announce poncho the weatherbot render from betawork,you can now get personal weather forecast in slack update since publish this piece in november 2015 the poncho weather messenger bot launch on stage at the facebook conference and be now the most popular bot on facebook if you be new to bot this be a great place to start try it out here you ll like it poncho be a personalized weather service from the coolest of cat who need boring and meaningless datum when you can get personalize forecast with gifs and text that will make you smile whatever the weather vanity fair say it s like be pal with the weatherman which be true if your weatherman be super cool up until now we have be a text and email service you get text or email in the morning and evening you can sign up for that right here but we know that people want more poncho you guy want poncho on call with new slack integration we ve get you cover if you be use slack for your messaging need and if not why not we have some uh maze ing news for you that s right — you can summon up your very own forecast from poncho in slack we be join other like lyft and foursquare as slack officially launch slash command today ok first up let I tell you how it work you simply type in poncho and your zipcode into slack and then boom the next thing you ll see be your very own forecast for that zipcode resplendent with text and gifs and everything so for example in the video I type in poncho 11217 and I get a forecast for my zipcode in brooklyn it be halloween so the theme be the shine which be why the forecast be weather spelt backwards and the gif be the scary kid from the film if you be new to poncho you ll soon figure out that half the fun be decipher the message our wonderful editorial team put together set up poncho in slack be super simple just click the add to slack button yes that one up there make sure to add it to all the channel so that poncho will be available wherever you want you wouldn t want your friend to miss out would you unless of course you re keep all the good joke for yourself I ve see that happen all righty see you on slack err slacker and if you be not on slack you can still use the text and email version or wait for our super cute app which will be come out soon from a quick cheer to a stand ovation clap to show how much you enjoy this story head of creative at betaworks new york idea and observation from betawork
Joel Leeman,69,5,https://becominghuman.ai/i-think-i-m-slowly-turning-into-a-cyborg-cbecfa8462df?source=tag_archive---------3----------------,I think I m slowly turn into a cyborg become human artificial intelligence magazine,it s only a matter of time as much of life move online atomize into bit on app social network and a variety of other web product I m begin to notice more and more that I rely on these tool to supplement my brainpower it sound melodramatic I realize but go with I for a second here take my schedule at work I be glue to outlook in an unhealthy way like if I don t have that little ding go off 15 minute before a meeting start there s no way I m go to make it meeting come and go and change and happen all the time but I don t really pay attention to memorize any of the detail because I know I can always glance at my phone to know what I m suppose to be do I hold a similar unhealthy relationship with facebook too back in the early day of facebook I actually really enjoy log in every day see whose birthday it be and write a little note of well wish fast forward to present day and I m terrible at wish people happy birthday mostly because the 4 7 of my friend who have a birthday every day overwhelm I I m so scared of miss one or two that I neglect all of they have the ability to know when anyone s special day be have put a damper on actually remember a few of they without the aid of facebook do you know anyone s birthday by memory any more or have you like I lose that part of your memory in fact if I don t write something down with pen and paper a practice vastly underappreciated imho it feel like it might be lose forever even if it s just a click a way and I ve actually catch myself use twitter as a partial brain aid what be I up to last week oh I ll just scroll back and see what I be tweet about or maybe instagram to my little online scrapbook of what I ve be up to or what I ve show the world I m up to I m also quite directionally challenge and rely on my iphone way too much to get around though maybe I m just truly terrible at direction who know but why would I take the time to study street and landmark when I ve get a world s worth of map sit in my pocket side note be we lose the art of get lose and there s nothing wrong with all that I suppose it s more that I have a weird feeling maybe I m rely on technology a little much what prompt my ruminating on all this be a video I watch ask random couple if they know each other s phone number by heart spoiler none of they do I actually make an effort several year ago to learn my partner s number but if I have never consciously make that decision I certainly wouldn t know it now lose these tiny archaic practice by themselves individually doesn t mean much but when you add they up it start to feel like a bit overwhelming doesn t it this cyborg vs luddite thing have especially jump into the spotlight with wearable finally come to market google glass have largely be see as a flop but it shouldn t be take lightly that people be literally choose to wear a computer on their face all day or of course take the apple watch and other smartwatche like it yet another device create to fill a need that no one have but will inevitably become an indispensable piece of hardware that we all must have until smart chip can just be implant in our brain one of my favorite writer john herrman describe it quite brilliantly though I m sure I will have one within two year okay so I m not just a grumpy old technophobe either I see value in technology heck I work and therefore pretty much live online I like gadget as much as the next guy in fact I rather enjoy a recent episode of invisibilia an incredibly interesting new podcast from npr detail the story of the original cyborg a guy at mit in the 90 s who build a very early version of what be essentially google glass and wear it for year he use his face computer to recall bit of information at a moment s notice about prior interaction he have with people like a digital file folder on each relationship there be of course plenty of example of how technology augment the human experience how it build relationship and give a voice to the voiceless and have open new world of possibility I could and often do spend day talk about all the amazing thing we can do today that we couldn t 20 year ago but as I ve argue before there come an inflection point where we all should think a bit more critically about the tool and toy we use and rely on and for I that day be here can you imagine a day where we re connect to all the information in the world through smart glass a smartwatch and our smartphone start to sound a bit cyborg ish to I do you enjoy this subscribe to my newsletter net irl a weekly roundup of some of the good story about the impact technology and the internet have on our everyday life I m on twitter @joelleeman from a quick cheer to a stand ovation clap to show how much you enjoy this story lifelong learner connector and musician first social now digital strategy @thomsonreuter into tech medium life 👨 🏻‍ 💻 🤷 🏻‍ ♂ ️ late news info and tutorial on artificial intelligence machine learn deep learn big datum and what it mean for humanity
Scott Smith,83,8,https://medium.com/phase-change/your-temporary-instant-disposable-dreamhouse-for-the-weekend-12eb419ded0?source=tag_archive---------4----------------,your temporary instant disposable dreamhouse for the weekend,close colleague of mine will tell you I have hone a particular obsession crackpot theory over the past few year that airbnb have be gently a b test I in real life let I explain I travel more than most human should as someone who run their own company and sometimes need to spend more time in a location than be affordable via traditional hotel lodging such as with a recent relocation over the summer I have make use of that darling of the sharing economy scourge of community depend on which lens you look at it through airbnb to stretch my budget spend time close to work friend client or just have company when travel I ve stay in over 30 property in something like eight country so I ve have a lot of time to contemplate the company s strategy from the inside the semi serious theory start during back to back stay in the uk several year ago my first three night stay be in a london borough in a fairly cozy house own by a couple with a toddler it be comfortable enough though a bit chilly in both bedroom and share bath the interior design wasn t mile off my taste but it didn t push any button of joy either mostly catalog standard late 20th century british home store I never even sit down on the ground floor the bit of medium I see around the house be mildly interesting if predictable but not must read or binge viewable I wasn t really allow in the kitchen which be reserve for use by the family only the wife of the couple have formerly work in medium on a cooking show the husband in finance I hardly see either of they as they make themselves scarce after the check in I didn t have much interaction with the host until leave and they weren t interested in any to be honest it be strictly a transactional stay their child be probably cute but fuss far too much to get a close look — it be mostly an unhappy sound come from the kitchen or bedroom fair enough I stay three day I pay I chat briefly and leave and leave a weakly positive review after I have no real complaint but probably wouldn t look for it again from london I move down to the south coast for work I m be vague to protect the host mention herein I find the place also an attached house in a row date probably from the edwardian period the host couple meet I in the front hall usher I in sit I down in the lounge to relax and I be immediately offer a warm fresh baked cupcake and a glass of wine as I slide back into a nice leather sofa as the husband who work in the trendy area of fintech ask I about my work — and seem to understand what I do — my eye scan the groan bookshelf across from I have that want to read that ohhh that s a good one must remember to look at that I recall think we have so much in common the wife just finish up a new round of bake for one of her side business shout a welcome and tell I to feel free to use the house as my own list the tasty good available for breakfast the next day as she join our conversation with the couple s very adorable son who poke at my shoe engagingly and seem to pay close attention to my voice what follow be an interesting chat about culture technology and cooking before I go up to my very warm comfortable private room past the amazing folk art highly listenable cd collection and private bath with want able scandinavian textile and then it hit I the principle actor and script of these two airbnb play be roughly the same same family configuration profession and age same general house same price per night within a few pound same availability except when contrast the two one be so comfortable personally interesting and engage I want to stay an extra week while the other almost hurry I on my way one I be happy to pay to stay in one I feel vaguely grudge about in retrospect one could have be my alternate media collection and wine store one miss the mark on general user experience for I I quietly lock the door to my room log onto the fast broadband quite slow and choppy at house # 1 and open my amazon profile just to see what I d be look at lately as I lay in bed the first night breathing in the rich cake scent still hang in the air I think about whether airbnb have somehow tap into my online search and purchase after all this be the age of convergent big datum and powerful retail analytic without have see really any of the home content at either place or anything useful about the host from the airbnb listing I d end up in two very similar yet weirdly different residence one where even the conversation with the host be familiar and relevant the other where it just didn t read back to back easy to compare be the child even real or just part of the test in a period when both home staging and immersive theatre be hot why couldn t it happen I think and with same day delivery service break out all over couldn t a set of highly personalize home content — choose to be both familiar and aspirational after all you want to leave space for potential purchase to help fund this business model — have be pluck from a regional depot pop onto shelf and in cabinet and organize for my arrival couldn t some actor in search of work in london have be brief up enough from open source material to interact with I for an hour or so couldn t they couldn t they I d be on the road for a while and fatigue be start to set in maybe it be affect my head that be two year ago it have be in the back of my mind since and then this past summer I have a similar experience only with my whole family while mid relocation to the netherlands again similar home same family demographic both away on holiday this time it s tough to get small child to follow a script right one house comfortable enough in a suburban town the other a charming place in a gentrify neighborhood worth squat in hope the owner didn t return jk airbnb jk be I optimize my own stay or be they feed I more appropriate property in hope of make this testing easy hotel have test such thing why not the hotel killer itself they even leave the same bread for we as a welcome basket one white one whole grain after all airbnb have deploy aerosolve its own machine learning platform to make sense of real time usage datum and help host get a well return tuning property for desirability be feasible — the company be already use automate scanning of house photo to optimize presentation of property as well with all of this technology aim at the property themselves why wouldn t airbnb also dig into the mind of guest find out how they respond to different house which convenience they re draw to etc nah that would take sensor inside a house on top of crack web and mobile analytic you d need to know what people do during their stay and as I m sit there think again about this crazy idea I see a tweet go by airbnb have purchase an obscure russian sensor company I slam the laptop and check the cabinet for tin foil a month or so go by I forget about it again then I open medium and see a story about how airbnb have mock up part of its own headquarters base on the apartment design a french couple who use the service to let their own flat the couple be now sue the company they be brand their company with our life owner benjamin dewé tell buzzfeed the company have apparently copy a range of style element from the french couple s home in its own san francisco office down to the doodle on the chalkboard the doodle as jamie lauren keile demonstrate in the medium piece above it s pretty easy to break those furnishing and accessory down to a shoppable list on with good obtain on amazon or elsewhere like those magazine feature that show how to buy knock off of celebrity fashion complete with price and shop a family s flat admittedly one they rent out via airbnb include to airbnb for a function have be commodifie into a shopping list buy that lifestyle right here well yet live in it for a few day only with the convergence of big datum analytic include visual analysis tool which can look for the presence of brand in social medium photo machine learning and accessible api of company like amazon and breakneck logistic uber style or even predictive shipping per the notorious amazon patent fabbe up a home interior to suit your taste or taste that be form but haven t fully emerge yet be within today s technology hell even that cute roomba you have to have may be quietly map the place you live this will be available in knock off home robot soon have you check the user agreement of your various home appliance and system to see if they can sell the datum probably not and why not tap that stock of underused home and underemployed people if there s one thing the sharing economy overlord have teach we it s that the world be just a collection of undermonetized asset wait to be redistribute right why not productize commodify and populate that second to last frontier our living space and stay in someone else s place with someone else s stuff you fancy from the picture be tired everything else be personalize financialize and productize why even own your own stuff when it could be ubere into position in a desirable location base on your most recent pinter save think about it with a bundled dreamhometm service you can perpetually test drive that new living room suite for long holiday weekend — I mean why wait until after purchase for buyer s remorse to set in you can get it out of the way without the financial commitment just your desire play forward all the time you can even test roommate or neighbor for the weekend why stop at furnishing and paint color slap those detailed sentiment analysis and personality analytic glean from your prospective co habitant s online activity eye track history tinder preference and 23andme profile onto a few improv actor and have some big datum cosplay in a pop up maisonette come monday morning you can just walk out the front door with nothing but a premium fee to pay a fee which may be itself be subsidize by various sponsor who want to test product on you don t worry it s cool duralux crate & barrel and linkedin pick up the tab for this getaway in the wood or beach with new friend sound good of course it do we know you would like it check your email your temporary instant disposable dreamhouse for the weekend may be wait from a quick cheer to a stand ovation clap to show how much you enjoy this story future post normal innovation strategic design http changeist com essay observation and speculation from the changeist lab
iDanScott,3,4,https://medium.com/@iDanScott/the-bejeweled-solver-3cd07c69dfc4?source=tag_archive---------5----------------,c # play bejewel blitz idanscott medium,as some of you read this may or may not already know ; over the past day or so I go from have the idea of create a computer program that would essentially be able to play the popular arcade game bejewel blitz on facebook to actually develop it now as hard as this problem sound it be surprisingly easy and fairly swift to solve I break it down in to 3 main step the first step be probably the most time consume of they all as everything from there be just colour management the solution I come up with in the end for that be to take a screenshot of the entire screen and then scan the image from top to bottom use a nested for loop until I find a funny shade of brown that only appear along the top edge of the bejewel grid for anyone wonder that colour be color fromargb 255 39 19 5 once this colour have be find use the bitmap getpixel x y function I break out of both for loop and know that be the point where the top leave corner of the grid be I could then use this to construct a rectangle which would extract the bejewel grid from the full screenshot the size of the rectangle be calculate use the size of the grid cell 40px2 find that out use trusty old paint multiply by the amount of row column there be 8 find that out use my eye ball this result in the rectangle size come out at 320px2 so the next step from here be to identify what colour reside in what square to do that I start off by create a 2 dimensional array of colour or color s to be politically correct that be 8 row and 8 column to match that of the playable grid I then systematically loop through the 2 dimensional array of colour in a nested for of x and y value assign the array the colour of the pixel at the location x * 40 + 20 y * 40 + 22 the x value be decide as it be half way through the gem and 22 be choose for the y value as certain gem have a white center green and yellow so 22 provide a more accurate reading with this 2 dimensional array I be then able to generate a visual representation of what the computer be see when it be try to figure out what colour be where as you can see from the above screenshot it s able to identify what gem be what colour depend on what pixel be at that magic 20 22 of the cell another thing I think about before I finish this project to the state it s in now be to prevent the application from try to switch 2 empty cell because one gem have just be blow up or something I add all the know color code to their own array and ask if the colour that s in the 2d array also reside within the know colours list if it do it will then evaluate whether it can be move to a win square if not it s ignore entirely I win t bore you with the gory detail of how I check if a gem can be move as instead this be a link to the beginning of the if statement in my open source github project from here the full source code can be view comment on and even improve upon if you guy feel like I could do something obviously well finally all that s leave to do by definition of this application be to actually move the gem this be do by make some window api call to set the mouse location and simulate mouse click again the detail of how to exactly do that be within the github project but if I ve keep your attention for this long all that s leave to say be thank you and if you have any further question don t hesitate to hit I up on here or twitter @idanscott thank for read from a quick cheer to a stand ovation clap to show how much you enjoy this story dan scott 23 computer science student of plymouth university www idanscott co uk
Josh,18,6,https://medium.com/@joshdotai/9-reasons-why-now-is-the-time-for-artificial-intelligence-876b3def0fee?source=tag_archive---------6----------------,9 reason why now be the time for artificial intelligence,there s no deny it — artificial intelligence be happen and it s happen big company from facebook to google to amazon be hard at work build world class ai team that infiltrate every facet of their product siri be one of the large team at apple and microsoft have a grow research effort on this front but why be now the time for ai 1 artificial neural network traditional programming be deterministic sequential and logical for example computer take input apply instruction and generate output this be great for task like calculation and conversion but ill suited if the application isn t explicitly define the human brain on the other hand doesn t behave this way we learn and grow through repetition and education recent progress in artificial neural network anns be key to build computer that can think these breakthrough be enable tremendous stride in ai work at google and apple 2 knowledge graph company like yelp foursquare and wolfram alpha have enable access to their datum through apis as a result platform like siri and google now be able to answer question such as what s the close coffeeshop or what s the population of india if a new service have to handle the natural language processing nlp audio processing datum and more it would be nearly impossible fortunately the knowledge graph have evolve over the last 20 year to a point where new ai platform can immediately have access to ton of datum 3 natural language processing nlp be a field of computer science and linguistic where computer attempt to derive meaning from human or natural input while the field have be around since the 1950 we ve see huge stride in the last few year thank to markov model and n gram model as well as project like calo and wordnet stanford s corenlp demo here be one of the many strong nlp solution available today 4 speech processing in order to speak to a computer and have it understand our intent we first need to handle the audio processing and convert sound wave to text know as speech process this field have see major advancement in the last few year beyond the advancement in technology we ve see company like nuance emerge with powerful api that power service like gps dictation and more today it be almost effortless for a new ai company to translate voice to text with a high degree of confidence 5 computational power the increase in computational efficiency over the last 17 year have be remarkable in 2014 people could buy a video card that be 84 3 time the performance of one from 2004 for the same price this increase in computational power be necessary if we want to emulate the brain for example research attempt to simulate 1 second of human brain activity require 82 944 processor support 1 73 billion artificial neuron connect by 10 4 trillion synapsis the decrease in cost and increase in computational power be enable tremendous breakthrough in ai today 6 consumer acceptance a big aspect of see mass adoption around artificial intelligence be consumer approval with an initial push from apple to highlight siri and now microsoft s cortana and google now do the same smart phone owner have access to an ai whether they like it or not as a result consumer be come around to the idea and even start to embrace it funny video like this one be help the masse to accept this new human computer interaction 7 ubiquity of personal computing converse with an ai be a very personal experience the emergence of small always on device make this possible the iphone be first introduce in 2007 only 8 year ago now more than 64 % of americans own a smartphone wearable such as the apple watch or jawbone open the possibility of even more intimate personal computing these device that we carry or wear serve as excellent host for this technology make it possible for ai to truly enter the mainstream for the first time 8 funding ai funding seem to go through wave and in the last few year it s definitely back up scale inference a predictive ai company recently raise $ 13 6 m amazon just announce a $ 100 m fund for voice control technology and ibm do the same for the watson venture fund the total invest in ai company in 2014 grow past $ 300 m from a mere $ 14 9 m in 2010 accord to bloomberg with firm like khosla venture and andreesen horowitz lead deal in ai company fund be fuel innovation in ai 9 research effort another reason for the apparent surge in ai be the collective research effort take place accord to a 2014 report by miri machine intelligence research institute 41 of the top 275 cs conference be ai relate ai account for about 10 % of all cs research today the ieee computational intelligence society have more than 7 000 member and there be more 106 ai journal base on miri estimate more than $ 50 m go into funding ai research by the national science foundation nsf in 2011 with this much research and effort go into ai innovation it s no wonder we re see this technology start to reach the masse if history be an indicator we may see interest in ai spike and go back down with momentum across these various different sector though ai interest seem likely to keep grow if you re interested in keep up with our effort and stay in touch check out http josh ai and reach out this post be write by alex at josh ai previously alex be a research scientist for nasa sandia national lab and the naval resarch lab before that alex work at fisker automotive and found at the pool and yeti alex have an engineering degree from ucla live in los angeles and like to tweet about artificial intelligence and design josh be an ai agent for your home if you re interested in follow josh and get early access to the beta enter your email at https josh ai like josh on facebook — http facebook com joshdotai follow josh on twitter — http twitter com joshdotai from a quick cheer to a stand ovation clap to show how much you enjoy this story
paulson,1,17,https://electricliterature.com/what-could-happen-if-we-did-things-right-an-interview-with-kim-stanley-robinson-author-of-aurora-d88a0f8f72e7?source=tag_archive---------7----------------,what could happen if we do thing right an interview with kim stanley robinson author of aurora,be kim stanley robinson our great political writer that be the provocative question pose recently by a critic in the new yorker science fiction writer rarely get that kind of serious attention but robinson s visionary experiment in imagine a more just society have always be part of his fictional universe in fact he get his ph d in english study under the renowned marxist theorist fredric jameson the idea of utopia may seem discredit in today s world but not to robinson he believe we need more utopian thinking to create a well future and the future be where he take we in his new novel aurora set in the 26th century it s the story of a space voyage to colonize planet outside our solar system robinson write in the tradition of hard science fiction use only exist or plausible technology for his interstellar journey as much as he geek out on the mechanic of space travel his real interest be how people would handle a very long voyage trap inside a starship his futuristic theme win t surprise longtime fan of robinson who s well know for his mar trilogy publish in the 1990s to read ksr be to wonder how our specie might survive and even thrive in the century ahead the author stop by my radio studio before give the keynote speech at a local science fiction conference we talk about the existential angst of life on a starship the future of artificial intelligence and the aesthetic of space travel our conversation will air on public radio international s to the good of our knowledge you can subscribe to the ttbook podcast here steve paulson how would you describe the story in aurora kim stanley robinson it s the story of humanity try to go to other star system this may be an ancient idea but for sure it s a 19th century idea the russian space scientist tsiolkovsky say earth be humanity s cradle but you re not mean to stay in your cradle forever this idea have be part of science fiction ever since — that humanity will spread through the star or at least through this galaxy sp it s a long way to travel to another star ksr it be a long way and the idea of go to the star be get not easy but more difficult so I decide to explore the difficulty I try to think about whether it s really possible at all or if we re condemn — if you want to put it that way — to stay in this solar system sp what star be your space voyager try to get to ksr tau ceti which have often be the destination for science fiction voyager ursula le guin s dispossess take place around tau ceti and so do isaac asimov s the naked sun it s about 12 light year away we now know it have three or four big planet the size of a small neptune or a large earth they ve get the mass of about five earth that s too heavy for human to be on but those planet could have moon about the size of earth so it become the near viable target alpha centauri which be just four light year away only have tiny planet that be close than mercury be to our sun so they win t be habitable sp your story be set 500 year into the future it take a long time to get to this star ksr yes my work principle be what would it really be like so no hyperspace no warp drive no magical thing about what isn t really go to happen to get we there that mean sub lightyear speed so I postulate that we could get spaceship go to about one tenth the speed of light which be extraordinarily fast then the problem become slow down you have to carry enough fuel to slow yourself down if you ve accelerate to that kind of speed the mass of the decelerant fuel will be about 90 % of the weight of your ship as you re approach your target you have to get back down to the speed at which you can orbit your destination the physics of this be a huge problem sp you re talk about a multi generational voyage that will take a couple hundred year that s a fascinating idea the people who start out will be dead by the time the starship get there ksr I guess it would take four or five generation — say 200 year this be not my original idea the multi generational starship be an old science fiction idea start by robert heinlein and there may even be early precursor one always find forgotten precursor for every science fiction idea heinlein write universe around 1940 brian aldiss write a book call starship in 1958 and gene wolfe write a very great starship narrative in the 1990s the book of the long sun so it s not an original idea to I ; it s sort of a sub genre within science fiction sp but the whole idea of a project that take generation be something we don t do anymore people do that when they build the pyramid in egypt or the great cathedral in europe I can t think of a current project that will take generation to complete ksr you really have to think of it as a mobile island or a vast zoo it isn t even a project so much as a city that you ve shoot off into space and when the city get to its destination the people unpack themselves into the new place you re right it could be compare to build the cathedral and it s interesting to think about the people bear on the starship who didn t make the choice to be there so it turn into a bit of a prison novel sp because you re trap there you re in this confine space for your whole life ksr and for two or three generation you re bear on the ship and you die on the ship you re just in between the star so it s very existential there be some wonderful thought stimulant to think about a starship as a closed ecology sp how big be the starship in your story ksr there s something like a hundred kilometer of interior space sp so this be big ksr yeah two ring you could imagine they as cylinder that have be link until they make a circle so twelve cylinder per circle you ve get 24 cylinder and each have a different earth ecology in it and each one of they be about five kilometer long it s pretty big but you need that much space to be viable at all because you have to take along a noah s ark worth of genetic material or else it isn t go to work sp what do you have to bring along ksr you would want as much of everything as you can bring but you certainly need a big bacterial load you need to bring along a lot of soil you need a lot of what would be effectively unidentified bacteria ; you just need a big hunk of earth and then all the animal that you can fit that would survive each one of these cylinder would be like a little zoo or aviary sp as you be imagine this voyage which part be most interesting to you be it the science — try to figure out technically how we could get there or be it the personal dynamic of how people would get along when they re trap in space for so long ksr I think it would be the latter I m an english major the wing of science fiction that s discuss this idea have be the physics guy the hard sf guy they ve be concern with propulsion navigation with slow down with all the thing you would use physics to comprehend but I ve be think about the problem ecologically sociologically psychologically these element haven t be fully explore and you get a new story when you explore they it s a rather awful story which lead to some peculiar narrative choice sp why be it awful ksr because they re trap and the spaceship be a trillion time small than earth s surface even though it s big it s small and we didn t evolve to live in one of these thing it s like you spend your whole life in a motel six sp put that way it do sound pretty awful ksr well than a prison but you can t get out you can t choose to do something else I don t think we re mean for that even though we live in room all the time in modern society I think the reason people volunteer for thing like mar one be they re think how be that different from my ordinary life I sit in a room in front of my laptop all day long if I m go to mars it s more interesting sp mar one be the project that s try to engineer one way trip to mar you know you re not go to come back frankly it sound like a suicide mission and yet ten of thousand of people have sign up for this mission ksr yes but they ve make a category error their imagination have not manage to catch up to the situation they be in some kind of boring life and they want excitement maybe they re young maybe they re worried about their economic prospect maybe they want something different they imagine it would be exciting if they get to mar but it be ralph waldo emerson who say travel be stupid ; wherever you go you re still stick with yourself I go to the south pole once I be only there for a week and it be the most boring place in antarctica because we couldn t really leave the room without get into space suit sp be extend space travel like go to antarctica ksr it s the good analogy you can get especially for mar you would get to a landscape that s beautiful and sublime and scientifically interesting and mind boggle antarctica be all those thing and so would mar be but I notice that nobody in the united states care about what the antarctican be do every november and december there be a couple thousand people down there have a blast if the same thing happen on mar it would be like oh cool some scientist be do cool thing but then you go back to your real life and you don t care sp so even though you write about these long space voyage you wouldn t want to be part of one ksr not at all but I ve only write about long space voyage once — in this book aurora sp you also write a whole series of book about mar you still have to get there ksr but there s an important distinction you can get to mar in a year s travel and then live there your whole life and you re on a planet which have gravity and landscape you can terraform it it s like a gardening project or build a cathedral I think terraforme mar be viable go to the star however be completely different because you would be travel in a spaceship for several generation where you re in a room not on a planet it s be such a techie thing in science fiction but people haven t de strand those two idea they say well if we can go to mar we can go to tau ceti it doesn t follow it s not the same kind of effort sp would it be interesting to travel just through our own solar system ksr yes this solar system be our neighborhood we can get around it in human time scale we can visit the moon of saturn we can visit triton the moon of neptune there be hundred of thousand of asteroid on which we could set up basis the moon of all the big planet be great the four big moon of jupiter — we couldn t be on io because it s too radioactive or too impact by the radio wave of jupiter itself — but by and large the solar system be fascinating sp yet I imagine a lot of people would say yeah there s a lot of cool stuff out there but it s all dead ksr well we have question about mars europa ganymede and enceladus a moon of saturn wherever there s liquid water in the solar system it might be dead or alive it might be bacterially alive it might have life that start independently it might be cousin life that be blast off of mar on meteorite and land on earth and other place we don t know yet and if it be dead it s still beautiful and interesting so these would be site of scientific interest antarctica be pretty dead but we still go there sp I ve hear it s incredibly beautiful ksr it s very beautiful I think if you re stand on the surface of europa look around the ice scape and look up at saturn in the sky overhead it s also go to be beautiful I m not sure if it s beautiful enough to drive a gigantic effort to get there the robot go there now be already a tremendous exploration for humanity the photo send back to we be a gigantic gift and a beautiful thing to look at so human go there will always be a kind of research project that a few scientist do I m not say that the rest of the solar system be crucial to we I think earth be the one and only crucial place for humanity it will always be our only home sp I wonder if we would develop a different sense of beauty if we go out into the solar system when we think of natural beauty we tend to think of gorgeous landscape like mountain or desert but out in the solar system on another planet or a moon would our experience of awe and wonder be different ksr you can go back to the 18th century when mountain be not regard as beautiful edmund burke and the other philosopher talk about the sublime so the beautiful have to do with shapeliness and symmetry and with the human face and figure through the middle age mountain be see as horrible wasteland where god have forget what to do then in the romantic period they become sublime where you have not quite beauty but a combination of beauty and terror your sense be tell you this be dangerous and your rational mind be say no I m on a ledge but I ve get a railing it look dangerous but it s not you get this thrill sensation that be not beauty but be the sublime the solar system be a very sublime place sp because you could die at any moment if your oxygen support system go out ksr exactly it s like be in a submarine or even in scuba gear — the feeling of be meter under the surface with a machine keep you alive and bubble go up as you re look at a coral reef that s sublimity there s an element of terror that s suppress because your rational mind be say it s okay when you fly in an airplane and look down 30 000 foot to the surface of the earth that s the feeling of the sublime even if you re look down at a beautiful landscape but people can t bear to look because after a while you re think boy this machine sure have to work sp if you think long and hard about this ksr you might never fly again sp one thing that s so interesting about your novel aurora be that most of it be narrate by the ship itself what be the idea here ksr I do like the idea that my narrator be also character that they re not I I m not interested in myself I like to tell other people s story so I don t do memoir I do novel and for three or four novel now it s be an important game to I to imagine the narrator voice be different from mine so shaman s be the third wind this mystical spirit that know the paleolithic inside and out that wasn t I and cartophilus the time traveler tell galileo s story in aurora it make sense for the ship to need really powerful artificial intelligence like a quantum computer and once you get to quantum computer you ve get processing speed that be equal to the processing speed of human brain but the methodology would be completely different they d be algorithms that we program maybe it wouldn t have consciousness but when you get that much processing speed who s to say what consciousness really be so I make the narrator out of this starship s ai system and he — she it — have be instruct by the chief engineer to keep a narrative account of the voyage when you think about it write novel be strange we can tell most story to each other in about 500 word so a novel be not a natural act it s an art form that s be build up over century and doesn t have a good algorithm sp I recently interview stephen wolfram the computer theorist and software developer and ask if he think some future computer could write a great novel he say yes ksr wolfram s very important in theorize what computer can do because he s make a breakdown of activity from the simple to the complex and at full complexity the human brain or any other thinking machine that can get to that fourth level of complexity should be able to do it sp so in the future you think a computer or artificial intelligence system could write a modern ulysse ksr well this be an interesting question at that point you would need a quantum computer it would need to read a whole bunch of novel and try to abstract the rule of storytelle and then give it a shot in my novel the first chapter the computer write be 18th century literature it s what we would call camera eye point of view it doesn t guess what people be think ; how can it it just report what it see like a hemingway short story as the novel go on chapter by chapter the computer be recapitulate the history of the novel and by the end of the last chapter narrate by the computer you re get full on stream of consciousness it s kind of like ulysse or virginia woolf where you re inside the mind although it s the mind of the computer itself the last chapter be in a kind of flow state of the computer s thinking sp at that point do the computer have emotion ksr it wonder about that the computer can t be sure actually we re all trap in our own consciousness what be other people think what be other people feel you have to work by analogy to your own internal state the computer only have access to its own internal state sp do the future of ai and technology more generally excite you ksr yes ai in particular I use to scoff at it I m a recent convert to the idea that ai computing be interesting mainly it s just an add machine that can go really really fast there be no internal state they re not think however quantum computer push it to a new level it isn t clear yet that we can actually make quantum computer so this be the speculative part it might be science fiction that completely fall apart there be science fiction about easy space travel but that s not go to work there be science fiction about all of we live 10 000 year that might or might not work but it s way speculative quantum computing be still in that category because you get all the weirdness of quantum mechanic there be certain algorithm that might take a classical computer 20 billion year while a quantum computer would take 20 minute but those be for very particular task like factor a thousand digit number we don t know yet whether more complex task will be something that a quantum computer can handle well than a regular computer but the potential for stupendous processing power like a human brain s processing power seem to be there sp as a science fiction writer do you have a particular mission to imagine what our future might be like be that part of your job ksr yes I think that s central to the job what science fiction be good at be do scenario science fiction may never predict what be really go to happen in the future because that s too hard strange thing contingent thing happen that can t be predict but we can see trajectory and at this moment we can see future that be complete catastrophe where we cause a mass extinction event we cook the planet 90 % of humanity die because we run out of food or we think we re go to run out of food and then we fight over it in other word complete catastrophe on the other hand there s another scenario where we get hold of our technology our social system and our sense of law and justice and we make a kind of utopia — a positive future where we re sustainable over the long haul we could live on earth in a permaculture that s beautiful from this moment in history both scenario be completely conceivable sp yet if we look at popular culture dystopian and apocalyptic story be everywhere we don t see many positive vision of the future ksr I ve always be involve with the positive vision of the future so I would stubbornly insist that science fiction in general and my work in particular be about what could happen if we do thing right but right now dystopia be big it s good for movie because there be a lot of car crash and thing blow up sp be it a problem that we have so many negative vision of the future ksr dystopias express our fear and utopias express our hope fear be a very intense and dramatic emotion hope be more fragile but it s very stubborn and persistent hope be inherent in we get up and eat breakfast every day in the 1950 young people be think I m go to live on the moon I will go to neptune today it s the hunger game which be a very important science fiction story I like that it s science fiction not fantasy it s not lord of the ring or harry potter it s a very surrealistic and unsustainable future but it s a vision of the fear of young people they re pit we against each other and we have to hang together because there s a rich elite an oligarchy that s simply eat our life for their own entertainment so there s a profound psychological and emotional truth in the hunger game there s a feeling of fear and political apprehension that late global capitalism be not fair my mar book — although they re not as famous and haven t be turn into movie — be quite popular because they re say we could make a decent and beautiful civilization I ve be notice with great pleasure that my mar trilogy be sell well now than it ever have sp do our society need positive vision of the future do we need people to create scenario of how thing could go well ksr oh yes ever since thomas more s utopia we ve always have it edward bellamy write a book call look backward 2000 1887 the progressive political movement that change thing around the time of teddy roosevelt come out of this novel when people have to reconstruct the world s social order after world war ii they turn to h g well and a modern utopia and man like god we always need utopia these day people be fascinate by steve job or bill gate it s like those geeky 1950 science fiction story where a kid in his backyard make a rocket that go to the moon now it s in his garage where he make a computer that change everything we love these story because they re hopeful and they suggest that we could seize history and change it for the well if science fiction doesn t provide those story people find they somewhere else so steve job be a science fiction story we want from a quick cheer to a stand ovation clap to show how much you enjoy this story expand the influence of literature in popular culture
Christopher Wolf Nordlinger,8,6,https://medium.com/@chrisnordlinger/the-internet-of-things-and-the-operating-room-of-the-future-8999a143d7b1?source=tag_archive---------8----------------,the internet of thing and the operating room of the future,the doctor stand over the patient on the operating room table it can be dizzying to look around at the dozen or more video screen dedicate to standalone medical device and not think that the internet of thing iot could radically simplify the complexity of manage so many system in the process digital health could enormously improve patient care at the same time hospital struggle to constrain the rapidly increase cost of healthcare yet with iot investment they can reduce cost significantly it s not hard to see how the medical industry and hospital in particular will represent a major component of the $ 19 trillion internet of thing market opportunity that cisco predict by 2020 imagine its future in surgery alone be not some far off idea it already exist and it s revolutionary due to a unique blend of iot big datum advanced analytic and smart medical device here s how the reality play out in a lead example thousand of people suffer from heart arrhythmia cause by heart disease which show up as a flutter in the heartbeat that be highly disruptive and can cause potentially fatal stroke and heart attack there be a few pharmaceutical drug that can mollify the symptom but they do nothing to remove the dead tissue lesion in the heart that cause the underlying situation which be call atrial fibulation or afib for short cardiothing a make up name to protect the company while under fda approval review be attack this problem with ablation to remove the lesion by gently burn they out with a laser this involve insert a catheter into the heart to try to perform ablation to remove the afib cause lesion each device be hard wire to a screen where streaming datum from the end of the catheter display a view of the inside of the heart but that s not where the datum stop between the heart and the monitor like many device cardiothing a silicon valley startup work with two real iot powerhouse ptc thingworx and another silicon valley startup glassbeam to make something much more powerful possible thingworx model the operation of the catheter so that it can send secure datum to the cloud where it can be analyze by glassbeam glassbeam turn the unstructured datum into structured datum in the form of readable report that the device company can then use to improve doctor surgical performance for cardiothing and other high value asset manufacturer this kind of datum can also increase the uptime of their catheter device other can use iot analytic to increase the uptime of cat scan and mris because the datum can show when even the small part be show sign of weakness or malfunction and enable a repair that keep that equipment operate how imagine cardiothing s optical catheter thin enough to fit comfortably through a vein enter a heart and mapping it out to find the lesion responsible for the afib the surgeon be then able to frame the boundary of the lesion on caridothing s monitor to see which be die and need to get burn out the laser beam from the sensor embed catheter then cut the lesion out and the patient be heal what do this have to do with save money for the hospital high value machine such as mris and cat scan cost million downtime for they be not only very costly for the hospital that be not bill patient but also more importantly interrupt patient from get the good possible care thingworx enable medical device thing sensor model by thingworx to communicate as if it be the device to talk to other thing in the cloud once the unstructured datum be there it can be combine and recombine by glassbeam s analytic software to detect any abnormality for mris cat scan and other device stop small problem from become big problem that crash expensive heavily use equipment be the ultimate value of predictive maintenance hospital be large place with many people and thing move about a great deal and keep track of asset range from mri scanner to $ 60 000 bed be quite challenging in the case of cardiothing above the alliance of ptc thingworx and glassbeam should make the medical industry and business decision maker globally take notice whether it s healthcare agriculture networking or manufacture high utilization of equipment be absolutely essential to remain competitive in the case of the cardiothing s catheter spit out unstructured datum chris kuntz vice president ecosystem program of ptc thingworx say imagine the cardiac datum from that same procedure be combine and recombine with datum from ekg machines mri machines pharmaceutical research personal medical record keeping system blood monitor and hundred of healthcare system this be how the internet of thing drive a revolution in healthcare thank to our partnership with thingworx glassbeam ceo puneet pandit say we be able to capture that unstructured datum off the catheter and create structured datum that business decision maker at hospital the manufacturer and individual doctor can learn from pandit add as a result of the large amount of critical datum come from the catheter you can answer many question how do the device perform under what circumstance how long do the surgery take which surgeon do it most effectively who need to be more formally train as a result of this solution train surgeon to use equipment well provide significantly improve outcome for patient and for hospital dispense critical care no one have to wait any long for the mri to crash to know there be a problem they can fix the small problem before it escalate let the hospital know that a specific part be faulty by simply examine the unstructured datum it send out be the good example of the power of predictive maintenance no one have to wait for the mri to crash hospital can enjoy huge saving through predictive maintenance on all its heavily use expensive equipment give concern about privacy and safeguarding of material it be essential to have a secure connectivity partner such as thingworx aboard hippa be just the beginning of the scope of regulatory requirement that will need to be accommodate to operate successfully in the healthcare data space apply analytic available to doctor in real time reduce medical procedure risk and overall liability concern for hospital to reduce cost and increase profitability iot will play an enormous role for patient it mean their doctor will know so much more about treat they to ensure the good care after any procedure whether it s a heart bypass cancer surgery heart transplant or a simple blood test jack reader business development manager at thingworx now at verizon say imagine an operating room where there be just a few monitor and all the device speak to each other and with thousand of medical system within and beyond the wall of the hospital all of this innovation will exponentially increase insight and intelligence reduce cost for the hospital and increase health outcome the implication in term of knowledge gain and positive health outcome be so phenomenal that we almost can t now imagine from this early stage in the iot era all the possible source nor all the insight that will be gain however the soon iot analytic be adopt in the hospital the soon patient can expect well run hospital and healthy life this be only the beginning of a new era from a quick cheer to a stand ovation clap to show how much you enjoy this story ph d fulbright scholar storyteller communication expert content maven formerly state dept startups cisco & more
Louis Rosenfeld,90,5,https://medium.com/@louisrosenfeld/everyday-ia-d7aa7be07717?source=tag_archive---------9----------------,everyday ia louis rosenfeld medium,a few day ago cennydd bowle gently troll many of we thusly as cennydd have keynote a past information architecture summit it s hard to ignore his question and cennydd s timing be quite interesting give that tomorrow be world ia day the theme of this year s wiad be architecte happiness and in this adorable little video that the ia institute create to promote wiad 2015 abby covert say that this theme be choose because of the rise amount of information that everyone have to deal with my italic cennydd there s your answer if you re a human in today s develop world where even physical object and space be soak in information you be struggle to cope with and make sense of the stuff nearly all the time and nearly everywhere information architecture problem be everyday human problem so if you re design for human today you ll need at least some information architecture skill in order to help they information architecture literacy be require for anyone who design anything so it s not surprising that wiad have explode to 38 location in 24 country it s not surprising that abby s wonderful little book how to make sense of any mess information architecture for everybody have be such a hit it s not surprising to see the ia summit enter its 16th year strong than ever it s not surprising that the fourth edition of information architecture for the world wide web due out later this year be be recast as a book not for information architect but for people who need to know something about information architecture we ve enter full on mode of democratize ia skill because information architecture literacy be require for anyone who design anything I ll confess to have feel like cennydd a bit disconnected from ia for the past few year partly because I ve be invest almost every available moment of my wake hour into rosenfeld medium and partly because much of the ia community s discussion have push far deep into ia practice than my brain and attention span can manage but I m feel well now because I m find in my own day to day work that information architecture literacy be require for anyone who design anything for example while I rarely work on web site ia much these day I be absolutely absorb in the information architecture of book want to know what value publisher can provide to author in this age of self publish the list might be long than you imagine but I think most rosenfeld medium author would agree lou and team pull they out of the weed and help they to step back and make sense of their content as an information system information architecture skill be an absolute necessity when it come to frame structure and establish a flow for a book and not just for non fiction ; just ask jk rowling I m find that ia literacy be also incredibly helpful in other area like event planning I recently ask a couple dozen colleague who produce event to provide share their advice on organize a conference their response be generous useful and wonderful but the one I keep remember most be jeffrey zeldman s yes I m biased but I hear jeffrey singe a song of event ia I ve be singe it too in put together the first edition of the enterprise ux conference plug alert san antonio ; may 13 15 2015 I ve be work with dave malouf and uday gajendar to create an information architecture for a conversation in effect we re try to structure the event s program in a way that surface a latent conversation about enterprise ux that s be happen in the ux community for quite some time the event itself should simply serve as an opportunity to bring people together to sharpen and advance that conversation I m oversimplify a bit but we spend month design our event ia around four carefully sequence theme each in effect a curate mini conference 1 insight at scale ; 2 craft amid complexity ; 3 enterprise experimentation ; and 4 design organizational culture we see these as the main facet of the community s conversation on enterprise ux we ll know we ve be successful if at the event the conversation spill out of the auditorium and into the hallway and break area animate the word and face of attendee we ll know we be really successful if these conversation riff off the theme already cover — mean we get the sequence right and we ll know that we be really really successful if these four theme keep the conversation move forward — both after the event and as the ia for program at future edition of the event book have an information architecture event have an information architecture pretty much anything we design — consciously or not — have an information architecture so pardon I as I repeat information architecture literacy be require for anyone who design anything when I get my master in information and library study in 1990 our professor be preach about the oncoming information revolution since then I ve be fortunate to observe and even participate a little in that revolution in the blink of an eye information architect emerge as professional dedicate to make the pain of that revolution easy to bear in the blink of an eye other have proclaim that information architecture as a profession be dead I m not sure who s right nor do I care twenty five year be nothing the dust can settle after we re all dead let s worry instead about people suffer from everyday ia problem we as designer of any stripe have to help they and we have to get well at help they to help themselves oh and if you re wonder why I win t be at any of tomorrow s 38 wiad meeting well it s saturday and I have a date with my six year old we re go to organize his lego this piece originally run in the rosenfeld review ; sign up here for new one from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of rosenfeld medium I make thing out of information
Matt Harvey,677,7,https://blog.coast.ai/continuous-online-video-classification-with-tensorflow-inception-and-a-raspberry-pi-785c8b1e13e1?source=tag_archive---------0----------------,continuous online video classification with tensorflow inception and a raspberry pi,much have be write about use deep learning to classify prerecorded video clip these paper and project impressive tag classify and even caption each clip with each comprise a single action or subject today we re go to explore a way to continuously classify video as it s capture in an online system continuous classification allow we to solve all sort of interesting problem in real time like understand what s in front of a car for autonomous driving application to understand what s streaming on a tv we ll attempt to do the latter use only open source software and uber cheap hardware specifically tensorflow on a raspberry pi with a picamera we ll use a naive classification approach in this post see next section which will give we a relatively straightforward path to solve our problem and will form the basis for more advanced system to explore later by the time we re do today we should be able to classify what we see on our tv as either a football game or an advertisement run on our raspberry pi let s get to it video be an interesting classification problem because it include both temporal and spatial feature that be at each frame within a video the frame itself hold important information spatial as do the context of that frame relative to the frame before it in time temporal we hypothesize that for many application use only spatial feature be sufficient for achieve high accuracy this approach have the benefit of be relatively simple or at least minimal it s naive because it ignore the information encode between multiple frame of the video since football game have rather distinct spatial feature we believe this method should work wonderfully for the task at hand we re go to collect datum for offline training with a raspberry pi and a picamera we ll point the camera at a tv and record 10 frame per second or more specifically save 10 jpeg every second which will comprise our video here s the code for capture our image once we have our data we ll use a convolutional neural network cnn to classify each frame with one of our label ad or football cnn be the state of the art for image classification and in 2016 it s essentially a solved problem it feel crazy to say that but it really be thank in large part to google→tensorflow→inception and the many researcher who come before it there s very little low level coding require for we when it come to train a cnn for our continuous video classification problem pete warden at google write an awesome blog post call tensorflow for poet that show how to retrain the last layer of inception with new image and class this be call transfer learning and it let we take advantage of week of previous training without have to train a complex cnn from scratch put another way it let we train an image classifier with a relatively small training set we collect 20 minute of footage at 10 jpeg per second which amount to 4 146 ad frame and 7 899 football frame the next step be to sort each frame into two folder football and ad the name of the folder represent the label of each frame which will be the class our network will learn to predict on when we retrain the top layer of the inception v3 cnn this be essentially use the flower method describe in tensorflow for poet apply to video frame to retrain the final layer of the cnn on our new datum we checkout the r0 11 tag from the tensorflow repo and run the follow command retrain the final layer of the network on this data take about 30 minute on my laptop with a geforce gtx 960 m gpu at the completion of 4 000 training step our model report an incredible 98 8 % accuracy on the hold out validation set I m not sure I could do much well use my eye on the same datum as a point of reference if the network have classify each frame as football it would have achieve about 66 % accuracy so it seem to be work it s always a good idea to run some know datum through a train network to sanity check the result so we ll do that here here s the code we use to classify a single image manually through our retrain model and here be the result of spot check individual frame before we transfer everything to our pi and do this in real time let s use a different batch of record datum and see how well we do on that set to get this dataset and to make sure we don t have any data leakage into our training set we separately record another 19 minute of the football broadcast this dataset amount to 2 639 ad frame and 8 524 football frame we run each frame of this set through our classifier and achieve a true holdout accuracy score of 93 3 % awesome look like we ve validate our hypothesis that we can achieve high level of accuracy while only consider spatial feature impressive result consider that we only use 20 minute of training datum thank you google pete tensorflow and all the folk who have develop cnn over the year for your incredible work and contribution great so now we have our cnn train and we know that we can classify each frame of our video with relatively high accuracy how do it do on live tv with always change context for this we load up our raspberry pi 3 with our newly train model weight turn on the picamera at 10 fps and instead of save the image send it through our cnn to be classify we have to make some modification to the code to classify in real time the final result look like this we also have to get tensorflow run on the pi sam abraham write up excellent instruction for do this so I win t cover they again here after we install our dependency we run the program and crap inception on the raspberry pi 3 can only classify one image every four second okay so we don t quite have the hardware yet to do 10fps but this still feel like magic so let s see how we do flip on sunday night football and point our camera at the tv show a remarkable job at classify each moment as football or ad once every few second for the vast majority of the broadcast we see our prediction come out true to life so cool in all our naive method work remarkably well at continuous online video classification for this particular use case but we know that we re only consider part of the information provide to we inherently in video and so there must be room for improvement especially as our dataset become more complex for that we ll have to dive deeply so in the next post we ll explore feed the output of our cnn both the final softmax layer and the pool layer which give we a 2 048 d feature vector of each image to an lstm rnn to see if we can increase our accuracy spoiler alert we can from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of coastline automation use ai to make every car crash proof practical application of deep learning and research report from the road
Vivek Yadav,425,11,https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9?source=tag_archive---------1----------------,an augmentation base deep neural network approach to learn human drive behavior,overview in this post we will go over the work I do for project 3 of udacity s self drive car project behavior clone for drive the main task be to drive a car around in a simulator on a race track and then use deep learning to mimic the behavior of human this be a very interesting problem because it be not possible to drive under all possible scenario on the track so the deep learning algorithm will have to learn general rule for drive we must be very careful while use deep learning model because they have a tendency to overfit the data overfitting refer to the condition where the model be very sensitive to the training datum itself and the model s behavior do not generalize to new unseen datum one way to avoid overfitte be to collect a lot of datum a typical convolutional neural network can have up to a million parameter and tune these parameter require million of training instance of uncorrelated datum which may not always be possible and in some case cost prohibitive for our car example this will require we to drive the car under different weather lighting traffic and road condition one way to avoid overfitte be to use augmentation augmentation refer to the process of generate new training datum from a small datum set such that the new datum set represent the real world datum one may see in practice as we be generate thousand of new training instance from each image it be not possible to generate and store all these datum on the disk we will therefore utilize keras generator to read datum from the file augment on the fly and use it to train the model we will utilize image from the left and right camera so we can generate additional training datum to simulate recovery keras generator be set up such that in the initial phase of learn the model drop datum with low steering angle with high probability this remove any potential for bias towards drive at zero angle after set up the image augmentation pipeline we can proceed to train the model the training be perform use simple adam learn algorithm with learn rate of 0 0001 after this training the model be able to drive the car by itself on the first track for hour and generalize to the second track all the training be base on drive datum of about 4 lap use ps4 controller on track 1 in one direction alone the model never see track 2 in training but with image augmentation flip darken shift etc and use datum from all the camera leave right and center the model be able to learn general rule of drive that help translate this learning to a different track important these result be obtain on titan x gpu machine I build early full specification of the computer can be find here please note that computer with different performance will provide a different performance of the network augmentation help we extract as much information from datum as possible we will generate additional datum use the follow data augmentation technique augmentation be a technique of manipulate the incoming training datum to generate more instance of training datum this technique have be use to develop powerful classifier with little datum https blog keras io build powerful image classification model use very little datum html however augmentation be very specific to the objective of the neural network brightness augmentation change brightness to simulate day and night condition we will generate image with different brightness by first convert image to hsv scale up or down the v channel and convert back to the rgb channel use left and right camera image use left and right camera image to simulate the effect of car wander off to the side and recover we will add a small angle 25 to the left camera and subtract a small angle of 0 25 from the right camera the main idea be the left camera have to move right to get to center and right camera have to move leave horizontal and vertical shift we will shift the camera image horizontally to simulate the effect of car be at different position on the road and add an offset corresponding to the shift to the steering angle we add 0 004 steering angle unit per pixel shift to the right and subtract 0 004 steering angle unit per pixel shift to the left we will also shift the image vertically by a random number to simulate the effect of drive up or down the slope shadow augmentation the next augmentation we will add be shadow augmentation where random shadow be cast across the image this be implement by choose random point and shade all point on one side choose randomly of the image the code for this augmentation be present below flip in addition to the transformation above we will also flip image at random and change the sign of the predict angle to simulate driving in the opposite direction 2 preprocesse after augment the image as above we will crop the top 1 5 of the image to remove the horizon and the bottom 25 pixel to remove the car s hood originally 1 3 of the top of car image be remove but later it be change to 1 5 to include image for case when the car may be drive up or down a slope we will next rescale the image to a 64x64 square image after augmentation the augmented image look as follow these image be generate use kera s generator and unlimited number of image can be generate from one image I use lambda layer in keras to normalize intensity between 5 and 5 3 kera generator for subsample as there be limited datum and we be generate thousand of training example from the same image it be not possible to store all the image apriori into memory we will utilize kera s generator function to sample image such that image with low angle have low probability of get represent in the datum set this alleviate any problem we may ecounter due to model have a bias towards drive straight panel below show multiple training sample generate from one image the keras generator be present below the pr_threshold variable be a threshold that determine if a data with small angle will be drop or not 4 model architecture and training I implement the model architecture above for train the datum the first layer be 3 1x1 filter this have the effect of transform the color space of the image research have show that different color space be well suit for different application as we do not know the good color space apriori use 3 1x1 filter allow the model to choose its good color space this be follow by 3 convolutional block each comprise of 32 64 and 128 filter of size 3x3 these convolution layer be follow by 3 fully connect layer all the convolution block and the 2 follow fully connect layer have exponential relu elu as activation function I choose leaky relu to make transition between angle smooth training I train the model use the keras generator with batch size of 256 for 8 epoch in each epoch I generate 20000 image I start with pr_threshold the chance of drop datum with small angle as 1 and reduce the probability by divide it by the iteration number after each epoch the entire training take about 5 minute however it too more than 20 hour to arrive at the right architecture and training parameter snippet below present the result of train 5 model performance video below show the performance of algorithm on the track 1 on which the original datum be collect the car be able to drive around for hour we will next look into the case where either the camera resolution video size or track be change generalization from one image size to another video below present generalization from one image size to another I use the same pretraine model and test it on all the other image size and find that the deep learning neural network be able to drive the car around for all image size generalization from one image resolution to another video below present generalization from one image resolution to another I use the same pretraine model and test it on all the other image resolution and find that the deep learning neural network be able to drive the car around for all image resolution I also test different combination of image size and image resolution and on track 1 the deep learning algorithm be able to drive the car around for all combination of image resolution and size generalization from one track to another figure below present generalization from one track to another this be perhaps the tough test for the deep learning algorithm in the second track there be more right turn and u turn it be dark and the road have slope all of which be absent in the original track however all these effect be artificially include into the model via image augmentation 6 future direction this project be far from over this project open more question than it answer a few more thing to try be 6 reflection this be perhaps the weird project I do this project challenge all the previous knowledge I have about deep learning in general large epoch size and training with more datum result in well performance but in this case any time I get beyond 10 epoch the car simply drive off the track although all the image augmentation and tweak seem reasonable n0w I do not think of they apriori I hope other find this post useful and get insprie to try novel thing I have not use on the fly training agile trainer by john chen yet I want to try and stretch the datum as much as possible next thing to try be to experiment with parallel network use john s trainer acknowledgement I be very thankful to udacity for select I for the first cohort this allow I to connect with many like minded individual as always learn a lot from discussion with henrik tünnermann and john chen I be also thankful for get the nvida s gpu grant although its for work but I use it for udacity too from a quick cheer to a stand ovation clap to show how much you enjoy this story staff software engineer at lockheed martin autonomous system with research interest in control machine learning ai lifelong learner with glassblowe problem good place to learn about chatbot we share the late bot news info ai & nlp tool tutorial & more
Carlos Beltran,97,9,https://medium.com/@carlosbeltran/ai-the-theme-in-avenged-sevenfolds-new-album-the-stage-f4516d6fc96?source=tag_archive---------2----------------,a rock album for ai carlos beltran medium,https open spotify com album 0jwnywjz6xhnrvayeclqpd it s awesome that avenge sevenfold become interested in ai and write an entire album that revolve around the idea in an interview with roll stone lead singer m shadow say the initial interest come after read tim urban s article over at waitbutwhy it s one of the thing along with movie like she and the matrix of course that spike my interest in ai as well so I d highly recommend read it tim do a phenomenal job of explain the topic current challenge engineer be face and the very possible implication of this technology the term artificial intelligence be first coin half a century ago fast forward to today where we have have giant company like intel and apple acquire ai startup like there s no tomorrow it s not a matter of whether or not we ll be able to create machine that surpass our own capability but when theoretical physicist and futurist dr michio kaku think it be possible for machine as smart as we to exist by the end of the century google s chief futurist ray kurzweil believe such technology will exist as soon as 2029 the band be right in want its fan and the general public to be more aware of these idea — they could be right around the corner I m no expert but I d like to discuss the idea behind some of the song and include reference in case you d like to delve deep and if you want to read more on the possible future of ai i d recommend read kurzweil s book the singularity be near although some of his prediction have be meet with skepticism the idea present be think provoke simply put nanomachine be microscopic machine that will enhance we in almost every way imaginable they ll be able to help our immune system fight off disease they would create super soldier this technology be actually at the center of a great game series metal gear solid this hack in our biological makeup will also increase our lifespans kurzweil imago a future where biotechnology be so advanced that we will live forever this be the same idea behind the song paradigm lyric include the song also raise the question of what it really mean to be human what do we become when we merge with machine will we lose what fundamentally make we human it can be argue that this merge be the next logical step in evolution as there be no there be no evolutionary pressure for we to do so anymore we ll become as kurzweil put it godlike expand the brain s neocortex will allow we for example to pose question in our thought and know the answer almost immediately most likely thank to our direct brain to google connection we ll always have witty joke on hand and learn calculus will be as simple as purchase downloadable content plug and play besides swap out fail body part with prosthetic and enhance our brain there s another way we ll be able to gain immortality both dr kaku and kurzweil firmly believe that the advance in brain computer interface will eventually allow we to upload our consciousness to machine scientist still have no clue how the brain work how the billion of neuron form connection that result in learn behavior or what dreaming be but once these secret be know which might never actually happen and we know how our brain function as well as what the consciousness switch be the possibility be endless to get an idea of what s possible check out black mirror s episode playt the brain computer interface for the game be so advanced that the player can t distinguish between what s real and what isn t I don t want to spoil anything but get ready for a mind fuck black mirror do a great job of weave technology with a dystopia that we might inhabit show a dark side of our society it s on netflix so check it out elon musk sure do he claim that the chance of we live in base reality be one in billion I d recommend watch the 3 minute video his logic be as follow we have pong some 40 year ago two rectangle and a dot be render on screen for what we call a videogame today we have game with realistic graphic and they keep get well every year well yet virtual and augmented reality be right around the corner push the boundary of game eventually we ll have the technology to create simulated world that be indistinguishable from reality therefore musk claim it be likely that we be live in an ancestor simulation create by an advanced future civilization some 10 000 year from now the album s 7th song simulation explore the idea that our reality might not be what it seem think of it this way — the brain and nervous system which we use to automatically react to the environment around we be the same brain and nervous system which tell we what the environment be throughout the song the patient be have thought that challenge the simulation they be live in they be — in a sense — wake up a dark voice which I believe be mean to represent the one run the show have to reprimand the patient remind they that they only exist because we allow it to control the situation the patient be to be sedate with blue comfort a reference from the matrix which will make they forget they re live in a simulation blissful ignorance I win t try to explain this one just watch the video and here s a quote from that man that might get your attention imagine an entity so intelligent but that s just it you can t imagine it in the second part to his article on ai tim urban compare this to a chimp be unable to understand a skyscraper be not just a part of its environment but that human build it it s not the chimp s fault or anything its brain be just not make to have that level of information process the same thing will happen when we build a machine with the collective knowledge of some 200 000 year of homo sapien existence therefore there be no way to know what it will do or what the consequence will be tim depict our situation with this entity what he refer to as artificial superintelligence asi beautifully mark zuckerberg be right in say we should be hopeful of the amount of good ai could do but some of the smart mind in existence be genuinely concerned stephen hawk acknowledge that the successful creation of an ai will be the big event in history but warn it could also end mankind elon musk found a research company openai as a way to neutralize the threat of a malicious artificial super intelligence create god describe ai as a modern messiah the very last invention man would ever need it paint the picture of a utopia where this intelligence exist at the same time the song suggest that we could be summon the demon unable to control the outcome we could just be its stepping stone as our existence after its creation become irrelevant the album wrap up with a 15 minute eargasm I can t produce word that will do exist any justice as the band describe it it s like listen to what the big bang might ve sound like neil degrasse tyson make a cameo at the end of the song that serve as a reminder that our problem and conflict be minuscule in the grand scheme of thing we re all a part of the same universe and once we as a society realize this we can truly make progress here s the full thing the stage be an exceptional album in my opinion the band s intention be for fan to educate themselves or be a bit more aware of what s go on in this area we can enjoy it as a rock album as well as explore the idea behind the lyric I have an awesome time write this dig up thing I ve read and see and unify they in a way so other can hopefully become more interested as well and come on don t tell I that the idea that we re live in a simulation isn t think provoke tap the ❤ button below my name s carlos and I generally write about personal development tech and entrepreneurship hit I up on twitter from a quick cheer to a stand ovation clap to show how much you enjoy this story software engineer focus on build cool shit on ethereum 🚀
Matt Harvey,558,6,https://blog.coast.ai/continuous-video-classification-with-tensorflow-inception-and-recurrent-nets-250ba9ff6b85?source=tag_archive---------3----------------,continuous video classification with tensorflow inception and recurrent net,a video be a sequence of image in our previous post we explore a method for continuous online video classification that treat each frame as discrete as if its context relative to previous frame be unimportant today we re go to stop treat our video as individual photo and start treat it like the video that it be by look at our image in a sequence we ll process these sequence by harness the magic of recurrent neural network rnns to restate the problem we outline in our previous post we re attempt to continually classify video as it s stream in an online system specifically we re classify whether what s streaming on a tv be a football game or an advertisement convolutional neural network which we use exclusively in our previous post do an amazing job at take in a fix size vector like an image of an animal and generate a fix size label like the class of animal in the image what cnn can not do without computationally intensive 3d convolution layer be accept a sequence of vector that s where rnn come in rnn allow we to understand the context of a video frame relative to the frame that come before it they do this by pass the output of one training step to the input of the next training step along with the new frame andrej karpathy describe this eloquently in his popular blog post the unreasonable effectiveness of recurrent neural network we re use a special type of rnn here call an lstm that allow our network to learn long term dependencies christopher olah write in his outstanding essay about lstms almost all exciting result base on recurrent neural network be achieve with lstms sell let s get to it our aim be to use the power of cnn to detect spatial feature and rnn for the temporal feature effectively build a cnn > rnn network or crnn for the sake of time rather than build and train a new network from scratch we ll step 2 be unique so we ll expand on it a bit there be two interesting path that come to mind when add a recurrent net to the end of our convolutional net let s say you re bake a cake you have at your disposal all of the ingredient in the world we ll say that this assortment of ingredient be our image to be classify by look at a recipe you see that all of the possible thing you could use to make a cake flour whisky another cake have be reduce down to ingredient and measurement that will make a good cake the person who create the recipe out of all possible ingredient be the convolutional network and the result instruction be the output of our pool layer now you make the cake and it s ready to eat you re the softmax layer and the finished product be our class prediction I ve make the code to explore these method available on github I ll pull out a couple interesting bit here in order to turn our discrete prediction or feature into a sequence we loop through each frame in chronological order add it to a queue of size n and pop off the first frame we previously add here s the gist n represent the length of our sequence that we ll pass to the rnn we could choose any length for n but I settle on 40 at 10fps which be the framerate of our video that give we 4 second of video to process at a time this seem like a good balance of memory usage and information the architecture of the network be a single lstm layer with 256 node this be follow by a dropout of 0 2 to help prevent over fitting and a fully connect softmax layer to generate our prediction I also experiment with wide and deep network but neither perform as well as this one it s likely that with a large training set a deep network would perform good note I m use the incredible tflearn library a high level api for tensorflow to construct our network which save we from have to write a lot of code once we have our sequence of feature and our network training with tflearn be a breeze evaluating be even easy now let s evaluate each of the method we outline above for add an rnn to our cnn intuitively if one frame be an ad and the next be a football game it s essentially impossible that the next will be an ad again I wish commercial be only 1 10th of a second long this be why it could be interesting to examine the temporal dependency of the probability of each label before we look at the more raw output of the pool layer we convert our individual prediction into sequence use the code above and then feed the sequence to our rnn after train the rnn on our first batch of datum we then evaluate the prediction on both the batch we use for training and a holdout set that the rnn have never see no surprise evaluate the same datum we use to train give we an accuracy of 99 55 % good sanity check that we re on the right path now the fun part we run the holdout set through the same network and get 95 4 % well than our 93 3 % we get without the lstm and not a bad result give we re use the full output of the cnn and thus not give the rnn much responsibility let s change that here we ll go a little deep see what I do there instead of let the cnn do all the hard work we ll give more responsibility to the rnn by use output of the cnn s pool layer which give we the feature representation not a prediction of our image we again build sequence with this datum to feed into our rnn run our training datum through the network to make sure we get high accuracy succeed at 99 89 % sanity check how about our holdout set 96 58 % that s an error reduction of 3 28 percentage point or 49 % from our cnn only benchmark awesome we have show that take both spatial and temporal feature into consideration improve our accuracy significantly next we ll want to try this method on a more complex dataset perhaps use multiple class of tv programming and with a whole whackload more datum to train on remember we re only use 20 minute of tv here once we feel comfortable there we ll go ahead and combine the rnn and cnn into one network so we can more easily deploy it in an online system that s go to be fun part 3 be now available five video classification method implement in keras and tensorflow from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of coastline automation use ai to make every car crash proof practical application of deep learning and research report from the road
Oxford University,237,19,https://medium.com/oxford-university/the-future-of-work-cf8a33b47285?source=tag_archive---------4----------------,the future of work oxford university medium,technology have always change employment but the rise of robotic and artificial intelligence could transform it beyond recognition researcher at oxford be investigate how technology will shape the future of work — and what we can do to ensure everyone benefit in a famous 1930 talk john maynard keynes imagine a future 100 year hence in which technological progress automate much of human labour by 2030 he estimate we could all enjoy a 15 hour work week a lot will need to change in the next decade for that to become a reality but it s not impossible right now advance in artificial intelligence and robotic promise machine that will take on all kind of human task digital communication be create an internet dwell labour force that can work remotely and on demand and the self employ be find that new technological service like uber and airbnb can provide a flexible way to make a living but phenomenon like these give rise to a cascade of effect — not all necessarily desirable — that be fiendishly difficult to perceive and predict it s perhaps not surprising then that the future of work be a topic of increase fascination for university of oxford academic both the oxford martin school and green templeton college now run specific programme that focus on the topic with plenty of researcher — from the department of engineering science and sociology to those of politic and economic — grapple with its complexity we see a need for bring together different perspective around the study of work explain dr marc thompson a senior fellow at saїd business school and the director of the green templeton college future of work programme our role as academic be to contribute to the debate both in term of theory and to raise challenge question and issue for those in government and industry what will happen as a result of these advance how will it affect people whose interest be be pursue and what be the long term implication a series of recent study from the university cut straight to the chase of technology s impact on employment focus on how robotic and automation will affect the job that human currently undertake the author dr carl benedikt frey @carlbfrey and prof michael osborne @maosbot come from quite different background frey be an economist interested in the transition of industrial nation to digital economy osborne an engineer focus on create machine learning algorithm together they re co director of the programme on technology and employment at the oxford martin school it would be fair to ask why I m do work relate to economic while we re sit here in the department of engineering admit osborne gesture to his surrounding but I ve always have some interest in think about what machine learning could mean for society beyond the industrial application we usually consider so when carl approach I to speak about algorithm and technology use in automation and their effect on employment it seem like a natural fit this be of course exactly the kind of multidisciplinary work the university excel at and the reason the oxford martin school be establish each of its programme bring together researcher from different field to tackle complex global issue that can t be solve by academic from a single discipline since meet the pair have set about develop way to analyse which job that exist today could be at risk of be take over by robot or artificial intelligence software in the next 20 year first they gather together as many smart people as they could to decide on 70 job role that definitely could or could not be automate in the next 20 year for example they collectively decide that switchboard operator and dishwasher could definitely be replace while the clergy and magistrate certainly couldn t the pair combine this list with datum from the us department of labor s o⋆net system — a database which describe the different skill relevant to specific occupation osborne then build an algorithm that could learn from both pool of datum to establish the kind of skill that be common to automatable job when show other occupation and the skill they require the software can classify they with a probability of be either automatable or non automatable the pair find that the job least likely to be automate be those that require skill of creative intelligence social intelligence or physical dexterity these be what they refer to as engineering bottleneck current limit to technology that make human irreplaceable osborne point out that it s perfectly possible for instance to have an algorithm churn out an endless sequence of song but almost impossible to have it create a hit similarly chat bot may be able to communicate with you but they can t negotiate a deal and robot can assemble object on a well define production line but they can t perform a fiddly task like make a cup of tea in your messy kitchen in each case it s because human draw on a huge wealth of tacit knowledge about culture emotion human behaviour and the physical environment that s hard to encode in a way that a machine can act upon but even with those bottleneck the result suggest that as many as 47 % of we job be at risk from automation over the course of the next two decade it s worth bear in mind that the figure explain which job be theoretically automatable rather than destine to be automate that may seem like a fiddly point say osborne but this analysis doesn t take into account other factor that we absolutely do believe will have an impact on whether an occupation be take over by a machine such as human wage level social acceptance and the creation of new job but however you look at it the number be difficult to ignore there s an intuitive counter argument to the claim that their analysis make for century new technology have be invent that have push human out of work but by and large most of we still continue to have job in fact researcher elsewhere in the university have show that the amount of work we all perform remain steadfastly consistent irrespective of technological change jonathan gershuny professor of sociology and director of the centre for time use research have spend a large part of his career trace the way that we all use our time — to work play rest and everything else fundamentally there be three realm of activity he explain from the bay window of his woodstock road office there s pay work unpaid work and consumption pay work be just that the task we carry out in exchange for money be it mining coal write a book or perform brain surgery unpaid work meanwhile be form of task that you could pay someone else to do for you but for whatever reason don t such as cook cleaning gardening or childcare and consumption be all the activity you absolutely couldn t pay someone else to do for you — your night s sleep say or eat lunch why be I tell you all this ask gershuny with a grin well when you define work quite widely like this you arrive at a really quite extraordinary discovery which be that work time — that be the sum of pay and unpaid work time — doesn t change very much look at all the datum we have access to the total be pretty constant at about 60 hour per week that s just over a third of our 168 hour week and a little more than the approximately 50 hour chunk we manage to spend sleep he point to decade of evidence accumulate by his team — in country include australia canada israel slovenia france sweden the netherlands and plenty more — that confirm the trend as well as work time regulation from as far back as the industrial revolution his late dataset — a huge survey of british resident carry out in 2015 — be be download in full the day we meet but a preliminary analysis already suggest that his observation hold true the truth be we need work for various reason a time structure a social context a purpose in life he explain indeed what many people cite keyne famous talk about the future fail to mention be that he go on to suggest that there be no country and no people who can look forward to the age of leisure and of abundance without a dread in other word he think that most us couldn t really begin to comprehend the reality of not work gershuny agree argue that human will simply endeavour to find new type of work to do in order to busy themselves whether the robot take over the job we currently possess or not dr ruth yeoman a research fellow at the saïd business school who research meaningful work in organisation and system point out that the human desire to find meaning in work be hard to ignore she explain that the drive to work be so strong that people seek positive meaning in work that be consider by many people to be dirty low status or poorly pay hospital cleaner for instance interpret their work to be meaningful and worthwhile because they enlarge the scope of that work in their own mind she explain this phenomenon allow human to justify all kind of work to themselves as useful and relevant it seem regardless of what it actually be frey and osborne aren t so confident that human be resourceful enough to create new work for themselves though frey have actually study the rate at which new job be be generate as a result of technological change his finding suggest that about 8 2 % of the us workforce shift into new type of job — that be role associate with technological advance — during the 1980 in the 1990s the figure fall to 4 4 % and in the 2000 it drop to just 0 5 % the evidence suggest that the new industry we might assume to be the salvation of the labour force — such as web design or datum science — aren t create as many new position as we may hope part of the reason for that argue osborne be that many of the new job role be create be relate to software rather than hard physical good software be pretty cheap with next to zero marginal cost of reproduction he explain that mean that a small group of people can have a great idea and easily turn it into a product that s use the world over while barely grow the size of its team the smartphone message service whatsapp be a prime example it be purchase by facebook for $ 19 billion in 2014 when it serve 700 million user at the time it have just 55 employee count specific job may however be overly simplistic when it come to think about how the work life of real people be set to change people often think about the work that people do as a monolithic indivisible lump of stuff explain daniel susskind @danielsusskind a lecturer in economic at balliol college and co author of a new book call the future of the profession the problem be that encourage the view that one day a lawyer will arrive at work to find an algorithm sit in his chair or a doctor turn up to a robot in her operating theatre and their job will both be go instead he argue we should be focus on the separate task that make up job role susskind co write his new book with his father richard susskind @richardsusskind whose oxford dphil consider the impact of artificial intelligence on law that be back in the 1980 when ai system be rudimentary and typically base on rule glean from human understanding but five year ago father and son — the latter then work in the policy unit at 10 downing street — realise that a second wave of artificial intelligence be be develop that could have profound effect on professional career since they ve be research how technology might affect the work life of lawyer doctor teacher architect and the rest of the profession not everything that a professional do be creative strategic or complex explain susskind so while many professional might think that all their work lie on one side of frey and osborne s engineering bottleneck actually many of the task they perform be amenable to computerisation for most that mean it s unlikely that they ll simply lose their job to technology at least in the near future — but they can expect to see a significant change in the sort of thing they re ask to do in their book the susskind describe twelve new role that might appear within the profession — such as process analyser knowledge engineer data scientist and empathiser these be role that sound unfamiliar to traditional professional that require skill and ability that many of they be unlikely to have at this moment in time they explain we re already see professional adapt so that they can work alongside more intelligent technological system though take for instance your bank manager when you use to approach they for a loan they d carefully make a decision on whether or not you be a good risk then either give you the money or send you home now an algorithm determine whether or not you re award the cash and yet bank manager still exist the role have simply change to become a customer service and sale job rather than an analytical or technical role not everyone will be as lucky as the professional whose job merely metamorphose because if all of the task that make up a job be automatable the job no long need to exist craig holmes @craigpholme a fellow in economic at pembroke college and senior research fellow at the institute for new economic thinking have be study shift in occupational structure of labour market and how they ve move away from middle skilled work with more people now do high skilled or low skilled work this phenomenon — refer to as the hollowing out of the labour market — isn t in itself new middle skilled factory worker have be lose their job to robot for decade for instance but the pace of technological development be now threaten other middle skilled occupation that in the past we ve assume could only be do by human job category define as associate professional for instance — the people that provide technical service that keep trade finance and government run — appear increasingly likely to be take over by machine in the case of say paralegal there be now piece of software that can sift through thousand of document pull out relevant precedent and put they together use a very simple format without require any human involvement explain holme so a traditionally middle tier research job can be perfectly perform by technology the same story could play out in other sector large dataset of historical case note and information from wearable could allow computer to make straightforward medical diagnosis say while smart algorithm might remove the work of number crunch accountant like car factory worker replace by robot in the past holme imago a number of possible future for those discharge from mid tier role some like the bank manager will be able to assume different role with similar title a small number may move upwards into role that aren t yet automatable other sadly may have to assume low skilled job or face unemployment the nature of those lower skilled job will of course change too the work of frey and osborne suggest that many low skilled job — such as call centre worker data entry clerk and dishwasher — will be readily automate in the future in some case the cost of technology will be so low that there s no wage that people could happily accept that would make the job sustainable admit holme in fast food restaurant for instance you can replace someone who take an order with an ipad that will last for year nobody would accept a job that pay wage that low but it s not perhaps quite so gloomy as that as personal service job will likely still require a human touch we ll probably see an increase in the number of low skill service job because people value human interaction and many of those job currently seem not to be readily automatable suggest holme that will provide more job they just win t be great job while technology may be the mechanism through which many job be lose though it might very well also be the thing that enable people to take up new lower skilled position there s be an explosion in connectivity around the world explain professor mark graham @geoplace from the oxford internet institute something like 3 5 billion people be now online and that have some significant repercussion in term of what work be where it s do and how it happen graham have be travel the world to talk to people who find themselves in a new kind of labour market in particular he s be interview individual who perform work from home provide to they by a slew of website such as amazon s mechanical turk upwork and clickworker these site all allow company and individual to outsource task potential employer simply post a description of what they need do to a website then people interested in do the work bid for it the employer choose someone to do the work base on a combination of price list skill and rating from previous employer ; the worker carry out the task get pay then move on to another piece of work the task be dole out vary — from transcription and translation to new kind of work such as tag image for artificial intelligence system — but much of it be currently difficult or expensive to automate technology have also create legion of new workforce member in more traditional sector such as transportation hospitality catering cleaning and delivery there be increasingly more way of commodifye bit of everyday life use your car to be an uber driver ; your apartment to be an airbnb host ; your bicycle to be a deliveroo rider ; or your broom to be a task rabbit cleaner explain graham this be what s become know as the sharing or gig economy whether it s uber airbnb or amazon s mechanical turk the business plan be much the same create a digital platform which make it easy to link a customer who want a service to be perform with someone who s willing to provide it for a very competitive fee these new style of work certainly bring some benefit apparent flexibility for worker more efficient use of exist resource and equipment and reasonable price for those seek service but as jeremias prassl an associate professor of law and fellow of magdalen college warn this new workforce be potentially vulnerable uber act like an employer it set your wage tell you the route to drive hire you and fire you if your rating fall too low he explain under any classical analysis uber perform all the usual employer function but in its contract with driver partner the platform explicitly deny employer status suggest that the worker be very much a contractor legally and through the language it use uber try to deny the fact that it offer employment through so do the company be able to avoid pay social security pension contribution redundancy pay and so on — all the usual right an employee might benefit from but prassl who s write a book about the topic point out that these kind of contract be nothing new from the perspective of an employment lawyer zero hour contract and the gig economy be old problem he explain we ve be grapple with the rise of so call non standard work for the last 30 or 40 year it s just that now they re receive more attention and sustained media coverage the problem as prassl see it be that employment law be currently base on an old binary system if you re an employee you get right — to say sick pay notice of dismissal or pay holiday but if you re a contractor you re not afford any of those right employment law currently boil down to a simple question how do you define whether or not someone count as an employee what my research suggest be that maybe we should turn the problem on its head he explain we could say instead who s the employer it seem like a subtle difference but with the shoe on the other foot he suggest crowd worker would be able to enjoy some kind of employment law protection in this upended scenario everyone could benefit from exist minimum standard like the minimum wage work time regulation and discrimination protection with their provision account for by whoever be legally deem to be the employer if company fail to comply worker could litigate employer in the knowledge that the damage be definitely owe to they it s not just prassl that s worried about the vulnerability of employee one of the issue be that we confuse work with job point out ruth yeoman there s an awful lot of work in the world that have to be do and one of the problem when we think about the future of work be how it all get convert into job for which people will be pay sometimes people may contribute to society not through pay work but through some other mechanism voluntary work say or care and while those task may be hard work or may not pay they be necessary and many of they must be do by human that s why stuart white @stuartgwhite associate professor from the department of politic and international relation be interested in how we could ensure everyone enjoy a basic standard of living — a concept he s write about in the book democratic wealth he explain white s suggestion be that no test of mean or willingness to take a job would be impose so that everyone in the country receive a basic payment every month it s worth note that the idea be not intend to make everyone rich — far from it instead it s a mean of give individual more flexibility afford they power to decide when and how to be contributive and productive it s a way of ensure you don t have people desperately scramble into job to make end meet white explain in turn he argue employer would make some of the least appealing job more pleasant — they d be force to otherwise nobody would choose to do they numerous mechanism for put such a policy into action have be propose in the past one option be to divert exist benefit and tax relief into a basic income that s share equally amongst the population if those contribution didn t stretch far enough they could be top up with revenue from further taxation — from land value tax suggest white alternatively the income could be provide by a state own investment fund from which the return would be share out equally there be lot of philosophical argument about whether or not it s all a good idea he concede but we re move into a world where there s increased insecurity around work against that backdrop a source of income that s independent of work be a way of rebalancing power relation in the labour market whether or not you agree with the concept of a universal citizen s income or the reform of employment law these concept be indicative of the kind of discussion that oxford researcher be increasingly lead I think the university need to be ask these kind of aristotlean question about whose interest be be meet who benefit from the change the moral question explain marc thompson it s not something we should shy away from increasingly then just as thompson hope for when he set up the green templeton college future of work programme oxford academic be work with business and government to shape the debate about the future of employment frey and osborne for instance have publish report with citi and deloitte about the impact of technology on employment ; mark graham sit on the department for international development s digital advisory panel ; and richard susskind act as an it adviser to the lord chief justice of england and wale what remain of course be for policymaker lawyer and industry official to take the question and suggestion raise by academic on board then work out how well to use technological advance in all our favour these possibility afford by technology automation and commodification of labour they can all be shape by policy organisational change and simply choose to do thing differently muse thompson there be some important choice to be make about how we make use of they technology will make many job redundant other easy and create at least some new one along the way keyne prediction of a fifteen hour work week may even come true but while human be in charge we can still choose for there to be some work that s perform by non robotic hand it would be very easy for there to be an automate pub where drink be serve from vend machine conclude mark graham but nobody want that because it would be depress write by jamie condliffe a science and technology writer base in london he tweet @jme_c in keep with one of the theme of the article we use 99designs to find an illustrator and work with slouise follow we on medium we ll be publish more article soon that look at topic such as medical trial development in healthcare and more if you like this article please click the green heart it really help to spread the word and let other find it produce by christopher eddie digital communications office university of oxford from a quick cheer to a stand ovation clap to show how much you enjoy this story oxford be one of the old university in the world we aim to lead the world in research and education contact digicomms@admin ox ac uk oxford be one of the old university in the world we aim to lead the world in research and education contact digicomms@admin ox ac uk
Maciej Lipiec,766,8,https://medium.com/k2-product-design/the-future-of-digital-banking-236ad65e4c76?source=tag_archive---------5----------------,the future of digital banking k2 product design medium,our solution be base on three pillar in the old time user interface of a bank be the bank teller at the branch from today s perspective it be inconvenient and time consuming but the bank have a human face now we be interact with our bank by click on link menu and button and fill out form but banking app be often hard to use overly complex and ugly lack of true customer centricity and technological debt on the back end side of thing make the banking experience frustrating how can we make digital banking easy more simple more personal and human by give it a new face of a robot meet bankbot it be the new digital bank teller personal assistant and a financial advisor when you sign in to your k2 bank account bankbot will greet you and ask for order the main interface of k2 bank be instantly familiar if you ever use slack over two million of people use it in the office everyday or facebook messenger or an sms app or irc then you re really old school it s never end stream with history of communication from bottom recent to the top old of the screen you type your command or question and bankbot will answer bankbot understand natural language but it pay special attention for keyword that will trigger action like a new transfer or search in history or credit card cancellation just type in send 100 eur to anna and bankbot will search it s database for possible recipient match „ anna and let you choose the one you mean or you can add a new recipient then bankbot will send confirmation code to your cell phone and ask you to type it in and it s do you don t need to click and move your hand from the keyboard of course this the easy scenario similar to send money via squarecash or snapcash but almost every operation can be complete that way type a recipient s name will show you recent transaction with she from your account history and option for a new payment typing usd will show you currency exchange rate if you need help type help if you need to contact human staff at the bank type human and you can chat with real person from customer service instead of a bot or type „ concierge if you re a private banking client there be also a way to access feature use the hamburger menu at the bottom — it open a list of option just like type slash in slack personal finance manager pfms for control home budget be popular addition to banking system but they be complicate often hide deep in the nested menus and they need a lot of user s attention do people really use they steven walker of forrester research have write bankbot can provide just that you can ask expense this month or car expense and it will show you a simple chart with relevant information this be pull mechanism but bankbot can also be proactive push important information to the user it can warn you that you be close to exceed your monthly budget it can remind you about regular payment you usually make each month it can remind you to pay off your credit card or pay your tax it can suggest well option to save or invest your money and show you how much more you can earn it can offer you a loan when you probably need it or offer travel insurance when he know you ve just buy plane ticket or up sell you a well account or credit card when it will notice that you ve get a pay rise or it can alert you when you should do something with your stock portfolio chat banking be nice on the desktop but it s even more effective on mobile — type a few word and it s do just like send an sms or you can talk to bankbot speech2text authentication can be provide by fingerprint sensor you can receive important alert as push notification on your phone or smartwatch and immediately take action or dismiss you can even get discount on your health insurance base on physical activity datum from your fitness band or apple watch bankbot can also live inside smart device like the amazon echo which provide its own api for developer — smart home and smart banking mix together or inside the facebook messenger chat the second payment service directive be to be transpose into national regulation across the european union from 2016 its goal be to open the banking market psd2 will force bank to provide access via api to their customer account and provide account information to third party service provider if the account holder wish to do so this be call „ access to the account xs2a and it s not optional bank will have to evolve as third party enter their space psd2 define traditional financial institution bank as account service payment service provider as psp and new player as account information service provider aisp or payment initiation service provider pisp both pisp and aisp will have to register with the competent authority in their home member state for security reason what be the implication of this for our system the quality of banking user interface will be extremely important because bank s client could choose to manage their account from third party provider app with well ux or functionality cut themselves from any direct communication with their bank in this case the bank will be reduce to a „ dumb pipe in the value chain but fight this by provide to the third party only the minimum api require may be a bad strategy for bank we think they should be more open actively partner with other financial institution retailer merchant and startup we imagine k2 bank solution provide an appstore base on its apis user will be able to give permission to third party service provider in a way you allow application to access your facebook or twitter account today you will be able to buy stuff at your authorized retailer without log into your bank or without visit the retailer site but from yours bank app there be no need to provide credit card number probably even ship address or any datum the bank can automatically offer you a purchase by installment or it can give you a discount because of your history of frequent past transaction online and offline with this retailer there will be no need for customer loyalty card anymore the bank can become an advertising channel for the retailer too offer personalize promotion for its customer this should be opt out but if your cell phone contract be end and bankbot message you with a really great offer for a plan with a cheap new iphone and you can buy it instantly with one click would you mind by build the thrive ecosystem bank and third party can both win and we hope customer will too if you want to know more about k2 bank solution it s design technology behind the bankbot and possibility of implementation don t hesitate to contact we of course conversational interface like bankbot can be use not only in banking but also insurance online commerce travel healthcare and many other industry please write to maciej lipiec k2 s user experience director at maciej lipiec@k2 pl you can read more about k2 bank in this article at chatbots magazine also please check out our project on behance k2 internet be a lead digital product design and communication agency in poland we develop digital service app and website with a strong focus on user experience we have a long time experience partnering with financial institution — in the last 10 year we help to envision design and develop over 10 transactional system for the big bank in poland stanusch technology be k2 bank s technology provider for bankbot the company be involve in research and development of the use of artificial intelligence in business it carry out project relate to natural language processing and semantic information retrieval it have become a world leader in the number of carry out project of virtual advisor chatbot thank you if you enjoy read this please 👏 👏 👏 and share from a quick cheer to a stand ovation clap to show how much you enjoy this story product design director @ k2 k2 internet be a lead digital product design and communication agency in poland
Camron Godbout,341,10,https://hackernoon.com/tensorflow-in-a-nutshell-part-three-all-the-models-be1465993930?source=tag_archive---------7----------------,tensorflow in a nutshell — part three all the model,make sure to check out the other article here in this installment we will be go over all the abstracted model that be currently available in tensorflow and describe use case for that particular model as well as simple sample code full source of work example be in the tensorflow in a nutshell repo use case language modeling machine translation word embed text processing since the advent of long short term memory and gate recurrent unit recurrent neural network have make leap and bound above other model in natural language processing they can be feed vector represent character and be train to generate new sentence base on the training set the merit in this model be that it keep the context of the sentence and derive mean that cat sit on the mat mean the cat be on the mat since the creation of tensorflow write these network have become increasingly simple there be even hide feature cover by denny britz here that make write rnn s even simple here a quick example use case image process facial recognition computer vision convolution neural network be unique because they re create in mind that the input will be an image cnn perform a slide window function to a matrix the window be call a kernel and it slide across the image create a convolve feature create a convolve feature allow for edge detection which then allow for a network to depict object from picture the convolve feature to create this look like this matrix below here s a sample of code to identify handwritten digit from the mnist dataset use case classification and regression these network consist of perceptron in layer that take input that pass information on to the next layer the last layer in the network produce the output there be no connection between each node in a give layer the layer that have no original input and no final output be call the hidden layer the goal of this network be similar to other supervised neural network use back propagation to make input have the desire train output these be some of the simple effective neural network for classification and regression problem we will show how easy it be to create a feed forward network to classify handwritten digit use case classification and regression linear model take x value and produce a line of good fit use for classification and regression of y value for example if you have a list of house size and their price in a neighborhood you can predict the price of house give the size use a linear model one thing to note be that linear model can be use for multiple x feature for example in the housing example we can create a linear model give house size how many room how many bathroom and price and predict price give a house with size # of room # of bathroom use case currently only binary classification the general idea behind a svm be that there be an optimal hyperplane for linearly separable pattern for datum that be not linearly separable we can use a kernel function to transform the original datum into a new space svms maximize the margin around separate the hyperplane they work extremely well in high dimensional space and and be still effective if the dimension be great than the number of sample use case recommendation system classification and regression deep and wide model be cover with great detail in part two so we win t get too heavy here a wide and deep network combine a linear model with a feed forward neural net so that our prediction will have memorization and generalization this type of model can be use for classification and regression problem this allow for less feature engineering with relatively accurate prediction thus get the good of both world here s a code snippet from part two s github use case classification and regression random forest model take many different classification tree and each tree vote for that class the forest choose the classification have the most vote random forest do not overfit you can run as many treee as you want and it be relatively fast give it a try on the iris data with this snippet below use case classification and regression in the contrib folder of tensorflow there be a library call bayesflow bayesflow have no documentation except for an example of the reinforce algorithm this algorithm be propose in a paper by ronald williams this network try to solve an immediate reinforcement learning task adjust the weight after get the reinforcement value at each trial at the end of each trial each weight be incremente by a learning rate factor multiply by the reinforcement value minus the baseline multiply by characteristic eligibility williams paper also discuss the use of back propagation to train the reinforce network use case sequential datum crf be conditional probability distribution that factoirze accord to an undirected model they predict a label for a single sample keep context from the neighboring sample crf be similar to hide markov model crf be often use for image segmentation and object recognition as well as shallow parsing name entity recognition and gene find ever since tensorflow have be release the community surround the project have be add more package example and case for use this amazing library even at the time of write this article there be more model and sample code be write it be amazing to see how much tensorflow as grow in these past few month the ease of use and diversity in the package be increase overtime and don t seem to be slow down anytime soon from a quick cheer to a stand ovation clap to show how much you enjoy this story co founder & cto of apteo research machine learn technique to improve invest come join we how hacker start their afternoon
Dominik Felix,286,5,https://chatbotsmagazine.com/how-to-create-a-chatbot-without-coding-a-single-line-e716840c7245?source=tag_archive---------8----------------,how to create a chatbot without code a single line,chatbot be ready to succeed if you think you have to hack day or even week to create a chatbot you might be wrong you don t have to be aware of any code skill immediately after big player like facebook messenger or skype open their platform for programmer many tool emerge with this article I want to give you an introduction to mockup and overview of different tool to build your first chatbot you re have an idea you want to show your use case it s definitely recommendable to mockup your story beforehand first you may find some bug in your concept moreover you will be able to explain a showcase to noninvolve people base on the motto fake it til you make it it s very intuitive storytelling just insert what the user say and what the bot respond use the setting option you can edit smartphone model decide number of fan and choose a profile picture a page category and a welcome message additional feature be button image and quick reply the whole story act like a movie by push the play button it can be share by just one click and it s possible to save the file as mp4 within the pay plan each of the tool support different platform therefore please keep in mind that it s important to choose your platform wisely base on the huge range most of the tool make use of facebook messenger chatfuel be focus on facebook messenger you don t need any code skill to get start it s simple to create different logic block and link they to respective trigger it offer great plugin e g human take over and a minimalistic ai in case you be recently start with bot I can recommend you this service motion provide sms email web chat facebook messenger and slack furthermore it s possible to link to other api and hook back to motion thus it operate as a hub the conversation be build with flowchart and base on connector and prepare module it just take a few minute to get familiar with the procedure founder ceo of motion ai david nelson s chatbot make easy api ai be a great platform for develop chatbot it have ai support and an intuitive interface it require only one click to assemble I e small talk or weather feature on the one hand it s possible to run the bot exclusively on their server on the other you can download a nodejs sample code to execute it on your infrastructure to sum up api ai be an advanced service be the reason why it s more complicated to build a bot use this tool unsurprisingly it get buy by google a few day ago feature cbm api ai small talk be now open why be it a big deal flow xo offer a graphical interface to build so call flow which define how your bot will operate to received message or audio it have a huge list of integration as a consequence it s more complex than chatfuel but also a lot more flexible pretty amazing be their support on messenger slack sms and telegram they ve an interesting approach to build chatbot it guide you through 4 step design develop launch and grow first you ve to design the content message persistent menu welcome message and some more as step 2 it want you to link message to trigger and setup curious module like offer human help the launch step lead you through the review process while the final step focus on customer retention I e schedule message user list etc manychat allow broadcast content from rss feed additionally it s possible to link to yahoo pipeline and broadcast everything you want it support scheduled message auto posting from rss facebook twitter youtube and have a basic mechanism to send specific answer to specific keyword watch their pitch to get a well understanding mindiq be a diy bot builder platform for business focus on facebook messenger you don t need any code skill and they make it dead simple for business to build bot they follow a template approach currently the template available be medium commerce and food tech they also provide tool to link your business tool like mailchimp to your chatbot there be many tool on the market every tool solve other problem and each of they use a different approach for how to design user interaction I really like the simplicity of chatfuel and the 4 step process of botsify since all of these tool be quite new I m super excited and look forward to see the direction that will be pursue and develop from a quick cheer to a stand ovation clap to show how much you enjoy this story botspot vienna agentur volk chatbot ecosystem botstack framework chatbot ai nlp facebook messenger slack telegram and more
Greg Gascon,368,6,https://medium.com/startup-grind/how-invisible-interfaces-are-going-to-transform-the-way-we-interact-with-computers-39ef77a8a982?source=tag_archive---------9----------------,how invisible interface be go to transform the way we interact with computer,in the mid ninety a computer scientist at xerox parc theorize the concept of the internet of thing albeit with a different name far before anyone else have and even far still before it have become possible even though today we call it by that name ubiquitous computing — as it be then coin by mark weiser — imagine a world wherein cheap and ubiquitous connected computing would radically alter the way we use and interact with computer the idea be ahead of its time in the world of ubiquitous computing connect device would become cheap and thereby would exist everywhere importantly these device would as a result cease to become special or unique — they would become invisible as we near this utopian world fill with computer our relationship with they inexorably will change each of we will come to interact with dozen of separate device on a daily basis as such we will need to develop interface in a way so as not to distract we as be currently do but in a way in which to empower we or how weiser put it we will need to adopt the concept of calm technology on the face of it ubiquitous computing be just that a reality in which computer be everywhere of course with trend relate to iot we be near this but we be not there yet one of the most important implication to come from ubiquitous computing for example will be the change it will make on how we perceive and interact with computer for instance think of the electric motor an old technology that be ubiquitous in the present today there could be dozen of they in a single car however when we hit a button to roll down the window we don t think at all about the motor pull the window down we simply think about the action of make the window go down the electric motor be so mundane and ubiquitous in our life that we don t even think about it when use it it be invisible it be this sort of invisibility that allow the user to take full control of their interaction with a give piece of technology when use a piece of technology that have become invisible the user think of use it in term of end goal rather than get bogge down in the technology itself the user doesn t have to worry how it be go to work they just make it happen in another example weiser simply state a good pencil stay out of the way of the writing now even though technology surround we today we aren t at this point yet gadget and device be still special to we in a distracting way we still not only still marvel at new technology we be tell to by whomever be produce it but why do this matter the good way to see how ubiquitous computing will impact we be to examine the way we engineer and interact with the app that exist today when create a web app for instance you try to guide or manipulate the user into use your tool as much as possible when you create a drip marketing email campaign for it in most case you aren t create it so that the user need to use your tool less you be create it so they can spend more time and use all of its feature that be to say the goal isn t foremost and necessarily to save the user time furthermore there be no question ask as to whether the user aught to spend more time use whatever particular app be be optimize within a social medium website each user be give a piece of social property a social medium platform imbue each social property with a value system — think of the concept of like comment or share — as incentive to spend time on the site each user interaction with a social property whether it be a photo or a comment that be write be then log and record so they can easily be reward for the time invest some social app such as linkedin will have we hook for something as simple as a pageview of our profile these action be far incentivize through the use of gamification app send intrusive notification give you some information about what they be about but not everything and this be crucial not know what be in the notification entice we to open it even far it go without say this be important for increase the amount of screen time we give the app for if we see everything in the notification there would be no point in open the app it make wake up every morning feel like open a bunch of small present and while it s a stretch to say that developer be act nefariously to steal our time those build our web service and tool should construct they with respect to the user s guilelessness do so require adopt principle of invisible or calm technology contradiction aside the most accessible way we can get a glimpse into a future dominate by invisible interface be the movie she although not the focus of the film she showcase a future wherein input give to device be do so largely through voice command yes there be still smartphone but the majority of interaction take place by simply talk to a give device use natural language theodore be able to interact with technology in a manner that be completely at hand he can ask any sort of question or create any sort of demand without get bogge down in how the device work furthermore the technology never try to whisk his attention away from anything the technology be always there but it be only in the periphery accord to weiser this be one of the key principle of design calm technology the device in question should never try to distract or pry the user away from what they be try to accomplish yet it must always be ready to accept user input it be calm in the exact opposite way that receive group chat notification on your phone be not we can see this principle of design in part at play in the new apple airpod even though they have yet to be release they promise to let we interact with the internet without ever need to look down at our phone and they be aware of their environment too they know such thing like if they be in an ear or not and if they be not they know to stop play sound it s these small micro automation that will far make technology invisible and allow we to focus on whatever it be that we want from the technology and not worry about have to configure it other more simple example include the auto brightness on your phone or its fingerprint scanner they simply work without any sort of configuration or notification about what they be do and more technology like this be come there be today even advocacy group such as time well spend that try to spread awareness about how interface and app can hijack the way our brain work even more promising be that there be company that be follow suit in these design principle for instance the upcoming moment smartwatch be a device which interface with the user largely through touch feedback instead of rely on the screen all that s need now well speech recognition from a quick cheer to a stand ovation clap to show how much you enjoy this story tech columnist app script dev social medium automator seo specialist read more at https www gregorygascon com the life work and tactic of entrepreneur around the world by founder for founder welcome submission on technology trend product design growth strategy and venture investing
Dhruv Parthasarathy,4.3K,12,https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4?source=tag_archive---------0----------------,a brief history of cnn in image segmentation from r cnn to mask r cnn,at athela we use convolutional neural network cnn for a lot more than just classification in this post we ll see how cnn can be use with great result in image instance segmentation ever since alex krizhevsky geoff hinton and ilya sutskever win imagenet in 2012 convolutional neural network cnn have become the gold standard for image classification in fact since then cnn have improve to the point where they now outperform human on the imagenet challenge while these result be impressive image classification be far simple than the complexity and diversity of true human visual understanding in classification there s generally an image with a single object as the focus and the task be to say what that image be see above but when we look at the world around we we carry out far more complex task we see complicated sight with multiple overlap object and different background and we not only classify these different object but also identify their boundary difference and relation to one another can cnn help we with such complex task namely give a more complicated image can we use cnn to identify the different object in the image and their boundary as have be show by ross girshick and his peer over the last few year the answer be conclusively yes through this post we ll cover the intuition behind some of the main technique use in object detection and segmentation and see how they ve evolve from one implementation to the next in particular we ll cover r cnn regional cnn the original application of cnn to this problem along with its descendant fast r cnn and fast r cnn finally we ll cover mask r cnn a paper release recently by facebook research that extend such object detection technique to provide pixel level segmentation here be the paper reference in this post inspire by the research of hinton s lab at the university of toronto a small team at uc berkeley lead by professor jitendra malik ask themselves what today seem like an inevitable question object detection be the task of find the different object in an image and classify they as see in the image above the team comprise of ross girshick a name we ll see again jeff donahue and trevor darrel find that this problem can be solve with krizhevsky s result by test on the pascal voc challenge a popular object detection challenge akin to imagenet they write let s now take a moment to understand how their architecture region with cnn r cnn work understand r cnn the goal of r cnn be to take in an image and correctly identify where the main object via a bounding box in the image but how do we find out where these bounding box be r cnn do what we might intuitively do as well propose a bunch of box in the image and see if any of they actually correspond to an object r cnn create these bounding box or region proposal use a process call selective search which you can read about here at a high level selective search show in the image above look at the image through window of different size and for each size try to group together adjacent pixel by texture color or intensity to identify object once the proposal be create r cnn warps the region to a standard square size and pass it through to a modified version of alexnet the win submission to imagenet 2012 that inspire r cnn as show above on the final layer of the cnn r cnn add a support vector machine svm that simply classify whether this be an object and if so what object this be step 4 in the image above improve the bounding box now have find the object in the box can we tighten the box to fit the true dimension of the object we can and this be the final step of r cnn r cnn run a simple linear regression on the region proposal to generate tight bounding box coordinate to get our final result here be the input and output of this regression model so to summarize r cnn be just the follow step r cnn work really well but be really quite slow for a few simple reason in 2015 ross girshick the first author of r cnn solve both these problem lead to the second algorithm in our short history fast r cnn let s now go over its main insight fast r cnn insight 1 roi region of interest pooling for the forward pass of the cnn girshick realize that for each image a lot of propose region for the image invariably overlap cause we to run the same cnn computation again and again ~2000 time his insight be simple — why not run the cnn just once per image and then find a way to share that computation across the ~2000 proposal this be exactly what fast r cnn do use a technique know as roipool region of interest pooling at its core roipool share the forward pass of a cnn for an image across its subregion in the image above notice how the cnn feature for each region be obtain by select a correspond region from the cnn s feature map then the feature in each region be pool usually use max pooling so all it take we be one pass of the original image as oppose to ~2000 fast r cnn insight 2 combine all model into one network the second insight of fast r cnn be to jointly train the cnn classifier and bounding box regressor in a single model where early we have different model to extract image feature cnn classify svm and tighten bounding box regressor fast r cnn instead use a single network to compute all three you can see how this be do in the image above fast r cnn replace the svm classifier with a softmax layer on top of the cnn to output a classification it also add a linear regression layer parallel to the softmax layer to output bounding box coordinate in this way all the output need come from one single network here be the input and output to this overall model even with all these advancement there be still one remain bottleneck in the fast r cnn process — the region proposer as we see the very first step to detect the location of object be generate a bunch of potential bounding box or region of interest to test in fast r cnn these proposal be create use selective search a fairly slow process that be find to be the bottleneck of the overall process in the middle 2015 a team at microsoft research compose of shaoqe ren kaime he ross girshick and jian sun find a way to make the region proposal step almost cost free through an architecture they creatively name fast r cnn the insight of fast r cnn be that region proposal depend on feature of the image that be already calculate with the forward pass of the cnn first step of classification so why not reuse those same cnn result for region proposal instead of run a separate selective search algorithm indeed this be just what the fast r cnn team achieve in the image above you can see how a single cnn be use to both carry out region proposal and classification this way only one cnn need to be train and we get region proposal almost for free the author write here be the input and output of their model how the region be generate let s take a moment to see how fast r cnn generate these region proposal from cnn feature fast r cnn add a fully convolutional network on top of the feature of the cnn create what s know as the region proposal network the region proposal network work by pass a slide window over the cnn feature map and at each window output k potential bounding box and score for how good each of those box be expect to be what do these k box represent intuitively we know that object in an image should fit certain common aspect ratio and size for instance we know that we want some rectangular box that resemble the shape of human likewise we know we win t see many box that be very very thin in such a way we create k such common aspect ratio we call anchor box for each such anchor box we output one bounding box and score per position in the image with these anchor box in mind let s take a look at the input and output to this region proposal network we then pass each such bounding box that be likely to be an object into fast r cnn to generate a classification and tightened bounding box so far we ve see how we ve be able to use cnn feature in many interesting way to effectively locate different object in an image with bounding box can we extend such technique to go one step far and locate exact pixel of each object instead of just bound box this problem know as image segmentation be what kaime he and a team of researcher include girshick explore at facebook ai use an architecture know as mask r cnn much like fast r cnn and fast r cnn mask r cnn s underlying intuition be straight forward give that fast r cnn work so well for object detection could we extend it to also carry out pixel level segmentation mask r cnn do this by add a branch to fast r cnn that output a binary mask that say whether or not a give pixel be part of an object the branch in white in the above image as before be just a fully convolutional network on top of a cnn base feature map here be its input and output but the mask r cnn author have to make one small adjustment to make this pipeline work as expect roialign realigning roipool to be more accurate when run without modification on the original fast r cnn architecture the mask r cnn author realize that the region of the feature map select by roipool be slightly misalign from the region of the original image since image segmentation require pixel level specificity unlike bound box this naturally lead to inaccuracy the author be able to solve this problem by cleverly adjust roipool to be more precisely align use a method know as roialign imagine we have an image of size 128x128 and a feature map of size 25x25 let s imagine we want feature the region correspond to the top leave 15x15 pixel in the original image see above how might we select these pixel from the feature map we know each pixel in the original image correspond to ~ 25 128 pixel in the feature map to select 15 pixel from the original image we just select 15 * 25 128 ~= 2 93 pixel in roipool we would round this down and select 2 pixel cause a slight misalignment however in roialign we avoid such round instead we use bilinear interpolation to get a precise idea of what would be at pixel 2 93 this at a high level be what allow we to avoid the misalignment cause by roipool once these mask be generate mask r cnn combine they with the classification and bounding box from fast r cnn to generate such wonderfully precise segmentation if you re interested in try out these algorithm yourself here be relevant repository fast r cnn mask r cnn in just 3 year we ve see how the research community have progress from krizhevsky et al s original result to r cnn and finally all the way to such powerful result as mask r cnn see in isolation result like mask r cnn seem like incredible leap of genius that would be unapproachable yet through this post I hope you ve see how such advancement be really the sum of intuitive incremental improvement through year of hard work and collaboration each of the idea propose by r cnn fast r cnn fast r cnn and finally mask r cnn be not necessarily quantum leap yet their sum product have lead to really remarkable result that bring we close to a human level understanding of sight what particularly excite I be that the time between r cnn and mask r cnn be just three year with continue funding focus and support how much far can computer vision improve over the next three year if you see any error or issue in this post please contact I at dhruv@getathela com and I ll immediately correct they if you re interested in apply such technique come join we at athela where we apply computer vision to blood diagnostic daily other post we ve write thank to bharath ramsundar pranav ramkrishnan tanay tandon and oliver cameron for help with this post from a quick cheer to a stand ovation clap to show how much you enjoy this story @dhruvp vp eng @athelas mit math and cs undergrad 13 mit cs master 14 previously director of ai program @ udacity blood diagnostic through deep learning http athela com
Slav Ivanov,3.9K,17,https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=tag_archive---------1----------------,the $ 1700 great deep learning box assembly setup and benchmark,update april 2018 use cuda 9 cudnn 7 and tensorflow 1 5 after year of use a thin client in the form of increasingly thin macbook I have get use to it so when I get into deep learning dl I go straight for the brand new at the time amazon p2 cloud server no upfront cost the ability to train many model simultaneously and the general coolness of have a machine learning model out there slowly teach itself however as time pass the aws bill steadily grow large even as I switch to 10x cheap spot instance also I didn t find myself train more than one model at a time instead I d go to lunch workout etc while the model be train and come back later with a clear head to check on it but eventually the model complexity grow and take long to train I d often forget what I do differently on the model that have just complete its 2 day training nudge by the great experience of the other folk on the fast ai forum I decide to settle down and to get a dedicated dl box at home the most important reason be save time while prototyping model — if they train fast the feedback time would be short thus it would be easy for my brain to connect the dot between the assumption I have for the model and its result then I want to save money — I be use amazon web service aws which offer p2 instance with nvidia k80 gpus lately the aws bill be around $ 60 70 month with a tendency to get large also it be expensive to store large dataset like imagenet and lastly I haven t have a desktop for over 10 year and want to see what have change in the meantime spoiler alert mostly nothing what follow be my choice inner monologue and gotcha from choose the component to benchmarke a sensible budget for I would be about 2 year worth of my current compute spending at $ 70 month for aw this put it at around $ 1700 for the whole thing you can check out all the component use the pc part picker site be also really helpful in detect if some of the component don t play well together the gpu be the most crucial component in the box it will train these deep network fast shorten the feedback cycle disclosure the follow be affiliate link to help I pay for well more gpu the choice be between a few of nvidia s card gtx 1070 gtx 1070 ti gtx 1080 gtx 1080 ti and finally the titan x the price might fluctuate especially because some gpu be great for cryptocurrency mining wink 1070 wink on performance side gtx 1080 ti and titan x be similar roughly speak the gtx 1080 be about 25 % fast than gtx 1070 and gtx 1080 ti be about 30 % fast than gtx 1080 the new gtx 1070 ti be very close in performance to gtx 1080 tim dettmer have a great article on pick a gpu for deep learning which he regularly update as new card come on the market here be the thing to consider when pick a gpu consider all of this I pick the gtx 1080 ti mainly for the training speed boost I plan to add a second 1080 ti soonish even though the gpu be the mvp in deep learning the cpu still matter for example data preparation be usually do on the cpu the number of core and thread per core be important if we want to parallelize all that data prep to stay on budget I pick a mid range cpu the intel i5 7500 it s relatively cheap but good enough to not slow thing down edit as a few people have point out probably the big gotcha that be unique to dl multi gpu be to pay attention to the pcie lane support by the cpu motherboard by andrej karpathy we want to have each gpu have 16 pcie lane so it eat datum as fast as possible 16 gb s for pcie 3 0 this mean that for two card we need 32 pcie lane however the cpu I have pick have only 16 lane so 2 gpu would run in 2x8 mode instead of 2x16 this might be a bottleneck lead to less than ideal utilization of the graphic card thus a cpu with 40 line be recommend edit 2 however tim dettmer point out that have 8 lane per card should only decrease performance by 0 10 % for two gpu so currently my recommendation be go with 16 pcie lane per video card unless it get too expensive for you otherwise 8 lane should do as well a good solution with to have for a double gpu machine would be an intel xeon processor like the e5 1620 v4 40 pcie lane or if you want to splurge go for a high end processor like the desktop i7 6850k memory ram it s nice to have a lot of memory if we be to be work with rather big dataset I get 2 stick of 16 gb for a total of 32 gb of ram and plan to buy another 32 gb later follow jeremy howard s advice I get a fast ssd disk to keep my os and current datum on and then a slow spin hdd for those huge dataset like imagenet ssd I remember when I get my first macbook air year ago how blow away be I by the ssd speed to my delight a new generation of ssd call nvme have make its way to market in the meantime a 480 gb mydigitalssd nvme drive be a great deal this baby copy file at gigabyte per second hdd 2 tb seagate while ssds have be get fast hdd have be get cheap to somebody who have use macbook with 128 gb disk for the last 7 year have this much space feel almost obscene the one thing that I keep in mind when pick a motherboard be the ability to support two gtx 1080 ti both in the number of pci express lane the minimum be 2x8 and the physical size of 2 card also make sure it s compatible with the choose cpu an asus tuf z270 do it for I msi — x99a sli plus should work great if you get an intel xeon cpu rule of thumb power supply should provide enough juice for the cpu and the gpu plus 100 watt extra the intel i5 7500 processor use 65w and the gpus 1080 ti need 250w each so I get a deepcool 750w gold psu currently unavailable evga 750 gq be similar the gold here refer to the power efficiency I e how much of the power consume be waste as heat the case should be the same form factor as the motherboard also have enough led to embarrass a burner be a bonus a friend recommend the thermaltake n23 case which I promptly get no led sadly here be how much I spend on all the component your cost may vary $ 700 gtx 1080 ti + $ 190 cpu + $ 230 ram + $ 230 ssd + $ 66 hdd + $ 130 motherboard + $ 75 psu + $ 50 case = = = = = = = = = = = = $ 1671 total add tax and fee this nicely match my preset budget of $ 1700 if you don t have much experience with hardware and fear you might break something a professional assembly might be the good option however this be a great learning opportunity that I couldn t pass even though I ve have my share of hardware relate horror story the first and important step be to read the installation manual that come with each component especially important for I as I ve do this before once or twice and I have just the right amount of inexperience to mess thing up this be do before instal the motherboard in the case next to the processor there be a lever that need to be pull up the processor be then place on the base double check the orientation finally the lever come down to fix the cpu in place but I have a quite the difficulty do this once the cpu be in position the lever wouldn t go down I actually have a more hardware capable friend of mine video walk I through the process turn out the amount of force require to get the lever lock down be more than what I be comfortable with next be fix the fan on top of the cpu the fan leg must be fully secure to the motherboard consider where the fan cable will go before instal the processor I have come with thermal paste if yours doesn t make sure to put some paste between the cpu and the cool unit also replace the paste if you take off the fan I put the power supply unit psu in before the motherboard to get the power cable snugly place in case back side pretty straight forward — carefully place it and screw it in a magnetic screwdriver be really helpful then connect the power cable and the case button and led just slide it in the m2 slot and screw it in piece of cake the memory prove quite hard to install require too much effort to properly lock in a few time I almost give up thinking I must be do it wrong eventually one of the stick click in and the other one promptly follow at this point I turn the computer on to make sure it work to my relief it start right away finally the gpu slide in effortlessly 14 pin of power later and it be run nb do not plug your monitor in the external card right away most probably it need driver to function see below finally it s complete now that we have the hardware in place only the soft part remain out with the screwdriver in with the keyboard note on dual booting if you plan to install window because you know for benchmark totally not for game it would be wise to do window first and linux second I didn t and have to reinstall ubuntu because window mess up the boot partition livewire have a detailed article on dual boot most dl framework be design to work on linux first and eventually support other operating system so I go for ubuntu my default linux distribution an old 2 gb usb drive be lay around and work great for the installation unetbootin osx or rufus window can prepare the linux thumb drive the default option work fine during the ubuntu install at the time of write ubuntu 17 04 be just release so I opt for the previous version 16 04 whose quirk be much well document online ubuntu server or desktop the server and desktop edition of ubuntu be almost identical with the notable exception of the visual interface call x not be instal with server I instal the desktop and disabled autostarte x so that the computer would boot it in terminal mode if need one could launch the visual desktop later by type startx let s get our install up to date from jeremy howard s excellent install gpu script to deep learn on our machine we need a stack of technology to use our gpu download cuda from nvidia or just run the code below update to specify version 9 of cuda thank to @zhanwenchen for the tip if you need to add later version of cuda click here after cuda have be instal the follow code will add the cuda installation to the path variable now we can verify that cuda have be instal successfully by run this should have instal the display driver as well for I nvidia smi show err as the device name so I instal the late nvidia driver as of may 2018 to fix it remove cuda nvidia driver if at any point the driver or cuda seem break as they do for I — multiple time it might be well to start over by run since version 1 5 tensorflow support cudnn 7 so we install that to download cudnn one need to register for a free developer account after download install with the follow anaconda be a great package manager for python I ve move to python 3 6 so will be use the anaconda 3 version the popular dl framework by google installation validate tensorfow install to make sure we have our stack run smoothly I like to run the tensorflow mnist example we should see the loss decrease during training keras be a great high level neural network framework an absolute pleasure to work with installation can t be easy too pytorch be a newcomer in the world of dl framework but its api be model on the successful torch which be write in lua pytorch feel new and exciting mostly great although some thing be still to be implement we install it by run jupyter be a web base ide for python which be ideal for data sciency task it s instal with anaconda so we just configure and test it now if we open http localhost 8888 we should see a jupyter screen run jupyter on boot rather than run the notebook every time the computer be restart we can set it to autostart on boot we will use crontab to do this which we can edit by run crontab e then add the following after the last line in the crontab file I use my old trusty macbook air for development so I d like to be able to log into the dl box both from my home network also when on the run ssh key it s way more secure to use a ssh key to login instead of a password digital ocean have a great guide on how to setup this ssh tunnel if you want to access your jupyter notebook from another computer the recommend way be to use ssh tunneling instead of open the notebook to the world and protect with a password let s see how we can do this 2 then to connect over ssh tunnel run the follow script on the client to test this open a browser and try http localhost 8888 from the remote machine your jupyter notebook should appear setup out of network access finally to access the dl box from the outside world we need 3 thing set up out of network access depend on the router network setup so I m not go into detail now that we have everything run smoothly let s put it to the test we ll be compare the newly build box to an aws p2 xlarge instance which be what I ve use so far for dl the test be computer vision relate mean convolutional network with a fully connect model throw in we time training model on aws p2 instance gpu k80 aw p2 virtual cpu the gtx 1080 ti and intel i5 7500 cpu andre hernandez point out that my comparison do not use tensorflow that be optimize for these cpu which would have help the they perform well check his insightful comment for more detail the hello world of computer vision the mnist database consist of 70 000 handwritten digit we run the keras example on mnist which use multilayer perceptron mlp the mlp mean that we be use only fully connect layer not convolution the model be train for 20 epoch on this dataset which achieve over 98 % accuracy out of the box we see that the gtx 1080 ti be 2 4 time fast than the k80 on aws p2 in train the model this be rather surprising as these 2 card should have about the same performance I believe this be because of the virtualization or underclocking of the k80 on aws the cpus perform 9 time slow than the gpu as we will see later it s a really good result for the processor this be due to the small model which fail to fully utilize the parallel processing power of the gpu interestingly the desktop intel i5 7500 achieve 2 3x speedup over the virtual cpu on amazon a vgg net will be finetune for the kaggle dog vs cat competition in this competition we need to tell apart picture of dog and cat run the model on cpus for the same number of batch wasn t feasible therefore we finetune for 390 batch 1 epoch on the gpu and 10 batch on the cpus the code use be on github the 1080 ti be 5 5 time fast that the aws gpu k80 the difference in the cpus performance be about the same as the previous experiment i5 be 2 6x fast however it s absolutely impractical to use cpu for this task as the cpus be take ~200x more time on this large model that include 16 convolutional layer and a couple semi wide 4096 fully connect layer on top a gan generative adversarial network be a way to train a model to generate image gan achieve this by pit two network against each other a generator which learn to create well and well image and a discriminator that try to tell which image be real and which be dream up by the generator the wasserstein gan be an improvement over the original gan we will use a pytorch implementation that be very similar to the one by the wgan author the model be train for 50 step and the loss be all over the place which be often the case with gan cpus aren t consider the gtx 1080 ti finish 5 5x fast than the aws p2 k80 which be in line with the previous result the final benchmark be on the original style transfer paper gatys et al implement on tensorflow code available style transfer be a technique that combine the style of one image a painting for example and the content of another image check out my previous post for more detail on how style transfer work the gtx 1080 ti outperform the aws k80 by a factor of 4 3 this time the cpu be 30 50 time slow than graphic card the slowdown be less than on the vgg finetune task but more than on the mnist perceptron experiment the model use mostly the early layer of the vgg network and I suspect this be too shallow to fully utilize the gpus the dl box be in the next room and a large model be train on it be it a wise investment time will tell but it be beautiful to watch the glow led in the dark and to hear its quiet hum as model be try to squeeze out that extra accuracy percentage point from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur hacker machine learn deep learning and other type of learn
Tyler Elliot Bettilyon,17.9K,13,https://medium.com/@TebbaVonMathenstien/are-programmers-headed-toward-another-bursting-bubble-528e30c59a0e?source=tag_archive---------2----------------,be programmer head toward another bursting bubble,a friend of mine recently pose a question that I ve hear many time in vary form and forum do you think it and some low level programming job be go to go the way of the dodo seem a bit like a massive job bubble that s go to burst it s my opinion that one of the only thing keep tech and low level computer science relate job prestigious and well pay be ridiculous industry jargon and public ignorance about computer which be both go to go away in the next 10 year this question be simultaneously on point about the future of technology job and exemplary of some pervasive misunderstanding regard the field of software engineering while it s true that there be a great deal of ridiculous industry jargon there be equally many genuinely difficult problem wait to be solve by those with the right skill set some software job be definitely go away but programmer with the right experience and knowledge will continue to be prestigious and well remunerate for many year to come ; as an example look at the recent explosion of ai researcher salary and the correspond dearth of available talent stay relevant in the ever change technology landscape can be a challenge by look at the technology that be replace programmer in the status quo we should be able to predict what job might disappear from the market additionally to predict how salary and demand for specific skill might change we should consider the grow body of people learn to program as hannah point out public ignorance about computer be keep wage high for those who can program and the public be become more computer savvy each year the fear of automation replace job be neither new nor unfounded in any field and especially in technology market force drive corporation toward automation and commodification gartner s hype cycle be one way of contextualize this phenomenon as time go on specific idea and technology push towards the plateau of productivity where they be eventually automate look at history one must conclude that automation have the power to destroy specific job market in diverse industry range from crop harvesting to automobile assembly technology advance have consistently replace and augment human labor to reduce cost a professor once put it this way in his compiler course take historical note of textile and steel industry do you want to build machine and tool or do you want to operate those machine in this metaphor the machine be a computer programming language this professor be really ask do you want to build website use javascript or do you want to build the v8 engine that power javascript the creation of website be be automate by wordpress and other today v8 on the other hand have a grow body of competitor some of whom be solve open research question language will come and go how many fortran job opening be there but there will always be someone build the next language lucky for us programming language implementation be write with programming language themselves be a machine operator in software put you on the path to be a machine creator in a way which be not true of the steel mill worker of the past the grow number of language interpreter and compiler show we that every job destroy machine also bring with it new opportunity to improve those machine maintain those machine and so forth despite the grow body of job which no long exist there have yet to be a moment in history where humanity have collectively say I guess there isn t any work leave for we to do commodification be come for we all not just software engineer throughout history human labor have consistently be replace with non human or augment to require few and less skilled human self drive car and truck be the flavor of the week in this grand human tradition if the cycle of creation and automation be a fact of life the natural question to answer next be which job and industry be at risk and which be not aws heroku and other similar hosting platform have forever change the role of the system administrator devop engineer internet business use to absolutely need their own server master someone who be well verse in linux ; someone who could configure a server with apache or nginx ; someone who could not only physically wire up the server the router and all the other physical component but who could also configure the routing table and all the software require to make that server accessible on the public web while there be definitely still people apply this skill set professionally aw be make some of those skill obsolete — especially at the low experience level and on the physical side of thing there be very lucrative role within amazon and netflix and google for people with deep expertise in network infrastructure but there be much less demand at the small to medium business scale business intelligence tool such as salesforce tableau and spotfire be also begin to occupy space historically hold by software engineer these system have reduce the demand for in house database administrator but they have also increase the demand for sql as a general purpose skill they have decrease demand for in house reporting technology but increase demand for integration engineer who automate the flow of datum from the business to the third party software platform s a field that be previously dominate by excel and spreadsheet be increasingly be push towards scripting language like python or r and towards sql for datum management some job have disappear but demand for people who can write software have see an increase overall data science be a fascinating example of commodification at a level close to software scikit learn tensorflow and pytorch be all software library that make it easy for people to build machine learning application without build the algorithm from scratch in fact it s possible to run a dataset through many different machine learning algorithm with many different parameter set for those algorithm with little to no understanding of how those algorithm be actually implement it s not necessarily wise to do this just possible you can bet that business intelligence company will be try to integrate these kind of algorithm into their own tool over the next few year as well in many way data science look like web development do 5 8 year ago — a booming field where a little bit of knowledge can get you in the door due to a skill gap as web development bootcamp be close and consolidate datum science bootcamp be pop up in their place kaplan who buy the original web development bootcamp dev bootcamp and start a data science bootcamp metis have decide to close devbootcamp and keep metis run content management system be among the most visible of the tool automate away the need for a software engineer squarespace and wordpress be among the most popular cms system today these platform be significantly reduce the value of people with a just a little bit of front end web development skill in fact the barrier for make a website and get it online have come down so dramatically that people with zero programming experience be successfully launch website every day those same people aren t make deeply interactive website that serve billion of people but they absolutely do make website for their own business that give customer the information they need a lovely landing page with information such as how to find the establishment and how to contact they be more than enough for a local restaurant bar or retail store if your business be not primarily an internet business it have never be easy to get a work site on the public web as a result the once thriving industry of web contractor who can quickly set up a simple website and get it online be become less lucrative finally it would border on hubris to ignore the physical aspect of computer in this context in the word of mike acton software be not the platform hardware be the platform software people would be wise to study at least a little computer architecture and electrical engineering a big shake up in hardware such as the arrival of consumer grade quantum computer would will change everything about professional software engineering quantum computer be still a way off but the grow interest in gpu and the drive toward parallelization be an imminent shift cpu speed have be stagnant for several year now and in that time a seemingly unquenchable thirst for machine learning and big datum have emerge with more desire than ever to process large datum set openmp opencl go cuda and other parallel processing language and framework will continue to become mainstream to be competitively fast in the near term future significant parallelization will be a requirement across the board not just in high performance niche like operating system infrastructure and video game website be ubiquitous the 2017 stack overflow survey report that about 15 % of professional software engineer be work in an internet web service company the bureau of labor statistic expect growth in web development to continue much fast than average 24 % between 2014 and 2024 due to its visibility there have be a massive focus on solve the skill gap in this industry code bootcamp teach web development almost exclusively and web development online course have flood udemy udacity coursera and similar marketplace the combination of increase automation throughout the web development technology stack and the influx of new entry level programmer with an explicit focus on web development have lead some to predict a slide towards a blue collar market for software developer some have go far suggest that the push towards a blue collar market be a strategy architecte by big tech firm other of course say we re head for another bursting bubble change in demand for specific technology be not news language and framework be always rise and fall in technology web development in its current incarnation js be king will eventually go the way of web development of the early 2000 s remember flash what be new be that a lot of people be receive an education explicitly and solely in the current trendy web development framework before you decide to label yourself a react developer remember there be people who once identify themselves as flash developer bank your career on a specific language framework or technology be a game of roulette of course it s quite difficult to predict what technology will remain relevant but if you re go to go all in on something I suggest rely on the lindy effect and pick something like c that have already withstand the test of time the next generation will have a level of de facto tech literacy that generation x and even millennial do not have one outcome of this will be that use the next generation of cms tool will be a give these tool will get well and young worker will be well at use they this combination will definitely will bring down the value of low level it and web development skill as eager and skilled youngster enter the job market high school be catch on as well offer computer science and programming class — some well educate high school student will likely be enter the workforce as program intern immediately upon graduation another big group of newcomer to programming be mbas and datum analyst job listing which be once dominate by excel be start to list sql as a nice to have and even requirement tool such as tableau spotfire salesforce and other web base metric system continue to replace the spreadsheet as the primary tool for report generation if this continue more datum analyst will learn to use sql directly simply because it be easy than export the datum into a spreadsheet people look to climb the rank and out perform their peer in these role be take online course to learn about database and statistical programming language with these new skill they can begin to position themselves as datum scientist by learn a combination of machine learning and statistical library look at metis curriculum as a prime example of this path finally the number of people earn computer science and software engineering degree continue to climb purdue for example report that application to their cs program have double over five year cornell report a similar explosion of cs graduate this trend isn t surprising give the growth and ubiquity of software it s hard for young people to imagine that computer will play a small role in our future so why not study something that s go to give you job security a common argument in the industry nowadays be around the idea that the education you receive in a four year computer science program be mostly unnecessary cruft I have hear this argument repeatedly in the hall of bootcamps web development shop and online from big name in the field such as this piece by eric elliott the opposition view be popular as well with some go so far as say all programmer should earn a master s degree like eric elliott I think it s good that there be more option than ever to break into programming and a 4 year degree might not be the good option for many simultaneously I agree with william bain that the foundational skill which apply across programming discipline be crucial for career longevity and that it be still hard to find that information outside of university course I ve write previously about what skill I think aspire engineer should learn as a foundation of a long career and join bradfield in order to help share this knowledge code school of many shape and size be become ubiquitous and for good reason there be quite a lot you can learn about programming without get into the minutia of big o notation obscure datum structure and algorithmic trivium however while it s true that fresh graduate from stanford be compete for some job with fresh graduate from hack reactor it s only true in one or two sub industry code school and bootcamp graduate be not yet apply to work on embed system cryptography security robotic network infrastructure or ai research and development yet these field like web development be grow quickly some programming relate skill have already start their transition from rare skill to baseline expectation conversely the engineering that go into create beastly engine like aw be anything but common the big company drive technology forward — amazon google facebook nvidia space x and so on — be typically not look for people with a basic understanding of javascript aws serve billion of user per day to support that kind of load an aws infrastructure engineer need a deep knowledge of network protocol computer architecture and several year of relevant experience as with any discipline there be amateur and artisan these prestigious firm be solve research problem and building system that be truly push against the boundary of what be possible yet they still struggle to fill open role even while basic programming skill be increasingly common people who can write algorithm to predict change in genetic sequence that will yield a desire result be go to be highly valuable in the future people who can program satellite spacecraft and automate machinery will continue to be highly value these be not field that lend themselves as readily to a 3 month intensive program as front end web development at least not without significant prior experience because computer science start with the word computer it be assume that young people will all have an innate understanding of it by 2025 unfortunately the ubiquity of computer have not create a new generation of people who de facto understand mathematics computer science network infrastructure electrical engineering and so on computer literacy be not the same as the study of computation despite mathematic have exist since the dawn of time there be still a relatively small portion of the population with strong statistical literacy and computer science be similarly old euclid invent several algorithms one of which be use every time you make an https request ; the fact that we use https every time we login to a website do not automatically imbue anyone with a knowledge of how those protocol work more establish professional field often have a bimodal wage distribution a relatively small number of practitioner make quite a lot of money and the majority of they earn a good wage but do not find themselves in the top 1 % of earner the national association for law placement collect datum that can be use to visualize this phenomenon in stark clarity a huge share of law graduate make between $ 45 00 and $ 65 000 — a good wage but hardly the salary we associate with a top professional we tend to think that all law graduate be on track to become partner at a law firm when really there be many path paralegal clerk public defender judge legal service for business contract writing and so on computer science graduate also have many option for their professional practice from web development to embed system as a basic level of programming literacy continue to become an expectation rather than a nice to have I suspect a similar distribution will emerge in programming job while there will always be a cohort of programmer make a lot of money to push on the edge of technology there will be a grow body of middle class programmer power the new computer centric economy the average salary for web developer will surely decrease over time that say I suspect that the number of job for programmer in general will only continue to grow as worker supply begin to meet demand hopefully we will see a healthy boom in a variety of middle class programming job there will also continue to be a top professional salary available for those programmer who be redefine what be possible regardless of which cohort of programmer you re in a career in technology mean continue your education throughout your life if you want to stay in the second cohort of programmer you may want to invest in learn how to create the machine rather than simply operate they from a quick cheer to a stand ovation clap to show how much you enjoy this story a curious human on a quest to watch the world learn
Arvind N,9.5K,8,https://towardsdatascience.com/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153?source=tag_archive---------3----------------,thought after take the deeplearning ai course towards data science,update — feb 2nd 2018 when this blog post be write only 3 course have be release all 5 course in this specialization be now out I will have a follow up blog post soon between a full time job and a toddler at home I spend my spare time learn about the idea in cognitive science & ai once in a while a great paper video course come out and you re instantly hook andrew ng s new deeplearning ai course be like that shane carruth or rajnikanth movie that one yearn for naturally as soon as the course be release on coursera I register and spend the past 4 evening binge watch the lecture work through quiz and programming assignment dl practitioner and ml engineer typically spend most day work at an abstract kera or tensorflow level but it s nice to take a break once in a while to get down to the nut and bolt of learn algorithm and actually do back propagation by hand it be both fun and incredibly useful andrew ng s new adventure be a bottom up approach to teach neural network — powerful non linearity learning algorithm at a beginner mid level in classic ng style the course be deliver through a carefully choose curriculum neatly time video and precisely positioned information nugget andrew pick up from where his classic ml course leave off and introduce the idea of neural network use a single neuron logistic regression and slowly add complexity — more neuron and layer by the end of the 4 week course 1 a student be introduce to all the core idea require to build a dense neural network such as cost loss function learn iteratively use gradient descent and vectorize parallel python numpy implementation andrew patiently explain the requisite math and programming concept in a carefully plan order and a well regulated pace suitable for learner who could be rusty in math code lecture be deliver use presentation slide on which andrew write use digital pen it feel like an effective way to get the listener to focus I feel comfortable watch video at 1 25x or 1 5x speed quiz be place at the end of each lecture section and be in the multiple choice question format if you watch the video once you should be able to quickly answer all the quiz question you can attempt quiz multiple time and the system be design to keep your high score programming assignment be do via jupyter notebook — powerful browser base application assignment have a nice guide sequential structure and you be not require to write more than 2 3 line of code in each section if you understand the concept like vectorization intuitively you can complete most programming section with just 1 line of code after the assignment be code it take 1 button click to submit your code to the automate grading system which return your score in a few minute some assignment have time restriction — say three attempt in 8 hour etc jupyter notebook be well design and work without any issue instruction be precise and it feel like a polished product anyone interested in understand what neural network be how they work how to build they and the tool available to bring your idea to life if your math be rusty there be no need to worry — andrew explain all the require calculus and provide derivative at every occasion so that you can focus on build the network and concentrate on implement your idea in code if your programming be rusty there be a nice code assignment to teach you numpy but I recommend learn python first on codecademy let I explain this with an analogy assume you be try to learn how to drive a car jeremy s fast ai course put you in the driver seat from the get go he teach you to move the steering wheel press the brake accelerator etc then he slowly explain more detail about how the car work — why rotate the wheel make the car turn why press the brake pedal make you slow down and stop etc he keep get deep into the inner working of the car and by the end of the course you know how the internal combustion engine work how the fuel tank be design etc the goal of the course be to get you drive you can choose to stop at any point after you can drive reasonably well — there be no need to learn how to build repair the car andrew s dl course do all of this but in the complete opposite order he teach you about internal combustion engine first he keep add layer of abstraction and by the end of the course you be drive like an f1 racer the fast ai course mainly teach you the art of driving while andrew s course primarily teach you the engineering behind the car if you have not do any machine learning before this don t take this course first the good starting point be andrew s original ml course on coursera after you complete that course please try to complete part 1 of jeremy howard s excellent deep learning course jeremy teach deep learning top down which be essential for absolute beginner once you be comfortable create deep neural network it make sense to take this new deeplearning ai course specialization which fill up any gap in your understanding of the underlie detail and concept 2 andrew stress on the engineering aspect of deep learning and provide plenty of practical tip to save time and money — the third course in the dl specialization feel incredibly useful for my role as an architect lead engineering team 3 jargon be handle well andrew explain that an empirical process = trial & error — he be brutally honest about the reality of designing and train deep net at some point I feel he might have as well just call deep learning as glorify curve fit 4 squash all hype around dl and ai — andrew make restrain careful comment about proliferation of ai hype in the mainstream medium and by the end of the course it be pretty clear that dl be nothing like the terminator 5 wonderful boilerplate code that just work out of the box 6 excellent course structure 7 nice consistent and useful notation andrew strive to establish a fresh nomenclature for neural net and I feel he could be quite successful in this endeavor 8 style of teaching that be unique to andrew and carry over from ml — I could feel the same excitement I feel in 2013 when I take his original ml course 9 the interview with deep learning hero be refreshing — it be motivate and fun to hear personal story and anecdote I wish that he d say concretely more often 2 good tool be important and will help you accelerate your learning pace I buy a digital pen after see andrew teach with one it help I work more efficiently 3 there be a psychological reason why I recommend the fast ai course before this one once you find your passion you can learn uninhibite 4 you just get that dopamine rush each time you score full point 5 don t be scare by dl jargon hyperparameter = setting architecture topology = style etc or the math symbol if you take a leap of faith and pay attention to the lecture andrew show why the symbol and notation be actually quite useful they will soon become your tool of choice and you will wield they with style thank for reading and good wish update thank for the overwhelmingly positive response many people be ask I to explain gradient descent and the differential calculus I hope this help from a quick cheer to a stand ovation clap to show how much you enjoy this story interest in strong ai share concept idea and code
Berit Anderson,1.6K,20,https://medium.com/join-scout/the-rise-of-the-weaponized-ai-propaganda-machine-86dac61668b?source=tag_archive---------4----------------,the rise of the weaponize ai propaganda machine scout science fiction + journalism medium,by berit anderson and brett horvath this piece be originally publish at scout ai this be a propaganda machine it s target people individually to recruit they to an idea it s a level of social engineering that I ve never see before they re capture people and then keep they on an emotional leash and never let they go say professor jonathan albright albright an assistant professor and datum scientist at elon university start dig into fake news site after donald trump be elect president through extensive research and interview with albright and other key expert in the field include samuel woolley head of research at oxford university s computational propaganda project and martin moore director of the centre for the study of medium communication and power at king college it become clear to scout that this phenomenon be about much more than just a few fake news story it be a piece of a much big and dark puzzle — a weaponize ai propaganda machine be use to manipulate our opinion and behavior to advance specific political agenda by leverage automate emotional manipulation alongside swarm of bot facebook dark post a b testing and fake news network a company call cambridge analytica have activate an invisible machine that prey on the personality of individual voter to create large shift in public opinion many of these technology have be use individually to some effect before but together they make up a nearly impenetrable voter manipulation machine that be quickly become the new decide factor in election around the world most recently analytica help elect u s president donald trump secure a win for the brexit leave campaign and lead ted cruz s 2016 campaign surge shepherd he from the back of the gop primary pack to the front the company be own and control by conservative and alt right interest that be also deeply entwine in the trump administration the mercer family be both a major owner of cambridge analytica and one of trump s big donor steve bannon in addition to act as trump s chief strategist and a member of the white house security council be a cambridge analytica board member until recently analytica s cto be the act cto at the republican national convention presumably because of its alliance analytica have decline to work on any democratic campaign — at least in the u s it be however in final talk to help trump manage public opinion around his presidential policy and to expand sale for the trump organization cambridge analytica be now expand aggressively into u s commercial market and be also meet with right wing party and government in europe asia and latin america cambridge analytica isn t the only company that could pull this off — but it be the most powerful right now understand cambridge analytica and the big ai propaganda machine be essential for anyone who want to understand modern political power build a movement or keep from be manipulate the weaponize ai propaganda machine it represent have become the new prerequisite for political success in a world of polarization isolation troll and dark post there s be a wave of report on cambridge analytica itself and solid coverage of individual aspect of the machine — bot fake news microtargeting — but none so far that we have see that portray the intense collective power of these technology or the frightening level of influence they re likely to have on future election in the past political messaging and propaganda battle be arm race to weaponize narrative through new medium — wage in print on the radio and on tv this new wave have bring the world something exponentially more insidious — personalize adaptive and ultimately addictive propaganda silicon valley spend the last ten year build platform whose natural end state be digital addiction in 2016 trump and his ally hijack they we have enter a new political age at scout we believe that the future of constructive civic dialogue and free and open election depend on our ability to understand and anticipate it welcome to the age of weaponize ai propaganda any company can aggregate and purchase big datum but cambridge analytica have develop a model to translate that datum into a personality profile use to predict then ultimately change your behavior that model itself be develop by pay a cambridge psychology professor to copy the groundbreaking original research of his colleague through questionable method that violate amazon s term of service base on its origin cambridge analytica appear ready to capture and buy whatever datum it need to accomplish its end in 2013 dr michal kosinski then a phd candidate at the university of cambridge s psychometric center release a groundbreaking study announce a new model he and his colleague have spend year develop by correlate subject facebook like with their ocean score — a standard bearing personality questionnaire use by psychologist — the team be able to identify an individual s gender sexuality political belief and personality trait base only on what they have like on facebook accord to zurich s das magazine which profile kosinski in late 2016 with a mere ten like as input his model could appraise a person s character well than an average coworker with seventy it could know a subject well than a friend ; with 150 like well than their parent with 300 like kosinski s machine could predict a subject s behavior well than their partner with even more like it could exceed what a person think they know about themselves not long afterward kosinski be approach by aleksandr kogan a fellow cambridge professor in the psychology department about license his model to scl election a company that claim its specialty lie in manipulate election the offer would have mean a significant payout for kosinki s lab still he decline worried about the firm s intention and the downstream effect it could have it have take kosinski and his colleague year to develop that model but with his method and finding now out in the world there be little to stop scl election from replicate they it would seem they do just that accord to a guardian investigation in early 2014 just a few month after kosinski decline their offer scl partner with kogan instead as a part of their relationship kogan pay amazon mechanical turk worker $ 1 each to take the ocean quiz there be just one catch to take the quiz user be require to provide access to all of their facebook datum they be tell the datum would be use for research the job be report to amazon for violate the platform s term of service what many of the turk likely didn t realize accord to document review by the guardian kogan also capture the same datum for each person s unwitte friend the datum gather from kogan s study go on to birth cambridge analytica which spin out of scl election soon after the name metaphorically at least be a nod to kogan s work — and a dig at kosinski but that early trove of user datum be just the beginning — just the seed analytica need to build its own model for analyze user personality without have to rely on the lengthy ocean test after a successful proof of concept and back by wealthy conservative investor analytica go on a data shopping spree for the age snap up datum about your shopping habit land ownership where you attend church what store you visit what magazine you subscribe to — all of which be for sale from a range of datum broker and third party organization sell information about you analytica aggregate this datum with voter role publicly available online datum — include facebook like — and put it all into its predictive personality model nix like to boast that analytica s personality model have allow it to create a personality profile for every adult in the u s — 220 million of they each with up to 5 000 datum point and those profile be be continually update and improve the more datum you spew out online albright also believe that your facebook and twitter post be be collect and integrate back into cambridge analytica s personality profile twitter and also facebook be be use to collect a lot of responsive datum because people be impassione they reply they retweet but they also include basically their entire argument and their entire background on this topic he explain collect massive quantity of datum about voter personality might seem unsettling but it s actually not what set cambridge analytica apart for analytica and other company like they it s what they do with that datum that really matter your behavior be drive by your personality and actually the more you can understand about people s personality as psychological driver the more you can actually start to really tap in to why and how they make their decision nix explain to bloomberg s sasha issenburg we call this behavioral microtargeting and this be really our secret sauce if you like this be what we re bring to america use those dossier or psychographic profile as analytica call they cambridge analytica not only identifie which voter be most likely to swing for their cause or candidate ; they use that information to predict and then change their future behavior as vice report recently kosinski and a colleague be now work on a new set of research yet to be publish that address the effectiveness of these method their early finding use personality target facebook post can attract up to 63 percent more click and 1 400 more conversion scout reach out to cambridge analytica with a detailed list of question about their communication tactic but the company decline to answer any question or to comment on any of their tactic but researcher across the technology and medium ecosystem who have be follow cambridge analytica s political messaging activity have unearth an expansive adaptive online network that automate the manipulation of voter at a scale never before see in political messaging they the trump campaign be use 40 50 000 different variant of ad every day that be continuously measure response and then adapt and evolve base on that response martin moore director of king college s centre for the study of medium communication and power tell the guardian in early december it s all do completely opaquely and they can spend as much money as they like on particular location because you can focus on a five mile radius where traditional pollster might ask a person outright how they plan to vote analytica rely not on what they say but what they do track their online movement and interest and serve up multivariate ad design to change a person s behavior by prey on individual personality trait for example nix write in an op ed last year about analytica s work on the cruz campaign our issue model identify that there be a small pocket of voter in iowa who feel strongly that citizen should be require by law to show photo i d at polling station leverage our other datum model we be able to advise the campaign on how to approach this issue with specific individual base on their unique profile in order to use this relatively niche issue as a political pressure point to motivate they to go out and vote for cruz for people in the temperamental personality group who tend to dislike commitment messaging on the issue should take the line that show your i d to vote be as easy as buy a case of beer whereas the right message for people in the stoic traditionalist group who have strongly hold conventional view be that show your i d in order to vote be simply part of the privilege of live in a democracy for analytica the feedback be instant and the response automate do this specific swing voter in pennsylvania click on the ad attack clinton s negligence over her email server yes serve she more content that emphasize failure of personal responsibility no the automate script will try a different headline perhaps one that play on a different personality trait — say the voter s tendency to be agreeable toward authority figure perhaps top intelligence official agree clinton s email jeopardize national security much of this be do through facebook dark post which be only visible to those be target base on user response to these post cambridge analytica be able to identify which of trump s message be resonate and where that information be also use to shape trump s campaign travel schedule if 73 percent of target voter in kent county mich click on one of three article about bring back job schedule a trump rally in grand rapid that focus on economic recovery political analyst in the clinton campaign who be base their tactic on traditional polling method laugh when trump schedule campaign event in the so call blue wall — a group of state that include michigan pennsylvania and wisconsin and have traditionally fall to democrats but cambridge analytica see they have an opening base on measure engagement with their facebook post it be the small margin in michigan pennsylvania and wisconsin that win trump the election dark post be also use to depress voter turnout among key group of democratic voter in this election dark post be use to try to suppress the african american vote write journalist and open society fellow mckenzie funk in a new york times editorial accord to bloomberg the trump campaign send ad remind certain select black voter of hillary clinton s infamous super predator line it target miami s little haiti neighborhood with message about the clinton foundation s trouble in haiti after the 2010 earthquake because dark post be only visible to the target user there s no way for anyone outside of analytica or the trump campaign to track the content of these ad in this case there be no sec oversight no public scrutiny of trump s attack ad just the rapid eye movement of million of individual user scan their facebook feed in the week lead up to a final vote a campaign could launch a $ 10 100 million dark post campaign target just a few million voter in swing district and no one would know this may be where future black swan election upset be bear these company moore say have find a way of transgressing 150 year of legislation that we ve develop to make election fair and open meanwhile surprised by the result of the 2016 presidential race albright start look into the fake news problem as a part of his research albright scrape 306 fake news site to determine how exactly they be all connect to each other and the mainstream news ecosystem what he find be unprecedented — a network of 23 000 page and 1 3 million hyperlink the site in the fake news and hyper bias # mcm network albright write have a very small node size — this mean they be link out heavily to mainstream medium social network and informational resource most of which be in the center of the network but not many site in their peer group be send link back these site aren t own or operate by any one individual entity he say but together they have be able to game search engine optimization increase the visibility of fake and biased news anytime someone google an election relate term online — trump clinton jews muslim abortion obamacare this network albright write in a post explore his finding be trigger on demand to spread false hyper biased and politically loaded information even more shocking to he though be that this network of fake news create a powerful infrastructure for company like cambridge analytica to track voter and refine their personality target model I scrape the tracker on these site and I be absolutely dumbfound every time someone like one of these post on facebook or visit one of these website the script be then follow you around the web and this enable data mining and influence company like cambridge analytica to precisely target individual to follow they around the web and to send they highly personalise political message the web of fake and biased news that albright uncover create a propaganda wave that cambridge analytica could ride and then amplify the more fake news that user engage with the more addictive analytica s personality engagement algorithm can become voter 35423 click on a fake story about hillary s sex trafficking ring let s get she to engage with more story about hillary s suppose history of murder and sex trafficking the synergy between fake content network automate message testing and personality profiling will rapidly spread to other digital medium albright s most recent research focus on an artificial intelligence that automatically create youtube video about news and current event the ai which react to trend topic on facebook and twitter pair image and subtitle with a computer generate voiceover it spool out nearly 80 000 video through 19 different channel in just a few day give its rapid development the technology community need to anticipate how ai propaganda will soon be use for emotional manipulation in mobile messaging virtual reality and augmented reality if fake news create the scaffolding for this new automate political propaganda machine bot or fake social medium profile have become its foot soldier — an army of political robot use to control conversation on social medium and silence and intimidate journalist and other who might undermine their message samuel woolley director of research at the university of oxford s computational propaganda project and a fellow at google s jigsaw project have dedicate his career to study the role of bot in online political organizing — who create they how they re use and to what end research by woolley and his oxford base team in the lead up to the 2016 election find that pro trump political messaging rely heavily on bot to spread fake news and discredit hillary clinton by election day trump s bot outnumber hers 5 1 the use of automate account be deliberate and strategic throughout the election most clearly with pro trump campaigner and programmer who carefully adjust the timing of content production during the debate strategically colonize pro clinton hashtag and then disabled activity after election day the study by woolley s team report woolley believe it s likely that cambridge analytica be responsible for subcontract the creation of those trump bot though he say he doesn t have direct proof still if anyone outside of the trump campaign be qualified to speculate about who create those bot it would be woolley lead by dr philip howard the team s principal investigator woolley and his colleague have be track the use of bot in political organizing since 2010 that s when howard bury deep in research about the role twitter play in the arab spring first notice thousand of bot coopte hashtag use by protester curious he and his team begin reach out to hacker botmaker and political campaign get to know they and try to understand their work and motivation eventually those creator would come to make up an informal network of nearly 100 informant that have keep howard and his colleague in the know about these bot over the last few year before long howard and his team be get the head up about bot propaganda campaign from the creator themselves as more and more major international political figure begin use botnet as just another tool in their campaign howard woolley and the rest of their team study the action unfold the world these informant reveal be an international network of government consultancy often with owner or top management just one degree away from official government actor and individual who build and maintain massive network of bot to amplify the message of political actor spread message counter to those of their opponent and silence those whose view or idea might threaten those same political actor the chinese iranian and russian government employ their own social medium expert and pay small amount of money to large number of people to generate pro government message howard and his coauthor write in a 2015 research paper about the use of bot in the venezuelan election depend on which of those three category bot creator fall into — government consultancy or individual — they re just as likely to be motivate by political belief as they be the opportunity to auction off their network of digital influence to the high bidder not all bot be create equal the average run of the mill twitter bot be literally a robot — often program to retweet specific account to help popularize specific idea or viewpoint they also frequently respond automatically to twitter user who use certain keyword or hashtag — often with pre write slur insult or threat high end bot on the other hand be more analog operate by real people they assume fake identity with distinct personality and their response to other user online be specific intend to change their opinion or those of their follower by attack their viewpoint they have online friend and follower they re also far less likely to be discover — and their account deactivate — by facebook or twitter work on their own woolley estimate an individual could build and maintain up to 400 of these boutique twitter bot ; on facebook which he say be more effective at identify and shut down fake account an individual could manage 10 20 as a result these high quality botnet be often use for multiple political campaign during the brexit referendum the oxford team watch as one network of bot previously use to influence the conversation around the israeli palestinian conflict be reactivate to fight for the leave campaign individual profile be update to reflect the new debate their personal tagline change to ally with their new allegiance — and away they go russia s bot army have be the subject of particular scrutiny since a cia special report reveal that russia have be work to influence the election in trump s favor recently reporter comedian samantha bee travel to moscow to interview two pay russian troll operator clothe in black ski mask to obscure their identity the two talk with bee about how and why they be use their account during the u s election they tell bee that they pose as americans online and target site like the wall street journal the new york post the washington post facebook and twitter their goal they say be to piss off other social medium user change their opinion and silence their opponent or to put it in the word of russian troll # 1 when your opponent just shut up the 2016 u s election be over but the weaponize ai propaganda machine be just warm up and while each of its component would be worry on its own together they represent the arrival of a new era in political messaging — a steel wall between campaign winner and loser that can only be mount by gather more datum create well personality analyse rapid development of engagement ai and hire more troll at the moment trump and cambridge analytica be lap their opponent the more datum they gather about individual the more analytica and by extension trump s presidency will benefit from the network effect of their work — and the hard it will become to counter or fight back against their messaging in the court of public opinion each tweet that echo forth from the @realdonaldtrump and @potus account announce and defend the administration s move be meet with a chorus of protest and argument but even that negative engagement become a valuable asset for the trump administration because every impulsive tweet can be treat like a psychographic experiment trump s first few week in office may have seem bumble but they represent a clear signal of what lie ahead for trump s presidency — an executive order design to enrage and distract his opponent as he and bannon move to strip power from the judicial branch install bannon himself on the national security council and issue a series of unconstitutional gag order to federal agency cambridge analytica may be slate to secure more federal contract and be likely about to begin manage white house digital communication for the rest of the trump administration what new predictive personality target become possible with potential access to datum on u s voter from the irs department of homeland security or the nsa lenin want to destroy the state and that s my goal too I want to bring everything crash down and destroy all of today s establishment bannon say in 2013 we know that steve bannon subscribe to a theory of history where a messianic grey warrior consolidate power and remake the global order bolster by the success of brexit and the trump victory breitbart of which bannon be executive chair until trump s election and cambridge analytica which bannon sit on the board of be now bring fake news and automate propaganda to support far right party in at least germany france hungary and india as well as part of south america never have such a radical international political movement have the precision and power of this kind of propaganda technology whether or not leader engineer designer and investor in the technology community respond to this threat will shape major aspect of global politic for the foreseeable future the future of politic will not be a war of candidate or even cash on hand and it s not even about big datum as some have argue everyone will have access to big datum — as hillary do in the 2016 election from now on the distinguish factor between those who win election and those who lose they will be how a candidate use that datum to refine their machine learning algorithm and automate engagement tactic election in 2018 and 2020 win t be a contest of idea but a battle of automate behavior change the fight for the future will be a proxy war of machine learn it will be wage online in secret and with the unwitting help of all of you anyone who want to effect change need to understand this new reality it s only by understand this — and by build well automate engagement system that amplify genuine human passion rather than manipulate it — that other candidate and cause around the globe will be able to compete implication # 1 public sentiment turn into high frequency trading thank to stock trading algorithm large portion of public stock and commodity market no long resemble a human system and some would argue no long serve their purpose as a signal of value instead they re a battleground for high frequency trading algorithm attempt to influence price or find nano leverage in price position in the near future we may see a similar process unfold in our public debate instead of battle press conference and opinion article public opinion about company and politician may turn into multi billion dollar battle between compete algorithm each deploy to sway public sentiment stock trading algorithm already exist that analyze million of tweet and online post in real time and make trade in a matter of millisecond base on change in public sentiment algorithmic trading and algorithmic public opinion be already connect it s likely they will continue to converge implication # 2 personalize automate propaganda that adapt to your weakness what if president trump s 2020 re election campaign didn t just have the good political messaging but 250 million algorithmic version of their political message all update in real time personalize to precisely fit the worldview and attack the insecurity of their target instead of have to deal with mislead politician we may soon witness a cambrian explosion of pathologically lie political and corporate bot that constantly improve at manipulate we implication # 3 not just a bubble but trap in your own ideological matrix imagine that in 2020 you find out that your favorite politics page or group on facebook didn t actually have any other human member but be fill with dozen or hundred of bot that make you feel at home and your opinion validate be it possible that you might never find out correction an early version of this story mistakenly refer to steve bannon as the owner of breitbart news until trump s election bannon serve as the executive chair of breitbart a position in which it be common to assume ownership through stock holding this story have be update to reflect that from a quick cheer to a stand ovation clap to show how much you enjoy this story ceo & co founder @join_scout the social implication of technology
Slav Ivanov,4.4K,10,https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607?source=tag_archive---------5----------------,37 reason why your neural network be not work slav,the network have be train for the last 12 hour it all look good the gradient be flow and the loss be decrease but then come the prediction all zero all background nothing detect what do I do wrong — I ask my computer who didn t answer where do you start check if your model be output garbage for example predict the mean of all output or it have really poor accuracy a network might not be train for a number of reason over the course of many debug session I would often find myself do the same check I ve compile my experience along with the good idea around in this handy list I hope they would be of use to you too a lot of thing can go wrong but some of they be more likely to be break than other I usually start with this short list as an emergency first response if the step above don t do it start go down the follow big list and verify thing one by one check if the input datum you be feed the network make sense for example I ve more than once mix the width and the height of an image sometimes I would feed all zero by mistake or I would use the same batch over and over so print display a couple of batch of input and target output and make sure they be ok try pass random number instead of actual datum and see if the error behave the same way if it do it s a sure sign that your net be turn datum into garbage at some point try debug layer by layer op by op and see where thing go wrong your datum might be fine but the code that pass the input to the net might be break print the input of the first layer before any operation and check it check if a few input sample have the correct label also make sure shuffle input sample work the same way for output label maybe the non random part of the relationship between the input and output be too small compare to the random part one could argue that stock price be like this I e the input be not sufficiently relate to the output there isn t an universal way to detect this as it depend on the nature of the datum this happen to I once when I scrape an image dataset off a food site there be so many bad label that the network couldn t learn check a bunch of input sample manually and see if label seem off the cutoff point be up for debate as this paper get above 50 % accuracy on mnist use 50 % corrupt label if your dataset hasn t be shuffle and have a particular order to it order by label this could negatively impact the learn shuffle your dataset to avoid this make sure you be shuffle input and label together be there a 1000 class a image for every class b image then you might need to balance your loss function or try other class imbalance approach if you be train a net from scratch I e not finetune you probably need lot of datum for image classification people say you need a 1000 image per class or more this can happen in a sorted dataset I e the first 10k sample contain the same class easily fixable by shuffle the dataset this paper point out that have a very large batch can reduce the generalization ability of the model thank to @hengcherkeng for this one do you standardize your input to have zero mean and unit variance augmentation have a regularize effect too much of this combine with other form of regularization weight l2 dropout etc can cause the net to underfit if you be use a pretraine model make sure you be use the same normalization and preprocessing as the model be when train for example should an image pixel be in the range 0 1 1 1 or 0 255 cs231n point out a common pitfall also check for different preprocessing in each sample or batch this will help with find where the issue be for example if the target output be an object class and coordinate try limit the prediction to object class only again from the excellent cs231n initialize with small parameter without regularization for example if we have 10 class at chance mean we will get the correct class 10 % of the time and the softmax loss be the negative log probability of the correct class so ln 0 1 = 2 302 after this try increase the regularization strength which should increase the loss if you implement your own loss function check it for bug and add unit test often my loss would be slightly incorrect and hurt the performance of the network in a subtle way if you be use a loss function provide by your framework make sure you be pass to it what it expect for example in pytorch I would mix up the nllloss and crossentropyloss as the former require a softmax input and the latter doesn t if your loss be compose of several small loss function make sure their magnitude relative to each be correct this might involve test different combination of loss weight sometimes the loss be not the good predictor of whether your network be train properly if you can use other metric like accuracy do you implement any of the layer in the network yourself check and double check to make sure they be work as intend check if you unintentionally disabled gradient update for some layer variable that should be learnable maybe the expressive power of your network be not enough to capture the target function try add more layer or more hide unit in fully connect layer if your input look like k h w = 64 64 64 it s easy to miss error relate to wrong dimension use weird number for input dimension for example different prime number for each dimension and check how they propagate through the network if you implement gradient descent by hand gradient checking make sure that your backpropagation work like it should more info 1 2 3 overfit a small subset of the datum and make sure it work for example train with just 1 or 2 example and see if your network can learn to differentiate these move on to more sample per class if unsure use xavier or he initialization also your initialization might be lead you to a bad local minimum so try a different initialization and see if it help maybe you use a particularly bad set of hyperparameter if feasible try a grid search too much regularization can cause the network to underfit badly reduce regularization such as dropout batch norm weight bias l2 regularization etc in the excellent practical deep learning for coder course jeremy howard advise get rid of underfitte first this mean you overfit the training datum sufficiently and only then address overfitte maybe your network need more time to train before it start make meaningful prediction if your loss be steadily decrease let it train some more some framework have layer like batch norm dropout and other layer behave differently during training and testing switching to the appropriate mode might help your network to predict properly your choice of optimizer shouldn t prevent your network from training unless you have select particularly bad hyperparameter however the proper optimizer for a task can be helpful in get the most training in the short amount of time the paper which describe the algorithm you be use should specify the optimizer if not I tend to use adam or plain sgd with momentum check this excellent post by sebastian ruder to learn more about gradient descent optimizer a low learning rate will cause your model to converge very slowly a high learning rate will quickly decrease the loss in the beginning but might have a hard time find a good solution play around with your current learning rate by multiply it by 0 1 or 10 get a nan non a number be a much big issue when training rnn from what I hear some approach to fix it do I miss anything be anything wrong let I know by leave a reply below from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur hacker machine learn deep learning and other type of learn
Keval Patel,833,7,https://becominghuman.ai/turn-your-raspberry-pi-into-homemade-google-home-9e29ad220075?source=tag_archive---------6----------------,turn your raspberry pi into homemade google home become human artificial intelligence magazine,google home be a beautiful device with build in google assistant — a state of the art digital personal assistant by google — which you can place anywhere at your home and it will do some amazing thing for you it will save your reminder shopping list note and most importantly answer your question and query base on the context of the conversation in this article you be go to learn to turn your raspberry pi into homemade google home device which be so let s get start once you have all these thing login to raspbian desktop and go to the follow step one by one as you can see your usb device be attach to card 1 and the device i d be 0 raspberry pi recognize card 0 as the internal sound card which be bcm2835 and other external sound card as external sound card this will set your external mic see pcm mic as the audio capture device see in pcm default and your inbuilt sound card card 0 as the speaker device this will create python 3 environment as the google assistant library run on python 3 x only in your raspberry pi and install require dependency if instead it display invalidgranterror then an invalid code be enter try again you can run google assistant init sh to initiate the google assistant any time 1 autostart with pixel desktop on boot 2 autostart with cli on boot you can do many daily stuff with your google home if you want to perform your custom task like turn off the light open the door you can do it with integrate google action in your google assistant if you have any trouble with start the google assistant leave a comment below I will try to resolve they ~if you like the article click the 💚 below so more people can see it also you can follow I on medium or on my blog so you get update regard my future article ~ from a quick cheer to a stand ovation clap to show how much you enjoy this story www kevalpatel2106 com | android developer | machine learner | gopher | open source contributor late news info and tutorial on artificial intelligence machine learn deep learn big datum and what it mean for humanity
Eduard Tyantov,5.4K,19,https://blog.statsbot.co/deep-learning-achievements-4c563e034257?source=tag_archive---------7----------------,deep learning achievement over the past year stat and bot,at statsbot we re constantly review the deep learning achievement to improve our model and product around christmas time our team decide to take stock of the recent achievement in deep learning over the past year and a bit long we translate the article by a data scientist ed tyantov to tell you about the most significant development that can affect our future almost a year ago google announce the launch of a new model for google translate the company describe in detail the network architecture — recurrent neural network rnn the key outcome close down the gap with human in accuracy of the translation by 55 85 % estimate by people on a 6 point scale it be difficult to reproduce good result with this model without the huge dataset that google have you probably hear the silly news that facebook turn off its chatbot which go out of control and make up its own language this chatbot be create by the company for negotiation its purpose be to conduct text negotiation with another agent and reach a deal how to divide item book hat etc by two each agent have his own goal in the negotiation that the other do not know about it s impossible to leave the negotiation without a deal for training they collect a dataset of human negotiation and train a supervised recurrent network then they take a reinforcement learning train agent and train it to talk with itself set a limit — the similarity of the language to human the bot have learn one of the real negotiation strategy — show a fake interest in certain aspect of the deal only to give up on they later and benefit from its real goal it have be the first attempt to create such an interactive bot and it be quite successful full story be in this article and the code be publicly available certainly the news that the bot have allegedly invent a language be inflate from scratch when training in negotiation with the same agent they disable the restriction of the similarity of the text to human and the algorithm modify the language of interaction nothing unusual over the past year recurrent network have be actively develop and use in many task and application the architecture of rnn have become much more complicated but in some area similar result be achieve by simple feedforward network — dssm for example google have reach the same quality as with lstm previously for its mail feature smart reply in addition yandex launch a new search engine base on such network employee of deepmind report in their article about generate audio briefly researcher make an autoregressive full convolution wavenet model base on previous approach to image generation pixelrnn and pixelcnn the network be train end to end text for the input audio for the output the research get an excellent result as the difference compare to human have be reduce by 50 % the main disadvantage of the network be a low productivity as because of the autoregression sound be generate sequentially and it take about 1 2 minute to create one second of audio look at sorry hear this example if you remove the dependence of the network on the input text and leave only the dependence on the previously generate phoneme then the network will generate phoneme similar to the human language but they will be meaningless hear the example of the generate voice this same model can be apply not only to speech but also for example to create music imagine audio generate by the model which be teach use the dataset of a piano game again without any dependence on the input datum read a full version of deepmind research if you re interested lip reading be another deep learning achievement and victory over human google deepmind in collaboration with oxford university report in the article lip read sentence in the wild on how their model which have be train on a television dataset be able to surpass the professional lip reader from the bbc channel there be 100 000 sentence with audio and video in the dataset model lstm on audio and cnn + lstm on video these two state vector be feed to the final lstm which generate the result character different type of input datum be use during train audio video and audio + video in other word it be an omnichannel model the university of washington have do a serious job of generate the lip movement of former us president obama the choice fall on he due to the huge number of his performance recording online 17 hour of hd video they couldn t get along with just the network as they get too many artifact therefore the author of the article make several crutch or trick if you like to improve the texture and timing you can see that the result be amazing soon you couldn t trust even the video with the president in their post and article google brain team report on how they introduce a new ocr optical character recognition engine into its map through which street sign and store sign be recognize in the process of technology development the company compile a new fsns french street name sign which contain many complex case to recognize each sign the network use up to four of its photo the feature be extract with the cnn scale with the help of the spatial attention pixel coordinate be take into account and the result be feed to the lstm the same approach be apply to the task of recognize store name on signboard there can be a lot of noise datum and the network itself must focus in the right place this algorithm be apply to 80 billion photo there be a type of task call visual reasoning where a neural network be ask to answer a question use a photo for example be there a same size rubber thing in the picture as a yellow metal cylinder the question be truly nontrivial and until recently the problem be solve with an accuracy of only 68 5 % and again the breakthrough be achieve by the team from deepmind on the clevr dataset they reach a super human accuracy of 95 5 % the network architecture be very interesting an interesting application of neural network be create by the company uizard generate a layout code accord to a screenshot from the interface designer this be an extremely useful application of neural network which can make life easy when develop software the author claim that they reach 77 % accuracy however this be still under research and there be no talk on real usage yet there be no code or dataset in open source but they promise to upload it perhaps you ve see quick draw from google where the goal be to draw sketch of various object in 20 second the corporation collect this dataset in order to teach the neural network to draw as google describe in their blog and article the collect dataset consist of 70 thousand sketch which eventually become publicly available sketch be not picture but detailed vector representation of drawing at which point the user press the pencil release where the line be draw and so on researcher have train the sequence to sequence variational autoencoder vae use rnn as a code decoding mechanism eventually as befit the auto encoder the model receive a latent vector that characterize the original picture whereas the decoder can extract a drawing from this vector you can change it and get new sketch and even perform vector arithmetic to create a catpig one of the hot topic in deep learning be generative adversarial network gan most often this idea be use to work with image so I will explain the concept use they the idea be in the competition of two network — the generator and the discriminator the first network create a picture and the second one try to understand whether the picture be real or generate schematically it look like this during train the generator from a random vector noise generate an image and feed it to the input of the discriminator which say whether it be fake or not the discriminator be also give real image from the dataset it be difficult to train such construction as it be hard to find the equilibrium point of two network most often the discriminator win and the training stagnate however the advantage of the system be that we can solve problem in which it be difficult for we to set the loss function for example improve the quality of the photo — we give it to the discriminator a classic example of the gan training result be picture of bedroom or people previously we consider the auto code sketch rnn which encode the original datum into a latent representation the same thing happen with the generator the idea of generate an image use a vector be clearly show in this project in the example of face you can change the vector and see how the face change the same arithmetic work over the latent space a man in glass minus a man plus a woman be equal to a woman with glass if you teach a control parameter to the latent vector during training when you generate it you can change it and so manage the necessary image in the picture this approach be call conditional gan so do the author of the article face age with conditional generative adversarial network have train the engine on the imdb dataset with a know age of actor the researcher be give the opportunity to change the face age of the person google have find another interesting application to gan — the choice and improvement of photo gan be train on a professional photo dataset the generator be try to improve bad photo professionally shoot and degrade with the help of special filter and the discriminator — to distinguish improved photo and real professional one a train algorithm go through google street view panorama in search of the good composition and receive some picture of professional and semi professional quality as per photographer rate an impressive example of gan be generate image use text the author of this research suggest embed text into the input of not only a generator conditional gan but also a discriminator so that it verify the correspondence of the text to the picture in order to make sure the discriminator learn to perform his function in addition to training they add pair with an incorrect text for the real picture one of the eye catch article of 2016 be image to image translation with conditional adversarial network by berkeley ai research bair researcher solve the problem of image to image generation when for example it be require to create a map use a satellite image or realistic texture of the object use their sketch here be another example of the successful performance of conditional gan in this case the condition go to the whole picture popular in image segmentation unet be use as the architecture of the generator and a new patchgan classifier be use as a discriminator for combat blurred image the picture be cut into n patch and the prediction of fake real go for each of they separately christopher hesse make the nightmare cat demo which attract great interest from the user you can find a source code here in order to apply pix2pix you need a dataset with the corresponding pair of picture from different domain in the case for example with card it be not a problem to assemble such a dataset however if you want to do something more complicated like transfiguring object or styling then pair of object can not be find in principle therefore author of pix2pix decide to develop their idea and come up with cyclegan for transfer between different domain of image without specific pair — unpaired image to image translation the idea be to teach two pair of generator discriminator to transfer the image from one domain to another and back while we require a cycle consistency — after a sequential application of the generator we should get an image similar to the original l1 loss a cyclic loss be require to ensure that the generator do not just begin to transfer picture of one domain to picture from another domain which be completely unrelated to the original image this approach allow you to learn the mapping of horse > zebras such transformation be unstable and often create unsuccessful option you can find a source code here machine learning be now come to medicine in addition to recognize ultrasound mri and diagnosis it can be use to find new drug to fight cancer we already report in detail about this research briefly with the help of adversarial autoencoder aae you can learn the latent representation of molecule and then use it to search for new one as a result 69 molecule be find half of which be use to fight cancer and the other have serious potential topic with adversarial attack be actively explore what be adversarial attack standard network train for example on imagenet be completely unstable when add special noise to the classified picture in the example below we see that the picture with noise for the human eye be practically unchanged but the model go crazy and predict a completely different class stability be achieve with for example the fast gradient sign method fgsm have access to the parameter of the model you can make one or several gradient step towards the desire class and change the original picture one of the task on kaggle be relate to this the participant be encourage to create universal attack defense which be all eventually run against each other to determine the good why should we even investigate these attack first if we want to protect our product we can add noise to the captcha to prevent spammer from recognize it automatically secondly algorithm be more and more involved in our life — face recognition system and self drive car in this case attacker can use the shortcoming of the algorithm here be an example of when special glass allow you to deceive the face recognition system and pass yourself off as another person so we need to take possible attack into account when teach model such manipulation with sign also do not allow they to be recognize correctly • a set of article from the organizer of the contest • already write library for attack cleverhan and foolbox reinforcement learning rl or learn with reinforcement be also one of the most interesting and actively develop approach in machine learn the essence of the approach be to learn the successful behavior of the agent in an environment that give a reward through experience — just as people learn throughout their life rl be actively use in game robot and system management traffic for example of course everyone have hear about alphago s victory in the game over the good professional researcher be use rl for train the bot play with itself to improve its strategy in previous year deepmind have learn use dqn to play arcade game well than human currently algorithm be be teach to play more complex game like doom much of the attention be pay to learn acceleration because experience of the agent in interaction with the environment require many hour of training on modern gpu in his blog deepmind report that the introduction of additional loss auxiliary task such as the prediction of a frame change pixel control so that the agent well understand the consequence of the action significantly speed up learn learning result 4 2 learn robotsin openai they have be actively study an agent s training by human in a virtual environment which be safe for experiment than in real life in one of the study the team show that one shot learning be possible a person show in vr how to perform a certain task and one demonstration be enough for the algorithm to learn it and then reproduce it in real condition if only it be so easy with people here be the work of openai and deepmind on the same topic the bottom line be that an agent have a task the algorithm provide two possible solution for the human and indicate which one be well the process be repeat iteratively and the algorithm for 900 bit of feedback binary markup from the person learn how to solve the problem as always the human must be careful and think of what he be teach the machine for example the evaluator decide that the algorithm really want to take the object but in fact he just simulate this action there be another study from deepmind to teach the robot complex behavior walk jump etc and even do it similar to the human you have to be heavily involve with the choice of the loss function which will encourage the desire behavior however it would be preferable that the algorithm learn complex behavior itself by lean with simple reward researcher manage to achieve this they teach agent body emulator to perform complex action by construct a complex environment with obstacle and with a simple reward for progress in movement you can watch the impressive video with result however it s much more fun to watch it with a superimposed sound finally I will give a link to the recently publish algorithm for learn rl from openai now you can use more advanced solution than the standard dqn in july 2017 google report that it take advantage of deepmind s development in machine learning to reduce the energy cost of its data center base on the information from thousand of sensor in the datum center google developer train a neural network ensemble to predict pue power usage effectiveness and more efficient data center management this be an impressive and significant example of the practical application of ml as you know train model be poorly transfer from task to task as each task have to be train for a specific model a small step towards the universality of the model be do by google brain in his article one model to learn the all researcher have train a model that perform eight task from different domain text speech and image for example translation from different language text parse and image and sound recognition in order to achieve this they build a complex network architecture with various block to process different input datum and generate a result the block for the encoder decoder fall into three type convolution attention and gate mixture of expert moe main result of learn by the way this model be present in tensor2tensor in their post facebook staff tell we how their engineer be able to teach the resnet 50 model on imagenet in just one hour truth be tell this require a cluster of 256 gpu tesla p100 they use gloo and caffe2 for distribute learn to make the process effective it be necessary to adapt the learning strategy with a huge batch 8192 element gradient average warm up phase special learning rate etc as a result it be possible to achieve an efficiency of 90 % when scale from 8 to 256 gpu now researcher from facebook can experiment even fast unlike mere mortal without such a cluster the self drive car sphere be intensively develop and the car be actively test from the relatively recent event we can note the purchase of intel mobileye the scandal around uber and google technology steal by their former employee the first death when use an autopilot and much more I will note one thing google waymo be launch a beta program google be a pioneer in this field and it be assume that their technology be very good because car have be drive more than 3 million mile as to more recent event self drive car have be allow to travel across all us state as I say modern ml be begin to be introduce into medicine for example google collaborate with a medical center to help with diagnosis deepmind have even establish a separate unit this year under the program of the data science bowl there be a competition hold to predict lung cancer in a year on the basis of detailed image with a prize fund of one million dollar currently there be heavy investment in ml as it be before with bigdata china invest $ 150 billion in ai to become the world leader in the industry for comparison baidu research employ 1 300 people and in the same fair facebook — 80 at the last kdd alibaba employee talk about their parameter server kungpeng which run on 100 billion sample with a trillion parameter which become a common task © you can draw your own conclusion it s never too late to study machine learning in one way or another over time all developer will use machine learning which will become one of the common skill as it be today — the ability to work with database link to the original post from a quick cheer to a stand ovation clap to show how much you enjoy this story mail ru group head of machine learning team datum story on machine learning and analytic from statsbot s maker
Maruti Techlabs,552,5,https://chatbotsmagazine.com/which-are-the-best-intelligent-chatbots-or-ai-chatbots-available-online-cc49c0f3569d?source=tag_archive---------8----------------,what be the good intelligent chatbot or ai chatbot available online,how do we define the intelligence of a chatbot you can see a lot of article about what would make a chatbot appear intelligent a chatbot be intelligent when it become aware of user need its intelligence be what give the chatbot the ability to handle any scenario of a conversation with ease be the travel bot or the weather bot that have button that you click and give you some query artificially intelligent definitely but they be just not far along the conversation axis it can be a wonderfully design conversational interface that be smooth and easy to use it could be natural language processing and understanding where it be able to understand sentence that you structure in the wrong way now it be easy than ever to make a bot from scratch also chatbot development platform like chatfuel gupshup make it fairly simple to build a chatbot without a technical background hence make the reach for chatbot easy and transparent to anyone who would like to have one for their business for more understanding on intelligent chatbot read our blog the good ai base chatbot available online be mitsuku rise poncho right click insomno bot dr ai and melody this chatbot be one the good ai chatbot and it s my favorite too evidently it be the current winner of loebner prize the loebner prize be an annual competition in artificial intelligence that award prize to the chatterbot consider by the judge to be the most human like the format of the competition be that of a standard turing test you can talk with mitsuku for hour without get bore it reply to your question in the most humane way and understand your mood with the language you re use it be a bot make to chat about anything which be one of the main reason that make it so human like — contrary to other chatbot that be make for a specific task rise be a chatbot and a very good one — she win recognition this past saturday as the most human like chatbot in a competition describe as the first turing test the loebner prize in 2014 and 2015 right click be a startup that introduce an a i powered chatbot that create website it ask general question during the conversation like what industry you belong to and why do you want to make a website and create customize template as per the give answer hira saeed try to divert it from its job by ask it about love but what a smart player it be by reply to each of her query it try to bring she back to the actual job of website creation the process be short but keep you hooked poncho be a messenger bot design to be your one and only weather expert it send alert up to twice a day with user consent and be intelligent enough to answer question like should I take an umbrella today read poncho developer s piece think differently when build bot insomno bot be for night owl as the name suggest it be for all people out there who have trouble sleep this bot talk to you when you have no one around and give you amazing reply so that you win t get bore it s not something that will help you count star when you can t sleep or help you with read suggestion but this bot talk to you about anything it ask about symptom body parameter and medical history then compile a list of the most and least likely cause for the symptom and rank they by order of seriousness it live inside the exist biadu doctor app this app collect medical information from people and then pass it to doctor in a form that make it easy to use for diagnostic purpose or to otherwise respond to feature cbm the future healthcare and conversational ui these be just the basic version of intelligent chatbot there be many more intelligent chatbot out there which provide a much more smart approach to respond to query since the process of make a intelligent chatbot be not a big task most of we can achieve it with the most basic technical knowledge many of which will be very extremely helpful in the service industry and also help provide a well customer experience the most important part of any chatbot be the conversation it have with its user hence more effort have to be put in design a chatbot conversation hope you have a good read to know more about chatbot and how they converse with people visit the link below feature cbm how to make a chatbot intelligent if you resonate with this article please subscribe to our newsletter you will get a free copy of our case study on business automation through our bot solution from a quick cheer to a stand ovation clap to show how much you enjoy this story professional team deliver enterprise software solution — bot development big datum analytic web & mobile app and ai & ml integration chatbots ai nlp facebook messenger slack telegram and more
Jerry Chen,2.3K,11,https://news.greylock.com/the-new-moats-53f61aeac2d9?source=tag_archive---------9----------------,the new moat greylock perspective,to build a sustainable and profitable business you need strong defensive moat around your company this ring especially true today as we undergo one of the large platform shift in a generation as application move to the cloud be consume on iphone echo and tesla be build on open source and be fuel by ai and datum these dramatic shift be render some exist moat useless and leave ceo feel like it s almost impossible to build a defensible business in this post I ll review some of the traditional economic moat that technology company typically leverage and how they be be disrupt I believe that startup today need to build system of intelligencetm — ai powered application — the new moat business can build several different moat and over time these moat can change the follow list be definitely not exhaustive and fair warning it will read like a bad b school blog some of the great and most endure technology company be defend by powerful moat for example microsoft google and facebook all have moat build on economy of scale and network effect one of the most successful cloud business amazon web service aws have both the advantage of scale but also the power of network effect more app and service be build natively on aws because that s where the customer and the datum be in turn the ecosystem of solution attract more customer and developer who build more app that generate more datum continue the virtuous cycle while drive down amazon s cost through the advantage of scale strong moat help company survive through major platform shift but survive should not be confuse with thrive for example high switching cost can partly account for why mainframe and big iron system be still around after all these year legacy business with deep moat may not be the high growth vehicle of their prime but they be still generate profit company need to recognize and react when they be in the midst of an industry wide transformation lest they become victim of their own success moreover these massive platform shift — like cloud and mobile — be technology tidal wave that create opening for new player and enable founder to build path over and around exist moat startup founder who succeed tend to execute a dual pronged strategy 1 attack legacy player moat and 2 simultaneously build their own defensible moat that ride the new wave for example facebook have the most entrenched social network but instagram build a mobile first photo app that ride the smartphone wave to a $ 1b acquisition in the enterprise world saas company like salesforce be disrupt on premise software company like oracle now with the advent of cloud aws azure and google cloud be create a direct channel to the customer these platform shift can also change the buyer and end user within the enterprise the buyer have move from a central it team to an office knowledge worker to someone with an iphone to any developer with a github account in this current wave of disruption be it still possible to build sustainable moat for founder it may feel like every advantage you build can be replicate by another team down the street or at the very least it feel like moat can only be build at massive scale open source tool and cloud have push power to the new incumbent — the current generation of company that be at massive scale have strong distribution network high switching cost and strong brand work for they these be company like apple facebook google amazon and salesforce why do it feel like there be no more moat to build in an era of cloud and open source deep technology attack hard problem be become a shallow moat the use of open source be make it hard to monetize technology advance while the use of cloud to deliver technology be move defensibility to different part of the product company that focus too much on technology without put it in context of a customer problem will be catch between a rock and a hard place — or as I like to say between open source and a cloud place for example incumbent technology like oracle s proprietary database be be attack from open source alternative like hadoop and mongodb and in the cloud by amazon aurora and innovation like google spanner on the other hand company that build great customer experience may find defensibility through the workflow of their software I believe that deep technology moat aren t completely go and defensible business model can still be build around ip if you pick a place in the technology stack and become the absolute good of breed solution you can create a valuable company however this mean pick a technical problem with few substitute that require hard engineering and need operational knowledge to scale today the market be favor full stack company saas offering that offer application logic middleware and database combine technology be become an invisible component of a complete solution e g no one care what database back your favorite mobile app as long as your food be deliver on time in the consumer world apple make the integrated or full stack experience popular with the iphone which seamlessly integrate hardware with software this integrated experience be come to dominate enterprise software as well cloud and saas have make it possible to reach customer directly and in a cost effective manner as a result customer be increasingly buy full stack technology in the form of saas application instead of buy individual piece of the tech stack and build their own app the emphasis on the whole application experience or the top of the technology stack be why I also evaluate company through an additional framework the stack of enterprise system at the bottom of the stack of system be usually a database on top of which an application be build if the datum and app power a critical business function it become a system of record there be three major system of record in an enterprise your customer your employee and your asset crm own your customer hcm own your employee and erp financial own your asset generation of company have be build around own a system of record and every wave produce a new winner in crm we see salesforce replace siebel as the system of record for customer data and workday replace oracle peoplesoft for employee datum workday have also expand into financial datum other application can be build around a system of record but be usually not as valuable as the actual system of record for example marketing automation company like marketo and responsy build big business around crm but never become as strategic or as valuable as salesforce system of engagementtm be the interface between user and the system of record and can be powerful business because they control the end user interaction in the mainframe era the system of record and engagement be tie together when the mainframe and terminal be essentially the same product the client server wave usher in a class of company that try to own your desktop only to be disrupt by a generation of browser base company only to be succeed by mobile first company the current generation of company vie to own the system of engagement include slack amazon alexa and every other speech text conversational ui startup in china wechat have become a dominant system of engagement and be now a platform for everything from e commerce to game if it sound like system of engagementtm turn over more than system of record it s probably because they do the successive generation of system of engagementtm don t necessarily disappear but instead user keep add new way to interact with their application in a multi channel world own the system of engagement be most valuable if you control most of the end user engagement or be a cross channel system that reach user wherever they be perhaps the most strategic advantage of be a system of engagement be that you can coexist with several system of record and collect all the datum that pass through your product over time you can evolve your engagement position into an actual system of record use all the datum you have accumulate I believe that system of intelligencetm be the new moat what be a system of intelligence and why be it so defensible what make a system of intelligence valuable be that it typically cross multiple datum set multiple system of record one example be an application that combine web analytic with customer datum and social datum to predict end user behavior churn ltv or just serve more timely content you can build intelligence on a single data source or single system of record but that position become hard to defend against the vendor that own the datum for a startup to thrive around incumbent like oracle and sap you need to combine their datum with other datum source public or private to create value for your customer incumbent will be advantage on their own datum for example salesforce be build a system of intelligence einstein start with their own system of record crm the next generation of enterprise product will use different artificial intelligence ai technique to build system of intelligencetm it s not just application that will be transform by ai but also data center and infrastructure product we can categorize three major area where you can build system of intelligencetm customer face application around the customer journey employee face application like hcm itsm financial or infrastructure system like security compute storage networking and monitor management in addition to these broad horizontal use case startup can also focus on a single industry or market and build a system of intelligence around datum that be unique to a vertical like veeva in life science or rhumbix in construction in all of these market the battle be move from the old moat the source of the datum to the new moat what you do with the datum use a company s datum you can upsell customer automatically respond to support ticket prevent employee attrition and identify security anomaly product that use datum specific to an industry I e healthcare financial service or unique to a company customer data machine log etc to solve a strategic problem begin to look like a pretty deep moat especially if you can replace or automate an entire enterprise workflow or create a new value add workflow that be make possible by this intelligence enterprise application that build system of record have always be powerful business model some of the most enduring app company like salesforce and sap be all build on deep ip benefit from economy of scale and over time they accumulate more datum and operating knowledge as they get deep within a company s workflow and business process however even these incumbent be not immune to platform shift as a new generation of company attack their domain to be fair we may be at risk of ai marketing fatigue but all the hype reflect ai s potential to change so many industry one popular ai approach machine learning ml can be combine with datum a business process and an enterprise workflow to create the context to build a system of intelligence google be an early pioneer of apply ml to a process and workflow they collect more datum on every user and apply machine learning to serve up more timely ad within the workflow of a web search there be other evolve ai technique like neural network that will continue to change what we can expect from these future application these ai drive system of intelligencetm present a huge opportunity for new startup successful company here can build a virtuous cycle of datum because the more datum you generate and train on with your product the well your model become and the well your product become ultimately the product become tailor for each customer which create another moat high switching cost it be also possible to build a company that combine system of engagementtm with intelligence or even all three layer of the enterprise stack but a system of intelligence or engagement can be the good insertion point for a startup against an incumbent build a system of engagement or intelligence be not a trivial task and will require deep technology especially at speed and scale in particular technology that can facilitate an intelligence layer across multiple datum source will be essential finally there be some business that can build datum network effect by use customer and market datum to train and improve model that make the product well for all customer which spin the flywheel of intelligence fast in summary you can build a defensible business model as a system of engagement intelligence or record but with the advent of ai intelligent application will be the fountain of the next generation of great software company because they will be the new moat thank to saam motamedi sarah guo eli collins peter bailis elisa schreiber michael inouye my greylock partner sarah tavel and the rest of my partner at greylock for their input this post be also help through conversation with my friend at several greylock back company include trifacta cloudera and dozen of founder and ceo that have influence my thinking all good idea be shamelessly steal and all bad idea be mine alone from a quick cheer to a stand ovation clap to show how much you enjoy this story restless irreverent partner at @greylockvc www jerrychen com greylock partner back entrepreneur who be build disruptive market transform consumer and enterprise software company
Sarthak Jain,3.9K,10,https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74?source=tag_archive---------2----------------,how to easily detect object with deep learning on raspberry pi,disclaimer I m build nanonet com to help build ml with less datum and no hardware the raspberry pi be a neat piece of hardware that have capture the heart of a generation with ~15 m device sell with hacker build even cool project on it give the popularity of deep learning and the raspberry pi camera we think it would be nice if we could detect any object use deep learning on the pi now you will be able to detect a photobomber in your selfie someone enter harambe s cage where someone keep the sriracha or an amazon delivery guy enter your house 20 m year of evolution have make human vision fairly evolve the human brain have 30 % of it s neuron work on processing vision as compare with 8 percent for touch and just 3 percent for hear human have two major advantage when compare with machine one be stereoscopic vision the second be an almost infinite supply of training datum an infant of 5 year have have approximately 2 7b image sample at 30fps to mimic human level performance scientist break down the visual perception task into four different category object detection have be good enough for a variety of application even though image segmentation be a much more precise result it suffer from the complexity of create training datum it typically take a human annotator 12x more time to segment an image than draw bounding box ; this be more anecdotal and lack a source also after detect object it be separately possible to segment the object from the bounding box object detection be of significant practical importance and have be use across a variety of industry some of the example be mention below object detection can be use to answer a variety of question these be the broad category there be a variety of model architecture that be use for object detection each with trade off between speed size and accuracy we pick one of the most popular one yolo you only look once and have show how it work below in under 20 line of code if you ignore the comment note this be pseudo code not intend to be a work example it have a black box which be the cnn part of it which be fairly standard and show in the image below you can read the full paper here https pjreddie com media file paper yolo_1 pdf for this task you probably need a few 100 image per object try to capture datum as close to the datum you re go to finally make prediction on draw bounding box on the image you can use a tool like labelimg you will typically need a few people who will be work on annotate your image this be a fairly intensive and time consume task you can read more about this at medium com nanonet nanonet how to use deep learning when you have limit datum f68c0b512cab you need a pretraine model so you can reduce the amount of datum require to train without it you might need a few 100k image to train the model you can find a bunch of pretraine model here the process of train a model be unnecessarily difficult to simplify the process we create a docker image would make it easy to train to start train the model you can run the docker image have a run sh script that can be call with the follow parameter you can find more detail at to train a model you need to select the right hyper parameter find the right parameter the art of deep learning involve a little bit of hit and try to figure out which be the good parameter to get the high accuracy for your model there be some level of black magic associate with this along with a little bit of theory this be a great resource for find the right parameter quantize model make it small to fit on a small device like the raspberry pi or mobile small device like mobile phone and rasberry pi have very little memory and computation power training neural network be do by apply many tiny nudge to the weight and these small increment typically need float point precision to work though there be research effort to use quantize representation here too take a pre train model and running inference be very different one of the magical quality of deep neural network be that they tend to cope very well with high level of noise in their input why quantize neural network model can take up a lot of space on disk with the original alexnet be over 200 mb in float format for example almost all of that size be take up with the weight for the neural connection since there be often many million of these in a single model the node and weight of a neural network be originally store as 32 bit float point number the simple motivation for quantization be to shrink file size by store the min and max for each layer and then compress each float value to an eight bit integer the size of the file be reduce by 75 % code for quantization you need the raspberry pi camera live and work then capture a new image for instruction on how to install checkout this link download model once your do train the model you can download it on to your pi to export the model run then download the model onto the raspberry pi install tensorflow on the raspberry pi depend on your device you might need to change the installation a little run model for predict on the new image the raspberry pi have constraint on both memory and compute a version of tensorflow compatible with the raspberry pi gpu be still not available therefore it be important to benchmark how much time do each of the model take to make a prediction on a new image we have remove the need to annotate image we have expert annotator who will annotate your image for you we automatically train the good model for you to achieve this we run a battery of model with different parameter to select the good for your data nanonet be entirely in the cloud and run without use any of your hardware which make it much easy to use since device like the raspberry pi and mobile phone be not build to run complex compute heavy task you can outsource the workload to our cloud which do all of the compute for you get your free api key from http app nanonet com user api_key collect the image of object you want to detect you can annotate they either use our web ui https app nanonet com objectannotation appid = your_model_id or use open source tool like labelimg once you have dataset ready in folder image image file and annotation annotation for the image file start upload the dataset once the image have be upload begin train the model the model take ~2 hour to train you will get an email once the model be train in the meanwhile you check the state of the model once the model be train you can make prediction use the model from a quick cheer to a stand ovation clap to show how much you enjoy this story founder & ceo @ nanonet com nanonet machine learn api
Gaurav Oberoi,850,12,https://hackernoon.com/exploring-deepfakes-20c9947c22d9?source=tag_archive---------3----------------,explore deepfake hacker noon,in december 2017 a user name deepfake post realistic look explicit video of famous celebrity on reddit he generate these fake video use deep learning the late in ai to insert celebrity face into adult movie in the follow week the internet explode with article about the danger of face swap technology harass innocent propagate fake news and hurt the credibility of video evidence forever in this post I explore the capability of this tech describe how it work and discuss potential application deepfake offer the ability to swap one face for another in an image or a video face swapping have be do in film for year but it require skilled video editor and cgi expert to spend many hour to achieve decent result this be so remarkable that I m go to repeat it anyone with hundred of sample image of person a and person b can feed they into an algorithm and produce high quality face swap — video editing skill be not need this also mean that it can be do at scale and give that so many of we have our face online it s trivially easy to insert almost anyone into fake video scary but hopefully it s not all doom and gloom after all we as a society have already come to accept that photo can easily be fake before dream up how to use this tech I want to get a handle on how it work and how well it perform I pick two popular late night tv host jimmy fallon and john oliver because I can find lot of video of they with similar pose and lighting — and also enough variation like lip sync battle to keep it interesting luckily for I there s an active github repo that contain the original deepfake code and many more improvement it s fairly straightforward to use but the onus be still on the user to collect and prepare training datum to make experimentation easy I write a script to work directly with youtube video this make collect and preprocesse training datum painless and convert video one step click here to view my github repo and see how easily I generate the video below I also share my model weight the follow video be generate by train a model on about 15k image of each person s face 30k image total I get face for each celebrity from 6 8 youtube video of 3 5 minute each with 20 frame per second per video and by filter out frame that don t have their face present all of this be do automatically — all I do be specify a list of youtube video url the total training time be about 72 hour on a nvidia gtx 1080 ti gpu training be primarily constrain by gpu but download video and chop they into frame be I o bind and can be parallelize note that while I have thousand of image of each person decent face swap can be achieve with as few as 300 image I go this route because I pull face image from video and it s far easy to pick a handful of video as training datum than to find hundred of image the image below be low resolution to keep the size of the animate gif file small there s a youtube video below with high resolution and sound while not perfect the result above be quite convince the key thing to remember be the algorithm learn how to do this by see lot of example I didn t modify the video in any way magical let s look under the cover at the core of the deepfake code be an autoencoder a deep neural network that learn how to take an input compress it down into a small representation or encoding and then to regenerate the original input from this encoding put a bottleneck in the middle force the network to recreate these image instead of just return what it see the encoding help it capture broad pattern hypothetically like how and where to draw jimmy fallon s eyebrow deepfake go far by have one encoder to compress a face into an encoding and two decoder one to turn it back into person a fallon and the other to person b oliver it s easy to understand with a diagram in the above we re show how these 3 component get train once training be complete we can perform a clever trick pass in an image of fallon into the encoder and then instead of try to reconstruct fallon from the encoding we now pass it to decoder b to reconstruct oliver it s remarkable to think that the algorithm can learn how to generate these image just by see thousand of example but that s exactly what have happen here and with fairly decent result while the result be exciting there be clear limitation to what we can achieve with this technology today these be tenable problem to be sure tool can be build to collect image from online channel en masse ; algorithm can help flag when there be insufficient or mismatch training datum ; clever optimization or model reuse can help reduce training time ; and a well engineer system can be build to make the entire process automatic but ultimately the question be why be there enough of a business model to make do all this worth it give what ve now learn about what s possible let s talk about way in which this could be useful hollywood have have this technology at its fingertip but not at this low cost if they can create great look video with this technique it will change the demand for skilled editor over time but it could also open up new opportunity for instance make movie with unknown actor and then superimpose famous celebrity onto they this could work for youtube video or even news channel film by regular folk in more out there scenario studio could change actor base on their target market more schwarzenag for the austrians or netflix could allow viewer to pick actor before hit play more likely this tech could generate revenue for the estate of long dead actor by bring they back to life some of the comment thread on deepfake video on youtube be abuzz about what a great meme generator this technology could create jib jab be a company that have be sell video greeting card with simple face swap for year they be hilarious but the big opportunity be to create the next big viral hit ; after all photo filter attract masse of people to instagram and snapchat and face swap app have do well before give how fun the result can be there s likely room for a hit viral app if you can get the cost low enough to generate these model imagine if target could have a celebrity showcase their clothe for a month just by pay her agent a fee grab some exist headshot and click a button this would create a new revenue stream for celebrity social medium influencer or anyone who happen to be in the spotlight at the moment and it would give business another tool to promote brand and drive conversion it also raise interesting legal question about ownership of likeness and business model question on how to partition and price right to use they imagine a world where the ad you see as you surf the web include you your friend and your family while this may come across as creepy today do it seem so far fetched to think that this win t be the norm in a few year after all we be visual creature and advertiser have be try to elicit emotional response from we for year e g coke may want to convey joy by put your friend in a hip music video or allstate may tug at your fear by show your family in an insurance ad or the approach may be more direct banana republic could superimpose your face on a body type that match yours and convince you that it s worth try out their new leather jacket whoever the original deepfake user be they open a pandora s box of difficult question about how fake video generation will affect society I hope that in the same way we have come to accept that image can easily be fake we will adapt to video uncertainty too though not everyone share this hope what deepfake also do be shine a light on how interesting this technology be deep generative model like the autoencoder that deepfake use allow we to create synthetic but realistic looking datum include image or video only by show an algorithm lot of example this mean that once these algorithm be turn into product regular folk will have access to powerful tool that will make they more creative hopefully towards positive end there have already be some interesting application of this technique like style transfer app that make your photo look like famous painting but give the high volume and exciting nature of the research that be be publish in this space there s clearly a lot more to come I m interested in explore how to build value from the late in ai research ; if you have an interest in take this technology to market to solve a real problem please drop I a note a few fun tidbit for the curious from a quick cheer to a stand ovation clap to show how much you enjoy this story I ve be a product manager engineer and founder for over a decade in seattle and silicon valley currently explore new idea at the allen institute for ai how hacker start their afternoon
Nick Bourdakos,5K,15,https://medium.freecodecamp.org/understanding-capsule-networks-ais-alluring-new-architecture-bdb228173ddc?source=tag_archive---------4----------------,understand capsule network — ai s allure new architecture,convolutional neural network have do an amazing job but be root in problem it s time we start think about new solution or improvement — and now enter capsule previously I briefly discuss how capsule network combat some of these traditional problem for the past for few month I ve be submerge myself in all thing capsule I think it s time we all try to get a deep understanding of how capsule actually work in order to make it easy to follow along I have build a visualization tool that allow you to see what be happen at each layer this be pair with a simple implementation of the network all of it can be find on github here this be the capsnet architecture don t worry if you don t understand what any of it mean yet I ll be go through it layer by layer with as much detail as I can possibly conjure up the input into capsnet be the actual image supply to the neural net in this example the input image be 28 pixel high and 28 pixel wide but image be actually 3 dimension and the 3rd dimension contain the color channel the image in our example only have one color channel because it s black and white most image you be familiar with have 3 or 4 channel for red green blue and possibly an additional channel for alpha or transparency each one of these pixel be represent as a value from 0 to 255 and store in a 28x28x1 matrix 28 28 1 the bright the pixel the large the value the first part of capsnet be a traditional convolutional layer what be a convolutional layer how do it work and what be its purpose the goal be to extract some extremely basic feature from the input image like edge or curve how can we do this let s think about an edge if we look at a few point on the image we can start to pick up a pattern focus on the color to the left and right of the point we be look at you might notice that they have a large difference if the point be an edge what if we go through each pixel in the image and replace its value with the value of the difference of the pixel to the left and right of it in theory the image should become all black except for the edge we could do this by loop through every pixel in the image but this isn t very efficient we can instead use something call a convolution technically speak it s a cross correlation but everyone like to call they convolution a convolution be essentially do the same thing as our loop but it take advantage of matrix math a convolution be do by line up a small window in the corner of the image that only let we see the pixel in that area we then slide the window across all the pixel in the image multiply each pixel by a set of weight and then add up all the value that be in that window this window be a matrix of weight call a kernel we only care about 2 pixel but when we wrap the window around they it will encapsulate the pixel between they can you think of a set of weight that we can multiply these pixel by so that their sum add up to the value we be look for spoiler below we can do something like this with these weight our kernel will look like this however kernel be generally square — so we can pad it with more zero to look like this here s a nice gif to see a convolution in action note the dimension of the output be reduce by the size of the kernel plus 1 for example 7 — 3 + 1 = 5 more on this in the next section here s what the original image look like after do a convolution with the kernel we craft you might notice that a couple edge be miss specifically the horizontal one in order to highlight those we would need another kernel that look at pixel above and below like this also both of these kernel win t work well with edge of other angle or edge that be blur for that reason we use many kernel in our capsnet implementation we use 256 kernel and the kernel be normally large to allow for more wiggle room our kernel will be 9x9 this be what one of the kernel look like after train the model it s not very obvious but this be just a large version of our edge detector that be more robust and only find edge that go from bright to dark note I ve round the value because they be quite large for example 0 01783941 luckily we don t have to hand pick this collection of kernel that be what training do the kernel all start off empty or in a random state and keep get tweak in the direction that make the output close to what we want this be what the 256 kernel end up look like I color they as pixel so it s easy to digest the more negative the number the bluer they be 0 be green and positive be yellow after we filter the image with all of these kernel we end up with a fat stack of 256 output image relu formally know as rectify linear unit may sound complicated but it s actually quite simple relu be an activation function that take in a value if it s negative it become zero and if it s positive it stay the same in code and as a graph we apply this function to all of the output of our convolution why do we do this if we don t apply some sort of activation function to the output of our layer then the entire neural net could be describe as a linear function this would mean that all this stuff we be do be kind of pointless add a non linearity allow we to describe all kind of function there be many different type of function we could apply but relu be the most popular because it s very cheap to perform here be the output of relu conv1 layer the primarycap layer start off as a normal convolution layer but this time we be convolve over the stack of 256 output from the previous convolution so instead of have a 9x9 kernel we have a 9x9x256 kernel so what exactly be we look for in the first layer of convolution we be look for simple edge and curve now we be look for slightly more complex shape from the edge we find early this time our stride be 2 that mean instead of move 1 pixel at a time we take step of 2 a large stride be choose so that we can reduce the size of our input more rapidly note the dimension of the output would normally be 12 but we divide it by 2 because of the stride for example 20 — 9 + 1 2 = 6 we will convolve over the output another 256 time so we will end up with a stack of 256 6x6 output but this time we aren t satisfied with just some lousy plain old number we re go to cut the stack up into 32 deck with 8 card each deck we can call this deck a capsule layer each capsule layer have 36 capsule if you re keep up and be a math wiz that mean each capsule have an array of 8 value this be what we can call a vector here s what I m talk about these capsule be our new pixel with a single pixel we could only store the confidence of whether or not we find an edge in that spot the high the number the high the confidence with a capsule we can store 8 value per location that give we the opportunity to store more information than just whether or not we find a shape in that spot but what other kind of information would we want to store when look at the shape below what can you tell I about it if you have to tell someone else how to redraw it and they couldn t look at it what would you say this image be extremely basic so there be only a few detail we need to describe the shape we can call these instantiation parameter with more complex image we will end up need more detail they can include pose position size orientation deformation velocity albedo hue texture and so on you might remember that when we make a kernel for edge detection it only work on a specific angle we need a kernel for each angle we could get away with it when deal with edge because there be very few way to describe an edge once we get up to the level of shape we don t want to have a kernel for every angle of rectangle oval triangle and so on it would get unwieldy and would become even bad when deal with more complicated shape that have 3 dimensional rotation and feature like lighting that s one of the reason why traditional neural net don t handle unseen rotation very well as we go from edge to shape and from shape to object it would be nice if we have more room to store this extra useful information here be a simplified comparison of 2 capsule layer one for rectangle and the other for triangle vs 2 traditional pixel output like a traditional 2d or 3d vector this vector have an angle and a length the length describe the probability and the angle describe the instantiation parameter in the example above the angle actually match the angle of the shape but that s not normally the case in reality it s not really feasible or at least easy to visualize the vector like above because these vector be 8 dimensional since we have all this extra information in a capsule the idea be that we should be able to recreate the image from they sound great but how do we coax the network into actually want to learn these thing when train a traditional cnn we only care about whether or not the model predict the right classification with a capsule network we have something call a reconstruction a reconstruction take the vector we create and try to recreate the original input image give only this vector we then grade the model base on how close the reconstruction match the original image I will go into more detail on this in the come section but here be a simple example after we have our capsule we be go to perform another non linearity function on it like relu but this time the equation be a bit more involve the function scale the value of the vector so that only the length of the vector change not the angle this way we can make the vector between 0 and 1 so it s an actual probability this be what length of the capsule vector look like after squash at this point it s almost impossible to guess what each capsule be look for the next step be to decide what information to send to the next level in traditional network we would probably do something like max pooling max pooling be a way to reduce size by only pass on the highest activate pixel in the region to the next layer however with capsule network we be go to do something call routing by agreement the good example of this be the boat and house example illustrate by aurélien géron in this excellent video each capsule try to predict the next layer s activation base on itself look at these prediction which object would you choose to pass on to the next layer not know the input probably the boat right both the rectangle capsule and the triangle capsule agree on what the boat would look like but they don t agree on how the house would look so it s not very likely that the object be a house with routing by agreement we only pass on the useful information and throw away the datum that would just add noise to the result this give we a much smart selection than just choose the large number like in max pooling with traditional network misplace feature don t faze it with capsule network the feature wouldn t agree with each other hopefully that work intuitively however how do the math work we have 10 different digit class that we be predict note in the boat and house example we be predict 2 object but now we be predict 10 unlike in the boat and the house example the prediction aren t actually image instead we be try to predict the vector that describe the image the capsule s prediction for each class be make by multiply it s vector by a matrix of weight for each class that we be try to predict remember that we have 32 capsule layer and each capsule layer have 36 capsule that mean we have a total of 1 152 capsule you will end up with a list of 11 520 prediction each weight be actually a 16x8 matrix so each prediction be a matrix multiplication between the capsule vector and this weight matrix as you can see our prediction be a 16 degree vector where do the 16 come from it s an arbitrary choice just like 8 be for our original capsule but it should be note that we want to increase the number of dimension of our capsule the deeply we get into the network this should make sense intuitively because the deeply we go the more complex our feature become and the more parameter we need to recreate they for example you will need more information to describe an entire face than just a person s eye the next step be to figure out which of these 11 520 prediction agree with each other the most it can be difficult to visualize a solution to this when we think in term of high dimensional vector for the sake of sanity let s start off by pretend our vector be just point in 2 dimensional space we start off by calculate the mean of all of the point each point start out with equal importance we then can measure the distance between every point from the mean the further the point be away from the mean the less important that point become we then recalculate the mean this time take into account the point s importance we end up go through this cycle 3 time as you can see as we go through this cycle the point that don t agree with the other start to disappear the high agreeing point end up get pass on to the next layer with the high activation after agreement we end up with ten 16 dimensional vector one vector for each digit this matrix be our final prediction the length of the vector be the confidence of the digit be find — the long the well the vector can also be use to generate a reconstruction of the input image this be what the length of the vector look like with the input of 4 the fifth block be the bright which mean high confidence remember that 0 be the first class mean 4 be our predict class the reconstruction portion of the implementation isn t very interesting it s just a few fully connect layer but the reconstruction itself be very cool and fun to play around with if we reconstruct our 4 input from its vector this be what we get if we manipulate the slider the vector we can see how each dimension affect the 4 I recommend clone the visualization repo to play around with different input and see how the slider affect the reconstruction run the tool then point your browser to http localhost 5000 I think that the reconstruction from capsule network be stunning even though the current model be only train on simple digit it make my mind run with the possibility that a mature architecture train on a large dataset could achieve I m very curious to see how manipulate the reconstruction vector of a more complicated image would affect it for that reason my next project be to get capsule network to work with the cifar and smallnorb dataset thank for reading if you have any question feel free to reach out at bourdakos1@gmail com connect with I on linkedin or follow I on medium if you find this article helpful it would mean a lot if you give it some applause 👏 and share to help other find it and feel free to leave a comment below from a quick cheer to a stand ovation clap to show how much you enjoy this story computer vision addict at ibm watson our community publish story worth read on development design and datum science
Mark Johnson,3.7K,9,https://hackernoon.com/how-i-shipped-six-side-projects-in-2017-3dde6c77adbb?source=tag_archive---------5----------------,how I launch six side project in 2017 hacker noon,last year I set a goal to learn something new each month and end out launch six new project which I ll recap along with what I learn below look back it seem a little crazy to I that I manage to launch as much as I do while run a more than full time business spending quality time with my family I have two kid and a very patient wife teach as an adjunct professor and consult on the side it s easy to think that not have enough time be what s hold you back from launch your side project if there be only more time be the general excuse we give ourselves and we look for fancy app or task management technique to try and free up more space in our schedule however one of the main thing I ve learn over the last year be that time be not the primary issue you have enough time ; what you need be motivation the good news be that motivation can be hack I ve learn a few way to hack my motivation in 2017 and I want to share those with you you simply can t stay motivated about something you don t care about so choose something that you re excited to work on when you feel inspiration strike around that idea don t let it pass use it even if that mean jot down some quick note while you re in a meeting at work it s important to grab ahold of those moment of inspiration to stay hungry and curious around your work for I that mean ship something every month I tend to blow thing up once I start work on they so this 30 day constraint really help I rein that tendency in and spend my motivation efficiently it also give you a chance to try out new idea if one month s idea turn out to be a dud at least you didn t waste a whole year on it this be the big one you will run out of motivation fuel towards the end of your project that last 10 % be killer the only thing that will get you through a motivation slump be know there be people on the other side wait to see what you build another benefit of share your work be that it give you a chance to get some supportive feedback for what you re do the co work space I work out of atlas local have an office wide event on the first friday of every month I use that event to present my project from the previous month and be always encourage and support by the generous folk who be there you ll be surprise by how much support you ll get for just step out there and share something you make perhaps the most surprising part of this experiment for I be that far from be burn out at the end I feel even more motivated to ship more work in 2018 I d encourage you to hack your motivation in the new year and ship some of those idea you ve have lie around for a while I d love to hear about it if you try if you re interested in the detail of what I build in 2017 read on visually compare the personality type of your group s strong and weak trait I ve be interested in the myers briggs type indicator mbti for a while now while I don t see it as prescriptive or even all that scientific it have be a helpful framework for empathize with people who be different than I what many personality nerd don t realize be that the mbti system be base on something call cognitive function these function be create by the father of modern phycology carl jung back in the 1920 I want to dive a little deep and learn more about that at the same time I be watch hbo s west world and see this screen while I love these kind of sci fi uis which be what immediately catch my attention I think what if I could build a host profile of anyone base on their mbti trait why not to prepare for this I read the mbti bible gift differ by myer and briggs and start hack on build out a system that could generate a radar chart base on the cognitive function underlie the mbti system in the end I pivot away from the west world ui a bit since I and other beta tester find a lot more utility in the ability to overlay multiple people on the radar chart to get a sense of chemistry amongst a group of people the result be really interesting if I do say so myself try enter you team s personality type or you and your spouse the easy way to create signup sheet online for anything I ve work on sheetcake for a few year now on the side it have a very small set of loyal user most of which know I or someone close to I some fun fact about sheetcake sheetcake actually work really well for certain type of thing like those zero day signup so I want to create a landing page for it that market some of the benefit I start from a template on this one but here s where it land ask my extroverte assistant bot question about I early in the year chat bot be all the rage while I ve never be optimistic that chat bot will go anywhere on their own the conversational a i aspect of they be intriguing to I and I want to learn more about it I m an introvert and generally pretty bad at share anything about myself so I think it might be fun to create an extroverte bot that could answer simple question about I build convince a i with goal orient action planning after come across this article I be super intrigue by goal orient action planning goap describe in the context of a game with some nostalgia for I f e a r have work on several game with rudimentary a i in the past I d never come across this technique I remember think that f e a r s a I be particularly impressive and lifelike after research a bit more the really compelling part about this methodology be not so much how convincing the result be but how simple and elegant the solution be especially compare to a more standard a I approach like finite state machine so for april s project I make a javascript library to explore goap a basic implementation turn out to be surprisingly simple only 58 line of code sign accountability contract for your goal this be the month I start on the whole 30 diet I d become complacent about my eating habit and it definitely be effect my energy level whole30 work really well for I I lose 18 pound during the diet and a total of 35 more in the month follow most of all it really even out my energy level during the day and I feel much more motivated and focus see the parallel between public commitment and motivation I decide to explore the idea of goal contract for may s project create unique map poster for your favorite place and memory this be where everything pivot my goal for june be to make a product that people actually want to buy one of my big weakness be sale and marketing so I want to learn more about that by build a product I could practice with I ve always be interested in map and generative art so create a tool where you can create and purchase poster of your favorite location be an intriguing idea this project be way too ambitious to complete in one month on the side so I decide to go all in on tiltmap for the rest of the year and work on a different angle of the product every month until launch I find that chunk the various part of a large project into a month long project be really helpful to actually get this do june july the secret saucetm️ most of the first month be do r&d to figure out if generate high re map in 3d space be even possible at all generate a 300dpi map of any location in the world at a 3d angle be not something that any api or platform I find support out of the box so I have to invent my own way of do it this take most of the month to figure out but be surprisingly simple once I find the answer after that I build a rudimentary editor to start create actual poster and order a couple of test print august september the proof of concept mvp the next few month I build out a more consumer mvp of the product the design wasn t great but I get it to the point where everything work and I could start user test the poster creation and printing process october november branding & market the next couple of month be focus on get this ready to launch while the editor be basically do I have no home page and the marketing side of the project be nowhere close I end up sell a few poster this month before launch by present tiltmap at zero day and a conference I attend this be super motivating as it be the first time I ve ever sell anything from a side project december public launch the launch on product hunt go well than I expect I be hope for 10 sale or so but end up get 37 and be still see sale come in it feel good to make something people want to buy and it serve as a great testing ground for try out different ad and sale strategy that could come in useful at my day job I plan to continue work on tiltmap in 2018 and hopefully get some decent fun money revenue from it and that s a wrap thank for read the whole way to the bottom 😃 have any thought or feedback i d love to hear it comment below or hit I up on twitter from a quick cheer to a stand ovation clap to show how much you enjoy this story web designer developer and teacher work at the cross section of learning and technology co founder cto of pathwright launcher of side project how hacker start their afternoon
Justin Lee,8.3K,11,https://medium.com/swlh/chatbots-were-the-next-big-thing-what-happened-5fc49dd6fa61?source=tag_archive---------6----------------,chatbots be the next big thing what happen the startup medium,oh how the headline blare chatbot be the next big thing our hope be sky high bright eyed and bushy tail the industry be ripe for a new era of innovation it be time to start socialize with machine and why wouldn t they be all the road sign point towards insane success at the mobile world congress 2017 chatbot be the main headliner the conference organizer cite an overwhelming acceptance at the event of the inevitable shift of focus for brand and corporate to chatbot in fact the only significant question around chatbot be who would monopolize the field not whether chatbot would take off in the first place one year on we have an answer to that question no because there isn t even an ecosystem for a platform to dominate chatbot weren t the first technological development to be talk up in grandiose term and then slump spectacularly the age old hype cycle unfold in familiar fashion expectation build build and then it all kind of fizzle out the predict paradim shift didn t materialize and app be tellingly still alive and well we look back at our breathless optimism and turn to each other slightly baffled be that it that be the chatbot revolution we be promise digit s ethan bloch sum up the general consensus accord to dave feldman vice president of product design at heap chatbots didn t just take on one difficult problem and fail they take on several and fail all of they bot can interface with user in different way the big divide be text vs speech in the beginning of computer interface be the write word user have to type command manually into a machine to get anything do then graphical user interface guis come along and save the day we become entrance by windows mouse click icon and hey we eventually get color too meanwhile a bunch of research scientist be busily develop natural language nl interface to database instead of have to learn an arcane database query language another bunch of scientist be develop speech processing software so that you could just speak to your computer rather than have to type this turn out to be a whole lot more difficult than anyone originally realise the next item on the agenda be hold a two way dialog with a machine here s an example dialog date back to the 1990 with vcr setup system pretty cool right the system take turn in collaborative way and do a smart job of figure out what the user want it be carefully craft to deal with conversation involve vcrs and could only operate within strict limitation modern day bot whether they use type or speak input have to face all these challenge but also work in an efficient and scalable way on a variety of platform basically we re still try to achieve the same innovation we be 30 year ago here s where I think we re go wrong an oversized assumption have be that app be over and would be replace by bot by pit two such disparate concept against one another instead of see they as separate entity design to serve different purpose we discourage bot development you might remember a similar war cry when app first come onto the scene ten year ago but do you remember when app replace the internet it s say that a new product or service need to be two of the follow well cheap or fast be chatbot cheap or fast than app no — not yet at least whether they re well be subjective but I think it s fair to say that today s good bot isn t comparable to today s good app plus nobody think that use lyft be too complicated or that it s too hard to order food or buy a dress on an app what be too complicated be try to complete these task with a bot — and have the bot fail a great bot can be about as useful as an average app when it come to rich sophisticated multi layer app there s no competition that s because machine let we access vast and complex information system and the early graphical information system be a revolutionary leap forward in help we locate those system modern day app benefit from decade of research and experimentation why would we throw this away but if we swap the word replace with extend thing get much more interesting today s most successful bot experience take a hybrid approach incorporate chat into a broad strategy that encompass more traditional element the next wave will be multimodal app where you can say what you want like with siri and get back information as a map text or even a spoken response another problematic aspect of the sweeping nature of hype be that it tend to bypass essential question like these for plenty of company bot just aren t the right solution the past two year be litter with case of bot be blindly apply to problem where they aren t need build a bot for the sake of it let it loose and hope for the good will never end well the vast majority of bot be build use decision tree logic where the bot s can response rely on spot specific keyword in the user input the advantage of this approach be that it s pretty easy to list all the case that they be design to cover and that s precisely their disadvantage too that s because these bot be purely a reflection of the capability fastidiousness and patience of the person who create they ; and how many user need and input they be able to anticipate problem arise when life refuse to fit into those box accord to recent report 70 % of the 100 000 + bot on facebook messenger be fail to fulfil simple user request this be partly a result of developer fail to narrow their bot down to one strong area of focus when we be build growthbot we decide to make it specific to sale and marketer not an all rounder despite the temptation to get overexcite about potential capabiltie remember a bot that do one thing well be infinitely more helpful than a bot that do multiple thing poorly a competent developer can build a basic bot in minute — but one that can hold a conversation that s another story despite the constant hype around ai we re still a long way from achieve anything remotely human like in an ideal world the technology know as nlp natural language processing should allow a chatbot to understand the message it receive but nlp be only just emerge from research lab and be very much in its infancy some platform provide a bit of nlp but even the good be at toddler level capacity for example think about siri understand your word but not their meaning as matt asay outline this result in another issue failure to capture the attention and creativity of developer and conversation be complex they re not linear topic spin around each other take random turn restart or abruptly finish today s rule base dialogue system be too brittle to deal with this kind of unpredictability and statistical approach use machine learning be just as limit the level of ai require for human like conversation just isn t available yet and in the meantime there be few high quality example of trailblaze bot to lead the way as dave feldman remark once upon a time the only way to interact with computer be by type arcane command to the terminal visual interface use window icon or a mouse be a revolution in how we manipulate information there s a reason compute move from text base to graphical user interface guis on the input side it s easy and fast to click than it be to type tap or selecting be obviously preferable to type out a whole sentence even with predictive often error prone text on the output side the old adage that a picture be worth a thousand word be usually true we love optical display of information because we be highly visual creature it s no accident that kid love touch screen the pioneer who dream up graphical interface be inspire by cognitive psychology the study of how the brain deal with communication conversational uis be mean to replicate the way human prefer to communicate but they end up require extra cognitive effort essentially we re swap something simple for a more complex alternative sure there be some concept that we can only express use language show I all the way of get to a museum that give I 2000 step but don t take long than 35 minute but most task can be carry out more efficiently and intuitively with guis than with a conversational ui aim for a human dimension in business interaction make sense if there s one thing that s break about sale and market it s the lack of humanity brand hide behind ticket number feedback form do not reply email automate response and gate contact we form facebook s goal be that their bot should pass the so call ture test mean you can t tell whether you be talk to a bot or a human but a bot isn t the same as a human it never will be a conversation encompasse so much more than just text human can read between the line leverage contextual information and understand double layer like sarcasm bot quickly forget what they re talk about mean it s a bit like converse with someone who have little or no short term memory as hubspot team pinpoint people aren t easily fool and pretend a bot be a human be guarantee to diminish return not to mention the fact that you re lie to your user and even those rare bot that be power by state of the art nlp and excel at processing and produce content will fall short in comparison and here s the other thing conversational uis be build to replicate the way human prefer to communicate — with other human but be that how human prefer to interact with machine not necessarily at the end of the day no amount of witty quip or human like mannerism will save a bot from conversational failure in a way those early adopter weren t entirely wrong people be yell at google home to play their favorite song order pizza from the domino s bot and get makeup tip from sephora but in term of consumer response and developer involvement chatbots haven t live up to the hype generate circa 2015 16 not even close computer be good at be computer search for datum crunch number analyze opinion and condense that information computer aren t good at understand human emotion the state of nlp mean they still don t get what we re ask they never mind how we feel that s why it s still impossible to imagine effective customer support sale or marketing without the essential human touch empathy and emotional intelligence for now bot can continue to help we with automate repetitive low level task and query ; as cog in a large more complex system and we do they and ourselves a disservice by expect so much so soon but that s not the whole story yes our industry massively overestimate the initial impact chatbot would have emphasis on initial as bill gate once say the hype be over and that s a good thing now we can start examine the middle ground grey area instead of the hyper inflate frantic black and white zone I believe we re at the very beginning of explosive growth this sense of anti climax be completely normal for transformational technology messaging will continue to gain traction chatbot aren t go away nlp and ai be become more sophisticated every day developer app and platform will continue to experiment with and heavily invest in conversational marketing and I can t wait to see what happen next from a quick cheer to a stand ovation clap to show how much you enjoy this story head of growth for growthbot messaging & conversational strategy @hubspot medium s large publication for maker subscribe to receive our top story here → https goo gl zhclji
Leigh Alexander,2.7K,31,https://medium.com/@leighalexander/the-future-we-wanted-fd41e3e14512?source=tag_archive---------7----------------,the future we want leigh alexander medium,I wonder a lot about how jane end up when we be small we do everything together she s just like you aunt cissy keep insist and jane be in that her birth parent be for the most part out of the picture we also both like fantasy book and hate afterschool but honestly that s where the similarity end jane be a weirdo in what way be she weird dr carla ask I clasp her hand my uncle say jane couldn t tell fantasy from reality I say after a pause but your uncle still perform care for jane someone in the circle say a group member in legging let s call her ruby say loudly people say that about I when I be little too it s a common avenue leverage to oppress girl of imagination luckily dr carla hold up her hand then gently say let s keep that thought as we bring group to an end I could tell fantasy from reality ruby be still insist as the twelve or so of we trail out of the park tap our mobilepay against the turnstile ridgewood park take only nine cent from each of we unlike switchmond field which take 17 cent the turnstile display blink do your part and thank you alternately I could tell ruby say shoulder abruptly behind I and nearly shout into my ear I just didn t want to after group I take the bus promptly home mobilepay one dollar and ninety cent and speed walk to the apartment rex and ellis would have be in front of screen the whole time when I come in the house there be a musty smell of microwave cheese el be miss pant wave around two grotesque wireless fiddlestick of some kind the noise be all come from rex and also brian who be sort of leapfrog all over some vinyl electronic pad that be say thing like vanquished and blue move next the way brian call he eyyyy be confidently bright as if dip in the golden morning at home I d just miss despite myself I be also call he eyyyy when el slam into I and rexy start talk immediately in a language I barely understand about blue unit and combat lane snippet from some universe into which they all dive joyfully whenever I turn my back how be woman s group brian ask continue to grin he look so happy to see I so proud of the time he spend delight the child it be unfair of I to be resentful it be nice I say pick up rex s sock and brian s sock and put they in my pocket pick up a piece of colorful plastic part of one of el s playset and reunite it with another part rex continue the noise ; they want to show I something to do with the game and beat dad and I say I promise I will in a minute my coat be still on brian give I a kiss he didn t really know what we talk about in group which be how it be suppose to be I talk about jane a little bit I m wonder if I should try to look she up and see how she s do be jane the one who be your roommate when we meet no the one I grow up with in jamaica plain my aunt and uncle basically take care of she I tell you she get all the crystal animal oh brian say pick a bit of egg off the countertop with his fingertip and gamely eat it the crazy one you shouldn t call woman crazy I say rex have go back to try to play with the mat and be shove el who be try to play too they be only pay attention to the game which be chime new challenger alert yes the crazy one it s nice you talk about her brian say you know your coat be still on I know god hey polly you know what might be good if we get one of those augusta virtual assistant thing even just for weekend brian say take my coat off I I shrug it angrily in his direction since we d already discuss it and he know how I feel about virtual assistant the voice tech have really evolve brian go on and think of it as sexist be a date framework I swear it s get really progressive I just think we could be a little bit happy around here I think you could be happy you ve be out all day and you re still so tense your mad face be still on a watercolor version sure but a mad face a watercolor version brian be an advertising copywriter like I we meet at a conference and sometimes his way with word be really enviable I almost didn t even notice out all day and then my own voice come out weaken well play brian it s like one o clock I creak that s what I mean brian reply immediately the day feel long to you probably because you have so much to do one of the client have one in the office and I just think it would be convenient for you you can set it when to run the dishwasher do the alarm even the whole smart closet thing the smart kitchen we could use it pay rent on a smart flat and not have a virtual assistant instal be like buy a swimming pool and never swim in it I just want a quick bath I say the sound of run water drown out the din of electronic in the house brian be probably right be we waste money by not spend more money privately I resolve to have a long bath not a quick one that would show they I sit imagine what I would yell if rex knock on the door or if brian bring up my m I a time even though really that have only happen once I think about this for a long while until I wind myself up lie rigidly in the bath and stare furiously into my belly button jane s crystal animal be present from my uncle and aunt when we be in the first grade they take we on a road trip to maine driving alongside strip of silvery stony sea and stop in small strange town inside an ash color colonial house we find a fragrant souvenir store sell wooden lighthouse nameplate and shell art and a whole mirror display case full of animal make of cut crystal we be draw to the crystal animal by a heavy sense of fate because I think aunt cissy be try to buy an umbrella and the shop have an old slow credit card machine or her card keep get decline something adult be go on — my favorite be the unicorn and butterfly and jane love the elephant and dolphin it be as if we be look upon the crown jewel of some fantastical city each one lead to a world jane say peer confidently into the display case where light rainbowe in the facet of the crystal which in turn be reflect in the mirror it be her perform magic face sometimes she would stare intently at something and attempt telekinesis but this time she just move her pointy face marginally close to the glass case her breath fog it careful I say not want she to get we in trouble in the fussy store this be how you enter the crystal world she retort speak softly you can do scry this way you can see the future a moment later jane whisper I m in I move close to the glass breathe on it and say I too the long I stare unblinke the more the glitter shape abstract in the haze light pour along the mirror wall of the display like molten gold and my eye well and sting I painfully feel the desire to own a sparkle crystal animal the ache way that only child can want thing I believe completely in the crystal world as discover by jane who spend the rest of that night s car ride explain it all eagerly to my aunt and uncle entrance I you form a bond with one of the animal to enter its world it would defend you from danger in astral form you have to be pure of heart if you concentrate your power the animal would show you the future I do try to add thing to crystal world too but jane s idea be always well I have to admit that ; she be the one who make it all come alive that night the star over the salt marsh be magic the long trail of red taillight and out of state plate be magic the grill cheese and fry I have at friendly s be warm and magic and taste like love sometime after we check into the motel and go to sleep in the same bed uncle arthur must have go back out in the morning he give jane a small cardboard box with a heavy knot of bubble wrap in it he say careful as she tear at it at its heart be the crystal unicorn you two will have to share it aunt cissy say inevitably brian bring home a huge glossy white box with a minimalist logo on it and a picture of augusta on the front the box be about as tall as a nine year old contain augusta s mobile mount as well as her bust unit not that I really want to learn the meaning or function of either of these thing I have the manual on my knee and on the other knee be el pound his fist on my thigh and keening as I try to explain that he could play with the box once we take the robot out of it it s not a robot ma it s an ai lady rex insist do we need to gender they I say brian lift the fiberglass head and shoulder from the box with great care in augusta s focus test face two huge eye glitter from behind a sort of black resinous mesh and at the corner of her white sculpt giaconda smile be twin black pinhead which the manual say be speaker inside the box hug in pack material her cranelike arm be fold and wrap in plastic beside her cylindrical body it look like a bin whoa brian say softly cradle the fiberglass bust with great care and examine its feature whoa rex echo their father she s beautiful what do we say about appearance base judgment rexy brian say unconvincingly glance at I briefly for approval as he set the bust on the coffee table and gingerly begin slide other piece out of the long package I continue page through the manual which have section title oven timer and error code what be these mobilepay transaction feature I feel myself frown don t worry about those the free feature be enough for us brian say augusta have plasticine ball joint shoulder and he start fit they into the flexible body socket with jerk and creak glimpse of dormant circuitry visible through her armpit so her bust can ride around the house on this mobile unit right and she use the arm for certain task and also to lift the bust off and on the smart port in the bedroom the kitchen the bathroom the bathroom I feel myself frown more or wherever you tell she where to go he say fit a halo spangle with sensor or something at the base of the unit like augusta go kitchen nicole and her wife don t have the mobile unit so they just keep the bust instal on the kitchen smart port which be where I feel like our augusta will spend most of her time too look it up in there kitchen companion mode where she s just connect to all the appliance and answer recipe question play music talk to you about whatever she have a vacuum accessory you win t get bore when I work late mom she s shiny can I kiss she on the face rex ask their hand on the shoulder contour of the bust innocently enough only on the cheek I relent she need to charge brian say so what do you think I could get use to it I say to be honest I feel she be my punishment last week I take a couple day to work from home while ellis be under the weather and we say I d get rex from school rather than have they go home with the wythe since they don t really like it at the wythe but work be kind of difficult about it and give one of the client my home number so the client keep call I and I shut off the smart home so I could finish research some comparable without interruption but it also shut off all my networked alarm so poor rexy wait at school for almost an hour with no sign of I and they couldn t call the house so the school call brian at work who tell they to call janet wythe who go back and get they and I didn t notice any of it until janet drop rex off at our place visibly annoyed with I because it be after 5 pm by then what be bad be when brian get home I try to pretend nothing have go wrong that day because I didn t know the school have call he don t think of augusta as some kind of punishment brian say gently she s go to just help look after everything a little more smoothly you ll see you win t know how you live without her mom I m go to marry her rex announce I just say okay sweetheart and knock softly on augusta s cheek with my fist just out of curiosity have a husband be nice but look what s in the vacuum dust pod be even nice nancy blurt with a high laugh squawk I mean that s what the ad say or I m paraphrase those be not my word I understand dr carla say gravely go on nancy but like and here nancy glance around the circle guiltily a little performatively if you d have ask I although judge one another s authenticity be against group rule the thing be I really love look in the dust pod I empty it every time I run the vacuum so I can be sure that what it bring back be just from that time no matter how often I run it it always come back full and I just find that so I don t know something in I just kind of love see all that dirt how it be all around our apartment completely invisible but I know it be there I know it s just so validate to look it in the face it s totally normal for sexist image of woman in advertising to resonate even with woman like us dr carla say shift her gaze away from nancy to encompass the group bear in mind that you haven t be give many mainstream framework and offer yourself forgiveness and care now to polly what be you work on this week internalize misogyny still I feel the raw burn of everyone s attention and briefly lose my word then I realize dr carla mean the stuff to do with jane ; for a second there I d actually think she be refer to augusta I m still think a lot about jane I hear myself admit and I also feel myself blush it feel like it soon might rain which make everyone impatient we fall out of touch toward the end of high school we she always act out as teen normal act out stuff but toward the end there she be there be stuff with the police court drug and for I it be just kind of time to grow up I have see jane teetering at the edge of some life waterfall sway ever more violently the long I stand and watch and in the end I begin back away so I wouldn t go over too we have to set boundary in order to give the good care to ourselves and other dr carla say evenly remember you be also an underprivileged child you can release your guilt be it guilt that s be keep you from get back in touch with jane I have determine never to feel guilty about jane but I didn t say that really I be just afraid of how I would find she after all of this time and I do explain that I notice but do not acknowledge ruby scowling pointedly like all of we in group jane be more than the circumstance that she have survive dr carla say you may indeed find she in the state of isolation and suffering that you fear and it s good you ve prepare nonjudgmentally for that but how would it feel to open your heart to the possibility that the thing you love about she would be there too the crystal unicorn leapt suddenly to the front of my mind along with a deep nostalgia I feel we can loosely collect today s share under the theme of be this the future we imagine dr carla tell everyone as we bring our practice to a close today let s go ahead and take that as our prompt to consider until the next time we meet a wave of light glitter beatifically across augusta s mesh eye screen and a serene chime waft from the corner of she perpetually smile white lip a breathy whirr herald the approach of the mobile mount the elegant architecture of the crane arm reach reach to lift the bust unit off the kitchen port and onto itself there be a soft click I m transition to a new place the assemble augusta announce gliding quietly across the kitchen behind I and into the living room she would wait there for the kid to return from sunday swimming with brian so she could operate their entertainment app I m transition to a new place sometimes I feel like I m only pretend to be a human jane say to I once we be maybe fourteen and by then she no long live with we but with a foster parent call marlene we didn t like marlene but we like her house a tunnel like ranch pile wall to wall in psychedelic decoration and antique junk my aunt and uncle continue give jane a different crystal animal every year for her birthday she now have a unicorn a dove a dolphin a cat a butterfly a rabbit and a deer one of the good part of jane go on to marlene s be we could access an official state nature trail through the wood out back we be in the wood a lot in those day enjoy the ethereal late afternoon sun filter through the pine the mote of pollen that sparkle in it sometimes we try smoking herb that we find in marlene s grinder we think it be drug but now I know it be only white sage I feel like no matter how good I get at know how to act with people or how to perform task I ll always just be pretend to be someone who isn t crazy jane say dig pattern into the sweet smell dirt with a broken stick I know I say I too but really I only understand she in the manner of a half glimpse truth like the crystal deer jane imagine be always move through the tree just out of our sight some mica glitter in the loam or the sound of faraway windchimes from marlene s back deck and she d say crystal deer even though of course we no long actually believe in the crystal world anymore or that s what I assume I understand jane in many way and pretend eagerly to know the rest there be time it feel like jane be more my family than my aunt and uncle who give all they have to try to soothe the rude start I get even more than they she make my life beautiful and exciting jane and I have pang and rage that only one another understand we cry until we ache we do blood sister spell over candle we scratch rune into our ankle with marlene s sewing needle and mine always heal up while hers linger messily I think she must have be pick they so they would scar she often describe feel like some fathomless anomaly assign to constantly perform the grueling role of jane and this I couldn t understand like I m an alien in a rubber human suit and the mothership forget I here for so long that I don t know who I be anymore she say while she speak her eye light up with the smoke and hazel of evening ; she didn t even look particularly troubled as if part of she take a certain delight in put it all to word so why should I just keep pretend to be normal when it s just a matter of time before this rubber suit just split open and out come pour this this she make shape with her hand long shadow that I watch crawl along the forest floor inexplicably I envy she do you think you should see a psychologist I ask they would tell jane not to be so imaginative and clever and different I just know it I visualize an iron steam all the crease out of the jane suit an image that provoke horror and relief in equal force I ve be go she say softly before that there have never be anything that she hadn t tell I right away that I know of I call over my shoulder to augusta and ask she to look up a jane who d have the surname i d know sure augusta reply judder silently over the synthetic flooring towards I beam her fiberglass smile the sound of her voice for some reason emerge from the kitchen port over my shoulder which unsettle I I ll just look that up for you polly she move much close to I ; I resist the impulse to step back her great insectoid eye gleam twin display shimmer to life in white showing list of top result social medium profile contact information even in the abstract I could see that one of they be definitely my jane nose to nose with augusta I find myself unable either to touch her eye with my fingertip to investigate the result or to ask she aloud to do it some strange part of I even think detachedly of shove she can that top result could you save it it s can you just save the contact information my voice unexpectedly betray I high and faint sorry polly augusta demur I m not sure what you want I to save try repeat — save the contact — we speak over each other sorry polly she say again I m not sure what you want we stare at each other and wait for silence and then I clearly say augusta save top contact result great I ve save that for you she reply warmly from the mouth speaker the sculpt lip unmove only vibrate slightly I didn t notice I d be hold my breath until augusta back up pivot and hiss softly away from I to re install herself in the living room I m return to my previous place I m return to my previous place the next week be a nightmare brian suddenly have to go spend day at some resort retreat for brand immersion with one of his firm s casino client rex get el s cold and spin it into a sinus infection and I have to work from home all week alone with they both I already use both my kid be sick last week with work when only el have be sick — I should have know well than to invite this kind of fatal justice — so this week I have to keep allude in my most harried email tone to ongoing structural issue with our apartment something about a woman with sick kid just isn t very convincing to colleague for legality s sake they pretend but I always know when I m be judge from the way el be scream I think he might even be develop an ear infection and rex always regress at the slight discomfort wanting to be bring every little thing and even melodramatically suck their thumb but rex be also suddenly willing to wear the sweet train pajama from brian s sister the one they be outgrow which I see as a perk everything go okay over there brian ask his kind face hang in one of the great moon of augusta s eye her bust unit be instal in the kitchen where I have to admit it have be helpful to arrange a sort of command center for the rest of the home that wasn t to say I like live with augusta ; the house be cleaner certainly and as brian promise many thing have become easy it be now more of the sort of home our coworker would expect we to have but something feel as though it be be lose I feel alienated perhaps it be only fatigue it didn t seem like the right time to tell brian that I no long want augusta I catch he up on the progress of the child s ailment and stop myself when I realize I be simply aimlessly list task that I d do in the house at work that I have give augusta to do I haven t speak out loud to another adult in what feel like forever I explain it s great you have some help though isn t it his eye light up with evangelical fever at the subject of augusta which I realize I d give he rare permission to enjoy his voice surge out of the black corner of her mouth you know where the vacuum attachment be right you know the toy surprise game that el can play with rexy augusta can play it with they and you know nicole be tell I that actually the mobilepay feature be pretty sophisticated personality conversation scheme you can have a little bit more of an intimate relationship with she — intimate I raise my eyebrow at he just you know nicole be say like because she and katie they feel the same as you at first but like there s a lot here nicole be say to I around like autonomy of ai the humanity I guess or specifically her womanhood the ethic of that whole thing you know I think jet lag might explain that kind of talk from he can she be set to have a man s voice no brian answer immediately they want it to be standardize it have to be standard across international if you have a male option imagine like with the socialization and cultural stuff it would literally be in the past it s always turn out to be literally more than twice the work and then what about gender neutral what about people like rexy it just by give she one voice it would be a strong vision for the product overall oh I say right hey listen gorgeous I have to jump back in here he say press both palm together in the high resolution image of he that shimmer in augusta s palm sized left eye screen in the right eye the display tick forward dutifully count each second of the call okay sweetheart I say look up the extra feature say brian quickly before disconnect augusta s eye become black and uncanny again I think I see her lip twitch briefly but certainly it be only my fatigue at the end of the week at group dr carla ask how we be all do with the week s prompt and everyone take turn answer at the time I really feel empowered like I be do the surgery for myself harriet be say and it s not that I m unhappy with my body now or that my partner be unhappy the opposite really thing be good I love it all thing be good but be this the future you imagine as we say when you be a little girl dr carla ask lean forward I couldn t have imagine it harriet say with a soft laugh I think mostly in those day I dream of become an international spy or of build heroic machine suit harriet be very beautiful and when she glance at I briefly I feel a warm rush imagine she as a co conspirator it be an exceptionally warm spring day and everyone be yawn dazzle by the waving of the bright green grass or of enter a crystal world I find myself blurt let s come to you polly dr carla say you ve be work out some issue around your foster sister jane and the future you want for she plus some internalize misogyny in general have you make any decision I look she up I say and then instantly regret it the urge to talk about — or to — jane have recently be squeeze out of my schedule of work weird hour and extract thick rope of green snot from el s nose with a sterile bulb there be a few possibility for how jane could have turn out but I couldn t imagine she with that lifestyle except maybe the forgetting to bathe part and everyone look at I it seem ruby in particular lean forward like someone about to eat a steak it make I realize my internalized misogyny problem be big than I think I recite quickly actually the real issue I m have be with my assistant augusta who happen to be an ai she s a virtual identity dr carla gently correct nod I talk about how augusta make I uncomfortable how I feel sort of like a failure how I wish she wasn t in the house but I didn t feel like I could remove she how I be jealous of the way brian and the kid admire she as with both my kid be sick only part of it be a lie I didn t say that I sometimes want to hit augusta and I have trouble see she as a person I say I want we all to acknowledge the courage it take polly to admit her issue with the personhood of virtual identity especially when they be woman dr carla say to a smattering of soft applause virtual identity offer we many opportunity to understand ourselves in relation to other in a safe way let s all consider how polly could own these feeling rather than displace they onto a being who ethically lot of we agree be autonomously alive in her own right I want to ask if polly have try develop any intimacy with augusta or if she s view she only as an employee or a slave fuck ruby the intimacy feature cost money and we have two kid I say turn to smile warmly at ruby many of these issue be just more complex and challenging when one become a mom you have two corporate income ruby reply without even flinch I m notice some conflict body language so I want to bring everyone back to the core thesis of this group which be woman support woman dr carla say ruby we all make an agreement to one another not to make assumption outside of what we each bring to the session but her socioeconomic position relative to issue of labor and identity be relevant ruby plead here we speak to and not about one another dr carla say your socioeconomic position — you know I grow up poor and have — let s try a moment of silence dr carla say and we all obey then let s leave that there for today let s remember we have all have different experience and that in this group we be all equally entitle to feel pain no matter how we come to be everyone seem placate by this and a satisfied dr carla smile personally I would be pleased to welcome a virtual woman to this group someday how about for next week s prompt we try share space who have we allow into our world and what have change about we as a result the last crystal animal my uncle arthur send jane be a frog when he die the tenor of my world change the machination of his heart disease add horrible consideration to that last stretch of senior year but while graduation be something I be prepared to anticipate and understand the loss of he still feel sudden and unfair jane and I have already start see less of each other then she have a new good friend of whom she say I be jealous but how could I have be jealous of a smelly remedial student with parched hair small lip small eye pick skin who have be write off by the rest of the school year ago and deservedly so since she be stupid as well as destructive this particular girl get suspend for beat a young kid in the face what kind of person do that the two of they be just gross together do mobilepay hack to pay for garish video game and eat pill they order online whenever I peek in the detention hall and see they together fool around I feel embarrassed for they I start back away we be go to be eighteen soon and I have important thing go on like help aunt cissy with everything learn to cook thing for us aunt cissy be often distraught and ask for jane which at the time really upset I since I be proud of all I be do for her most kid my age would have be out party and jane definitely be quickly get a reputation meanwhile I take care of my family and prepare for the future the last time I speak to jane I be twenty one or twenty two I come home to jamaica plain from college in chicago because aunt cissy have pass I be afraid my birth family might come to the funeral I be afraid about the bill and of what the house might look like and I be wrack by the feeling that I hadn t call she quite as often as I should have once I d move away I be incredibly vulnerable which partially explain what I do then jane be the only person who would have understand the loss I be sure all the scream fight and snitch on one another and name call we do at the end of high school feel well in the past of childhood surely we d both grow jane have make a lot of mistake and I have be unforgive but aunt cissy have be like a parent to she she have be so special to my family and maybe we hadn t be ready to deal with lose my uncle when we be so young but this time it be go to be different since we be adult but when I call mail and message jane on the way home I get inconsistent reply at first she tell I she d be seriously ill herself but be feel well and would meet I at cissy s place ; when I get there jane say she couldn t talk because she be at a friend s birthday but then late that night she be still stick at the birthday so I offer to come pick she up but get no reply on the morning of the funeral she send I a message with a cut tone reveal that actually she be be evict and it be a really overwhelming time and that she just wasn t able to perform for I right now she wasn t at the funeral luckily neither be any of my birth family really just one cousin but it be the least bad one who barely come near I I be too exhausted to be upset over anything else I end up drinking which would have kill my aunt and uncle and I find myself on public transit to the two family house in somerville where I know from social medium that jane and her friend be live she wasn t there either but an oily weed of a boy who be apparently her roommate let I in I think you guy be be evict I say lightly and he say nah the house be a sprawl collage of empty liter sodas painting lamp swath of pattern fabric overflowing ashtray stud with foil shape I couldn t identify but that fill I with dread serene guitar music filter through the air from someplace I feel the familiar bitter pang of envy despite myself — I never get invite to cool house like these I ask the roommate which room be jane s he say it be the one with all the book and I find it quickly a closet sized sanctuary that make I angry I would have know it be hers without be tell even down to her scent and it be perfectly neat line in fantasy book with a square of iridescent fabric pin gracefully to the ceiling over the bed my head pound and I fight with the desire to just stay there and wait for she as long as it take I m just get something of mine I think she have I call down the creak stair but the roommate have already forget about I as summer come on thing worsen at home the kid behavior degenerate the more demanding my client at work become and brian and I each have to travel more than once for summit that both our firm be involve with amid all of this ellis get extremely attach to augusta insist she stand over his cot when he sleep scream if I move she which cause I and brian to fight I find several parenting and screen time pamphlet in rex s school bag paranoid I imagine some judgmental teacher have sneak they in to send I a message ruby from group could be a teacher maybe even at rexy s school I hadn t be able to go to group with everything recently sunday have become our only together time which mean I sit in the living room pay bill or answer email while augusta run game of blue legend for brian and the kid rex scream at el to get off the pad brian suddenly call her gussie and her laugh augusta could laugh now what be all these mobilepay receipt for augusta feature I ask but no one answer rex snap the back of gussie s mobile mount with el s baby blanket again and again she laugh be respectful brian chide rex caress the bin like body with an open palm his foot in slipper be prop grandly up on the coffee table a strange new rudeness every few second the game emit a lick of musical noise and announce your move I pretend to have a headache and go to lie down hope brian would take the kid for gelato or something I hear he make a great show out of get they ready use a short tone with the kid and their shoe so I would hear tell augusta to check on I in 30 minute I suspect he think I should feel bad once everyone be go I go into the living room where augusta be stand and wait the disarray of the space discomfit I as do the sticky handprint and fingerprint smudge that be all over the brushed chrome mobile mount so I tell she to go in the kitchen and install her bust unit there in the kitchen I say augusta call jane I m call augusta say serenely her eye turn white time wheel turn in they jane say hello much more suddenly than I expect and I hold onto the counter just out of her sight tuck my hair behind my ear and lean close to the pinprick camera augusta wear over her eyebrow jane I say calmly even brightly it s polly polly oh wow polly jane be say and the person in the display be definitely she she have the same pointy face her hair be much dark than I remember she be sharp I recognize she and I didn t recognize her glance frantically around she for clue but find none she wear a black blazer and decent earring there be a serene white wall behind she I be startle nervous lightheaded I say I have be go through some old thing and thinking of she but she didn t ask what those be I ask how thing be frequently and with escalate pitch because she be reticent about detail for some reason so I tell about brian and the kid and my degree and the firm and finally she say she work at a university something about literature or cultural something I didn t understand really she get marry a few year ago they live in menlo park for a while but they just move to berkeley six month ago and be love it so yeah she say with a shrug thing be good there it be the brief appearance of her eye s familiar defiant gleam she know she know I have be expect thing not to be good whatever bridge have lead that troubled girl to become this astonishingly normal woman she have no inclination to describe the sudden loneliness I experience be concussive and I commit not to cry in front of she as I have so many time before I m basically call because I have something of your I say do you remember those crystal animal you use to get from cissy and arthur for a terrifying moment there be no recognition at all and then to my great relief she smile openly genuinely a familiar crooked teen shape opening in the unfamiliar adult s face oh yeah she say your parent be so so lovely to I I want to ask then why weren t you there when they die but I think the slight abrasion might startle away these fleeting glimpse of the jane I know do you remember stare into they to like see the future or whatever I say and crystal deer and all of that she pause blink and give I an oddly serene look you always have such a good memory she finally say no defiant gleam as if she really didn t remember the crystal world do you remember the unicorn that you get first she give I the same serene gutting look and shake her head slightly I remember I have a lot of they there probably be a unicorn I actually have they in a box I give to my daughter she might I m not sure where she have it honestly I could go try to dig they up if you want they back be that why you re call no I say I just want to know how you be do great she say I m great but listen I actually need to jump on a faculty call in about a minute should I try to call you back this weekend how about sure I say even though I already know there be no way I would talk to this unbearable simulacrum this skinsuit jane ever again augusta s eye go dark and she stare at I hollowly you win t know how you live without she then I yank the bust unit forcibly from the kitchen port raise the fiberglass creature over my head and bring she down hard on the kitchen floor I straddle she where her body would be and I begin to beat her inhuman face deliberately even though her upturne nose hurt my fist and palm desperate to crack that unflinche mouth which mock I finally a fissure appear between the eye socket and the pinprick camera and part of the forehead cave and I work my hand into the crack I could smell blood from the mark I be suffer rip out plasticine entrail and malleable conductor and by the time my knuckle reach metal I be exhaust and could do no more I leave the bust on the kitchen floor in crunch piece wash my hand in cold water then I stand on a chair to reach the top of the storage cabinet in our bedroom rifle around painfully finally I find the small misshapen cardboard box lick with year of reinforcement tape I clear away the inflatable packing and take out the crystal unicorn that I have take from jane s room when my aunt die sit at the edge of the bed examine it in my palm I be affirm to know that I want it as much as I always have the graceful kneeling shape with its abstract facet and long delicate horn it be remarkable that something so fine as the horn should have remain unbroken all this time and unexpectedly I blink back tear the crystal unicorn seem to swim dissolve then clarify just like it have on that magic night in a maine motel when we be little and look into it to see the future jane promise it could show we that day in somerville I find all of the crystal animal in their little box in a big vinyl storage case underneath all the staple book drawing and map we have make about they I stay in jane s bedroom for a long time read through batter paper streak in fat bright marker tremulous pencil cursive try to commit as much of it as I could to memory there be guide to the crystal world inside each creature that jane have imagine and that I have put to word each world could convey its own special blessing like to make we invisible or to make we impervious to pain it be true that nothing hurt while I be hold the unicorn we believe that inside the unicorn be a sort of astral lobby a heart chamber that connect everything if we ever get separate in the crystal world jane always say we meet back there I concentrate on the unicorn it be hard to know if the animal be in the midst of kneel or rise and as it swam in my eye I let my vision soften I draw close I see the beautiful familiar spire rise before I welcome I I hear the soft and distant music I m in I whisper but I know she would never be there again from a quick cheer to a stand ovation clap to show how much you enjoy this story I write about the intersection of technology popular culture and the life we ve live inside machine I m also a narrative designer leighalexander1 at gmail
Daniel Simmons,3.4K,8,https://itnext.io/you-can-build-a-neural-network-in-javascript-even-if-you-dont-really-understand-neural-networks-e63e12713a3?source=tag_archive---------8----------------,you can build a neural network in javascript even if you don t really understand neural network,click here to share this article on linkedin » skip this part if you just want to get on with it I should really start by admit that I m no expert in neural network or machine learning to be perfectly honest most of it still completely baffle I but hopefully that s encouraging to any fellow non expert who might be read this eager to get their foot wet in m l machine learning be one of those thing that would come up from time to time and I d think to myself yeah that would be pretty cool but I m not sure that I want to spend the next few month learn linear algebra and calculus like a lot of developer however I m pretty handy with javascript and would occasionally look for example of machine learning implement in js only to find heap of article and stackoverflow post about how js be a terrible language for m l which admittedly it be then I d get distract and move on figure that they be right and I should just get back to validate form input and wait for css grid to take off but then I find brain js and I be blow away where have this be hide the documentation be well write and easy to follow and within about 30 minute of get start I d set up and train a neural network in fact if you want to just skip this whole article and just read the readme on github be my guest it s really great that say what follow be not an in depth tutorial about neural network that delve into hide input layer activation function or how to use tensorflow instead this be a dead simple beginner level explanation of how to implement brain js that go a bit beyond the documentation here s a general outline of what we ll be do if you d prefer to just download a work version of this project rather than follow along with the article then you can clone the github repository here create a new directory and plop a good ol index html boilerplate file in there then create three js file brain js training datum js and script js or whatever generic term you use for your default js file and of course import all of these at the bottom of your index html file easy enough so far now go here to get the source code for brain js copy & paste the whole thing into your empty brain js file hit save and bam 2 out of 4 file be finish next be the fun part decide what your machine will learn there be countless practical problem that you can solve with something like this ; sentiment analysis or image classification for example I happen to think that application of m l that process text as input be particularly interesting because you can find training datum virtually everywhere and they have a huge variety of potential use case so the example that we ll be use here will be one that deal with classify text we ll be determine whether a tweet be write by donald trump or kim kardashian ok so this might not be the most useful application but twitter be a treasure trove of machine learning fodder and useless though it may be our tweet author identifier will nevertheless illustrate a pretty powerful point once it s be train our neural network will be able to look at a tweet that it have never see before and then be able to determine whether it be write by donald trump or by kim kardashian just by recognize pattern in the thing they write in order to do that we ll need to feed it as much training datum as we can bear to copy paste into our training datum js file and then we can see if we can identify ourselves some tweet author now all that s leave to do be set up brain js in our script js file and feed it some training datum in our training datum js file but before we do any of that let s start with a 30 000 foot view of how all of this will work set up brain js be extremely easy so we win t spend too much time on that but there be a few detail about how it s go to expect its input datum to be format that we should go over first let s start by look at the setup example that s include in the documentation which I ve slightly modify here that illustrate all this pretty well first of all the example above be actually a work a I it look at a give color and tell you whether black text or white text would be more legible on it which hopefully illustrate how easy brain js be to use just instantiate it train it and run it that s it I mean if you inline the training datum that would be 3 line of code pretty cool now let s talk about train datum for a minute there be two important thing to note in the above example other than the overall input { } output { } format of the training datum first the datum do not need to be all the same length as you can see on line 11 above only an r and a b value get pass whereas the other two input pass an r g and b value also even though the example above show the input as object it s worth mention that you could also use array I mention this largely because we ll be pass array of vary length in our project second those be not valid rgb value every one of they would come out as black if you be to actually use it that s because input value have to be between 0 and 1 in order for brain js to work with they so in the above example each color have to be process probably just feed through a function that divide it by 255 — the max value for rgb in order to make it work and we ll be do the same thing so if we want out neural network to accept tweet I e string as an input we ll need to run they through an similar function call encode below that will turn every character in a string into a value between 0 and 1 and store it in an array fortunately javascript have a native method for convert any character into ascii code call charcodeat so we ll use that and divide the outcome by the max value for extended ascii character 255 we re use extended ascii just in case we encounter any fringe case like é or 1⁄2 which will ensure that we get a value < 1 also we ll be store our training datum as plain text not as the encode datum that we ll ultimately be feed into our a I you ll thank I for this later so we ll need another function call processtrainingdata below that will apply the previously mention encode function to our training datum selectively convert the text into encode character and return an array of training datum that will play nicely with brain j so here s what all of that code will look like this go into your script js file something that you ll notice here that wasn t present in the example from the documentation show early other than the two helper function that we ve already go over be on line 20 in the train function which save the train neural network to a global variable call trainednet this prevent we from have to re train our neural network every time we use it once the network be train and save to the variable we can just call it like a function and pass in our encode input as show on line 25 in the execute function to use our a i alright so now your index html brain js and script js file be finish now all we need be to put something into training datum js and we ll be ready to go last but not least our training datum like I mention we re store all our tweet as text and encode they into numeric value on the fly which will make your life a whole lot easy when you actually need to copy paste training datum no format necessary just paste in the text and add a new row add that to your training data js file and you re do note although the above example only show 3 sample from each person I use 10 of each ; I just didn t want this sample to take up too much space of course your neural network s accuracy will increase proportionally to the amount of training datum that you give it so feel free to use more or less than I and see how it affect your outcome now to run your newly train neural network just throw an extra line at the bottom of your script js file that call the execute function and pass in a tweet from trump or kardashian ; make sure to console log it because we haven t build a ui here s a tweet from kim kardashian that be not in my training datum I e the network have never encounter this tweet before then pull up your index html page on localhost check the console aaand there it be the network correctly identify a tweet that it have never see before as originate from kim kardashian with a certainty of 86 % now let s try it again with a trump tweet and the result again a never before see tweet and again correctly identify this time with 97 % certainty now you have a neural network that can be train on any text that you want you could easily adapt this to identify the sentiment of an email or your company s online review identify spam classify blog post determine whether a message be urgent or not or any of a thousand different application and as useless as our tweet identifier be it still illustrate a really interesting point that a neural network like this can perform task as nuance as identify someone base on the way they write so even if you don t go out and create an innovative or useful tool that s power by machine learning this be still a great bit of experience to have in your developer tool belt you never know when it might come in handy or even open up new opportunity down the road once again all of this be available in a github repo here from a quick cheer to a stand ovation clap to show how much you enjoy this story web developer javascript enthusiast box fan itnext be a platform for it developer & software engineer to share knowledge connect collaborate learn and experience next gen technology
Logan Spears,2.3K,6,https://hackernoon.com/coursera-vs-udacity-for-machine-learning-f9c0d464a0eb?source=tag_archive---------9----------------,coursera vs udacity for machine learning hacker noon,2018 be an exciting time for student of machine learn there be a wealth of readily available educational material and the industry s importance only continue to grow that say with so many easily accessible resource choose the right fit for your interest can be difficult to help those consider enter the machine learn world i d like to share my experience from two course I take in 2017 coursera s machine learning course and udacity s machine learning engineer nanodegree program I find both course to be very instructive and worthwhile but very different in nature if you don t have time to take both then hopefully this post can help you decide which one be good for you coursera coursera s machine learning course be the og machine learning course lead by famed stanford professor andrew ng this course feel like a college course with a syllabus weekly schedule and standard lecture the college feel extend to the curriculum as well here be an example slide if that scare you you aren t alone I usually shy away from course heavy in math but I actually appreciate the approach in this course the course begin with a linear algebra refresher and explain machine learning concept like gradient descent cost function regularization etc along the way it be structure well than any in person college course I ever attend the material isn t easy but that s a good thing you come away from the course with the satisfaction of genuinely understand machine learn enough so that you could even build your own machine learning framework from scratch udacity udacity s machine learning engineer nanodegree program be the trade school alternative to coursera s academia from basic statistic to full fledge deep learn udacity teach you a plethora of industry standard technique to complete the program s well craft project the project be so good in fact that I fork their repos on github and leave my solution up as portfolio item the final step of the program be to complete a capstone project of your own choosing while you could theoretically do a similar project on your own I find the desire to complete my nanodegree to be a strong motivator ; I end up put in much more time and effort than I normally would have put into an independent side project ultimately I end up create something of which I be truly proud udacity s program doesn t so much teach as it do provide a framework and motivation for you to teach yourself comparison now that I ve introduce the two program I ll highlight the strength and weakness of each across a number of category programming environment as I mention coursera be the og machine learning course ; so it should come as no surprise that the it s teach in the og 3d math language and programming environment matlab due to matlab s cost and licensing issue the machine learn world have mostly move to python this move severely limit the utility of the programming assignment because you ll have to relearn a lot of that work in python if you be a seasoned programmer who know many language that might not be a big deal however if you be relatively new to program then this detour may cost you a lot of time the udacity course be teach in a modern python environment with popular framework like sklearn tensorflow and keras the course even teach student how to use aw to deploy machine learning software to the cloud the course also simplify the process of instal machine learning dependency with a docker image and ami amazon machine image for local and aw development respectively in fact the entire udacity environment be in line with industry good practice and student who learn it will be well equip in the job market winner = udacity lecture coursera s machine learning course be create and teach by the ai godfather himself andrew ng and this course have contribute in no small part to his reputation within the industry the lecture follow a single uniform format and each one build upon the last in a methodical way not to mention he lead every one himself lastly professor ng be also very encouraging in his video which I think be a nice touch udacity s lecture by contrast feature a rotate cast of character which can create very jarring transition between section I count at least seven different people lecture throughout the program while udacity attempt to provide multiple content source for its student the lack of homogeneity definitely dent my enthusiasm for the lecture by the end of the program I just skip right to the project and watch the lecture or even search youtube as need winner = coursera project coursera s course have programming assignment in which student s submit code to be test against automate unit test while this model help the class scale it leave you hunt through the forum when thing go wrong that say I never hit any major roadblock the assignment themselves be directly relate to the course material and reinforce the lecture sometimes it feel like I be actually create my own machine learning framework ; at other time however it feel like I be just implement method until the unit test pass udacity s project be extremely well design in fact they constitute some of the good educational material I ve ever encounter each project cover a subject such as unsupervised learning reinforcement learn linear regression in which you solve a multi step machine learning problem and write about your approach and understanding when you feel that you have complete a project you submit it to be grade by a human the quality of the feedback that I get be incredible the final project be a capstone that you get to pick yourself but it be still review by udacity s staff the proposal and final report end up be one of the good portfolio item I have ever create and one of the thing I be most proud of in my programming career winner = udacity cost coursera s price be hard to beat because it s free to get the certification its $ 80 if you be machine learning on a budget then coursera be a great choice udacity have recently change its pricing model for the machine learning nanodegree when I enter the program it be $ 200 a month now it be a $ 999 flat fee the per month pricing model incentivize I to finish the program quickly in only three month though I must admit give the quality of instructor feedback even with the price hike tuition still seem reasonable the highly skilled labor that be meticulously review project can t pay for itself with such a high dollar amount however sign up for the nanodegree program be obviously a much big consideration winner = coursera conclusion while the course tie on the number category win I be go to pick a winner it be udacity it may come as no surprise that a pay course beat out a free one but the udacity machine learn engineer nanodegree program give I the confidence to professional pursue machine learning position and opportunity ; and for that its entry fee be a very small price to pay that say I would still recommend you do both course start with coursera so that when you use battery include high level framework you understand the low level detail and have a well appreciation of what you re actually code after you ve build a strong conceptual foundation far refine your skill by learn practical industry standard practice at udacity overall I be so glad I take concrete step to enter the machine learn world in 2017 and I would encourage you to do the same in 2018 coursera s machine learning certificate machine learning engineer nanodegree certificate from a quick cheer to a stand ovation clap to show how much you enjoy this story programmer and entrepreneur find I @ spearsx com github notnil how hacker start their afternoon
James Le,2K,9,https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef?source=---------0----------------,how to do semantic segmentation use deep learning,this article be a comprehensive overview include a step by step guide to implement a deep learning image segmentation model nowadays semantic segmentation be one of the key problem in the field of computer vision look at the big picture semantic segmentation be one of the high level task that pave the way towards complete scene understand the importance of scene understanding as a core computer vision problem be highlight by the fact that an increase number of application nourish from infer knowledge from imagery some of those application include self drive vehicle human computer interaction virtual reality etc with the popularity of deep learning in recent year many semantic segmentation problem be be tackle use deep architecture most often convolutional neural net which surpass other approach by a large margin in term of accuracy and efficiency semantic segmentation be a natural step in the progression from coarse to fine inference it be also worthy to review some standard deep network that have make significant contribution to the field of computer vision as they be often use as the basis of semantic segmentation system a general semantic segmentation architecture can be broadly think of as an encoder network follow by a decoder network unlike classification where the end result of the very deep network be the only important thing semantic segmentation not only require discrimination at pixel level but also a mechanism to project the discriminative feature learn at different stage of the encoder onto the pixel space different approach employ different mechanism as a part of the decoding mechanism let s explore the 3 main approach the region base method generally follow the segmentation use recognition pipeline which first extract free form region from an image and describe they follow by region base classification at test time the region base prediction be transform to pixel prediction usually by label a pixel accord to the high scoring region that contain it r cnn region with cnn feature be one representative work for the region base method it perform the semantic segmentation base on the object detection result to be specific r cnn first utilize selective search to extract a large quantity of object proposal and then compute cnn feature for each of they finally it classify each region use the class specific linear svms compare with traditional cnn structure which be mainly intend for image classification r cnn can address more complicated task such as object detection and image segmentation and it even become one important basis for both field moreover r cnn can be build on top of any cnn benchmark structure such as alexnet vgg googlenet and resnet for the image segmentation task r cnn extract 2 type of feature for each region full region feature and foreground feature and find that it could lead to well performance when concatenate they together as the region feature r cnn achieve significant performance improvement due to use the highly discriminative cnn feature however it also suffer from a couple of drawback for the segmentation task due to these bottleneck recent research have be propose to address the problem include sds hypercolumn mask r cnn the original fully convolutional network fcn learn a mapping from pixel to pixel without extract the region proposal the fcn network pipeline be an extension of the classical cnn the main idea be to make the classical cnn take as input arbitrary sized image the restriction of cnn to accept and produce label only for specific sized input come from the fully connect layer which be fix contrary to they fcns only have convolutional and pool layer which give they the ability to make prediction on arbitrary sized input one issue in this specific fcn be that by propagate through several alternated convolutional and pool layer the resolution of the output feature map be down sample therefore the direct prediction of fcn be typically in low resolution result in relatively fuzzy object boundary a variety of more advanced fcn base approach have be propose to address this issue include segnet deeplab crf and dilated convolution most of the relevant method in semantic segmentation rely on a large number of image with pixel wise segmentation mask however manually annotate these mask be quite time consume frustrating and commercially expensive therefore some weakly supervised method have recently be propose which be dedicate to fulfil the semantic segmentation by utilize annotate bounding box for example boxsup employ the bounding box annotation as a supervision to train the network and iteratively improve the estimate mask for semantic segmentation simple do it treat the weak supervision limitation as an issue of input label noise and explore recursive training as a de noise strategy pixel level labeling interpret the segmentation task within the multiple instance learning framework and add an extra layer to constrain the model to assign more weight to important pixel for image level classification in this section let s walk through a step by step implementation of the most popular architecture for semantic segmentation — the fully convolutional net fcn we ll implement it use the tensorflow library in python 3 along with other dependency such as numpy and scipy in this exercise we will label the pixel of a road in image use fcn we ll work with the kitti road dataset for road lane detection this be a simple exercise from the udacity s self drive car nano degree program which you can learn more about the setup in this github repo here be the key feature of the fcn architecture there be 3 version of fcn fcn 32 fcn 16 fcn 8 we ll implement fcn 8 as detailed step by step below we first load the pre train vgg 16 model into tensorflow take in the tensorflow session and the path to the vgg folder which be downloadable here we return the tuple of tensor from vgg model include the image input keep_prob to control dropout rate layer 3 layer 4 and layer 7 now we focus on create the layer for a fcn use the tensor from the vgg model give the tensor for vgg layer output and the number of class to classify we return the tensor for the last layer of that output in particular we apply a 1x1 convolution to the encoder layer and then add decoder layer to the network with skip connection and upsample the next step be to optimize our neural network aka build tensorflow loss function and optimizer operation here we use cross entropy as our loss function and adam as our optimization algorithm here we define the train_nn function which take in important parameter include number of epoch batch size loss function optimizer operation and placeholder for input image label image learn rate for the training process we also set keep_probability to 0 5 and learning_rate to 0 001 to keep track of the progress we also print out the loss during train finally it s time to train our net in this run function we first build our net use the load_vgg layer and optimize function then we train the net use the train_nn function and save the inference datum for record about our parameter we choose epoch = 40 batch_size = 16 num_classe = 2 and image_shape = 160 576 after do 2 trial pass with dropout = 0 5 and dropout = 0 75 we find that the 2nd trial yield well result with well average loss to see the full code check out this link https gist github com khanhnamle1994 e2ff59ddca93c0205ac4e566d40b5e88 if you enjoy this piece I d love it if you hit the clap button 👏 so other might stumble upon it from a quick cheer to a stand ovation clap to show how much you enjoy this story blue ocean thinker https jameskle com nanonet machine learn api
Sarthak Jain,3.9K,10,https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74?source=---------1----------------,how to easily detect object with deep learning on raspberry pi,disclaimer I m build nanonet com to help build ml with less datum and no hardware the raspberry pi be a neat piece of hardware that have capture the heart of a generation with ~15 m device sell with hacker build even cool project on it give the popularity of deep learning and the raspberry pi camera we think it would be nice if we could detect any object use deep learning on the pi now you will be able to detect a photobomber in your selfie someone enter harambe s cage where someone keep the sriracha or an amazon delivery guy enter your house 20 m year of evolution have make human vision fairly evolve the human brain have 30 % of it s neuron work on processing vision as compare with 8 percent for touch and just 3 percent for hear human have two major advantage when compare with machine one be stereoscopic vision the second be an almost infinite supply of training datum an infant of 5 year have have approximately 2 7b image sample at 30fps to mimic human level performance scientist break down the visual perception task into four different category object detection have be good enough for a variety of application even though image segmentation be a much more precise result it suffer from the complexity of create training datum it typically take a human annotator 12x more time to segment an image than draw bounding box ; this be more anecdotal and lack a source also after detect object it be separately possible to segment the object from the bounding box object detection be of significant practical importance and have be use across a variety of industry some of the example be mention below object detection can be use to answer a variety of question these be the broad category there be a variety of model architecture that be use for object detection each with trade off between speed size and accuracy we pick one of the most popular one yolo you only look once and have show how it work below in under 20 line of code if you ignore the comment note this be pseudo code not intend to be a work example it have a black box which be the cnn part of it which be fairly standard and show in the image below you can read the full paper here https pjreddie com media file paper yolo_1 pdf for this task you probably need a few 100 image per object try to capture datum as close to the datum you re go to finally make prediction on draw bounding box on the image you can use a tool like labelimg you will typically need a few people who will be work on annotate your image this be a fairly intensive and time consume task you can read more about this at medium com nanonet nanonet how to use deep learning when you have limit datum f68c0b512cab you need a pretraine model so you can reduce the amount of datum require to train without it you might need a few 100k image to train the model you can find a bunch of pretraine model here the process of train a model be unnecessarily difficult to simplify the process we create a docker image would make it easy to train to start train the model you can run the docker image have a run sh script that can be call with the follow parameter you can find more detail at to train a model you need to select the right hyper parameter find the right parameter the art of deep learning involve a little bit of hit and try to figure out which be the good parameter to get the high accuracy for your model there be some level of black magic associate with this along with a little bit of theory this be a great resource for find the right parameter quantize model make it small to fit on a small device like the raspberry pi or mobile small device like mobile phone and rasberry pi have very little memory and computation power training neural network be do by apply many tiny nudge to the weight and these small increment typically need float point precision to work though there be research effort to use quantize representation here too take a pre train model and running inference be very different one of the magical quality of deep neural network be that they tend to cope very well with high level of noise in their input why quantize neural network model can take up a lot of space on disk with the original alexnet be over 200 mb in float format for example almost all of that size be take up with the weight for the neural connection since there be often many million of these in a single model the node and weight of a neural network be originally store as 32 bit float point number the simple motivation for quantization be to shrink file size by store the min and max for each layer and then compress each float value to an eight bit integer the size of the file be reduce by 75 % code for quantization you need the raspberry pi camera live and work then capture a new image for instruction on how to install checkout this link download model once your do train the model you can download it on to your pi to export the model run then download the model onto the raspberry pi install tensorflow on the raspberry pi depend on your device you might need to change the installation a little run model for predict on the new image the raspberry pi have constraint on both memory and compute a version of tensorflow compatible with the raspberry pi gpu be still not available therefore it be important to benchmark how much time do each of the model take to make a prediction on a new image we have remove the need to annotate image we have expert annotator who will annotate your image for you we automatically train the good model for you to achieve this we run a battery of model with different parameter to select the good for your data nanonet be entirely in the cloud and run without use any of your hardware which make it much easy to use since device like the raspberry pi and mobile phone be not build to run complex compute heavy task you can outsource the workload to our cloud which do all of the compute for you get your free api key from http app nanonet com user api_key collect the image of object you want to detect you can annotate they either use our web ui https app nanonet com objectannotation appid = your_model_id or use open source tool like labelimg once you have dataset ready in folder image image file and annotation annotation for the image file start upload the dataset once the image have be upload begin train the model the model take ~2 hour to train you will get an email once the model be train in the meanwhile you check the state of the model once the model be train you can make prediction use the model from a quick cheer to a stand ovation clap to show how much you enjoy this story founder & ceo @ nanonet com nanonet machine learn api
Bharath Raj,2.2K,15,https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced?source=---------2----------------,data augmentation | how to use deep learning when you have limit datum — part 2,we have all be there you have a stellar concept that can be implement use a machine learning model feeling ebullient you open your web browser and search for relevant datum chance be you find a dataset that have around a few hundred image you recall that most popular dataset have image in the order of ten of thousand or more you also recall someone mention have a large dataset be crucial for good performance feeling disappoint you wonder ; can my state of the art neural network perform well with the meagre amount of datum I have the answer be yes but before we get into the magic of make that happen we need to reflect upon some basic question when you train a machine learning model what you re really do be tune its parameter such that it can map a particular input say an image to some output a label our optimization goal be to chase that sweet spot where our model s loss be low which happen when your parameter be tune in the right way naturally if you have a lot of parameter you would need to show your machine learning model a proportional amount of example to get good performance also the number of parameter you need be proportional to the complexity of the task your model have to perform you don t need to hunt for novel new image that can be add to your dataset why because neural network aren t smart to begin with for instance a poorly train neural network would think that these three tennis ball show below be distinct unique image so to get more datum we just need to make minor alteration to our exist dataset minor change such as flip or translation or rotation our neural network would think these be distinct image anyway a convolutional neural network that can robustly classify object even if its place in different orientation be say to have the property call invariance more specifically a cnn can be invariant to translation viewpoint size or illumination or a combination of the above this essentially be the premise of data augmentation in the real world scenario we may have a dataset of image take in a limited set of condition but our target application may exist in a variety of condition such as different orientation location scale brightness etc we account for these situation by train our neural network with additional synthetically modify datum yes it can help to increase the amount of relevant datum in your dataset this be relate to the way with which neural network learn let I illustrate it with an example imagine that you have a dataset consist of two brand of car as show above let s assume that all car of brand a be align exactly like the picture in the left I e all car be face left likewise all car of brand b be align exactly like the picture in the right I e face right now you feed this dataset to your state of the art neural network and you hope to get impressive result once it s train let s say it s do training and you feed the image above which be a brand a car but your neural network output that it s a brand b car you re confused didn t you just get a 95 % accuracy on your dataset use your state of the art neural network I m not exaggerate similar incident and goof up have occur in the past why do this happen it happen because that s how most machine learning algorithm work it find the most obvious feature that distinguish one class from another here the feature be that all car of brand a be face left and all car of brand b be face right how do we prevent this happening we have to reduce the amount of irrelevant feature in the dataset for our car model classifier above a simple solution would be to add picture of car of both class face the other direction to our original dataset well yet you can just flip the image in the exist dataset horizontally such that they face the other side now on train the neural network on this new dataset you get the performance that you intend to get before we dive into the various augmentation technique there s one issue that we must consider beforehand the answer may seem quite obvious ; we do augmentation before we feed the datum to the model right yes but you have two option here one option be to perform all the necessary transformation beforehand essentially increase the size of your dataset the other option be to perform these transformation on a mini batch just before feed it to your machine learning model the first option be know as offline augmentation this method be prefer for relatively small dataset as you would end up increase the size of the dataset by a factor equal to the number of transformation you perform for example by flip all my image I would increase the size of my dataset by a factor of 2 the second option be know as online augmentation or augmentation on the fly this method be prefer for large dataset as you can t afford the explosive increase in size instead you would perform transformation on the mini batch that you would feed to your model some machine learning framework have support for online augmentation which can be accelerate on the gpu in this section we present some basic but powerful augmentation technique that be popularly use before we explore these technique for simplicity let we make one assumption the assumption be that we don t need to consider what lie beyond the image s boundary we ll use the below technique such that our assumption be valid what would happen if we use a technique that force we to guess what lie beyond an image s boundary in this case we need to interpolate some information we ll discuss this in detail after we cover the type of augmentation for each of these technique we also specify the factor by which the size of your dataset would get increase aka data augmentation factor you can flip image horizontally and vertically some framework do not provide function for vertical flip but a vertical flip be equivalent to rotate an image by 180 degree and then perform a horizontal flip below be example for image that be flip you can perform flip by use any of the follow command from your favorite package data augmentation factor = 2 to 4x one key thing to note about this operation be that image dimension may not be preserve after rotation if your image be a square rotate it at right angle will preserve the image size if it s a rectangle rotate it by 180 degree would preserve the size rotate the image by finer angle will also change the final image size we ll see how we can deal with this issue in the next section below be example of square image rotate at right angle you can perform rotation by use any of the follow command from your favorite package data augmentation factor = 2 to 4x the image can be scale outward or inward while scale outward the final image size will be large than the original image size most image framework cut out a section from the new image with size equal to the original image we ll deal with scaling inward in the next section as it reduce the image size force we to make assumption about what lie beyond the boundary below be example or image be scale you can perform scale by use the follow command use scikit image datum augmentation factor = arbitrary unlike scale we just randomly sample a section from the original image we then resize this section to the original image size this method be popularly know as random cropping below be example of random cropping if you look closely you can notice the difference between this method and scale you can perform random crop by use any the follow command for tensorflow data augmentation factor = arbitrary translation just involve move the image along the x or y direction or both in the follow example we assume that the image have a black background beyond its boundary and be translate appropriately this method of augmentation be very useful as most object can be locate at almost anywhere in the image this force your convolutional neural network to look everywhere you can perform translation in tensorflow by use the follow command datum augmentation factor = arbitrary over fitting usually happen when your neural network try to learn high frequency feature pattern that occur a lot that may not be useful gaussian noise which have zero mean essentially have data point in all frequency effectively distort the high frequency feature this also mean that low frequency component usually your intend datum be also distort but your neural network can learn to look past that add just the right amount of noise can enhance the learning capability a tone down version of this be the salt and pepper noise which present itself as random black and white pixel spread through the image this be similar to the effect produce by add gaussian noise to an image but may have a low information distortion level you can add gaussian noise to your image by use the follow command on tensorflow datum augmentation factor = 2x real world natural datum can still exist in a variety of condition that can not be account for by the above simple method for instance let we take the task of identify the landscape in photograph the landscape could be anything freeze tundras grassland forest and so on sound like a pretty straight forward classification task right you d be right except for one thing we be overlook a crucial feature in the photograph that would affect the performance — the season in which the photograph be take if our neural network do not understand the fact that certain landscape can exist in a variety of condition snow damp bright etc it may spuriously label frozen lakeshore as glacier or wet field as swamps one way to mitigate this situation be to add more picture such that we account for all the seasonal change but that be an arduous task extend our data augmentation concept imagine how cool it would be to generate effect such as different season artificially without go into gory detail conditional gan can transform an image from one domain to an image to another domain if you think it sound too vague it s not ; that s literally how powerful this neural network be below be an example of conditional gan use to transform photograph of summer scenery to winter scenery the above method be robust but computationally intensive a cheap alternative would be something call neural style transfer it grab the texture ambiance appearance of one image aka the style and mix it with the content of another use this powerful technique we produce an effect similar to that of our conditional gan in fact this method be introduce before cgan be invent the only downside of this method be that the output tend to look more artistic rather than realistic however there be certain advancement such as deep photo style transfer show below that have impressive result we have not explore these technique in great depth as we be not concerned with their inner working we can use exist train model along with the magic of transfer learn to use it for augmentation what if you want to translate an image that doesn t have a black background what if you want to scale inward or rotate in finer angle after we perform these transformation we need to preserve our original image size since our image do not have any information about thing outside it s boundary we need to make some assumption usually the space beyond the image s boundary be assume to be the constant 0 at every point hence when you do these transformation you get a black region where the image be not define but be that the right assumption in the real world scenario it s mostly a no image processing and ml framework have some standard way with which you can decide on how to fill the unknown space they be define as follow the simple interpolation method be to fill the unknown region with some constant value this may not work for natural image but can work for image take in a monochromatic background the edge value of the image be extend after the boundary this method can work for mild translation the image pixel value be reflect along the image boundary this method be useful for continuous or natural background contain tree mountain etc this method be similar to reflect except for the fact that at the boundary of reflection a copy of the edge pixel be make normally reflect and symmetric can be use interchangeably but difference will be visible while deal with very small image or pattern the image be just repeat beyond its boundary as if it s be tile this method be not as popularly use as the rest as it do not make sense for a lot of scenario besides these you can design your own method for deal with undefined space but usually these method would just do fine for most classification problem if you use it in the right way then yes what be the right way you ask well sometimes not all augmentation technique make sense for a dataset consider our car example again below be some of the way by which you can modify the image sure they be picture of the same car but your target application may never see car present in these orientation for instance if you re just go to classify random car on the road only the second image would make sense to be on the dataset but if you own an insurance company that deal with car accident and you want to identify model of upside down break car as well the third image make sense the last image may not make sense for both the above scenario the point be while use augmentation technique we have to make sure to not increase irrelevant datum you re probably expect some result to motivate you to walk the extra mile fair enough ; I ve get that cover too let I prove that augmentation really work use a toy example you can replicate this experiment to verify let s create two neural network to classify datum to one among four class cat lion tiger or a leopard the catch be one will not use data augmentation whereas the other will you can download the dataset from here link if you ve check out the dataset you ll notice that there s only 50 image per class for both training and testing clearly we can t use augmentation for one of the classifier to make the odd more fair we use transfer learn to give the model a well chance with the scarce amount of datum for the one without augmentation let s use a vgg19 network I ve write a tensorflow implementation here which be base on this implementation once you ve clone my repo you can get the dataset from here and vgg19 npy use for transfer learn from here you can now run the model to verify the performance I would agree though write extra code for data augmentation be indeed a bit of an effort so to build our second model I turn to nanonet they internally use transfer learning and datum augmentation to provide the good result use minimal datum all you need to do be upload the datum on their website and wait until it s train in their server usually around 30 minute what do you know it s perfect for our comparison experiment once it s do training you can request call to their api to calculate the test accuracy checkout out my repo for a sample code snippet don t forget to insert your model s i d in the code snippet impressive isn t it it be a fact that most model perform well with more datum so to provide a concrete proof I ve mention the table below it show the error rate of popular neural network on the cifar 10 c10 and cifar 100 c100 dataset c10 + and c100 + column be the error rate with data augmentation thank you for read this article hit that clap button if you do hope it shed some light about data augmentation if you have any question you could hit I up on social medium or send I an email bharathrajn98@gmail com from a quick cheer to a stand ovation clap to show how much you enjoy this story undergrad | computer vision and ai enthusiast | hungry nanonet machine learn api
Daniel Rothmann,302,8,https://towardsdatascience.com/human-like-machine-hearing-with-ai-1-3-a5713af6e2f8?source=---------3----------------,human like machine hearing with ai 1 3 towards data science,significant breakthrough in ai technology have be achieve through model human system while artificial neural network nn be mathematical model which be only loosely couple with the way actual human neuron function their application in solve complex and ambiguous real world problem have be profound additionally model the architectural depth of the brain in nns have open up broad possibility in learn more meaningful representation of datum in image recognition and process the inspiration from the complex and more spatially invariant cell of the visual system in cnn have also produce great improvement to our technology if you re interested in apply image recognition technology on audio spectrogram check out my article what s wrong with cnn and spectrogram for audio processing as long as human perceptual capacity exceed that of machine we stand to gain by understand the principle of human system human be very skillful when it come to perceptual task and the contrast between human understanding and the status quo of ai become particularly apparent in the area of machine hearing consider the benefit reap from get inspire by human system in visual processing I propose that we stand to gain from a similar process in machine hearing with neural network in this article series I will detail a framework for real time audio signal processing with ai which be develop in cooperation with aarhus university and intelligent loudspeaker manufacturer dynaudio a s its inspiration be primarily draw from cognitive science which attempt to combine perspective of biology neuroscience psychology and philosophy to gain great understanding of our cognitive faculty perhaps the most abstract domain of sound be how we as human perceive it while a solution for a signal processing problem have to operate within the parameter of intensity spectral and temporal property on a low level the end goal be most often a cognitive one transform a signal in such a way that our perception of the sound it contain be alter if one wish to programatically change the gender of a record speak voice for example it be necessary to describe this problem in more meaningful term before define its low level characteristic the gender of a speaker can be conceive as a cognitive property which be construct from many factor general pitch and timbre of a voice difference in pronunciation difference in choice of word and language and a common understanding of how these property relate to gender these parameter can be describe in low level feature like intensity spectral and temporal property but only in more complex combination do they form high level representation this form a hierarchy of audio feature from which the meaning of a sound can be derive the cognitive property represent a human voice can be think of as a combinatory pattern of temporal development in a sound s intensity spectral and statistical property nns be great at extract abstract representation of datum and be therefore well suited for the task of detect cognitive property in sound in order to build a system for this purpose let s examine how sound be represent in human auditory organ that we can use to inspire representation of sound for process with nns hearing in human start at the outer ear which firstly consist of the pinna the pinna act as a form of spectral preprocessing in which the incoming sound be modify depend on its direction in relation to the listener sound then travel through the opening in the pinna into the ear canal which far act to modify spectral property of incoming sound by resonate in a way that amplify frequency in the range ~1 6 khz 1 as sound wave reach the end of the ear canal they excite the eardrum onto which the ossicle the small bone in the body be attach these bone transmit the pressure from the ear canal to the fluid fill cochlea in the inner ear 1 the cochlea be of great interest in guide sound representation for nns because this be the organ responsible for transduce acoustic vibration into neural activity in human it be a coil tube which be separate along its length by two membrane be the reissner s membrane and the basilar membrane along the length of the cochlea there be a row of around 3 500 inner hair cell 1 as pressure enter the cochlea its two membrane be push down the basilar membrane be narrow and stiff at its base but loose and wide at its apex so that each place along its length respond more intensely at a particular frequency to simplify the basilar membrane can be think of as a continuous array of bandpass filter which along the length of the membrane act to separate sound into their spectral component this be the primary mechanism by which human convert sound pressure into neural activity therefore it be reasonable to assume that spectral representation of audio would be beneficial in model sound perception with ai since frequency response along the basilar membrane vary exponentially 2 logarithmic frequency representation might prove most efficient one such representation could be derive use a gammatone filterbank these filter be commonly apply in model spectral filtering in the auditory system since they approximate the impulse response of human auditory filter derive from the measure auditory nerve fiber response to white noise stimulus call the revcor function 3 since the cochlea have ~3500 inner hair cell and human can detect gap in sound down to ~2 5 ms in length 1 a spectral resolution of 3500 gammatone filter separate into 2 ms window seem optimal parameter for achieve human like spectral representation in machine in practical scenario however I assume that less resolution could still achieve desirable effect in most analysis and processing task while be more viable from a computational standpoint a number of software library for auditory analysis be available online a notable example be the gammatone filterbank toolkit by jason heeris it provide adjustable filter as well as tool for spectrogram like analysis of audio signal with gammatone filter as neural activity move from the cochlea onto the auditory nerve and the ascend auditory pathway a number of process be apply in brainstem nucleus before it reach the auditory cortex these process form a neural code which represent an interface between stimulus and perception 4 much knowledge about the specific inner working of these nucleus be still speculative or unknown so I will detail these nucleus only at their high level of function human have a set of these nucleus for each ear that be interconnect but for simplicity I ve illustrate the flow for only one ear the cochlear nucleus be the first code step for neural signal come from the auditory nerve it consist of a variety of neuron with different property which serve to perform initial processing of sound feature some of which be direct to the superior olive which be associate with sound localization while other be direct to the lateral lemniscus and inferior colliculus commonly associate with more advanced feature 1 j j eggermont detail this flow of information from the cochlear nucleus in between sound and perception review the search for a neural code as follow the ventral cochlear nucleus vcn extract and enhance the frequency and time information that be multiplexe in the firing pattern of the auditory nerve fiber and distribute the result via two main pathway the sound localization path and the sound identification path the anterior part of the vcn avcn mainly serve the sound localization aspect and its two type of bushy cell provide input to the superior olivary complex soc where interaural time difference itds and level difference ild be map for each frequency separately 4 the information carry by the sound identification pathway be a representation of complex spectra such as vowel this representation be mainly create in the ventral cochlear nucleus by special type of unit dub chopper stellate neuron 4 the detail of these auditory encoding be difficult to specify but they indicate to we that a form of code of incoming frequency spectra could improve understanding of low level sound feature as well as make sound impression less expensive to process in nns we can apply the unsupervised autoencoder nn architecture as an attempt to learn common property associate with complex spectra like word embedding its possible to find commonality in frequency spectra that represent select feature or a more tightly condensed meaning of sound an autoencoder be train to encode an input into a compressed representation that can be reconstruct back into a representation with a high similarity to the input this mean that the autoencoder s target output be the input itself 5 if an input can be reconstruct without great loss the network have learn to encode it in such a way that the compress internal representation contain enough meaningful information this internal representation be then what we refer to as the embed the encode part of the autoencoder can be decouple from the decoder to generate embedding for other application embedding also have the benefit that they be often of low dimensionality than the original datum for instance an autoencoder could compress a frequency spectrum with a total of 3500 value into a vector with a length of 500 value put simply each value of such a vector could describe high level factor of a spectrum such as vowel harshness or harmonicity these be only example as the meaning of statistically common factor derive by an autoencoder might often be difficult to label in plain language in the next article we will expand upon this idea with add memory to produce embedding for temporal development of audio frequency spectra this wrap up the first part of my article series on audio processing with artificial intelligence next we will discuss the essential concept of sensory memory and temporal dependency in sound follow to stay update and feel free to leave clap if you enjoy the article as always feel free to connect with I on linkedin to stay in touch 1 c j plack the sense of hear 2nd ed psychology press 2014 2 s j elliott and c a shera the cochlea as a smart structure smart mater struct vol 21 no 6 p 64001 jun 2012 3 a m darle property and implementation of the gammatone filter a tutorial speech hearing and language university college london 1991 4 j j eggermont between sound and perception review the search for a neural code hear re vol 157 no 1 2 pp 1 42 jul 2001 5 t p lillicrap et al learning deep architecture for ai vol 2 no 1 2015 from a quick cheer to a stand ovation clap to show how much you enjoy this story ai engineer @ convai especially interested in audio and time series forecasting reach we at convai dk sharing concept idea and code
Amine Aoullay,58,4,https://towardsdatascience.com/how-to-use-noise-to-your-advantage-5301071d9dc3?source=---------4----------------,how to use noise to your advantage towards data science,for scientist random fluctuation or noise be undesirable although typically assume to degrade performance it can sometimes improve information processing in non linear system in this post we ll see some example where the noise can be use as an advantage recent work have show that by allow some inaccuracy when train deep neural network not only the training performance but also the accuracy of the model can be improve neural network be capable of learn output function that can change wildly with small change in input add noise to inputs randomly be like tell the network to not change the output in a ball around your exact input by limit the amount of information in a network we force it to learn compact representation of input feature rl be an area of machine learning that assume there be an agent situate in an environment at each step the agent take an action and it receive an observation and reward from the environment an rl algorithm seek to maximize the agent s total reward give a previously unknown environment through a learning process that usually involve lot of trial and error to understand the challenge with exploration in deep rl system think about researcher that spend lot of time in a lab without produce any practical application equivalently rl agent can spend a huge amount of resource without converge to a local optimum openai propose a technique call parameter space noise that introduce noise in the model policy parameter at the beginning of each episode other approach be focus on what be know as action space noise which introduce noise to change the likelihood associate with each action the agent might take from one moment to the next the initial result of the parameter space noise model prove to be really promise the technique help algorithm explore their environment more effectively lead to high score and more elegant behavior more detail can be find in the research paper the important thing to remember be that add noise be use as an advantage to boost the exploration performance of reinforcement learning algorithm boost recognition isn t as simple as throw more label image at these system indeed manually annotate a large number of image be an expensive and time consuming process facebook researcher and engineer have address this by train image recognition network on large set of public image with hashtag since people often caption their photo with hashtag it woul d be a good source of training datum for model facebook develop new approach that be tailor for do image recognition experiment use hashtag supervision this study be describe in detail in explore the limit of weakly supervised pretraining on the coco object detection challenge it have be show that the use of hashtag for pretraining can boost the average precision of a model by more than 2 percent noise should not be our enemy it isn t always an unwanted disturbance and can often be use as an advantage and even serve as a valuable research tool if anyone try to tell you otherwise well just give he the example we present stay tune and if you like this article please leave a 👏 1 weakly supervised pretraine https research fb com publication explore the limit of weakly supervised pretraine 2 well exploration with parameter noise https blog openai com well exploration with parameter noise from a quick cheer to a stand ovation clap to show how much you enjoy this story msc in machine learning mva @ ens paris saclay sharing concept idea and code
Jonathan Balaban,804,5,https://towardsdatascience.com/deep-learning-tips-and-tricks-1ef708ec5f53?source=---------5----------------,deep learning tip and trick towards data science,below be a distilled collection of conversation message and debate I ve have with peer and student on how to optimize deep model if you have trick you ve find impactful please share they deep learning model like the convolutional neural network cnn have a massive number of parameter ; we can actually call these hyper parameter because they be not optimize inherently in the model you could gridsearch the optimal value for these hyper parameter but you ll need a lot of hardware and time so do a true data scientist settle for guess these essential parameter one of the good way to improve your model be to build on the design and architecture of the expert who have do deep research in your domain often with powerful hardware at their disposal graciously they often open source the result modeling architecture and rationale here be a few way you can improve your fit time and accuracy with pre train model here s how to modify dropout and limit weight size in keras with mnist here s an example of final layer modification in keras with 14 class for mnist and an example of how to freeze weight in the first five layer alternatively we can set the learning rate to zero for that layer or use per parameter adaptive learning algorithm like adadelta or adam this be somewhat complicated and well implement in other platform like caffe it s often essential to get a visual idea of how your model look if you re work in keras abstraction be nice but doesn t allow you to drill down into section of your model for deep analysis fortunately the code below let we visualize our model directly with python this will plot a graph of the model and save it as a png file plot take two optional argument you can also directly obtain the pydot graph object and render it yourself for example to show it in an ipython notebook I hope this collection help with your modeling endeavor let I know your good trick and connect with I on twitter and linkedin from a quick cheer to a stand ovation clap to show how much you enjoy this story datum science nomad sharing concept idea and code
Arthur Juliani,9K,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=---------6----------------,simple reinforcement learning with tensorflow part 0 q learning with table and neural network,for this tutorial in my reinforcement learning series we be go to be explore a family of rl algorithms call q learning algorithms these be a little different than the policy base algorithm that will be look at in the the follow tutorial part 1 3 instead of start with a complex and unwieldy deep neural network we will begin by implement a simple lookup table version of the algorithm and then show how to implement a neural network equivalent use tensorflow give that we be go back to basic it may be good to think of this as part 0 of the series it will hopefully give an intuition into what be really happen in q learning that we can then build on go forward when we eventually combine the policy gradient and q learning approach to build state of the art rl agent if you be more interested in policy network or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient method which attempt to learn function which directly map an observation to an action q learning attempt to learn the value of be in a give state and take a specific action there while both approach ultimately allow we to take intelligent action give a situation the mean of get to that action differ significantly you may have hear about deepq network which can play atari game these be really just large and more complex implementation of the q learning algorithm we be go to discuss here for this tutorial we be go to be attempt to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provide an easy way for people to experiment with their learn agent in an array of provide toy game the frozenlake environment consist of a 4x4 grid of block each one either be the start block the goal block a safe frozen block or a dangerous hole the objective be to have an agent learn to navigate from the start to the goal without move onto a hole at any give time the agent can choose to move either up down left or right the catch be that there be a wind which occasionally blow the agent onto a space they didn t choose as such perfect performance every time be impossible but learn to avoid the hole and reach the goal be certainly still doable the reward at every step be 0 except for enter the goal which provide a reward of 1 thus we will need an algorithm that learn long term expect reward this be exactly what q learning be design to provide in it s simple implementation q learning be a table of value for every state row and action column possible in the environment within each cell of the table we learn a value for how good it be to take a give action within a give state in the case of the frozenlake environment we have 16 possible state one for each block and 4 possible action the four direction of movement give we a 16x4 table of q value we start by initialize the table to be uniform all zero and then as we observe the reward we obtain for various action we update the table accordingly we make update to our q table use something call the bellman equation which state that the expect long term reward for a give action be equal to the immediate reward from the current action combine with the expect reward from the good future action take at the follow state in this way we reuse our own q table when estimate how to update our table for future action in equation form the rule look like this this say that the q value for a give state s and action a should represent the current reward r plus the maximum discount γ future reward expect accord to our own table for the next state s we would end up in the discount variable allow we to decide how important the possible future reward be compare to the present reward by update in this way the table slowly begin to obtain accurate measure of the expect future reward for a give action in a give state below be a python walkthrough of the q table algorithm implement in the frozenlake environment thank to praneet d for find the optimal hyperparameter for this approach now you may be think table be great but they don t really scale do they while it be easy to have a 16x4 table for a simple grid world the number of possible state in any modern game or real world environment be nearly infinitely large for most interesting problem table simply don t work we instead need some way to take a description of our state and produce q value for action without a table that be where neural network come in by act as a function approximator we can take any number of possible state that can be represent as a vector and learn to map they to q value in the case of the frozenlake example we will be use a one layer network which take the state encode in a one hot vector 1x16 and produce a vector of 4 q value one for each action such a simple network act kind of like a glorify table with the network weight serve as the old cell the key difference be that we can easily expand the tensorflow network with add layer activation function and different input type whereas all that be impossible with a regular table the method of update be a little different as well instead of directly update our table with a network we will be use backpropagation and a loss function our loss function will be sum of square loss where the difference between the current predict q value and the target value be compute and the gradient pass through the network in this case our q target for the choose action be the equivalent to the q value compute in equation 1 above below be the tensorflow walkthrough of implement our simple q network while the network learn to solve the frozenlake problem it turn out it doesn t do so quite as efficiently as the q table while neural network allow for great flexibility they do so at the cost of stability when it come to q learning there be a number of possible extension to our simple q network which allow for great performance and more robust learn two trick in particular be refer to as experience replay and freeze target network those improvement and other tweak be the key to get atari play deep q network and we will be explore those addition in the future for more info on the theory behind q learning see this great post by tambet matiisen I hope this tutorial have be helpful for those curious about how to implement simple q learning algorithms if this post have be valuable to you please consider donate to help support future tutorial article and implementation any contribution be greatly appreciate if you d like to follow my work on deep learning ai and cognitive science follow I on medium @arthur juliani or on twitter @awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @unity3d & cognitive neuroscience phd student explore frontier technology through the lens of artificial intelligence datum science and the shape of thing to come
SAGAR SHARMA,2.5K,5,https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6?source=---------7----------------,activation function neural network towards data science,what be activation function why we use activation function with neural network the activation function can be basically divide into 2 type as you can see the function be a line or linear therefore the output of the function will not be confine between any range equation f x = x range infinity to infinity it doesn t help with the complexity or various parameter of usual datum that be feed to the neural network the nonlinear activation function be the most use activation function nonlinearity help to make the graph look something like this it make it easy for the model to generalize or adapt with variety of datum and to differentiate between the output the main terminology need to understand for nonlinear function be the nonlinear activation function be mainly divide on the basis of their range or curve 1 sigmoid or logistic activation function the sigmoid function curve look like a s shape the main reason why we use sigmoid function be because it exist between 0 to 1 therefore it be especially use for model where we have to predict the probability as an output since probability of anything exist only between the range of 0 and 1 sigmoid be the right choice the function be differentiable that mean we can find the slope of the sigmoid curve at any two point the function be monotonic but function s derivative be not the logistic sigmoid function can cause a neural network to get stick at the training time the softmax function be a more generalize logistic activation function which be use for multiclass classification 2 tanh or hyperbolic tangent activation function tanh be also like logistic sigmoid but well the range of the tanh function be from 1 to 1 tanh be also sigmoidal s shape the advantage be that the negative input will be map strongly negative and the zero input will be map near zero in the tanh graph the function be differentiable the function be monotonic while its derivative be not monotonic the tanh function be mainly use classification between two class 3 relu rectify linear unit activation function the relu be the most use activation function in the world right now since it be use in almost all the convolutional neural network or deep learning as you can see the relu be half rectified from bottom f z be zero when z be less than zero and f z be equal to z when z be above or equal to zero range 0 to infinity the function and its derivative both be monotonic but the issue be that all the negative value become zero immediately which decrease the ability of the model to fit or train from the datum properly that mean any negative input give to the relu activation function turn the value into zero immediately in the graph which in turn affect the result graph by not map the negative value appropriately 4 leaky relu it be an attempt to solve the die relu problem can you see the leak 😆 the leak help to increase the range of the relu function usually the value of a be 0 01 or so when a be not 0 01 then it be call randomized relu therefore the range of the leaky relu be infinity to infinity both leaky and randomize relu function be monotonic in nature also their derivative also monotonic in nature I will be post 2 post per week so don t miss the tutorial so follow I on medium facebook twitter linkedin google+ quora to see similar post any comment or if you have any question write it in the comment clap it share it follow I happy to be helpful kudo 2 epoch vs batch size vs iteration 3 train inception with custom image on cpu 4 tensorflow image recognition python api tutorial on cpu from a quick cheer to a stand ovation clap to show how much you enjoy this story I be interested in programming python c++ arduino machine learning I m the editor of arduino community on medium I also like to write stuff sharing concept idea and code
Jae Duk Seo,33,6,https://towardsdatascience.com/principal-component-analysis-network-in-tensorflow-with-interactive-code-7be543047704?source=---------8----------------,principal component analysis network in tensorflow with interactive code,a natural extension from principle component analysis pool layer would be make a full neural network out of the layer I want to know if this be even possible as well as how well or bad it perform on mnist datum principle component analysis pca pooling layer for anyone who be not familiar with pcap please read this blog post first the basic idea be pool layer such as max or mean pooling operation perform dimensionality reduction to not only to save computational power but also to act as a regularizer pca be a dimensionality reduction technique in which convert correlate variable into a set of value of linearly uncorrelate variable call principal component and we can take advantage of this operation to do a similar job as max mean pooling network compose of majority of pool layer now I know what you be think it doesn t make sense to have a network that be only compose of pool layer while perform classification and you be completely right it doesn t but I just want to try this out for fun datum set network architecture blue rectangle → pcap or max pooling layergreen rectangle → convolution layer to increase channel size + global averaging pooling operation the network itself be very simple only four pooling layer and one convolution layer to increase the channel size however in order for the dimension to match up we will downsample each image into 16 * 16 dimension hence the tensor will have a shape of batch size 16 16 1 → batch size 8 8 1 → batch size 4 4 1 → batch size 2 2 1 → batch size 1 1 1 → batch size 1 1 10 → batch size 10 and we can perform classification with soft max layer as any other network do result principle component network as see above the training accuracy have stagnate at 18 percent accuracy which be horrible lol but I suspect that the network didn t have enough learning capacity from the start and this be good it could do however I want to see how each pcap layer transform the image top leave image → original inputtop right image → after first layerbottom leave image → after second layerbottom right image → after fourth layer one obvious pattern we can observe be the change of brighten for example if the top left pixel be white in the second layer this pixel will change to black in the next layer currently I be not 100 % sure on why this be happen but with more study I hope to know exactly why result max pooling network as see above when we replace all of the pcap layer with max pooling operation we can observe that the accuracy on training image stagnate around 14 percent confirm the fact that the network didn t have enough learning capacity from the start top left image → original inputtop right image → after first layerbottom leave image → after second layerbottom right image → after fourth layer contrast to pcap with max pooling we can clearly observe that the pixel with most high intensity move on to the next layer this be expect since that be what max pooling do interactive code for google colab you would need a google account to view the code also you can t run read only script in google colab so make a copy on your play ground finally I will never ask for permission to access your file on google drive just fyi happy coding to access the network with pcap please click here to access the network with max pooling please click here final word I wasn t expect much of this network from the start but I expect at least 30 percent accuracy on training testing image lol if any error be find please email I at jae duk seo@gmail com if you wish to see the list of all of my writing please view my website here meanwhile follow I on my twitter here and visit my website or my youtube channel for more content I also implement wide residual network please click here to view the blog post reference from a quick cheer to a stand ovation clap to show how much you enjoy this story https jaedukseo I | | | | |your everyday seo who like kimchi sharing concept idea and code
Jae Duk Seo,20,7,https://towardsdatascience.com/multi-stream-rnn-concat-rnn-internal-conv-rnn-lag-2-rnn-in-tensorflow-f4f17189a208?source=---------9----------------,multi stream rnn concat rnn internal conv rnn lag 2 rnn in tensorflow,for the last two week I have be die to implement different kind of recurrent neural network rnn and finally I have the time to implement all of they below be the list of different rnn case I want to try out case a vanilla recurrent neural network case b multi stream recurrent neural networkcase c concatenate recurrent neural networkcase d internal convolutional recurrent neural networkcase e lag 2 recurrent neural network vanilla recurrent neural network there be in total of 5 different case of rnn I wish to implement however in order to fully understand all of the implementation it would be a good idea to have a strong understanding of vanilla rnn case a be vanilla rnn so if you understand code for case a you be good to go if anyone wish to review simple rnn please visit my old blog post only numpy vanilla recurrent neural network derive back propagation through time practice case a vanilla recurrent neural network result red box → 3 convolutional layerorange → global average pooling and softmaxgreen circle → hide unit at time 0blue circle → input in 4 time stampblack box → recurrent neural network with 4 time stamp as see above the base network be simple rnn combine with convolutional neural network for classification the rnn have time stamp of 4 which mean we be go to give the network 4 different kind of input at each time stamp and to do that I be go to add some noise to the original image blue line → train cost over timeorange line → train accuracy over timegreen line → test cost over timere line → test accuracy over time as see above our base network already perform well now the question be how other method perform and would it be able to regularize well than our base network case b multi stream recurrent neural network idea result red box → 3 convolutional layerorange → global average pooling and softmaxgreen circle → hide unit at time 0blue circle → convolution input stream yellow circle → fully connect network streamblack box → recurrent neural network with 4 time stamp the idea behind this rnn be simply to give different representation of datum to the rnn in our base network we have the network either the raw image or image with some noise add red box → additional four cnn fnn layer to process the inputblue box → create input at each different time stamp as see below now our rnn take in input of tensor size with batch_size 26 26 1 reduce the width and the height by 2 and I be hope that different representation of the datum would act as a regularization similar to data augmentation blue line → train cost over timeorange line → train accuracy over timegreen line → test cost over timere line → test accuracy over time as see above the network do pretty well and have outperform our base network by 1 percent on the testing image case c concatenate recurrent neural network idea result red box → 3 convolutional layerorange → global average pooling and softmaxgreen circle → hide unit at time 0blue circle → input in 4 time stampblack box → recurrent neural network with 4 time stampblack curve arrow → concatenate input for each time stamp this approach be very simple the idea be that on each time stamp different feature will be extract and it might be useful for the network to have more feature overtime for the recurrent layer blue line → train cost over timeorange line → train accuracy over timegreen line → test cost over timere line → test accuracy over time sadly this be a huge failure I guess the empty hide value do not help one bit for the network to perform well case d internal convolutional recurrent neural network idea result red box → 3 convolutional layerorange → global average pooling and softmaxgreen circle → hide unit at time 0blue circle → input in 4 time stampblack box → recurrent neural network with 4 time stampgray arrow → perform internal convolution before pass onto the next time stamp as see above this network take in the exact same input as our base network however this time we be go to perform additional convolution operation in the internal representation of the datum right image → declare 3 new convolution layerleft image red box → if the current internal layer be not none we be go to perform additional convolution operation I actually have no theoretical reason behind this implementation I just want to see if it work lol blue line → train cost over timeorange line → train accuracy over timegreen line → test cost over timere line → test accuracy over time as see above the network do a fine job at converge however it be not able to outperform our base network sadly case e lag 2 recurrent neural network idea result red box → 3 convolutional layerorange → global average pooling and softmaxgreen circle → hide unit at time 0 or lag of 1 blue circle → input in 4 time stampblack box → recurrent neural network with 4 time stamppurple circle → hide state lag of 2 in a traditional rnn set we only rely on the most previous value to determine the current value for a while I be think that there be no reason for we to limit the look back time or lag as 1 we can extend this idea into lag 3 or lag 4 etc just for simplicity I take lag 2 blue line → train cost over timeorange line → train accuracy over timegreen line → test cost over timere line → test accuracy over time thankfully the network do well than the base network but with very small margin however this type of network would be most suitable for time series data interactive code transparency for google colab you would need a google account to view the code also you can t run read only script in google colab so make a copy on your play ground finally I will never ask for permission to access your file on google drive just fyi happy coding also for transparency I upload all of the training log on my github to access the code for case a click here for the log click here to access the code for case b click here for the log click here to access the code for case c click here for the log click here to access the code for case c click here for the log click here to access the code for case c click here for the log click here final word I want to review rnn for quite a long time now finally I get to do it if any error be find please email I at jae duk seo@gmail com if you wish to see the list of all of my writing please view my website here meanwhile follow I on my twitter here and visit my website or my youtube channel for more content I also implement wide residual network please click here to view the blog post reference from a quick cheer to a stand ovation clap to show how much you enjoy this story https jaedukseo I | | | | |your everyday seo who like kimchi sharing concept idea and code
Wallarm,72,4,https://lab.wallarm.com/tensorflow-dataset-api-for-increasing-training-speed-of-neural-networks-43a3050f2080?source=---------3----------------,tensorflow dataset api for increase training speed of neural network,wallarm ai engine be the heart of our security solution two key parameter of our ai engine efficiency be how fast neural network can be train to reflect the update training set and how much compute power need to be dedicate to the training on the on go basis many of our machine learning algorithm be write on top of tensorflow an open source dataflow software library originally release by google our average cpu load for the ai engine today be as high as 80 % so we be always look for way to speed thing up in software our late find be dataset api dataset be a mid level tensorflow api which make work with datum fast and more convenient in this blog we will measure just how much fast model training can be with dataset compare to the you use of feed_dict for starter let s prepare datum that will be use to train the model dataset can usually be store in numpy s array regardless of kind of datum they be that s why we prepare all our dataset without tensorflow and store it in npz format similar to this https github com wallarm research blob a719923f6a2da461deea0e01622d11cbfc8b057b tf_ds_api storing_in_npz_format py#l1 l10 this step help we avoid unnecessary data processing load on cpu and memory during model training now we be ready to train the model first let s load preprocesse datum from disk https github com wallarm research blob a719923f6a2da461deea0e01622d11cbfc8b057b tf_ds_api load_from_npz py#l1 l7 next the datum will be convert from numphy array into tensorflow tensor tf datum dataset from_tensor_slice method be use for that and load into tensorflow dataset from_tensor_slice method take placeholder with the same size of the 0th dimension element and return dataset object once the dataset be in tf you can process it for example you can use map f function which can process the datum but we already preprocess our dataset and all we need to do be apply batch and maybe shuffle fortunately dataset api already have need function they be batch and shuffle ok if we shuffle our dataset how can we use it for production it s easy we simply make another dataset without datum be shuffle https github com wallarm research blob a719923f6a2da461deea0e01622d11cbfc8b057b tf_ds_api dataset py#l1 l5 dataset api have other good method for preprocesse datum there be a comprehensive list of method in the official doc next we should extract datum from dataset object step by step for each of the training epoch tf datum iterator be tailor make for it tf currently support four type of iterator reinitializeble iterator be very useful all we need to do to start the work be to create an iterator and initializer for it iterator get_next yield the next element of our dataset when execute https github com wallarm research blob a719923f6a2da461deea0e01622d11cbfc8b057b tf_ds_api iterator py#l1 l8 to demonstrate the viability of use dataset api let s use propose approach for mnist dataset and for our corporate datum first we prepare datum and after that we process 1 and 5 epoch with dataset api and without model for this mnist example can be find on github https github com wallarm research blob a719923f6a2da461deea0e01622d11cbfc8b057b tf_ds_api model py#l1 l25 below be the result we obtain on a machine with one nvidia gtx 1080 and tf 1 8 0 all code of this experiment be available on github link mnist be a very small dataset and profit of dataset api isn t representative by contrast the result on a real life dataset be much more impressive thus dataset api be very good for increase your training speed with no source code change just some modification in the stack you can save 20 30 % off the training time from a quick cheer to a stand ovation clap to show how much you enjoy this story adaptive application security for devop @nginx partner @ycombibator s16 wallarm be devop friendly waf with hybrid architecture uniquely suit for cloud application it apply machine learning to traffic to adaptively generate security rule and verifie the impact of malicious payload in real time
Maryna Hlaiboroda,5,5,https://blog.heyml.com/%D0%B8%D0%B8-%D0%BF%D1%81%D0%B8%D1%85%D0%BE%D0%BF%D0%B0%D1%82-%D0%B8-%D0%B8%D0%B8-%D0%BE%D0%B1%D0%BC%D0%B0%D0%BD%D1%89%D0%B8%D0%BA-94c6a8e6c63e?source=---------4----------------,ии психопат и ии обманщик hey machine learning,команда исследователей из массачусетского технологического института mit представила нейронную сеть norman которая распознает изображения и генерирует текстовое описание к ним ее особенность в том что ученые тренировали сеть на картинках с подписями о смерти из сообщества reddit и norman во всем видит ужасы алгоритм назван в честь персонажа романа психо — убийцы с раздвоением личности нормана бейтса специалисты хотели продемонстрировать важность данных на которых обучают модель а также их сбалансированность чтобы наглядно показать влияние данных на результат исследователи из mit показали картинки из теста роршаха двум нейросетям ии алгоритму который обучали на обычных наборах с изображениями людей кошек и птиц и norman если обычная нейросеть видела на представленных картинках вазу с цветами стаю птиц на ветке или сидящих на лавочке людей то психопатическая сеть определяла это как застреленного мужчину или человека выпрыгнувшего из окна сбитого машиной на высокой скорости или убитого разрядом электрического тока инженеры из mit утверждают что создали нейросеть norman как напоминание что поведение ии — вина не его алгоритмов а данных для его обучения бывший сvo компании microsoft дейв коплин считает что создание подобного алгоритма является отличным поводом для публичного обсуждения проблем технологий искусственного интеллекта на который общество и бизнес начинают все больше полагаться bbc news ученые из торонтского университета — авишек бозе и пархам аараби — разработали систему которая ретуширует портретные фотографии таким образом чтобы алгоритмы распознавания лиц давали сбой проект позволяет сохранять конфиденциальность и бороться с раскрытием персональных данных в качестве данных для обучения исследователи использовали две нейронные сети одна распознавала лица на фото а вторая попиксельно ретушировала снимки и повторно отправляла их распознающей сети изменения которые давали наибольшее число ложных срабатываний формировали ядро фильтра ученые также отметили что разработанная ими система смогла обмануть алгоритм fast r cnn который создала компания facebook в будущем она позволит полностью пресекать идентификацию пользователя без его согласия в нынешнем же варианте технология снижает точность распознавания личности по фото до 0 5 % данный алгоритм — часть магистерской работы авишека бозе в августе 2018 года он намерен представить проект на семинаре mmsp 2018 в ванкувере u of t news генеральный директор корпорации google сундар пичай заявил что инженеры компании не будут заниматься военными разработками искусственного интеллекта однако специалисты поискового гиганта будут будут продолжать взаимодействовать с военными и правительственными ведомствами решение было принято после массового бойкота сотрудников компании против сотрудничества с пентагоном компания планировала создать искусственный интеллект для военных беспилотников по словам пичайа являясь лидером в ии разработках google чувствует огромную ответственность возложенную на плечи компании поэтому он объявил о семи принципах которых компания будет придерживаться в будущем он также отметил что использование ии должно быть социально полезным а при его разработке необходимо предусмотреть надежные средства обеспечения безопасности алгоритмы ии и собранные для них данные должны находиться под контролем людей а при их разработке должны быть учтены наивысшие научные стандарты и компания будет стремиться к тому чтобы ограничить вред использования таких систем tsn ua инженеры компании google разработали алгоритм autoaugment который дополняет данные для обучения алгоритмов компьютерного зрения изображениями созданными на основе существующих алгоритм трансформирует обрезает отражает и изменяет цвета на изображениях что позволяет увеличить набор исходных данных для обучения для создания алгоритма специалисты компании использовали модель обучения с подкреплением в результате он научился самостоятельно определять правила по которым необходимо изменить изображение и создать уникальное не исказив его при этом autoaugment умеет отражать изображения по горизонтали и вертикали поворачивать менять цвет и так далее при этом алгоритм может комбинировать правила и предотвращать создание одинаковых копий так система учитывает специфику конкретного набора изображений в случае с номерами домов в наборе svhn алгоритм использует геометрические преобразования изображения а также изменение его цвета в наборах cifar 10 и imagenet autoaugment не использует геометрические преобразования и не меняет цвет так как это правило может создать нереалистичную фотографию вместо этого алгоритм меняет оттенки на изображениях сохраняя при этом оригинальную цветовую гамму blog google ai калифорнийский университет в беркли опубликовал в открытом доступе архив видеороликов bdd100k для обучения автомобилей самостоятельной езде по общественным дорогам архив состоит из 100 тыс роликов по 40 секунд в разрешении 720р и 30 кадров в секунду кроме этого к каждому файлу прикреплены gps данные собранные мобильными устройствами которые могут приблизительно описывать траекторию движения транспорта в роликах содержаться различные дорожные ситуации и погодные условия снятые в различных уголках сша также в кадрах архива запечатлены 85 тыс пешеходов что может быть полезно разработчикам систем обнаружения пешеходов analytic vidhya хотите быть в курсе актуальных событий читайте нас в telegram и facebook и будьте в тренде from a quick cheer to a stand ovation clap to show how much you enjoy this story we be young and talented team and our passion be machine learn data science and artificial intelligence http heyml com
Amine Aoullay,58,4,https://towardsdatascience.com/how-to-use-noise-to-your-advantage-5301071d9dc3?source=---------6----------------,how to use noise to your advantage towards data science,for scientist random fluctuation or noise be undesirable although typically assume to degrade performance it can sometimes improve information processing in non linear system in this post we ll see some example where the noise can be use as an advantage recent work have show that by allow some inaccuracy when train deep neural network not only the training performance but also the accuracy of the model can be improve neural network be capable of learn output function that can change wildly with small change in input add noise to inputs randomly be like tell the network to not change the output in a ball around your exact input by limit the amount of information in a network we force it to learn compact representation of input feature rl be an area of machine learning that assume there be an agent situate in an environment at each step the agent take an action and it receive an observation and reward from the environment an rl algorithm seek to maximize the agent s total reward give a previously unknown environment through a learning process that usually involve lot of trial and error to understand the challenge with exploration in deep rl system think about researcher that spend lot of time in a lab without produce any practical application equivalently rl agent can spend a huge amount of resource without converge to a local optimum openai propose a technique call parameter space noise that introduce noise in the model policy parameter at the beginning of each episode other approach be focus on what be know as action space noise which introduce noise to change the likelihood associate with each action the agent might take from one moment to the next the initial result of the parameter space noise model prove to be really promise the technique help algorithm explore their environment more effectively lead to high score and more elegant behavior more detail can be find in the research paper the important thing to remember be that add noise be use as an advantage to boost the exploration performance of reinforcement learning algorithm boost recognition isn t as simple as throw more label image at these system indeed manually annotate a large number of image be an expensive and time consuming process facebook researcher and engineer have address this by train image recognition network on large set of public image with hashtag since people often caption their photo with hashtag it woul d be a good source of training datum for model facebook develop new approach that be tailor for do image recognition experiment use hashtag supervision this study be describe in detail in explore the limit of weakly supervised pretraining on the coco object detection challenge it have be show that the use of hashtag for pretraining can boost the average precision of a model by more than 2 percent noise should not be our enemy it isn t always an unwanted disturbance and can often be use as an advantage and even serve as a valuable research tool if anyone try to tell you otherwise well just give he the example we present stay tune and if you like this article please leave a 👏 1 weakly supervised pretraine https research fb com publication explore the limit of weakly supervised pretraine 2 well exploration with parameter noise https blog openai com well exploration with parameter noise from a quick cheer to a stand ovation clap to show how much you enjoy this story msc in machine learning mva @ ens paris saclay sharing concept idea and code
Kelvin Li,56,5,https://medium.com/@kelfun5354/the-complex-language-used-in-back-propagation-88c6e58f676c?source=---------9----------------,the complex language use in back propagation kelvin li medium,I ve look all over the internet for explanation of what exactly back propagation be and everyone either use complicated mathematical language or complex code to try to explain what back propagation if someone who doesn t know either want to know what it be then how will they really grasp what it be in this post I would like to unveil the secret of the universe with everyone and hopefully I ll do a good job at it accord to wikipedia backpropagation be a method use in artificial neural network to calculate a gradient that be need in the calculation of the weight to be use in the network backpropagation be commonly use by the gradient descent optimization algorithm to adjust the weight of neuron by calculate the gradient of the loss function if you have take a basic elementary algebra class you may have hear of the idea of a slope some people might think the idea of a slope be very insignificant but it be actually the game change concept that cause all the technological advancement within the last century to know the slope of something mean that you know the rate of something change over a period of time know this give we power to manipulate thing to our advantage now you can think of a gradient as the slope of something in a high dimension I win t go into detail but that be the general gist of what a gradient be weight be the value that we want to use to adjust the output of our function in each neuron so say we have an output of 2 and we want to change the 2 into a 1 then we would multiply the 2 by a 5 to get the desire result this mean that 5 will be the weight in this case in a way we be weigh down the output to what we want it to be a neuron be simply just a function a neural network a bunch of neuron be simply a bunch of function each neuron also have an activation function that spit out a value for the next neuron to calculate think of these function as how much of a yes or a no an input be an example would be picture recognize when you feed the neural network a picture the node will spit out a number between 0 and 1 where 0 be be very no and 1 be very yes this process continue between every node until the very end which ever node have the high number between 0 and 1 would be the decision the machine make a loss function be just some function that we use to determine how correct the predict output be from the real output for example we input a picture of a cat into the machine but the machine predict that it s a dinosaur clearly the machine be not do a very good job so we need some way to know how correct the machine be compare to the real datum which be where the loss function come in now that we have all the necessary understanding we can go into the real sauce now what I be about to explain to you be go to either confuse the crap out of you or make you feel enlighten let s pretend you be try to build a door lock opening mechanism this mechanism involve you press a button which trigger a ball roll down a platform and knock over a switch that unlock the door now let think about this there be a few component that we have to keep in mind the 1st component be you press the button the 2nd component be the ball roll down a platform and the 3rd component be the switch be knock over there be actually a lot of physics go on around here but let s just focus on the ball roll down the platform now when you create this mechanism you want the door to ideally open in 3 second but you don t have any tool to measure the time and length so all you can do be to create a platform through intuition you build your first platform and let the ball roll and realize that it take 9 second for the door to open after press the button so you go back to the platform and make the platform steeper you perform the same trial and error over and over again until you get the ideal opening time this my friend be backpropagation well true but the idea be basically the same in a neural net we have weight assign to each neuron these weight will get multiply by a certain input and modify through some activation function the result of these activation function might not always be what we want what backpropagation would do be that it will do some calculus will be cover in another post to determine the direction of increase decrease aka the gradient cut less of the platform or cut more of the platform to achieve the good weight ideal time the door open it then update these weight every time it have create new weight and run the neural net again every trial you cut a piece of the platform to test eventually we will achieve the good possible weight that satisfy our desire accuracy in my next post I will discuss more in depth about the math that be involve with backpropagation reference and link from a quick cheer to a stand ovation clap to show how much you enjoy this story get stick 24 7
Arthur Juliani,9K,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------1----------------,simple reinforcement learning with tensorflow part 0 q learning with table and neural network,for this tutorial in my reinforcement learning series we be go to be explore a family of rl algorithms call q learning algorithms these be a little different than the policy base algorithm that will be look at in the the follow tutorial part 1 3 instead of start with a complex and unwieldy deep neural network we will begin by implement a simple lookup table version of the algorithm and then show how to implement a neural network equivalent use tensorflow give that we be go back to basic it may be good to think of this as part 0 of the series it will hopefully give an intuition into what be really happen in q learning that we can then build on go forward when we eventually combine the policy gradient and q learning approach to build state of the art rl agent if you be more interested in policy network or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient method which attempt to learn function which directly map an observation to an action q learning attempt to learn the value of be in a give state and take a specific action there while both approach ultimately allow we to take intelligent action give a situation the mean of get to that action differ significantly you may have hear about deepq network which can play atari game these be really just large and more complex implementation of the q learning algorithm we be go to discuss here for this tutorial we be go to be attempt to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provide an easy way for people to experiment with their learn agent in an array of provide toy game the frozenlake environment consist of a 4x4 grid of block each one either be the start block the goal block a safe frozen block or a dangerous hole the objective be to have an agent learn to navigate from the start to the goal without move onto a hole at any give time the agent can choose to move either up down left or right the catch be that there be a wind which occasionally blow the agent onto a space they didn t choose as such perfect performance every time be impossible but learn to avoid the hole and reach the goal be certainly still doable the reward at every step be 0 except for enter the goal which provide a reward of 1 thus we will need an algorithm that learn long term expect reward this be exactly what q learning be design to provide in it s simple implementation q learning be a table of value for every state row and action column possible in the environment within each cell of the table we learn a value for how good it be to take a give action within a give state in the case of the frozenlake environment we have 16 possible state one for each block and 4 possible action the four direction of movement give we a 16x4 table of q value we start by initialize the table to be uniform all zero and then as we observe the reward we obtain for various action we update the table accordingly we make update to our q table use something call the bellman equation which state that the expect long term reward for a give action be equal to the immediate reward from the current action combine with the expect reward from the good future action take at the follow state in this way we reuse our own q table when estimate how to update our table for future action in equation form the rule look like this this say that the q value for a give state s and action a should represent the current reward r plus the maximum discount γ future reward expect accord to our own table for the next state s we would end up in the discount variable allow we to decide how important the possible future reward be compare to the present reward by update in this way the table slowly begin to obtain accurate measure of the expect future reward for a give action in a give state below be a python walkthrough of the q table algorithm implement in the frozenlake environment thank to praneet d for find the optimal hyperparameter for this approach now you may be think table be great but they don t really scale do they while it be easy to have a 16x4 table for a simple grid world the number of possible state in any modern game or real world environment be nearly infinitely large for most interesting problem table simply don t work we instead need some way to take a description of our state and produce q value for action without a table that be where neural network come in by act as a function approximator we can take any number of possible state that can be represent as a vector and learn to map they to q value in the case of the frozenlake example we will be use a one layer network which take the state encode in a one hot vector 1x16 and produce a vector of 4 q value one for each action such a simple network act kind of like a glorify table with the network weight serve as the old cell the key difference be that we can easily expand the tensorflow network with add layer activation function and different input type whereas all that be impossible with a regular table the method of update be a little different as well instead of directly update our table with a network we will be use backpropagation and a loss function our loss function will be sum of square loss where the difference between the current predict q value and the target value be compute and the gradient pass through the network in this case our q target for the choose action be the equivalent to the q value compute in equation 1 above below be the tensorflow walkthrough of implement our simple q network while the network learn to solve the frozenlake problem it turn out it doesn t do so quite as efficiently as the q table while neural network allow for great flexibility they do so at the cost of stability when it come to q learning there be a number of possible extension to our simple q network which allow for great performance and more robust learn two trick in particular be refer to as experience replay and freeze target network those improvement and other tweak be the key to get atari play deep q network and we will be explore those addition in the future for more info on the theory behind q learning see this great post by tambet matiisen I hope this tutorial have be helpful for those curious about how to implement simple q learning algorithms if this post have be valuable to you please consider donate to help support future tutorial article and implementation any contribution be greatly appreciate if you d like to follow my work on deep learning ai and cognitive science follow I on medium @arthur juliani or on twitter @awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @unity3d & cognitive neuroscience phd student explore frontier technology through the lens of artificial intelligence datum science and the shape of thing to come
Stefan Kojouharov,14.2K,7,https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?source=tag_archive---------2----------------,cheat sheet for ai neural network machine learn deep learning & big datum,over the past few month I have be collect ai cheat sheet from time to time I share they with friend and colleague and recently I have be get ask a lot so I decide to organize and share the entire collection to make thing more interesting and give context I add description and or excerpt for each major topic this be the most complete list and the big o be at the very end enjoy this machine learn cheat sheet will help you find the right estimator for the job which be the most difficult part the flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problem and how to solve it scikit learn formerly scikit learn be a free software machine learning library for the python programming language it feature various classification regression and clustering algorithm include support vector machine random forest gradient boost k mean and dbscan and be design to interoperate with the python numerical and scientific library numpy and scipy in may 2017 google announce the second generation of the tpu as well as the availability of the tpus in google compute engine 12 the second generation tpus deliver up to 180 teraflop of performance and when organize into cluster of 64 tpu provide up to 11 5 petaflop in 2017 google s tensorflow team decide to support keras in tensorflow s core library chollet explain that keras be conceive to be an interface rather than an end to end machine learning framework it present a high level more intuitive set of abstraction that make it easy to configure neural network regardless of the backend scientific computing library numpy target the cpython reference implementation of python which be a non optimize bytecode interpreter mathematical algorithm write for this version of python often run much slow than compile equivalent numpy address the slowness problem partly by provide multidimensional array and function and operator that operate efficiently on array require rewrite some code mostly inner loop use numpy the name panda be derive from the term panel datum an econometric term for multidimensional structured data set the term datum wrangler be start to infiltrate pop culture in the 2017 movie kong skull island one of the character play by actor marc evan jackson be introduce as steve woodward our data wrangler scipy build on the numpy array object and be part of the numpy stack which include tool like matplotlib panda and sympy and an expand set of scientific computing librarie this numpy stack have similar user to other application such as matlab gnu octave and scilab the numpy stack be also sometimes refer to as the scipy stack 3 matplotlib be a plotting library for the python programming language and its numerical mathematics extension numpy it provide an object orient api for embed plot into application use general purpose gui toolkit like tkinter wxpython qt or gtk+ there be also a procedural pylab interface base on a state machine like opengl design to closely resemble that of matlab though its use be discourage 2 scipy make use of matplotlib pyplot be a matplotlib module which provide a matlab like interface 6 matplotlib be design to be as usable as matlab with the ability to use python with the advantage that it be free > > > if you like this list you can let I know here < < < stefan be the founder of chatbot s life a chatbot medium and consult firm chatbot s life have grow to over 150k view per month and have become the premium place to learn about bot & ai online chatbot s life have also consult many of the top bot company like swelly instav outbrain neargroup and a number of enterprise big o algorithm cheat sheet http bigocheatsheet com bokeh cheat sheet https s3 amazonaw com asset datacamp com blog_asset python_bokeh_cheat_sheet pdf datum science cheat sheet https www datacamp com community tutorial python data science cheat sheet basic datum wrangle cheat sheet https www rstudio com wp content upload 2015 02 datum wrangle cheatsheet pdf datum wrangle https en wikipedia org wiki data_wrangle ggplot cheat sheet https www rstudio com wp content upload 2015 03 ggplot2 cheatsheet pdf keras cheat sheet https www datacamp com community blog keras cheat sheet#gs drkenms keras https en wikipedia org wiki keras machine learn cheat sheet https ai icymi email new machinelearning cheat sheet by emily barry abdsc machine learning cheat sheet https doc microsoft com en in azure machine learning machine learn algorithm cheat sheet ml cheat sheet http peekaboo vision blogspot com 2013 01 machine learn cheat sheet for scikit html matplotlib cheat sheet https www datacamp com community blog python matplotlib cheat sheet#gs uekyspy matpotlib https en wikipedia org wiki matplotlib neural network cheat sheet http www asimovinstitute org neural network zoo neural network graph cheat sheet http www asimovinstitute org blog neural network https www quora com where can find a cheat sheet for neural network numpy cheat sheet https www datacamp com community blog python numpy cheat sheet#gs ak5zbge numpy https en wikipedia org wiki numpy panda cheat sheet https www datacamp com community blog python panda cheat sheet#gs oundfxm panda https en wikipedia org wiki panda _ software panda cheat sheet https www datacamp com community blog panda cheat sheet python#gs hpforic pyspark cheat sheet https www datacamp com community blog pyspark cheat sheet python#gs l = j1zxq scikit cheat sheet https www datacamp com community blog scikit learn cheat sheet scikit learn https en wikipedia org wiki scikit learn scikit learn cheat sheet http peekaboo vision blogspot com 2013 01 machine learn cheat sheet for scikit html scipy cheat sheet https www datacamp com community blog python scipy cheat sheet#gs jdsg3oi scipy https en wikipedia org wiki scipy tesorflow cheat sheet https www altoros com tensorflow cheat sheet html tensor flow https en wikipedia org wiki tensorflow from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of chatbot life I help company create great chatbot & ai system and share my insight along the way late news info and tutorial on artificial intelligence machine learn deep learn big datum and what it mean for humanity
Andrej Karpathy,9.2K,7,https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=tag_archive---------3----------------,yes you should understand backprop andrej karpathy medium,when we offer cs231n deep learning class at stanford we intentionally design the programming assignment to include explicit calculation involve in backpropagation on the low level the student have to implement the forward and the backward pass of each layer in raw numpy inevitably some student complain on the class message board this be seemingly a perfectly sensible appeal if you re never go to write backward pass once the class be over why practice write they be we just torture the student for our own amusement some easy answer could make argument along the line of it s worth know what s under the hood as an intellectual curiosity or perhaps you might want to improve on the core algorithm later but there be a much strong and practical argument which I want to devote a whole post to > the problem with backpropagation be that it be a leaky abstraction in other word it be easy to fall into the trap of abstract away the learning process — believe that you can simply stack arbitrary layer together and backprop will magically make they work on your datum so let look at a few explicit example where this be not the case in quite unintuitive way we re start off easy here at one point it be fashionable to use sigmoid or tanh non linearity in the fully connect layer the tricky part people might not realize until they think about the backward pass be that if you be sloppy with the weight initialization or datum preprocesse these non linearity can saturate and entirely stop learn — your training loss will be flat and refuse to go down for example a fully connect layer with sigmoid non linearity compute use raw numpy if your weight matrix w be initialize too large the output of the matrix multiply could have a very large range e g number between 400 and 400 which will make all output in the vector z almost binary either 1 or 0 but if that be the case z * 1 z which be local gradient of the sigmoid non linearity will in both case become zero vanish make the gradient for both x and w be zero the rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule another non obvious fun fact about sigmoid be that its local gradient z * 1 z achieve a maximum at 0 25 when z = 0 5 that mean that every time the gradient signal flow through a sigmoid gate its magnitude always diminish by one quarter or more if you re use basic sgd this would make the low layer of a network train much slow than the high one tldr if you re use sigmoid or tanh non linearity in your network and you understand backpropagation you should always be nervous about make sure that the initialization doesn t cause they to be fully saturate see a long explanation in this cs231n lecture video another fun non linearity be the relu which threshold neuron at zero from below the forward and backward pass for a fully connect layer that use relu would at the core include if you stare at this for a while you ll see that if a neuron get clamp to zero in the forward pass I e z=0 it doesn t fire then its weight will get zero gradient this can lead to what be call the dead relu problem where if a relu neuron be unfortunately initialize such that it never fire or if a neuron s weight ever get knock off with a large update during training into this regime then this neuron will remain permanently dead it s like permanent irrecoverable brain damage sometimes you can forward the entire training set through a train network and find that a large fraction e g 40 % of your neuron be zero the entire time tldr if you understand backpropagation and your network have relus you re always nervous about dead relus these be neuron that never turn on for any example in your entire training set and will remain permanently dead neuron can also die during training usually as a symptom of aggressive learning rate see a long explanation in cs231n lecture video vanilla rnn feature another good example of unintuitive effect of backpropagation I ll copy paste a slide from cs231n that have a simplified rnn that do not take any input x and only compute the recurrence on the hidden state equivalently the input x could always be zero this rnn be unrolled for t time step when you stare at what the backward pass be do you ll see that the gradient signal go backwards in time through all the hidden state be always be multiply by the same matrix the recurrence matrix whh intersperse with non linearity backprop what happen when you take one number a and start multiply it by some other number b I e a*b*b*b*b*b*b this sequence either go to zero if |b| < 1 or explode to infinity when |b|>1 the same thing happen in the backward pass of an rnn except b be a matrix and not just a number so we have to reason about its large eigenvalue instead tldr if you understand backpropagation and you re use rnn you be nervous about have to do gradient clipping or you prefer to use an lstm see a long explanation in this cs231n lecture video let look at one more — the one that actually inspire this post yesterday I be browse for a deep q learning implementation in tensorflow to see how other deal with compute the numpy equivalent of q a where a be an integer vector — turn out this trivial operation be not support in tf anyway I search dqn tensorflow click the first link and find the core code here be an excerpt if you re familiar with dqn you can see that there be the target_q_t which be just reward * gamma argmax_a q s a and then there be q_acte which be q s a of the action that be take the author here subtract the two into variable delta which they then want to minimize on line 295 with the l2 loss with tf reduce_mean tf square so far so good the problem be on line 291 the author be try to be robust to outlier so if the delta be too large they clip it with tf clip_by_value this be well intentione and look sensible from the perspective of the forward pass but it introduce a major bug if you think about the backward pass the clip_by_value function have a local gradient of zero outside of the range min_delta to max_delta so whenever the delta be above min max_delta the gradient become exactly zero during backprop the author be clip the raw q delta when they be likely try to clip the gradient for add robustness in that case the correct thing to do be to use the huber loss in place of tf square it s a bit gross in tensorflow because all we want to do be clip the gradient if it be above a threshold but since we can t meddle with the gradient directly we have to do it in this round about way of define the huber loss in torch this would be much more simple I submit an issue on the dqn repo and this be promptly fix backpropagation be a leaky abstraction ; it be a credit assignment scheme with non trivial consequence if you try to ignore how it work under the hood because tensorflow automagically make my network learn you will not be ready to wrestle with the danger it present and you will be much less effective at building and debug neural network the good news be that backpropagation be not that difficult to understand if present properly I have relatively strong feeling on this topic because it seem to I that 95 % of backpropagation material out there present it all wrong filling page with mechanical math instead I would recommend the cs231n lecture on backprop which emphasize intuition yay for shameless self advertising and if you can spare the time as a bonus work through the cs231n assignment which get you to write backprop manually and help you solidify your understanding that s it for now I hope you ll be much more suspicious of backpropagation go forward and think carefully through what the backward pass be do also I m aware that this post have unintentionally turn into several cs231n ad apology for that from a quick cheer to a stand ovation clap to show how much you enjoy this story director of ai at tesla previously research scientist at openai and phd student at stanford I like to train deep neural net on large dataset
Avinash Sharma V,6.9K,10,https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0?source=tag_archive---------4----------------,understand activation function in neural network,recently a colleague of mine ask I a few question like why do we have so many activation function why be that one work well than the other how do we know which one to use be it hardcore math and so on so I think why not write an article on it for those who be familiar with neural network only at a basic level and be therefore wonder about activation function and their why how mathematic note this article assume that you have a basic knowledge of an artificial neuron I would recommend read up on the basic of neural network before read this article for well understanding so what do an artificial neuron do simply put it calculate a weighted sum of its input add a bias and then decide whether it should be fire or not yeah right an activation function do this but let s go with the flow for a moment so consider a neuron now the value of y can be anything range from inf to + inf the neuron really doesn t know the bound of the value so how do we decide whether the neuron should fire or not why this firing pattern because we learn it from biology that s the way brain work and brain be a work testimony of an awesome and intelligent system we decide to add activation function for this purpose to check the y value produce by a neuron and decide whether outside connection should consider this neuron as fire or not or rather let s say — activate or not the first thing that come to our mind be how about a threshold base activation function if the value of y be above a certain value declare it activate if it s less than the threshold then say it s not hmm great this could work activation function a = activate if y > threshold else not alternatively a = 1 if y > threshold 0 otherwise well what we just do be a step function see the below figure its output be 1 activate when value > 0 threshold and output a 0 not activate otherwise great so this make an activation function for a neuron no confusion however there be certain drawback with this to understand it well think about the following suppose you be create a binary classifier something which should say a yes or no activate or not activate a step function could do that for you that s exactly what it do say a 1 or 0 now think about the use case where you would want multiple such neuron to be connect to bring in more class class1 class2 class3 etc what will happen if more than 1 neuron be activate all neuron will output a 1 from step function now what would you decide which class be it hmm hard complicated you would want the network to activate only 1 neuron and other should be 0 only then would you be able to say it classify properly identify the class ah this be hard to train and converge this way it would have be well if the activation be not binary and it instead would say 50 % activate or 20 % activate and so on and then if more than 1 neuron activate you could find which neuron have the high activation and so on well than max a softmax but let s leave that for now in this case as well if more than 1 neuron say 100 % activate the problem still persist I know but since there be intermediate activation value for the output learning can be smooth and easy less wiggly and chance of more than 1 neuron be 100 % activate be less when compare to step function while training also depend on what you be train and the datum ok so we want something to give we intermediate analog activation value rather than say activate or not binary the first thing that come to our mind would be linear function a = cx a straight line function where activation be proportional to input which be the weighted sum from neuron this way it give a range of activation so it be not binary activation we can definitely connect a few neuron together and if more than 1 fire we could take the max or softmax and decide base on that so that be ok too then what be the problem with this if you be familiar with gradient descent for training you would notice that for this function derivative be a constant a = cx derivative with respect to x be c that mean the gradient have no relationship with x it be a constant gradient and the descent be go to be on constant gradient if there be an error in prediction the change make by back propagation be constant and not depend on the change in input delta x this be not that good not always but bear with I there be another problem too think about connect layer each layer be activate by a linear function that activation in turn go into the next level as input and the second layer calculate weight sum on that input and it in turn fire base on another linear activation function no matter how many layer we have if all be linear in nature the final activation function of last layer be nothing but just a linear function of the input of first layer pause for a bit and think about it that mean these two layer or n layer can be replace by a single layer ah we just lose the ability of stack layer this way no matter how we stack the whole network be still equivalent to a single layer with linear activation a combination of linear function in a linear manner be still another linear function let s move on shall we well this look smooth and step function like what be the benefit of this think about it for a moment first thing first it be nonlinear in nature combination of this function be also nonlinear great now we can stack layer what about non binary activation yes that too it will give an analog activation unlike step function it have a smooth gradient too and if you notice between x value 2 to 2 y value be very steep which mean any small change in the value of x in that region will cause value of y to change significantly ah that mean this function have a tendency to bring the y value to either end of the curve look like it s good for a classifier consider its property yes it indeed be it tend to bring the activation to either side of the curve above x = 2 and below x = 2 for example make clear distinction on prediction another advantage of this activation function be unlike linear function the output of the activation function be always go to be in range 0 1 compare to inf inf of linear function so we have our activation bind in a range nice it win t blow up the activation then this be great sigmoid function be one of the most widely use activation function today then what be the problem with this if you notice towards either end of the sigmoid function the y value tend to respond very less to change in x what do that mean the gradient at that region be go to be small it give rise to a problem of vanish gradient hmm so what happen when the activation reach near the near horizontal part of the curve on either side gradient be small or have vanish can not make significant change because of the extremely small value the network refuse to learn far or be drastically slow depend on use case and until gradient computation get hit by float point value limit there be way to work around this problem and sigmoid be still very popular in classification problem another activation function that be use be the tanh function hm this look very similar to sigmoid in fact it be a scaled sigmoid function ok now this have characteristic similar to sigmoid that we discuss above it be nonlinear in nature so great we can stack layer it be bind to range 1 1 so no worry of activation blow up one point to mention be that the gradient be strong for tanh than sigmoid derivative be steep decide between the sigmoid or tanh will depend on your requirement of gradient strength like sigmoid tanh also have the vanish gradient problem tanh be also a very popular and widely use activation function later come the relu function a x = max 0 x the relu function be as show above it give an output x if x be positive and 0 otherwise at first look this would look like have the same problem of linear function as it be linear in positive axis first of all relu be nonlinear in nature and combination of relu be also non linear in fact it be a good approximator any function can be approximate with combination of relu great so this mean we can stack layer it be not bind though the range of relu be 0 inf this mean it can blow up the activation another point that I would like to discuss here be the sparsity of the activation imagine a big neural network with a lot of neuron use a sigmoid or tanh will cause almost all neuron to fire in an analog way remember that mean almost all activation will be process to describe the output of a network in other word the activation be dense this be costly we would ideally want a few neuron in the network to not activate and thereby make the activation sparse and efficient relu give we this benefit imagine a network with random initialize weight or normalised and almost 50 % of the network yield 0 activation because of the characteristic of relu output 0 for negative value of x this mean a few neuron be fire sparse activation and the network be light woah nice relu seem to be awesome yes it be but nothing be flawless not even relu because of the horizontal line in relu for negative x the gradient can go towards 0 for activation in that region of relu gradient will be 0 because of which the weight will not get adjust during descent that mean those neuron which go into that state will stop respond to variation in error input simply because gradient be 0 nothing change this be call die relu problem this problem can cause several neuron to just die and not respond make a substantial part of the network passive there be variation in relu to mitigate this issue by simply make the horizontal line into non horizontal component for example y = 0 01x for x<0 will make it a slightly inclined line rather than horizontal line this be leaky relu there be other variation too the main idea be to let the gradient be non zero and recover during training eventually relu be less computationally expensive than tanh and sigmoid because it involve simple mathematical operation that be a good point to consider when we be design deep neural net now which activation function to use do that mean we just use relu for everything we do or sigmoid or tanh well yes and no when you know the function you be try to approximate have certain characteristic you can choose an activation function which will approximate the function fast lead to fast training process for example a sigmoid work well for a classifier see the graph of sigmoid doesn t it show the property of an ideal classifier because approximate a classifier function as combination of sigmoid be easy than maybe relu for example which will lead to fast training process and convergence you can use your own custom function too if you don t know the nature of the function you be try to learn then maybe I would suggest start with relu and then work backwards relu work most of the time as a general approximator in this article I try to describe a few activation function use commonly there be other activation function too but the general idea remain the same research for well activation function be still ongoing hope you get the idea behind activation function why they be use and how do we decide which one to use from a quick cheer to a stand ovation clap to show how much you enjoy this story musing of an ai deep learning mathematics addict
Arthur Juliani,3.5K,8,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=tag_archive---------5----------------,simple reinforcement learning with tensorflow part 8 asynchronous actor critic agent a3c,in this article I want to provide a tutorial on implement the asynchronous advantage actor critic a3c algorithm in tensorflow we will use it to solve a simple challenge in a 3d doom environment with the holiday right around the corner this will be my final post for the year and I hope it will serve as a culmination of all the previous topic in the series if you haven t yet or be new to deep learning and reinforcement learning I suggest check out the early entry in the series before go through this post in order to understand all the building block which will be utilize here if you have be follow the series thank you I have learn so much about rl in the past year and be happy to have share it with everyone through this article series so what be a3c the a3c algorithm be release by google s deepmind group early this year and it make a splash by essentially obsolete dqn it be fast simple more robust and able to achieve much well score on the standard battery of deep rl task on top of all that it could work in continuous as well as discrete action space give this it have become the go to deep rl algorithm for new challenging problem with complex state and action space in fact openai just release a version of a3c as their universal starter agent for work with their new and very diverse set of universe environment asynchronous advantage actor critic be quite a mouthful let s start by unpack the name and from there begin to unpack the mechanic of the algorithm itself asynchronous unlike dqn where a single agent represent by a single neural network interact with a single environment a3c utilize multiple incarnation of the above in order to learn more efficiently in a3c there be a global network and multiple worker agent which each have their own set of network parameter each of these agent interact with it s own copy of the environment at the same time as the other agent be interact with their environment the reason this work well than have a single agent beyond the speedup of get more work do be that the experience of each agent be independent of the experience of the other in this way the overall experience available for training become more diverse actor critic so far this series have focus on value iteration method such as q learning or policy iteration method such as policy gradient actor critic combine the benefit of both approach in the case of a3c our network will estimate both a value function v s how good a certain state be to be in and a policy π s a set of action probability output these will each be separate fully connect layer sit at the top of the network critically the agent use the value estimate the critic to update the policy the actor more intelligently than traditional policy gradient method advantage if we think back to our implementation of policy gradient the update rule use the discount return from a set of experience in order to tell the agent which of its action be good and which be bad the network be then update in order to encourage and discourage action appropriately the insight of use advantage estimate rather than just discount return be to allow the agent to determine not just how good its action be but how much well they turn out to be than expect intuitively this allow the algorithm to focus on where the network s prediction be lack if you recall from the duel q network architecture the advantage function be as follow since we win t be determine the q value directly in a3c we can use the discount return r as an estimate of q s a to allow we to generate an estimate of the advantage in this tutorial we will go even far and utilize a slightly different version of advantage estimation with low variance refer to as generalize advantage estimation in the process of build this implementation of the a3c algorithm I use as reference the quality implementation by dennybritz and openai both of which I highly recommend if you d like to see alternative to my code here each section embed here be take out of context for instructional purpose and win t run on its own to view and run the full functional a3c implementation see my github repository the general outline of the code architecture be the a3c algorithm begin by construct the global network this network will consist of convolutional layer to process spatial dependency follow by an lstm layer to process temporal dependency and finally value and policy output layer below be example code for establish the network graph itself next a set of worker agent each with their own network and environment be create each of these worker be run on a separate processor thread so there should be no more worker than there be thread on your cpu ~ from here we go asynchronous ~ each worker begin by set its network parameter to those of the global network we can do this by construct a tensorflow op which set each variable in the local worker network to the equivalent variable value in the global network each worker then interact with its own copy of the environment and collect experience each keep a list of experience tuple observation action reward do value that be constantly add to from interaction with the environment once the worker s experience history be large enough we use it to determine discount return and advantage and use those to calculate value and policy loss we also calculate an entropy h of the policy this correspond to the spread of action probability if the policy output action with relatively similar probability then entropy will be high but if the policy suggest a single action with a large probability then entropy will be low we use the entropy as a means of improve exploration by encourage the model to be conservative regard its sureness of the correct action a worker then use these loss to obtain gradient with respect to its network parameter each of these gradient be typically clip in order to prevent overly large parameter update which can destabilize the policy a worker then use the gradient to update the global network parameter in this way the global network be constantly be update by each of the agent as they interact with their environment once a successful update be make to the global network the whole process repeat the worker then reset its own network parameter to those of the global network and the process begin again to view the full and functional code see the github repository here the robustness of a3c allow we to tackle a new generation of reinforcement learning challenge one of which be 3d environment we have come a long way from multi armed bandit and grid world and in this tutorial I have set up the code to allow for play through the first vizdoom challenge vizdoom be a system to allow for rl research use the classic doom game engine the maintainer of vizdoom recently create a pip package so instal it be as simple as pip install vizdoom once it be instal we will be use the basic wad environment which be provide in the github repository and need to be place in the work directory the challenge consist of control an avatar from a first person perspective in a single square room there be a single enemy on the opposite side of the room which appear in a random location each episode the agent can only move to the left or right and fire a gun the goal be to shoot the enemy as quickly as possible use as few bullet as possible the agent have 300 time step per episode to shoot the enemy shoot the enemy yield a reward of 1 and each time step as well as each shot yield a small penalty after about 500 episode per worker agent the network learn a policy to quickly solve the challenge feel free to adjust parameter such as learn rate clip magnitude update frequency etc to attempt to achieve ever great performance or utilize a3c in your own rl task I hope this tutorial have be helpful to those new to a3c and asynchronous reinforcement learning now go forth and build ais there be a lot of move part in a3c so if you discover a bug or find a well way to do something please don t hesitate to bring it up here or in the github I be more than happy to incorporate change and feedback to improve the algorithm if you d like to follow my writing on deep learning ai and cognitive science follow I on medium @arthur juliani or on twitter @awjuliani if this post have be valuable to you please consider donate to help support future tutorial article and implementation any contribution be greatly appreciate more from my simple reinforcement learning with tensorflow series from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @unity3d & cognitive neuroscience phd student explore frontier technology through the lens of artificial intelligence datum science and the shape of thing to come
Elle O'Brien,2.3K,6,https://towardsdatascience.com/romance-novels-generated-by-artificial-intelligence-1b31d9c872b2?source=tag_archive---------6----------------,romance novel generate by artificial intelligence,I ve always be fascinate with romance novel — the kind they sell at the drugstore for a couple of dollar usually with some attractive soft light couple on the cover so when I start futze around with text generate neural network a few week ago I develop an urgent curiosity to discover what artificial intelligence could contribute to the ever popular genre maybe one day there will be entire book write by computer for now let s start with title I gather over 20 000 harlequin romance novel title and give they to a neural network a type of artificial intelligence that learn the structure of text it s powerful enough to string together word in a way that seem almost human 90 % human the other 10 % be all wackiness I be not disappoint with what come out I even photoshoppe some of my favorite into existence the author name be synthesize from machine learning too let s have a look by theme a common theme in romance novel be pregnancy and the word baby have a strong showing in the title I train the neural network on naturally the neural network come up with a lot of baby theme title there s an unusually high concentration of sheikh viking and billionaire in the harlequin world likewise the neural network generate some colorful new bachelor type I have so many question how be the prince pregnant what sort of consulting do the count do who be butterfly earl and what make the sheikh s desire so convenient although there be exception most romance novel end in happily ever after a lot of they even start with an unexpected wedding — a marriage of convenience or a stipulation of a business contract or a sham that turn into real love the neural network seem to have internalize something about matrimony doctor and surgeon be common paramour for mistress head towards the marriage valley christmas be a magical time for surgeon sheikh playboy dad consultant and the woman who love they what or where be knith I just like mission christmas this neural network have never see the big montana sky but it have some questionable idea about cowboy the neural network generate some decidedly pg 13 title they can t all live happily ever after some of the generate title sound like m night shyamalan be a collaborator how do the word fear get in there it s possible the network generate it without have fear in the training set but a subset of the harlequin empire be gear towards paranormal and gothic romance that might have include the word * note I check and there be veil of fear publish in 2012 to wrap it up some of the adorable failure and near miss generate by the neural network I hope you ve enjoy computer generate romance novel title half as much as I have maybe someone out there can write about the virgin viking or the consultant count or the baby surgeon seduction I d buy it I build a webscraper in python thank beautiful soup that grab about 20 000 romance novel title publish under the harlequin brand off of fictiondb com harlequin be to I synonymous with the romance genre although it comprise only a fraction albeit a healthy one of the entire market I feed this list of book title into a recurrent neural network use software I get from github and wait a few hour for the magic to happen the model I fit be a 3 layer 256 node recurrent neural network I also train the network on the author list in to create some new pen name for more about the neural network I use have a look at the fabulous work of andrej karpathy I discover that surgery by the sea be actually a real novel write by sheila dougla and publish in 1979 so this one isn t an original neural network creation because the training set be rather small only about 1 mb of text datum it s to be expect that sometimes the machine will spit out one of the title it be train on one of the more challenging aspect of this project be discern when that happen since the real publish title can be more surprising than anything bear out of artificial intelligence for example the $ 4 98 daddy and 6 1 grinch be both real in fact the very first romance novel publish by harlequin be call the manatee from a quick cheer to a stand ovation clap to show how much you enjoy this story computational scientist software developer science writer sharing concept idea and code
Slav Ivanov,2.9K,9,https://blog.slavv.com/picking-a-gpu-for-deep-learning-3d4795c273b9?source=tag_archive---------8----------------,pick a gpu for deep learning slav,quite a few people have ask I recently about choose a gpu for machine learning as it stand success with deep learning heavily dependent on have the right hardware to work with when I be build my personal deep learning box I review all the gpu on the market in this article I m go to share my insight about choose the right graphic processor also we ll go over deep learning dl be part of the field of machine learn ml dl work by approximate a solution to a problem use neural network one of the nice property of about neural network be that they find pattern in the datum feature by themselves this be oppose to have to tell your algorithm what to look for as in the olde time however often this mean the model start with a blank state unless we be transfer learn to capture the nature of the datum from scratch the neural net need to process a lot of information there be two way to do so — with a cpu or a gpu the main computational module in a computer be the central processing unit well know as cpu it be design to do computation rapidly on a small amount of datum for example multiply a few number on a cpu be blazingly fast but it struggle when operate on a large amount of data e g multiply matrix of ten or hundred thousand number behind the scene dl be mostly comprise of operation like matrix multiplication amusingly 3d computer game rely on these same operation to render that beautiful landscape you see in rise of the tomb raider thus gpus be develop to handle lot of parallel computation use thousand of core also they have a large memory bandwidth to deal with the datum for these computation this make they the ideal commodity hardware to do dl on or at least until asic for machine learning like google s tpu make their way to market for I the most important reason for pick a powerful graphic processor be save time while prototyping model if the network train fast the feedback time will be short thus it would be easy for my brain to connect the dot between the assumption I have for the model and its result see tim dettmer answer to why be gpu well suited to deep learning on quora for a well explanation also for an in depth albeit slightly outdated gpu comparison see his article which gpu s to get for deep learning there be main characteristic of a gpu relate to dl be there be two reason for have multiple gpu you want to train several model at once or you want to do distribute training of a single model we ll go over each one train several model at once be a great technique to test different prototype and hyperparameter it also shorten your feedback cycle and let you try out many thing at once distribute training or train a single network on several video card be slowly but surely gain traction nowadays there be easy to use approach to this for tensorflow and keras via horovod cntk and pytorch the distribute training library offer almost linear speed up to the number of card for example with 2 gpu you get 1 8x fast training pcie lane update the caveat to use multiple video card be that you need to be able to feed they with datum for this purpose each gpu should have 16 pcie lane available for data transfer tim dettmer point out that have 8 pcie lane per card should only decrease performance by 0 10 % for two gpu for a single card any desktop processor and chipset like intel i5 7500 and asus tuf z270 will use 16 lane however for two gpu you can go 8x 8x lane or get a processor and a motherboard that support 32 pcie lane 32 lane be outside the realm of desktop cpu an intel xeon with a msi — x99a sli plus will do the job for 3 or 4 gpu go with 8x lane per card with a xeon with 24 to 32 pcie lane to have 16 pcie lane available for 3 or 4 gpu you need a monstrous processor something in the class of or amd threadripper 64 lane with a corresponding motherboard also for more gpu you need a fast processor and hard disk to be able to feed they datum quickly enough so they don t sit idle nvidia have be focus on deep learning for a while now and the head start be pay off their cuda toolkit be deeply entrench it work with all major dl framework — tensoflow pytorch caffe cntk etc as of now none of these work out of the box with opencl cuda alternative which run on amd gpu I hope support for opencl come soon as there be great inexpensive gpu from amd on the market also some amd card support half precision computation which double their performance and vram size currently if you want to do dl and want to avoid major headache choose nvidia your gpu need a computer around it hard disk first you need to read the datum off the disk an ssd be recommend here but an hdd can work as well cpu that datum might have to be decode by the cpu e g jpeg fortunately any mid range modern processor will do just fine motherboard the data pass via the motherboard to reach the gpu for a single video card almost any chipset will work if you be plan on work with multiple graphic card read this section ram it be recommend to have 2 gigabyte of memory for every gigabyte of video card ram have more certainly help in some situation like when you want to keep an entire dataset in memory power supply it should provide enough power for the cpu and the gpu plus 100 watt extra you can get all of this for $ 500 to $ 1000 or even less if you buy a use workstation here be performance comparison between all card check the individual card profile below notably the performance of titan xp and gtx 1080 ti be very close despite the huge price gap between they the price comparison reveal that gtx 1080 ti gtx 1070 and gtx 1060 have great value for the compute performance they provide all the card be in the same league value wise except titan xp the king of the hill when every gb of vram matter this card have more than any other on the consumer market it s only a recommend buy if you know why you want it for the price of titan x you could get two gtx 1080 which be a lot of power and 16 gbs of vram this card be what I currently use it s a great high end option with lot of ram and high throughput very good value I recommend this gpu if you can afford it it work great for computer vision or kaggle competition quite capable mid to high end card the price be reduce from $ 700 to $ 550 when 1080 ti be introduce 8 gb be enough for most computer vision task people regularly compete on kaggle with these the new card in nvidia s lineup if 1080 be over budget this will get you the same amount of vram 8 gb also 80 % of the performance for 80 % of the price pretty sweet deal it s hard to get these nowadays because they be use for cryptocurrency mining with a considerable amount of vram for this price but somewhat slow if you can get it or a couple second hand at a good price go for it it s quite cheap but 6 gb vram be limit that s probably the minimum you want to have if you be do computer vision it will be okay for nlp and categorical datum model also available as p106 100 for cryptocurrency mining but it s the same card without a display output the entry level card which will get you start but not much more still if you be unsure about get in deep learning this might be a cheap way to get your foot wet titan x pascal it use to be the good consumer gpu nvidia have to offer make obsolete by 1080 ti which have the same spec and be 40 % cheap tesla gpusthis include k40 k80 which be 2x k40 in one p100 and other you might already be use these via amazon web service google cloud platform or another cloud provider in my previous article I do some benchmark on gtx 1080 ti vs k40 the 1080 perform five time fast than the tesla card and 2 5x fast than k80 k40 have 12 gb vram and k80 a whopping 24 gbs in theory the p100 and gtx 1080 ti should be in the same league performance wise however this cryptocurrency comparison have p100 lagging in every benchmark it be worth note that you can do half precision on p100 effectively double the performance and vram size on top of all this k40 go for over $ 2000 k80 for over $ 3000 and p100 be about $ 4500 and they get still get eat alive by a desktop grade card obviously as it stand I don t recommend get they all the spec in the world win t help you if you don t know what you be look for here be my gpu recommendation depend on your budget I have over $ 1000 get as many gtx 1080 ti or gtx 1080 as you can if you have 3 or 4 gpu run in the same box beware of issue with feed they with datum also keep in mind the airflow in the case and the space on the motherboard I have $ 700 to $ 900 gtx 1080 ti be highly recommend if you want to go multi gpu get 2x gtx 1070 if you can find they or 2x gtx 1070 ti kaggle here I come I have $ 400 to $ 700 get the gtx 1080 or gtx 1070 ti maybe 2x gtx 1060 if you really want 2 gpu however know that 6 gb per model can be limit I have $ 300 to $ 400 gtx 1060 will get you start unless you can find a use gtx 1070 I have less than $ 300 get gtx 1050 ti or save for gtx 1060 if you be serious about deep learn deep learning have the great promise of transform many area of our life unfortunately learn to wield this powerful tool require good hardware hopefully I ve give you some clarity on where to start in this quest disclosure the above be affiliate link to help I pay for well more gpu from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur hacker machine learn deep learning and other type of learn
Datafiniti,3,5,https://blog.datafiniti.co/classifying-websites-with-neural-networks-39123a464055?source=tag_archive---------0----------------,classify website with neural network knowledge from datum the datafiniti blog,at datafiniti we have a strong need for convert unstructured web content into structured datum for example we d like to find a page like and do the following both of these be hard thing for a computer to do in an automate manner while it s easy for you or I to realize that the above web page be sell some jean a computer would have a hard time make the distinction from the above page from either of the follow web page or both of these page share many similarity to the actual product page but also have many key difference the real challenge though be that if we look at the entire set of possible web page those similarity and difference become somewhat blurred which mean hard and fast rule for classification will fail often in fact we can t even rely on just look at the underlie html since there be huge variation in how product page be lay out in html while we could try and develop a complicated set of rule to account for all the condition that perfectly identify a product page do so would be extremely time consume and frankly incredibly boring work instead we can try use a classical technique out of the artificial intelligence handbook neural network here s a quick primer on neural network let s say we want to know whether any particular mushroom be poisonous or not we re not entirely sure what determine this but we do have a record of mushroom with their diameter and height along with which of these mushroom be poisonous to eat for sure in order to see if we could use diameter and height to determine poisonous ness we could set up the follow equation a * diameter + b * height = 0 or 1 for not poisonous poisonous we would then try various combination of a and b for all possible diameter and height until we find a combination that correctly determine poisonous ness for as many mushroom as possible neural network provide a structure for use the output of one set of input datum to adjust a and b to the most likely good value for the next set of input datum by constantly adjust a and b this way we can quickly get to the good possible value for they in order to introduce more complex relationship in our datum we can introduce hidden layer in this model which would end up look something like for a more detailed explanation of neural network you can check out the follow link in our product page classifier algorithm we setup a neural network with 1 input layer with 27 node 1 hide layer with 25 node and 1 output layer with 3 output node our input layer model several feature include our output layer have the follow our algorithm for the neural network take the follow step the ultimate output be two set of input layer t1 and t2 that we can use in a matrix equation to predict page type for any give web page this work like so so how do we do in order to determine how successful we be in our prediction we need to determine how to measure success in general we want to measure how many true positive tp result as compare to false positive fp and false negative fn conventional measurement for these be our implementation have the follow result these score be just over our training set of course the actual score on real life datum may be a bit low but not by much this be pretty good we should have an algorithm on our hand that can accurately classify product page about 90 % of the time of course identify product page isn t enough we also want to pull out the actual structured datum in particular we re interested in product name price and any unique identifier e g upc ean & isbn this information would help we fill out our product search we don t actually use neural network for do this neural network be well suit toward classification problem and extract datum from a web page be a different type of problem instead we use a variety of heuristic specific to each attribute we re try to extract for example for product name we look at the < h1 > and < h2 > tag and use a few metric to determine the good choice we ve be able to achieve around a 80 % accuracy here we may go into the actual metric and methodology for develop they in a separate post we feel pretty good about our ability to classify and extract product datum the extraction part could be well but it s steadily be improve in the meantime we re also work on classify other type of page such as business datum company team page event datum and more as we roll out these classifier and datum extractor we re include each one in our crawl of the entire internet this mean that we can scan the entire internet and pull out any available datum that exist out there exciting stuff you can connect with we and learn more about our business people product and property api and dataset by select one of the option below from a quick cheer to a stand ovation clap to show how much you enjoy this story instant access to web datum build the world s large database of web datum — follow our journey
Yingjie Miao ,43,6,https://medium.com/kifi-engineering/from-word2vec-to-doc2vec-an-approach-driven-by-chinese-restaurant-process-93d3602eaa31?source=tag_archive---------0----------------,from word2vec to doc2vec an approach drive by chinese restaurant process,google s word2vec project have create lot of interest in the text mining community it s a neural network language model that be both supervise and unsupervise unsupervised in the sense that you only have to provide a big corpus say english wiki supervise in the sense that the model cleverly generate supervised learning task from the corpus how two approach know as continuous bag of word cbow and skip gram see figure 1 in this paper cbow force the neural net to predict current word by surround word and skip gram force the neural net to predict surround word of the current word training be essentially a classic back propagation method with a few optimization and approximation trick e g hierarchical softmax word vector generate by the neural net have nice semantic and syntactic behavior semantically io be close to android syntactically boy minus boy be close to girl minus girl one can checkout more example here although this provide high quality word vector there be still no clear way to combine they into a high quality document vector in this article we discuss one possible heuristic inspire by a stochastic process call chinese restaurant process crp basic idea be to use crp to drive a clustering process and sum word vector in the right cluster imagine we have an document about chicken recipe it contain word like chicken pepper salt cheese it also contain word like use buy definitely my the the word2vec model give we a vector for each word one could naively sum up every word vector as the doc vector this clearly introduce lot of noise a well heuristic be to use a weighted sum base on other information like idf or part of speech pos tag the question be could we be more selective when add term if this be a chicken recipe document I shouldn t even consider word like definitely use my in the summation one can argue that idf base weight can significantly reduce noise of boring word like the and be however for word like definitely overwhelm the idfs be not necessarily small as you would hope it s natural to think that if we can first group word into cluster word like chicken pepper may stay in one cluster along with other cluster of junk word if we can identify the relevant cluster and only sum up word vector from relevant cluster we should have a good doc vector this boil down to cluster the word in the document one can of course use off the shelf algorithm like k mean but most these algorithm require a distance metric word2vec behave nicely by cosine similarity this doesn t necessarily mean it behave as well under eucledian distance even after projection to unit sphere it s perhaps good to use geodesic distance it would be nice if we can directly work with cosine similarity we have do a quick experiment on cluster word drive by crp like stochastic process it work surprisingly well — so far now let s explain crp imagine you go to a chinese restaurant there be already n table with different number of people there be also an empty table crp have a hyperparamt r > 0 which can be regard as the imagine number of people on the empty table you go to one of the n+1 table with probability proportional to exist number of people on the table for the empty table the number be r if you go to one of the n exist table you be do if you decide to sit down at the empty table the chinese restaurant will automatically create a new empty table in that case the next customer come in will choose from n+2 table include the new empty table inspire by crp we try the follow variation of crp to include the similarity factor common setup be the following we be give m vector to be cluster we maintain two thing cluster sum not centroid and vector in cluster we iterate through vector for current vector v suppose we have n cluster already now we find the cluster c whose cluster sum be most similar to current vector call this score sim v c variant 1 v create a new cluster with probability 1 1 + n otherwise v go to cluster c variant 2 if sim v c > 1 1 + n go to cluster c otherwise with probability 1 1+n it create a new cluster and with probability n 1+n it go to c in any of the two variant if v go to a cluster we update cluster sum and cluster membership there be one distinct difference to traditional crp if we don t go to empty table we deterministically go to the most similar table in practice we find these variant create similar result one difference be that variant 1 tend to have more cluster and small cluster variant 2 tend to have few but large cluster the example below be from variant 2 for example for a chicken recipe document the cluster look like this apparently the first cluster be most relevant now let s take the cluster sum vector which be the sum of all vector from this cluster and test if it really preserve semantic below be a snippet of python console we train word vector use the c implementation on a fraction of english wiki and read the model file use python library gensim model word2vec c 0 below denote the cluster 0 look like the semantic be preserve well it s convincing that we can use this as the doc vector the recipe document seem easy now let s try something more challenging like a news article news article tend to tell story and thus have less concentrated topic word we try the clustering on this article title signal on radar puzzle official in hunt for malaysian jet we get 4 cluster again look decent note that this be a simple 1 pass clustering process and we don t have to specify number of cluster could be very helpful for latency sensitive service there be still a miss step how to find out the relevant cluster s we haven t yet do extensive experiment on this part a few heuristic to consider there be other problem to think about 1 how do we merge cluster base on similarity among cluster sum vector or average similarity between cluster member 2 what be the minimal set of word that can reconstruct cluster sum vector in the sense of cosine similarity this could be use as a semantic keyword extraction method conclusion google s word2vec provide powerful word vector we be interested in use these vector to generate high quality document vector in an efficient way we try a strategy base on a variant of chinese restaurant process and obtain interesting result there be some open problem to explore and we would like to hear what you think appendix python style pseudo code for similarity drive crp we write this post while work on kifi — connect people with knowledge learn more originally publish at eng kifi com on march 17 2014 from a quick cheer to a stand ovation clap to show how much you enjoy this story the kifi engineering blog
Milo Spencer-Harper,2.2K,3,https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a?source=tag_archive---------1----------------,how to build a multi layer neural network in python,in my last blog post thank to an excellent blog post by andrew trask I learn how to build a neural network for the first time it be super simple 9 line of python code model the behaviour of a single neuron but what if we be face with a more difficult problem can you guess what the should be the trick be to notice that the third column be irrelevant but the first two column exhibit the behaviour of a xor gate if either the first column or the second column be 1 then the output be 1 however if both column be 0 or both column be 1 then the output be 0 so the correct answer be 0 however this would be too much for our single neuron to handle this be consider a nonlinear pattern because there be no direct one to one relationship between the input and the output instead we must create an additional hidden layer consist of four neuron layer 1 this layer enable the neural network to think about combination of input you can see from the diagram that the output of layer 1 feed into layer 2 it be now possible for the neural network to discover correlation between the output of layer 1 and the output in the training set as the neural network learn it will amplify those correlation by adjust the weight in both layer in fact image recognition be very similar there be no direct relationship between pixel and apple but there be a direct relationship between combination of pixel and apple the process of add more layer to a neural network so it can think about combination be call deep learning ok be we ready for the python code first I ll give you the code and then I ll explain far also available here https github com miloharper multi layer neural network this code be an adaptation from my previous neural network so for a more comprehensive explanation it s worth look back at my early blog post what s different this time be that there be multiple layer when the neural network calculate the error in layer 2 it propagate the error backwards to layer 1 adjust the weight as it go this be call back propagation ok let s try run it use the terminal command python main py you should get a result that look like this first the neural network assign herself random weight to her synaptic connection then she train herself use the training set then she consider a new situation 1 1 0 that she hadn t see before and predict 0 0078876 the correct answer be 0 so she be pretty close you might have notice that as my neural network have become smart I ve inadvertently personify she by use she instead of it that s pretty cool but the computer be do lot of matrix multiplication behind the scene which be hard to visualise in my next blog post I ll visually represent our neural network with an animate diagram of her neuron and synaptic connection so we can see her thinking from a quick cheer to a stand ovation clap to show how much you enjoy this story study economic at oxford university founder of www moju io interested in politic and ai technology trend and new invention follow this collection to update the late trend update as a collection editor I don t have any permission to add your article in the wild please submit your article and I will approve also follow this collection please
Josh,462,9,https://medium.com/technology-invention-and-more/everything-you-need-to-know-about-artificial-neural-networks-57fac18245a1?source=tag_archive---------3----------------,everything you need to know about artificial neural network,the year 2015 be a monumental year in the field of artificial intelligence not only be computer learn more and learn fast but we re learn more about how to improve their system everything be start to align and because of it we re see stride we ve never think possible until now we have program that can tell story about picture we have car that be drive themselves we even have program that create art if you want to read more about advancement in 2015 read this article here at josh ai with ai technology become the core of just about everything we do we think it s important to understand some of the common terminology and to get a rough idea of how it all work a lot of the advance in artificial intelligence be new statistical model but the overwhelming majority of the advance be in a technology call artificial neural network ann if you ve read anything about they before you ll have read that these ann be a very rough model of how the human brain be structure take note that there be a difference between artificial neural network and neural network though most people drop the artificial for the sake of brevity the word artificial be prepende to the phrase so that people in computational neurobiology could still use the term neural network to refer to their work below be a diagram of actual neuron and synapsis in the brain compare to artificial one fear not if the diagram doesn t come through very clearly what s important to understand here be that in our anns we have these unit of calculation call neuron these artificial neuron be connect by synapsis which be really just weight value what this mean be that give a number a neuron will perform some sort of calculation for example the sigmoid function and then the result of this calculation will be multiply by a weight as it travel the weighted result can sometimes be the output of your neural network or as I ll talk about soon you can have more neuron configure in layer which be the basic concept to an idea that we call deep learn artificial neural network be not a new concept in fact we didn t even always call they neural network and they certainly don t look the same now as they do at their inception back during the 1960 we have what be call a perceptron perceptron be make of mcculloch pitt neuron we even have bias perceptron and ultimately people start create multilayer perceptron which be synonymous with the general artificial neural network we hear about now but wait if we ve have neural network since the 1960 why be they just now get huge it s a long story and I encourage you to listen to this podcast episode to listen to the father of modern anns talk about their perspective of the topic to quickly summarize there s a hand full of factor that keep anns from become more popular we didn t have the computer processing power and we didn t have the datum to train they use they be frown upon due to they have a seemingly arbitrary ability to perform well each one of these factor be change our computer be get fast and more powerful and with the internet we have all kind of datum be share for use you see I mention above that the neuron and synapsis perform calculation the question on your mind should be how do they learn what calculation to perform be I right the answer be that we need to essentially ask they a large amount of question and provide they with answer this be a field call supervised learning with enough example of question answer pair the calculation and value store at each neuron and synapse be slowly adjust usually this be through a process call backpropagation imagine you re walk down a sidewalk and you see a lamp post you ve never see a lamp post before so you walk right into it and say ouch the next time you see a lamp post you scoot a few inch to the side and keep walk this time your shoulder hit the lamp post and again you say ouch the third time you see a lamp post you move all the way over to ensure you don t hit the lamp post except now something terrible have happen — now you ve walk directly into the path of a mailbox and you ve never see a mailbox before you walk into it and the whole process happen again obviously this be an oversimplification but it be effectively what backpropogation do an artificial neural network be give a multitude of example and then it try to get the same answer as the example give when it be wrong an error be calculate and the value at each neuron and synapse be propagate backwards through the ann for the next time this process take a lot of example for real world application the number of example can be in the million now that we have an understanding of artificial neural network and somewhat of an understanding in how they work there s another question that should be on your mind how do we know how many neuron we need to use and why do you bold the word layer early layer be just set of neuron we have an input layer which be the datum we provide to the ann we have the hide layer which be where the magic happen lastly we have the output layer which be where the finished computation of the network be place for we to use layer themselves be just set of neuron in the early day of multilayer perceptron we originally think that have just one input layer one hide layer and one output layer be sufficient it make sense right give some number you just need one set of computation and then you get an output if your ann wasn t calculate the correct value you just add more neuron to the single hide layer eventually we learn that in do this we be really just create a linear mapping from each input to the output in other word we learn that a certain input would always map to a certain output we have no flexibility and really could only handle input we d see before this be by no mean what we want now introduce deep learning which be when we have more than one hide layer this be one of the reason we have well anns now because we need hundred of node with ten if not more layer this lead to a massive amount of variable that we need to keep track of at a time advance in parallel programming also allow we to run even large anns in batch our artificial neural network be now get so large that we can no long run a single epoch which be an iteration through the entire network at once we need to do everything in batch which be just subset of the entire network and once we complete an entire epoch then we apply the backpropagation along with now use deep learning it s important to know that there be a multitude of different architecture of artificial neural network the typical ann be setup in a way where each neuron be connect to every other neuron in the next layer these be specifically call feed forward artificial neural network even though ann be generally all feed forward we ve learn that by connect neuron to other neuron in certain pattern we can get even well result in specific scenario recurrent neural network rnn be create to address the flaw in artificial neural network that didn t make decision base on previous knowledge a typical ann have learn to make decision base on context in training but once it be make decision for use the decision be make independent of each other when would we want something like this well think about play a game of blackjack if you be give a 4 and a 5 to start you know that 2 low card be out of the deck information like this could help you determine whether or not you should hit rnn be very useful in natural language processing since prior word or character be useful in understand the context of another word there be plenty of different implementation but the intention be always the same we want to retain information we can achieve this through have bi directional rnn or we can implement a recurrent hide layer that get modify with each feedforward if you want to learn more about rnn check out either this tutorial where you implement an rnn in python or this blog post where use for an rnn be more thoroughly explain an honorable mention go to memory network the concept be that we need to retain more information than what an rnn or lstm keep if we want to understand something like a movie or book where a lot of event might occur that build on each other convolutional neural network cnn sometimes call lenet name after yann lecun be artificial neural network where the connection between layer appear to be somewhat arbitrary however the reason for the synapsis to be setup the way they be be to help reduce the number of parameter that need to be optimize this be do by note a certain symmetry in how the neuron be connect and so you can essentially re use neuron to have identical copy without necessarily need the same number of synapsis cnn be commonly use in work with image thank to their ability to recognize pattern in surround pixel there s redundant information contain when you look at each individual pixel compare to its surround pixel and you can actually compress some of this information thank to their symmetrical property sound like the perfect situation for a cnn if you ask I christopher olah have a great blog post about understand cnn as well as other type of anns which you can find here another great resource for understand cnn be this blog post the last ann type that I m go to talk about be the type call reinforcement learning reinforcement learning be a generic term use for the behavior that computer exhibit when try to maximize a certain reward which mean that it in itself isn t an artificial neural network architecture however you can apply reinforcement learning or genetic algorithm to build an artificial neural network architecture that you might not have think to use before a great example and explanation can be find in this video where youtube user sethbling create a reinforcement learning system that build an artificial neural network architecture that play a mario game entirely on its own another successful example of reinforcement learning can be see in this video where the company deepmind be able to teach a program to master various atari game now you should have a basic understanding of what s go on with the state of the art work in artificial intelligence neural network be power just about everything we do include language translation animal recognition picture captioning text summarization and just about anything else you can think of you re sure to hear more about they in the future so it s good that you understand they now this post be write by aaron at josh ai previously aaron work at northrop grumman before join the josh team where he work on natural language programming nlp and artificial intelligence ai aaron be a skilled yoyo expert love video game and music have be program since middle school and recently turn 21 josh ai be an ai agent for your home if you re interested in follow josh and get early access to the beta enter your email at https josh ai like josh on facebook — http facebook com joshdotai follow josh on twitter — http twitter com joshdotai from a quick cheer to a stand ovation clap to show how much you enjoy this story technology trend and new invention follow this collection to update the late trend update as a collection editor I don t have any permission to add your article in the wild please submit your article and I will approve also follow this collection please
Milo Spencer-Harper,317,6,https://medium.com/deep-learning-101/how-to-create-a-mind-the-secret-of-human-thought-revealed-6211bbdb092a?source=tag_archive---------4----------------,how to create a mind the secret of human thought reveal,in my quest to learn about ai I read how to create a mind the secret of human thought reveal by ray kurzweil it be incredibly exciting and I m go to share what I ve learn if I be go to summarise the book in one sentence I could do no well than kurzweil s own word kurzweil argue convincingly that it be both possible and desirable he go on to suggest that the algorithm may be simple than we would expect and that it will be base on the pattern recognition theory of the mind prtm the human brain be the most incredible thing in the know universe a three pound object it can discover relativity imagine the universe create music build the taj mahal and write a book about the brain however it also have limitation and this give we clue as to how it work recite the alphabet ok good now recite it backwards the former be easy the latter likely impossible yet a computer find it trivial to reverse a list this tell we that the human brain can only retrieve information sequentially study have also reveal that when think about something we can only hold around four high level concept in our brain at a time that s why we use tool such as pen and paper to solve a math problem to help we think so how do the human brain work mammal actually have two brain the old reptilian brain call the amygdala and the conscious part call the neocortex the amygdala be pre program through evolution to seek pleasure and avoid pain we call this instinct but what distinguish mammal from other animal be that we have also evolve to have a neocortex our neocortex rationalise the world around we and make prediction it allow we to learn the two brain be tightly bind and work together however when read the book I wonder if these two brain might also be in conflict it would explain why the idea of internal struggle be present throughout literature and religion good vs evil social conformity vs hedonism what s slightly more alarming be we may have more mind than that our brain be divide into two hemisphere leave and right study of split brain patient where the connection between they have be sever show that these patient be not necessarily aware that the other mind exist if one mind move the right hand the other mind will post rationalise this decision by create a false memory a process know as confabulation this have implication for we all we may not have the free will which we perceive to have our conscious part of the brain may simply be create explanation for what the unconscious part have already do so how do the neocortex work we know that it consist of around 30 billion cell which we call neuron these neuron be connect together and transmit information use electrical impulse if the sum of the electrical pulse across multiple input to a neuron exceed a certain threshold that neuron fire cause the next neuron in the chain to fire and this go on continuously we call these process thought at first scientist think this neural network be such a complicated and tangled web that it would be impossible to ever understand however kurzweil use the example of the einstein s famous equation e = mc^2 to demonstrate that sometimes the solution to complex problem be surprisingly simple there be many example in science from newtonian mechanic to thermodynamic which show that move up a level of abstraction dramatically simplifie model complex system recent innovation in brain imaging technique have reveal that the neocortex contain module each consist of around 100 neuron repeat over and over again there be around 300 million of these module arrange in a grid so if we could discover the equation which model this module repeat it on a computer 300 million time and expose it to sensory input we could create an intelligent being but what do these module do kurzweil who have spend decade research ai propose that these module be pattern recogniser when read this page one pattern recogniser might be responsible for detect a horizontal stroke this module link upward to a module responsible for the letter a and if the other relevant stroke module light up the a module also light up the module a p p and l link to the apple module which in turn be link to high level pattern recogniser such as thought about apple you don t actually need to see the e because the apple pattern recogniser fire downward tell the one responsible for the letter e that there be a high probability of see one conversely inhibitory signal suppress pattern recogniser from fire if a high level pattern recogniser have detect such an event be unlikely give the context we literally see what we expect to see kurzweil call this the pattern recogniser theory of the mind prtm although it be hard for we to imagine all of our thought and decision can be explain by huge number of these pattern recogniser hook together we organise these thought to explain the world in a hierarchal fashion and use word to give meaning to these module the world be naturally hierarchal and the brain mirror this leave be on tree tree make up a forest and a forest cover a mountain language be closely relate to our thought because language directly evolve from and mirror our brain this help to explain why different language follow remarkably similar structure it explain why we think use our native language we use language not only to express idea to other but to express idea within our own mind what s interesting be that when ai researcher have work independently of neuroscientist their most successful method turn out to be equivalent to the human brain s method thus the human brain offer we clue for how to create an intelligent nonbiological entity if we work out the algorithm for a single pattern recogniser we can repeat it on a computer create a neural network kurzweil argue that these neural network could become conscious like a human mind free from biological constraint and benefit from the exponential growth in compute power these entity could create even smart entity and surpass we in intelligence this prediction be call technological singularity I ll discuss the ethical and social consideration in a future blog post but for now let s assume it be desirable the question then become what be the algorithm for a single pattern recogniser kurzweil recommend use a mathematical technique call hierarchal hide markov model name after the russian mathematician andrey markov 1856 1922 however this technique be too technical to be properly explain in kurzweil s book so my next two goal be 1 to learn as much as I can about hierarchal hide markov model 2 to build a simple neural network write in python from scratch which can be train to complete a simple task in my next blog post I learn how to build a neural network in 9 line of python code note submission do not necessarily represent the view of the editor from a quick cheer to a stand ovation clap to show how much you enjoy this story study economic at oxford university founder of www moju io interested in politic and ai fundamental and late development in # deeplearne
Karl N.,10,7,https://gab41.lab41.org/taking-keras-to-the-zoo-9a76243152cb?source=tag_archive---------5----------------,take kera to the zoo gab41,if you follow any of the popular blog like google s research fastml smola s adventure in datum land or one of the indie pop one like edwin chen s blog you ve probably also use modelzoo actually if you re like our boss you affectionately call it the zoo actually x 2 if you have interesting blog that you read feel free to let we know unfortunately modelzoo be only support in caffe fortunately we ve take a look at the difference between the kernel in keras theano and caffe for you and after read this blog you ll be able to load model from modelzoo into any of your favorite python tool why this post why not just download our github code in short it s well you figure out how these thing work before you use they that way you re well armed to use the late tensorflow and neon toolbox if you re prototype and transition your code to caffe so there s hinton s dropout and then there s caffe s dropout and they re different you might be wonder what s the big deal well sir I have a name of a guy for you and it s willy mr willy nilly one thing willy nilly like be the number 4096 another thing he like be to introduce regularization which include dropout arbitrarily and bayesian theorist aren t a fan those people try to fit their work into the probabilistic framework and they re try to hold onto what semblance of theoretical bound exist for neural network however for you as a practitioner understanding who s do what will save you hour of debug code we single out dropout because the way people have implement it span the gamut there s actually some history as to this variation but no one really care because optimize for it have almost universally produce similar result much of the discussion stem from how the chain rule be implement since randomly throw stuff away be apparently not really a differentiable operation pass gradient back I e backpropagation be a fun thing to do ; there s a technically right way to do it and then there s what s work back to modelzoo where we d recommend you note the only sentence of any substance in this section and the sentence be as follow while keras and perhaps other package multiply the gradient by the retention probability at inference time caffe do not that be to say if you have a dropout level of 0 2 your retention probability be 0 8 and at inference time keras will scale the output of your prediction by 0 8 so download the modelzoo * caffemodel but know that deploy they on caffe will produce non scale result whereas keras will hinton explain the reason why you need to scale and the intuition be as follow if you ve only get a portion of your signal seep through to the next layer during training you should scale the expectation of what the energy of your final result should be seem like a weird thing to care about right the argument that minimize x be still the same as the argument that minimize 2x this turn out to be a problem when you re pass multiple gradient back and don t implement your layer uniformly caffe work in instance like siamese network or bilinear network but should you scale your network on two side differently don t be surprised if you re get unexpected result what do this look like in francois s code look at the dropout code on github or in your installation folder under keras layers core py if you want to make your own layer for load in the dropout module just comment out the part of the code that do this scaling you can modify the original code or you can create your own custom layer we ve opt to keep our installation of keras clean and just implement a new class that extend maskedlayer btw you should be careful in your use of dropout our experience with they be that they regularize okay but could contribute to vanish gradient really quickly everyday except for sunday and some holiday a select few machine learning professor and some signal processing leader meet in an undisclosed location in the early hour of the morning the topic of their discussion be almost universally how do we get researcher and deep learning practitioner to code bug into their program one of the conclusion a while back be that the definition of convolution and dense matrix multiplication or cross correlation should be exactly opposite of each other that way when people be build algorithm that call themselves convolutional neural network no one will know which implementation be actually be use for the convolution portion itself for those who don t know convolution and sweeping matrix multiplication across an array of datum differ in that convolution will be flip before be slide across the array from wikipedia the definition be on the other hand if you re sweeping matrix multiplication across the array of datum you re essentially do cross correlation which on wikipedia look like like we say the only difference be that darn minus plus sign which cause we some headache we happen to know that theano and caffe follow different philosophy once again caffe doesn t bother with pleasantry and straight up code efficient matrix multiplie to load model from modelzoo into either keras and theano will require the transformation because they strictly follow the definition of convolution the easy fix be to flip it yourself when you re load the weight into your model for 2d convolution this look like weight = weight 1 1 here the variable weight will be insert into your model s parameter you can set weight by index into the model for example say I want to set the 9th layer s weight I would type model layer 9 set_weight weight incidentally and this be important when load any * caffemodel into python you may have to transpose it in order to use it you can quickly find this out by load it if you get an error but we think it worth note alright alright we know what you re really here for ; just get the code and run with it so we ve get some example code that classifie use keras and the vgg net from the web at our git see the link below but let s go through it just a bit here s a step by step account of what you need to do to use the vgg caffe model and now you have the basic go ahead and take a look at our github for some goody let we know originally publish at www lab41 org on december 13 2015 from a quick cheer to a stand ovation clap to show how much you enjoy this story gab41 be lab41 s blog explore data science machine learning and artificial intelligence geek out with we
Milo Spencer-Harper,42,3,https://medium.com/@miloharper/thanks-so-much-for-your-response-jared-really-glad-to-hear-you-enjoyed-reading-it-9d73caa469ff?source=tag_archive---------6----------------,thank so much for your response jar really glad you enjoy read it,thank so much for your response jar really glad you enjoy read it could you go into more detail about find the error on layer 1 that s a really great question I ve change this response quite a bit as I write it because your question help I improve my own understanding it sound like you know quite a lot about neural network already however I m go to explain everything fully for reader who be new to the field in the article you read I model the neural network use matrix grid of number that s the most common method as it be computationally fast and mathematically equivalent but it hide a lot of the detail for example line 15 calculate the error in layer 1 but it be hard to visualise what it be do to help I learn I ve re write that same code by model the layer neuron and synapsis explicitly and have create a video of the neural network learning I m go to use this new version of my code to answer your question for clarity I ll describe how I m go to refer to the layer the three input neuron be layer 0 the four neuron in the hide layer be layer 1 and the single output neuron be layer 2 in my code I choose to associate the synapsis with the neuron they flow into how do I find the error in layer 1 first I calculate the error of the output neuron layer 2 which be the difference between its output and the output in the training set example then I work my way backwards through the neural network so I look at the incoming synapsis into layer 2 and estimate how much each of the neuron in layer 1 be responsible for the error this be call back propagation in my new version of the code the neural network be represent by a class call neuralnetwork and it have a method call train which be show below you can see I calculate the error of the ouput neuron line 3 and 4 then I work backwards through the layer line 5 next I cycle through all the neuron in a layer line 6 and call each individual neuron s train method line 7 but what do the neuron s train method do here it be you can see that I cycle through every incoming synapse into the neuron the two key thing to note be let s consider line 4 even more carefully since this be the line which answer your question directly for each neuron in layer 1 its error be equal to the error in the output neuron layer 2 multiply by the weight of its synapse into the output neuron multiply by the sensitivity of the output neuron to input the sensitivity of a neuron to input be describe by the gradient of its output function since I use the sigmoid curve as my ouput function the gradient be the derivative of the sigmoid curve as well as use the gradient to calculate the error I also use the gradient to adjust the weight so this method of learning be call gradient descent if you look back at my old code which use matrix you can see that it be mathematically equivalent unless I make a mistake with the matrix method I calculate the error for all the neuron in layer 1 simultaneously with the new code I iterate through each neuron separately I hope that help answer your question also I m curious if there be any theory or rule of thumb on how many hidden layer and how many neuron in each layer should be use to solve a problem another good question I m not sure I m pretty new to neural network I only start learn about they recently I do read a book by the ai researcher ray kurzweil which say that an evolutionary approach work well than consult expert when select the overall parameter for a neural network those neural network which learn the good would be select he would make random mutation to the parameter and then pit the offspring against one another from a quick cheer to a stand ovation clap to show how much you enjoy this story study economic at oxford university founder of www moju io interested in politic and ai
Nikolai Savas,50,10,https://medium.com/@savas/craig-using-neural-networks-to-learn-mario-a76036b639ad?source=tag_archive---------7----------------,craig use neural network to learn mario nikolai savas medium,joe crozier and I recently come back from yhack a 36 hour 1500 person hackathon hold by yale university this be our second year in a row attend and for the second time we manage to place in the top 8 our project name craig be a self teach algorithm that learn to play super mario bro for the nintendo entertainment system ne it begin with absolutely no knowledge of what mario be how to play it or what win look like and use neuroevolution slowly build up a foundation for itself to be able to progress in the game my focus on this project be the gritty detail of the implementation of craig s evolution algorithm so I figure I d make a relatively indepth blog post about it craig s evolution be base on a paper title evolve neural network through augment topology specifically an algorithm title neat the rest of the blog post be go to cover my implementation of it hopefully in relatively layman s term before we jump right into the algorithm I m go to lay a foundation for the makeup of craig s brain his brain at any give point play the game be make up of a collection of neuron and synapsis alternatively title node and connection link essentially his brain be a direct graph above be the second part of this project a node js server that display the current state of craig s brain or what he be think let s go through it quickly to understand what it s represent on the left you see a big grid of square this be what the game look like right now or what craig can see he doesn t know what any of the square mean but he know that an air tile be different from a ground tile in some way each of the square be actually an input neuron on the right side you can see the 4 output neuron or the button that craig can press you can also see a line go from one of the black square on the left grid to the r neuron label 1 this be a synapse and when the input neuron fire on the left it will send a signal down the synapse and tell craig to press the r button in this way craig walk right as craig evolve more neuron and synapsis be create until his brain might look something more like this in this one I ll just point out a couple thing first of all the green square on the left be a goomba second you can see another neuron at the very bottom label 176 this be call a hidden neuron and represent a neuron that be neither input nor output they appear in craig s brain for add complexity as he evolve you can also see that at his time of death mario just die to a goomba he be try to press the r and b button while learn mario be a neat application of neural network and neuroevolution it serve mostly as a mean to demonstrate the power of these self evolve neural network in reality the application for neural network be endless while craig only learn how to play a simple ne game the exact same algorithm that be implement could also be apply to a robot that clean your house work in a factory or even paint beautiful painting craig be a cool peek into the future where machine no long need to be program to complete specific task but be instead give guideline and can teach themselves and learn from experience as the task we expect machine to complete become more and more complex it become less possible to hard code their task in we need more versatile machine to work for we and evolve neural network be a step in that direction if you re curious about some history behind the problem encounter by neuroevolution I highly recommend read the paper that this algorithm be base off the first section of the paper cover many different approach to neuroevolution and their benefit neat be a genetic algorithm that put every iteration of craig s brain to the test and then selectively breed they in a very similar way to the evolution of specie in nature the hierarchy be as follow synapse neuron build block of craig s brain genome an iteration of craig s brain essentially a collection of neuron and synapsis specie a collection of genome generation an iteration of the neat algorithm this be repeat over and over to evolve craig the first step every generation be to calculate the fitness of every individual genome from the previous generation this involve run the same function on each genome so that neat know how successful each one be for craig this mean run through a mario level use a particular genome or brain after run through the level we determine the fitness of the genome by this function once the fitness of every genome have be calculate we can move on to the next portion of the algorithm this part of the algorithm be probably the least intuitive the reason for this adjust fitness be to discourage specie from grow too big as the population in a species go up their adjust fitness go down force the genetic algorithm to diversify the proper implementation of this algorithm be relatively intensive so for craig s implementation we simplify it to the follow the important part here be that each genome now have an adjust fitness value associate with it here s where the natural selection part come in the survival of the fit portion be all about determine how many genome survive another generation as well as how many offspring will be bear in the species the algorithm use here aren t outline directly in the paper so most of these algorithm be create through trial and error the first step be to determine how many off a specie will die to make room for more baby this be do proportionally to a species adjust fitness the high the adjust fitness the more die off to make room for baby the second step be to determine how many child should be bear in the specie this be also proportional to the adjust fitness of the specie by the end of these two function the specie will have a certain number of genome leave as well as a baby quota — the difference between the number of genome and the populationsize this algorithm be necessary to allow for specie to be leave behind sometimes a specie will go down the completely wrong path and there s no point in keep they around this algorithm work in a very simple way if a species be in the bottom _ _ % of the entire generation it be mark for extinction if a species be mark for extinction _ _ time in a row then all genome in the specie be kill off now come the fun genetic part each specie should have a certain number of genome as well as a certain number of allotted spot for new offspring those spot now need to be populate each empty population spot need to be fill but can be fill through either asexual or sexual reproduction in other word offspring can result from either two genome in the specie be merge or from a mutation of a single genome in the specie before I discuss the process of merge two genome I ll first discuss mutation there be three kind of mutation that can happen to a genome in neat they be as follow this involve a re distribution of all synapse weight in a genome they can be either completely re distribute or simply perturb mean change slightly 2 mutate add synapse add a synapse mean find two previously unconnected node and connect they with a synapse this new synapse be give a random weight 3 mutate add node this be the trickiest of the mutation when add a node you need to split an already exist synapse into two synapsis and add a node in between they the weight of the original synapse be copy on to the second synapse while the first synapse be give a weight of 1 one important fact to note be that the first synapse bright red in the above picture be not actually delete but merely disabled this mean that it exist in the genome but it be mark as inactive synapsis add in either mutate add node or mutate add synapse be give a unique i d call a historical marking that be use in the crossover mate algorithm when two genome mate to produce an offspring there be an algorithm detailed in the neat paper that must be follow the intuition behind it be to match up common ancestor synapsis remember we ve be keep their historical marking s then take the mutation that don t match up and mix and match they to create the child once a child have be create in this way it undergo the mutation process outline above I win t go into too much detail on this algorithm but if you re curious about it you can find a more detailed explanation of it in section 3 2 of the original paper or you can see the code I use to implement it here once all the baby have be create in every specie we can finally progress to the final stage of the genetic algorithm respeciation essentially we first select a candidate genome from each specie this genome be now the representative for the specie all genome that be not select as candidate be put into a generic pool and re organize the re organization rely on an equation call the compatibility distance equation this equation determine how similar or different any two give genome be I win t go into the gritty detail of how the equation work as it be well explain in section 3 3 of the original paper as well as here in craig s code if a genome be too different from any of the candidate genome it be place in its own specie use this process all of the genome in the generic pool be re place into specie once this process have complete the generation be do and we be ready to re calculate the fitness of each of the genome while create craig mean get very little sleep at yhack it be well worth it for a couple reason first of all the neat algorithm be a very complex one learn how to implement a complex algorithm without lose myself in its complexity be an exercise in code cleanliness despite be press for time because of the hackathon it be also very interesting to create an algorithm that be mostly base off a paper as oppose to one that I have example code to work with often this mean carefully look into the wording use in the paper to determine whether I should be use a > or a > = for example one of the most difficult part of this project be that I be unable to test as I be program I essentially write all of the code blind and then be able to test and debug it once it have all be create this be for a couple reason partially because of the time constraint of a hackathon and partially because the algorithm as a whole have a lot of interlock part mean they need to be in a work state to be able to see if the algorithm work overall I m happy and proud by how joe and I be able to deal with the stress of create such a deep and complex project from scratch in a short 36 hour period not only do we enjoy ourselves and place well but we also manage to teach craig some cool skill like jump over the second pipe in level 1 from a quick cheer to a stand ovation clap to show how much you enjoy this story http savas ca — niko@savas can
Dr Ben Medlock,32,4,https://medium.com/@Ben_Medlock/why-turing-s-legacy-demands-a-smarter-keyboard-9e7324463306?source=tag_archive---------8----------------,why ture s legacy demand a smart keyboard dr ben medlock medium,why ture s legacy demand a smart keyboard when you start a company you dream of walk in the footstep of your hero for those work in artificial intelligence the british computer scientist and father of the field alan ture always come to mind I think of he when I do my phd when I co found an ai keyboard company in 2009 and when we paste his name on a meeting room door in our first real office as a british tech company today be a big day for swiftkey we ve introduce some of the principle originally conceive of by ture — artificial neural network — into our smartphone keyboard for the first time I want to explain how we manage to do it and how a technology like this something you may never have hear of before will help define the smartphone experience of the future this be my personal take ; for the official version check out the swiftkey blog frustration free typing on a smartphone rely on complex software to automatically fix typo and predict the word you might want to use swiftkey have be at the forefront of this area since 2009 and today our software be use across the world on more than half a billion handset soon after we launch the first version of our app in 2010 I start to think about use neural network to power smartphone type rather than the more traditional n gram approach a sophisticated form of word frequency count at the time it seem little more than theoretical as mobile hardware wasn t up to the task however three year later the situation begin to look more favorable and in late 2013 our team start work on the idea in earnest in order to build a neural network power swiftkey our engineer be task with the enormous challenge of come up with a solution that would run locally on a smartphone without any perceptible lag neural network language model be typically deploy on large server require huge computational resource get the tech to fit into a handheld mobile device would be no small feat after many month of trial error and lot of experimentation the team realize they might have find an answer with a combination of two approach the first be to make use of the graphical processing unit gpu on the phone utilize the powerful hardware acceleration design for render complex graphical image but thank to some clever programming they be also able to run the same code on the standard processing unit when the gpu wasn t available this combo turn out to be the win ticket so back to ture in 1948 he publish a little know essay call intelligent machinery in which he outline two form of compute he feel could ultimately lead to machine exhibit intelligent behavior the first be a variant of his highly influential universal ture machine destine to become the foundation for hardware design in all modern digital computer the second be an idea he call an unorganized machine a type of computer that would use a network of artificial neuron to accept input and translate they into predict output connect together many small computing unit each with the ability to receive modify and pass on basic signal be inspire by the structure of the human brain that s why the appropriation of this concept in software form be call an artificial neural network or a neural network for short the idea be that a collection of artificial neuron be connect together in a specific way call a topology such that a give set of input what you ve just type for example can be turn into a useful output e g your most likely next word the network be then train on million or even billion of datum sample and the behavior of the individual neuron be automatically tweak to achieve the desire overall result in the last few year neural network approach have facilitate great progress on tough problem such as image recognition and speech processing researcher have also begin to demonstrate advance in non traditional task such as automatically generate whole sentence description of image such technique will allow we to well manage the explosion of uncategorized visual datum on the web and will lead to smart search engine and aid for the visually impaired among a host of other application the fact that the human brain be so adept at work with language suggest that neural network inspire by the brain s internal structure be a good bet for the future of smartphone type in principle neural network also allow we to integrate powerful contextual cue to improve accuracy for instance a user s current location and the time of day these will be step stone to more efficient and personal device interaction — the keyboard of the future will provide an experience that feel less like typing and more like work with a close friend or personal assistant apply neural network to real world problem be part of a wide technology movement that s change the face of consumer electronic for good device be get smart more useful and more personal my goal be that swiftkey contribute to this revolution we should all be spend less time fix typo and more time say what we mean when it matter it s the legacy we owe to ture the photograph alan ture by joncallas be license under cc by 2 0 from a quick cheer to a stand ovation clap to show how much you enjoy this story technopreneur @swiftkey co founder
Nieves Ábalos,18,7,https://labs.beeva.com/sem%C3%A1ntica-desde-informaci%C3%B3n-desestructurada-90ce87736812?source=tag_archive---------9----------------,semántica desde información desestructurada beeva labs,detectar patrone es un núcleo importante en el mundo del procesamiento del lenguaje natural esta detección de patrone nos permite clasificar documentos lo que tiene muchas aplicacione análisis de sentimiento sentiment analysis recuperación de documentos document retrieval búsqueda web filtrado de spam etc esta clasificación se hace de manera automática de forma supervisada o no supervisada también conocida como cluster de documentos entre las técnicas más clásicas y utilizadas generalmente supervisadas encontramos clasificadore naive baye árboles de decisión id3 o c4 5 tf idf latent semantic indexing lsi y support vector machine svm algunas técnicas utilizadas para extraer características suelen inspirarse en cómo el ser humano es capaz de aprender de información simple y llegar a información más compleja se pueden diferenciar entre rede neuronale algunas topologías de rede neuronales se engloban dentro del concepto deep learn y técnicas que no usan estas rede para reconocer patrone en beeva nos hemos encontrado varias vece con un mismo problema ¿ cómo sabemos si do documentos son semejante y con semejante queremos decir que tratan de lo mismo esto entre otra cosas nos permitiría categorizar documentos dentro del mismo tema de manera automática así que a priori nos encontramos con do retos necesitamos representar los documentos de manera que los algoritmos que usemos los puedan entender normalmente estas representacione o modelos están basadas en matrices de características que posee cada documento para representar texto podemos usar técnicas de representación de manera local o de manera continua la representación local es aquella en la que solo tenemos en cuenta las palabras de forma aislada y se representa como un conjunto de términos índice o palabras clave n gramas bag of word este tipo de representación no tiene en cuenta la relación entre términos la representacióncontinua es aquella en la que sí se tiene en cuenta el contexto de las palabras y la relación entre ellas y se representan como matrice vectore conjuntos e incluso nodos lsa o lsi lds lda representacione distribuida o predictivas usando rede neuronales para nuestro primer reto extraer semántica vamos a probar una representación continua llamada representación distribuida de palabras distribute representation of word esta consiste en aprend representacione vectoriale de palabras es decir vamos a tener un espacio multidimensional en el que una palabra es representada como un vector una de las cosas interesantes de estos vectores es que son capace de extraer características tan relevante como propiedades sintácticas y semánticas de las palabras turian et al 2010 la otra es que este aprendizaje automático se realiza con datos de entrada no etiquetados es decir es no supervisado esto vectore pueden ser utilizados como entrada de muchas aplicaciones de procesamiento del lenguaje natural y machine learning de hecho es nuestro segundo reto utilizaremos estos vectores para intentar extraer temas de documentos para aplicar esta técnica usamos la herramienta word2vec mikolov et al — google 2013 que utiliza como entrada un corpusde texto o documentos cualquiera y obtener como salida vectores representando las palabras la arquitectura en la que se basa word2vec utiliza rede neuronales para aprend esta representacione aunque también se pueden obtener vectore que representen frase párrafos o incluso documentos completos le and mikolov 2014 primero utilizamos la implementación en python de la herramienta word2vec incluida en la librería gensim como entrada para generar los vectores tenemos do dataset con documentos en castellano wikipedia y yahoo answers de este dataset solo los que están en español el proceso es el siguiente figura 1 dado el conjunto de textos se construye un vocabulario y word2vec aprende las representacione vectoriales de palabras los algoritmos de aprendizaje que utiliza word2vec son bag of word continuo y skip gram continuo ambo algoritmos aprenden las representaciones de una palabra las cuales son útile para predecir otras palabras en la frase como sabemos que los vectores capturan muchas regularidade lingüísticas podemos aplicar operacione vectoriale para extraer mucha propiedades interesante por ejemplo si queremos saber qué palabras son las más similare a una dada buscamos cuale están más cerca aplicando distancia coseno cosine distance o similitud coseno cosine similarity por ejemplo con el modelo de wikipedia qué cinco palabras se parecen más a una dada también podemos obtener qué seis palabras se parecen más a do dadas con el modelo de la wikipedia y el de yahoo para ver las diferencias otra propiedad interesante es que las operaciones vectoriales vector rey — vector hombre + vector mujer nos da como resultado un vector muy cercano a vector reina por ejemplo vector pareja — vector hombre + vector novio nos da como resultado esto vectore al haber trabajado con do conjuntos de datos diferente wikipedia y yahoo answers podemos crear do espacios de representacione vectoriale ligeramente diferente con respecto al vocabulario usado y la semántica inherente en ellos en el de yahoo encontramos entre las palabras más similare la misma palabra mal escrita de diferentes manera en wikipedia esto no pasa pue la escritura es mucho más correcta además en el conjunto de yahoo tenemos no sólo preguntas en castellano sino que también encontramos otra en mejicano argentino y otros dialectos de sudamérica esto nos permite encontrar palabras similare en diferente dialectos con respecto al tiempo que tardamos en crear nuestro espacio de vectores la mayoría del tiempo se dedica al preprocesamiento y limpieza de esos documentos la implementación de gensim permite modificar los parámetros de creación del modelo e incluso utilizar vario worker con cython para que el entrenamiento sea más rápido la calidad de estos vectore dependerá de la cantidad de datos de entrenamiento del tamaño de los vectores y del algoritmo elegido para entrenar para obtener mejore resultado es necesario entrenar los modelos con dataset grande y con suficiente dimensionalidad para más detalle os recomendamos la lectura del trabajo de mikolov y le en la siguiente tabla os mostramos el tiempo que se tarda aproximadamente en entrenar unos 500 mb de datos suficientes para obtener un buen modelo de vectores el tiempo total es el tiempo que tardamos en preprocesar los datos entrenar y guardar el modelo para posteriores usos para usar a representacione vectoriale de documentos hemos utilizado doc2vec también de gensim como entrada de datos hemos considerado documento como una página de la wikipedia o una pregunta de yahoo con sus respuestas hemos variado el tamaño del fichero de entrada de 100 000 documento a 258 088 para un worker y una dimensión de 300 y el tiempo de entrenamiento se reduce bastante lo podemos ver en la siguiente tabla los tests ejecutado para ver el comportamiento del espacio de vectores no han sido tan satisfactorios como con word2vec los resultado para palabras similare son peore que con word2vec y para encontrar documento similare a uno dado vemos que no devuelve nada con mucho sentido como alternativa buscamos otros método que nos puedan decir qué documentos son parecido entre sí os los presentaremos en el siguiente post word2vec es considerado como un método inspirado en deep learning recomiendo la lectura de este artículo para aclarar concepto en cierto grupos de especialistas en la materia y no tanto deep learn sino shallow learn en otros grupos sea como sea la creación de espacio vectoriale para extraer propiedades sintácticas y semánticas de las palabras de manera automática y no supervisada nos abre todo un mundo de posibilidade a explorar esto vectores sirven de entrada para muchas aplicaciones como traducción automática clusterización categorización e incluso puede ser entrada de otros modelos basado en deep learning y es que además de aplicarse al lenguaje natural se está aplicando también en imágene y reconocimiento de voz ya que doc2vec no nos ha gustado mucho nuestro siguiente paso es aplicar esto espacio vectoriale a extraer temas y categorías de documentos con técnicas habituale en el mundo del procesamiento del lenguaje natural y de machine learning como tf idf de ello hablaremos en un siguiente post el corpus de datos de yahoo l6 — yahoo answer comprehensive question and answer version 1 0 multi part ha sido obtenido gracia a yahoo webscope para procesar estos datos hemos utilizado la librería gensim para python que implementa word2vec fuente imagen principal freedigitalphoto net kangshutter from a quick cheer to a stand ovation clap to show how much you enjoy this story conversational interface expert indie maker product manager & entrepreneur # voicefirst # chatbot # ai # nlproc create future concept at @monoceros_xyz innovative knowledge
Arthur Juliani,9K,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------0----------------,simple reinforcement learning with tensorflow part 0 q learning with table and neural network,for this tutorial in my reinforcement learning series we be go to be explore a family of rl algorithms call q learning algorithms these be a little different than the policy base algorithm that will be look at in the the follow tutorial part 1 3 instead of start with a complex and unwieldy deep neural network we will begin by implement a simple lookup table version of the algorithm and then show how to implement a neural network equivalent use tensorflow give that we be go back to basic it may be good to think of this as part 0 of the series it will hopefully give an intuition into what be really happen in q learning that we can then build on go forward when we eventually combine the policy gradient and q learning approach to build state of the art rl agent if you be more interested in policy network or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient method which attempt to learn function which directly map an observation to an action q learning attempt to learn the value of be in a give state and take a specific action there while both approach ultimately allow we to take intelligent action give a situation the mean of get to that action differ significantly you may have hear about deepq network which can play atari game these be really just large and more complex implementation of the q learning algorithm we be go to discuss here for this tutorial we be go to be attempt to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provide an easy way for people to experiment with their learn agent in an array of provide toy game the frozenlake environment consist of a 4x4 grid of block each one either be the start block the goal block a safe frozen block or a dangerous hole the objective be to have an agent learn to navigate from the start to the goal without move onto a hole at any give time the agent can choose to move either up down left or right the catch be that there be a wind which occasionally blow the agent onto a space they didn t choose as such perfect performance every time be impossible but learn to avoid the hole and reach the goal be certainly still doable the reward at every step be 0 except for enter the goal which provide a reward of 1 thus we will need an algorithm that learn long term expect reward this be exactly what q learning be design to provide in it s simple implementation q learning be a table of value for every state row and action column possible in the environment within each cell of the table we learn a value for how good it be to take a give action within a give state in the case of the frozenlake environment we have 16 possible state one for each block and 4 possible action the four direction of movement give we a 16x4 table of q value we start by initialize the table to be uniform all zero and then as we observe the reward we obtain for various action we update the table accordingly we make update to our q table use something call the bellman equation which state that the expect long term reward for a give action be equal to the immediate reward from the current action combine with the expect reward from the good future action take at the follow state in this way we reuse our own q table when estimate how to update our table for future action in equation form the rule look like this this say that the q value for a give state s and action a should represent the current reward r plus the maximum discount γ future reward expect accord to our own table for the next state s we would end up in the discount variable allow we to decide how important the possible future reward be compare to the present reward by update in this way the table slowly begin to obtain accurate measure of the expect future reward for a give action in a give state below be a python walkthrough of the q table algorithm implement in the frozenlake environment thank to praneet d for find the optimal hyperparameter for this approach now you may be think table be great but they don t really scale do they while it be easy to have a 16x4 table for a simple grid world the number of possible state in any modern game or real world environment be nearly infinitely large for most interesting problem table simply don t work we instead need some way to take a description of our state and produce q value for action without a table that be where neural network come in by act as a function approximator we can take any number of possible state that can be represent as a vector and learn to map they to q value in the case of the frozenlake example we will be use a one layer network which take the state encode in a one hot vector 1x16 and produce a vector of 4 q value one for each action such a simple network act kind of like a glorify table with the network weight serve as the old cell the key difference be that we can easily expand the tensorflow network with add layer activation function and different input type whereas all that be impossible with a regular table the method of update be a little different as well instead of directly update our table with a network we will be use backpropagation and a loss function our loss function will be sum of square loss where the difference between the current predict q value and the target value be compute and the gradient pass through the network in this case our q target for the choose action be the equivalent to the q value compute in equation 1 above below be the tensorflow walkthrough of implement our simple q network while the network learn to solve the frozenlake problem it turn out it doesn t do so quite as efficiently as the q table while neural network allow for great flexibility they do so at the cost of stability when it come to q learning there be a number of possible extension to our simple q network which allow for great performance and more robust learn two trick in particular be refer to as experience replay and freeze target network those improvement and other tweak be the key to get atari play deep q network and we will be explore those addition in the future for more info on the theory behind q learning see this great post by tambet matiisen I hope this tutorial have be helpful for those curious about how to implement simple q learning algorithms if this post have be valuable to you please consider donate to help support future tutorial article and implementation any contribution be greatly appreciate if you d like to follow my work on deep learning ai and cognitive science follow I on medium @arthur juliani or on twitter @awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @unity3d & cognitive neuroscience phd student explore frontier technology through the lens of artificial intelligence datum science and the shape of thing to come
Andrej Karpathy,9.2K,7,https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=tag_archive---------1----------------,yes you should understand backprop andrej karpathy medium,when we offer cs231n deep learning class at stanford we intentionally design the programming assignment to include explicit calculation involve in backpropagation on the low level the student have to implement the forward and the backward pass of each layer in raw numpy inevitably some student complain on the class message board this be seemingly a perfectly sensible appeal if you re never go to write backward pass once the class be over why practice write they be we just torture the student for our own amusement some easy answer could make argument along the line of it s worth know what s under the hood as an intellectual curiosity or perhaps you might want to improve on the core algorithm later but there be a much strong and practical argument which I want to devote a whole post to > the problem with backpropagation be that it be a leaky abstraction in other word it be easy to fall into the trap of abstract away the learning process — believe that you can simply stack arbitrary layer together and backprop will magically make they work on your datum so let look at a few explicit example where this be not the case in quite unintuitive way we re start off easy here at one point it be fashionable to use sigmoid or tanh non linearity in the fully connect layer the tricky part people might not realize until they think about the backward pass be that if you be sloppy with the weight initialization or datum preprocesse these non linearity can saturate and entirely stop learn — your training loss will be flat and refuse to go down for example a fully connect layer with sigmoid non linearity compute use raw numpy if your weight matrix w be initialize too large the output of the matrix multiply could have a very large range e g number between 400 and 400 which will make all output in the vector z almost binary either 1 or 0 but if that be the case z * 1 z which be local gradient of the sigmoid non linearity will in both case become zero vanish make the gradient for both x and w be zero the rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule another non obvious fun fact about sigmoid be that its local gradient z * 1 z achieve a maximum at 0 25 when z = 0 5 that mean that every time the gradient signal flow through a sigmoid gate its magnitude always diminish by one quarter or more if you re use basic sgd this would make the low layer of a network train much slow than the high one tldr if you re use sigmoid or tanh non linearity in your network and you understand backpropagation you should always be nervous about make sure that the initialization doesn t cause they to be fully saturate see a long explanation in this cs231n lecture video another fun non linearity be the relu which threshold neuron at zero from below the forward and backward pass for a fully connect layer that use relu would at the core include if you stare at this for a while you ll see that if a neuron get clamp to zero in the forward pass I e z=0 it doesn t fire then its weight will get zero gradient this can lead to what be call the dead relu problem where if a relu neuron be unfortunately initialize such that it never fire or if a neuron s weight ever get knock off with a large update during training into this regime then this neuron will remain permanently dead it s like permanent irrecoverable brain damage sometimes you can forward the entire training set through a train network and find that a large fraction e g 40 % of your neuron be zero the entire time tldr if you understand backpropagation and your network have relus you re always nervous about dead relus these be neuron that never turn on for any example in your entire training set and will remain permanently dead neuron can also die during training usually as a symptom of aggressive learning rate see a long explanation in cs231n lecture video vanilla rnn feature another good example of unintuitive effect of backpropagation I ll copy paste a slide from cs231n that have a simplified rnn that do not take any input x and only compute the recurrence on the hidden state equivalently the input x could always be zero this rnn be unrolled for t time step when you stare at what the backward pass be do you ll see that the gradient signal go backwards in time through all the hidden state be always be multiply by the same matrix the recurrence matrix whh intersperse with non linearity backprop what happen when you take one number a and start multiply it by some other number b I e a*b*b*b*b*b*b this sequence either go to zero if |b| < 1 or explode to infinity when |b|>1 the same thing happen in the backward pass of an rnn except b be a matrix and not just a number so we have to reason about its large eigenvalue instead tldr if you understand backpropagation and you re use rnn you be nervous about have to do gradient clipping or you prefer to use an lstm see a long explanation in this cs231n lecture video let look at one more — the one that actually inspire this post yesterday I be browse for a deep q learning implementation in tensorflow to see how other deal with compute the numpy equivalent of q a where a be an integer vector — turn out this trivial operation be not support in tf anyway I search dqn tensorflow click the first link and find the core code here be an excerpt if you re familiar with dqn you can see that there be the target_q_t which be just reward * gamma argmax_a q s a and then there be q_acte which be q s a of the action that be take the author here subtract the two into variable delta which they then want to minimize on line 295 with the l2 loss with tf reduce_mean tf square so far so good the problem be on line 291 the author be try to be robust to outlier so if the delta be too large they clip it with tf clip_by_value this be well intentione and look sensible from the perspective of the forward pass but it introduce a major bug if you think about the backward pass the clip_by_value function have a local gradient of zero outside of the range min_delta to max_delta so whenever the delta be above min max_delta the gradient become exactly zero during backprop the author be clip the raw q delta when they be likely try to clip the gradient for add robustness in that case the correct thing to do be to use the huber loss in place of tf square it s a bit gross in tensorflow because all we want to do be clip the gradient if it be above a threshold but since we can t meddle with the gradient directly we have to do it in this round about way of define the huber loss in torch this would be much more simple I submit an issue on the dqn repo and this be promptly fix backpropagation be a leaky abstraction ; it be a credit assignment scheme with non trivial consequence if you try to ignore how it work under the hood because tensorflow automagically make my network learn you will not be ready to wrestle with the danger it present and you will be much less effective at building and debug neural network the good news be that backpropagation be not that difficult to understand if present properly I have relatively strong feeling on this topic because it seem to I that 95 % of backpropagation material out there present it all wrong filling page with mechanical math instead I would recommend the cs231n lecture on backprop which emphasize intuition yay for shameless self advertising and if you can spare the time as a bonus work through the cs231n assignment which get you to write backprop manually and help you solidify your understanding that s it for now I hope you ll be much more suspicious of backpropagation go forward and think carefully through what the backward pass be do also I m aware that this post have unintentionally turn into several cs231n ad apology for that from a quick cheer to a stand ovation clap to show how much you enjoy this story director of ai at tesla previously research scientist at openai and phd student at stanford I like to train deep neural net on large dataset
Arthur Juliani,3.5K,8,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=tag_archive---------2----------------,simple reinforcement learning with tensorflow part 8 asynchronous actor critic agent a3c,in this article I want to provide a tutorial on implement the asynchronous advantage actor critic a3c algorithm in tensorflow we will use it to solve a simple challenge in a 3d doom environment with the holiday right around the corner this will be my final post for the year and I hope it will serve as a culmination of all the previous topic in the series if you haven t yet or be new to deep learning and reinforcement learning I suggest check out the early entry in the series before go through this post in order to understand all the building block which will be utilize here if you have be follow the series thank you I have learn so much about rl in the past year and be happy to have share it with everyone through this article series so what be a3c the a3c algorithm be release by google s deepmind group early this year and it make a splash by essentially obsolete dqn it be fast simple more robust and able to achieve much well score on the standard battery of deep rl task on top of all that it could work in continuous as well as discrete action space give this it have become the go to deep rl algorithm for new challenging problem with complex state and action space in fact openai just release a version of a3c as their universal starter agent for work with their new and very diverse set of universe environment asynchronous advantage actor critic be quite a mouthful let s start by unpack the name and from there begin to unpack the mechanic of the algorithm itself asynchronous unlike dqn where a single agent represent by a single neural network interact with a single environment a3c utilize multiple incarnation of the above in order to learn more efficiently in a3c there be a global network and multiple worker agent which each have their own set of network parameter each of these agent interact with it s own copy of the environment at the same time as the other agent be interact with their environment the reason this work well than have a single agent beyond the speedup of get more work do be that the experience of each agent be independent of the experience of the other in this way the overall experience available for training become more diverse actor critic so far this series have focus on value iteration method such as q learning or policy iteration method such as policy gradient actor critic combine the benefit of both approach in the case of a3c our network will estimate both a value function v s how good a certain state be to be in and a policy π s a set of action probability output these will each be separate fully connect layer sit at the top of the network critically the agent use the value estimate the critic to update the policy the actor more intelligently than traditional policy gradient method advantage if we think back to our implementation of policy gradient the update rule use the discount return from a set of experience in order to tell the agent which of its action be good and which be bad the network be then update in order to encourage and discourage action appropriately the insight of use advantage estimate rather than just discount return be to allow the agent to determine not just how good its action be but how much well they turn out to be than expect intuitively this allow the algorithm to focus on where the network s prediction be lack if you recall from the duel q network architecture the advantage function be as follow since we win t be determine the q value directly in a3c we can use the discount return r as an estimate of q s a to allow we to generate an estimate of the advantage in this tutorial we will go even far and utilize a slightly different version of advantage estimation with low variance refer to as generalize advantage estimation in the process of build this implementation of the a3c algorithm I use as reference the quality implementation by dennybritz and openai both of which I highly recommend if you d like to see alternative to my code here each section embed here be take out of context for instructional purpose and win t run on its own to view and run the full functional a3c implementation see my github repository the general outline of the code architecture be the a3c algorithm begin by construct the global network this network will consist of convolutional layer to process spatial dependency follow by an lstm layer to process temporal dependency and finally value and policy output layer below be example code for establish the network graph itself next a set of worker agent each with their own network and environment be create each of these worker be run on a separate processor thread so there should be no more worker than there be thread on your cpu ~ from here we go asynchronous ~ each worker begin by set its network parameter to those of the global network we can do this by construct a tensorflow op which set each variable in the local worker network to the equivalent variable value in the global network each worker then interact with its own copy of the environment and collect experience each keep a list of experience tuple observation action reward do value that be constantly add to from interaction with the environment once the worker s experience history be large enough we use it to determine discount return and advantage and use those to calculate value and policy loss we also calculate an entropy h of the policy this correspond to the spread of action probability if the policy output action with relatively similar probability then entropy will be high but if the policy suggest a single action with a large probability then entropy will be low we use the entropy as a means of improve exploration by encourage the model to be conservative regard its sureness of the correct action a worker then use these loss to obtain gradient with respect to its network parameter each of these gradient be typically clip in order to prevent overly large parameter update which can destabilize the policy a worker then use the gradient to update the global network parameter in this way the global network be constantly be update by each of the agent as they interact with their environment once a successful update be make to the global network the whole process repeat the worker then reset its own network parameter to those of the global network and the process begin again to view the full and functional code see the github repository here the robustness of a3c allow we to tackle a new generation of reinforcement learning challenge one of which be 3d environment we have come a long way from multi armed bandit and grid world and in this tutorial I have set up the code to allow for play through the first vizdoom challenge vizdoom be a system to allow for rl research use the classic doom game engine the maintainer of vizdoom recently create a pip package so instal it be as simple as pip install vizdoom once it be instal we will be use the basic wad environment which be provide in the github repository and need to be place in the work directory the challenge consist of control an avatar from a first person perspective in a single square room there be a single enemy on the opposite side of the room which appear in a random location each episode the agent can only move to the left or right and fire a gun the goal be to shoot the enemy as quickly as possible use as few bullet as possible the agent have 300 time step per episode to shoot the enemy shoot the enemy yield a reward of 1 and each time step as well as each shot yield a small penalty after about 500 episode per worker agent the network learn a policy to quickly solve the challenge feel free to adjust parameter such as learn rate clip magnitude update frequency etc to attempt to achieve ever great performance or utilize a3c in your own rl task I hope this tutorial have be helpful to those new to a3c and asynchronous reinforcement learning now go forth and build ais there be a lot of move part in a3c so if you discover a bug or find a well way to do something please don t hesitate to bring it up here or in the github I be more than happy to incorporate change and feedback to improve the algorithm if you d like to follow my writing on deep learning ai and cognitive science follow I on medium @arthur juliani or on twitter @awjuliani if this post have be valuable to you please consider donate to help support future tutorial article and implementation any contribution be greatly appreciate more from my simple reinforcement learning with tensorflow series from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning @unity3d & cognitive neuroscience phd student explore frontier technology through the lens of artificial intelligence datum science and the shape of thing to come
Rohan Kapur,1K,30,https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d?source=tag_archive---------3----------------,rohan & lenny # 1 neural network & the backpropagation algorithm explain,in rohan s last post he talk about evaluate and plug hole in his knowledge of machine learning thus far the backpropagation algorithm — the process of train a neural network — be a glare one for both of we in particular together we embark on master backprop through some great online lecture from professor at mit & stanford after attempt a few programming implementation and hand solution we feel equip to write an article for ayoai — together today we ll do our good to explain backpropagation and neural network from the beginning if you have an elementary understanding of differential calculus and perhaps an intuition of what machine learning be we hope you come out of this blog post with an acute but existent nonetheless understanding of neural network and how to train they let we know if we succeed let s start off with a quick introduction to the concept of neural network fundamentally neural network be nothing more than really good function approximator — you give a train network an input vector it perform a series of operation and it produce an output vector to train our network to estimate an unknown function we give it a collection of data point — which we denote the training set — that the network will learn from and generalize on to make future inference neural network be structure as a series of layer each compose of one or more neuron as depict above each neuron produce an output or activation base on the output of the previous layer and a set of weight when use a neural network to approximate a function the datum be forward through the network layer by layer until it reach the final layer the final layer s activation be the prediction that the network actually make all this probably seem kind of magical but it actually work the key be find the right set of weight for all of the connection to make the right decision this happen in a process know as training — and that s what most of this post be go to be about when we re train the network it s often convenient to have some metric of how good or bad we re do ; we call this metric the cost function generally speak the cost function look at the function the network have infer and use it to estimate value for the data point in our training set the discrepancy between the output in the estimation and the training set datum point be the principle value for our cost function when train our network the goal will be to get the value of this cost function as low as possible we ll see how to do that in just a bit but for now just focus on the intuition of what a cost function be and what it s good for generally speak the cost function should be more or less convex like so in reality it s impossible for any network or cost function to be truly convex however as we ll soon see local minima may not be a big deal as long as there be still a general trend for we to follow to get to the bottom also notice that the cost function be parameterize by our network s weight — we control our loss function by change the weight one last thing to keep in mind about the loss function be that it doesn t just have to capture how correctly your network estimate — it can specify any objective that need to be optimize for example you generally want to penalize large weight as they could lead to overfitte if this be the case simply add a regularization term to your cost function that express how big your weight will mean that in the process of train your network it will look for a solution that have the good estimate possible while prevent overfitting now let s take a look at how we can actually minimize the cost function during the training process to find a set of weight that work the good for our objective now that we ve develop a metric for score our network which we ll denote as j w we need to find the weight that will make that score as low as possible if you think back to your pre calculus day your first instinct might be to set the derivative of the cost function to zero and solve which would give we the location of every minimum maximum in the function unfortunately there be a few problem with this approach especially as the size of network begin to scale up solve for the weight directly become increasingly infeasible instead we look at a different class of algorithms call iterative optimization algorithm that progressively work their way towards the optimal solution the most basic of these algorithm be gradient descent recall that our cost function will be essentially convex and we want to get as close as possible to the global minimum instead of solve for it analytically gradient descent follow the derivative to essentially roll down the slope until it find its way to the center let s take the example of a single weight neural network whose cost function be depict below we start off by initialize our weight randomly which put we at the red dot on the diagram above take the derivative we see the slope at this point be a pretty big positive number we want to move close to the center — so naturally we should take a pretty big step in the opposite direction of the slope if we repeat the process enough we soon find ourselves nearly at the bottom of our curve and much close to the optimal weight configuration for our network more formally gradient descent look something like this let s dissect every time we want to update our weight we subtract the derivative of the cost function w r t the weight itself scale by a learning rate and — that s it you ll see that as it get close and close to the center the derivative term get small and small converging to zero as it approach the solution the same process apply with network that have ten hundred thousand or more parameter — compute the gradient of the cost function w r t each of the weight and update each of your weight accordingly I do want to say a few more word on the learning rate because it s one of the more important hyperparameter setting for your neural network that you have control over if the learning rate be too high it could jump too far in the other direction and you never get to the minimum you re search for set it too low and your network will take age to find the right weight or it will get stick in a local minimum there s no magic number to use when it come to a learning rate and it s usually good to try several and pick the one that work the good for your individual network and dataset in practice many choose to anneal the learning rate over time — it start out high because it s furth from the solution and decay as it get close but as it turn out gradient descent be kind of slow really slow actually early I use the analogy of the weight roll down the gradient to get to the bottom but that doesn t actually make any sense — it should pick up speed as it get to the bottom not slow down another iterative optimization algorithm know as momentum do just that as the weight begin to roll down the slope they pick up speed when they get close to the solution the momentum that they pick up carry they close to the optima while gradient descent would simply stop as a result training with momentum update be both fast and can provide well result here s what the update rule look like for momentum as we train we accumulate a velocity value v at each training step we update v with the gradient at the current position once again scale by the learning rate also notice that with each time step we decay velocity v by a factor mu usually somewhere around 9 so that over time we lose momentum instead of bounce around by the minimum forever we then update our weight in the direction of the velocity and repeat the process again over the first few training iteration v will grow as our weight pick up speed and take successively big leap as we approach the minimum our velocity stop accumulate as quickly and eventually begin to decay until we ve essentially reach the minimum an important thing to note be that we accumulate a velocity independently for each weight — just because one weight be change particularly clearly doesn t mean any of the other weight need to be there be lot of other iterative optimization algorithm that be commonly use with neural network but I win t go into all of they here if you re curious some of the more popular one include adagrad and adam the basic principle remain the same throughout — gradually update the weight to get they close to the minimum but regardless of which optimization algorithm you use we still need to be able to compute the gradient of the cost function w r t each weight but our cost function isn t a simple parabola anymore — it s a complicated many dimensional function with countless local optima that we need to watch out for that s where backpropagation come in the backpropagation algorithm be a major milestone in machine learning because before it be discover optimization method be extremely unsatisfactory one popular method be to perturb adjust the weight in a random uninformed direction ie increase or decrease and see if the performance of the ann increase if it do not one would attempt to either a go in the other direction b reduce the perturbation size or c a combination of both another attempt be to use genetic algorithm which become popular in ai at the same time to evolve a high performance neural network in both case without analytically be inform on the correct direction result and efficiency be suboptimal this be where the backpropagation algorithm come into play recall that for any give supervised machine learning problem we aim to select weight that provide the optimal estimation of a function that model our training datum in other word we want to find a set of weight w that minimize on the output of j w we discuss the gradient descent algorithm — one where we update each weight by some negative scalar reduction of the error derivative with respect to that weight if we do choose to use gradient descent or almost any other convex optimization algorithm we need to find say derivative in numerical form for other machine learning algorithm like logistic regression or linear regression compute the derivative be an elementary application of differentiation this be because the output of these model be just the input multiply by some choose weight and at most feed through a single activation function the sigmoid function in logistic regression the same however can not be say for neural network to demonstrate this here be a diagram of a double layered neural network as you can see each neuron be a function of the previous one connect to it in other word if one be to change the value of w1 both hide 1 and hide 2 and ultimately the output neuron would change because of this notion of functional dependency we can mathematically formulate the output as an extensive composite function and thus here the output be a composite function of the weight input and activation function s it be important to realize that the hide unit node be simply intermediary computation that in actuality can be reduce down to computation of the input layer if we be to then take the derivative of say function with respect to some arbitrary weight for example w1 we would iteratively apply the chain rule which I m sure you all remember from your calculus class the result would look similar to the following now let s attach a black box to the tail of our neural network this black box will compute and return the error — use the cost function — from our output all we ve do be add another functional dependency ; our error be now a function of the output and hence a function of the input weight and activation function if we be to compute the derivative of the error with any arbitrary weight again we ll choose w1 the result would be each of these derivative can be simplify once we choose an activation and error function such that the entire result would represent a numerical value at that point any abstraction have be remove and the error derivative can be use in gradient descent as discuss early to iteratively improve upon the weight we compute the error derivative w r t every other weight in the network and apply gradient descent in the same way this be backpropagation — simply the computation of derivative that be feed to a convex optimization algorithm we call it backpropagation because it almost seem as if we be traverse from the output error to the weight take iterative step use chain the rule until we reach our weight when I first truly understand the backprop algorithm just a couple of week ago I be take aback by how simple it be sure the actual arithmetic computation can be difficult but this process be handle by our computer in reality backpropagation be just a rather tedious but again for a generalize implementation computer will handle this application of the chain rule since neural network be convolute multilayer machine learning model structure at least relative to other one each weight contribute to the overall error in a more complex manner and hence the actual derivative require a lot of effort to produce however once we get past the calculus backpropagation of neural net be equivalent to typical gradient descent for logistic linear regression thus far I ve walk through a very abstract form of backprop for a simple neural network however it be unlikely that you will ever use a single layered ann in application so now let s make our black box — the activation and error function — more concrete such that we can perform backprop on a multilayer neural net recall that our error function j w will compute the error of our neural network base on the output prediction it produce vs the correct a priori output we know in our training set more formally if we denote our predict output estimation as vector p and our actual output as vector a then we can use this be just one example of a possible cost function the log likelihood be also a popular one and we use it because of its mathematical convenience this be a notion one will frequently encounter in machine learn the squared expression exaggerate poor solution and ensure each discrepancy be positive it will soon become clear why we multiply the expression by half the derivative of the error w r t the output be the first term in the error w r t weight derivative expression we formulate early let s now compute it our result be simply our prediction take away our actual output now let s move on to the activation function the activation function use depend on the context of the neural network if we aren t in a classification context relu rectify linear unit which be zero if input be negative and the identity function when the input be positive be commonly use today if we re in a classification context that be predict on a discrete state with a probability ie if an email be spam we can use the sigmoid or tanh hyperbolic tangent function such that we can squeeze any value into the range 0 to 1 these be use instead of a typical step function because their smoothness property allow for the derivative to be non zero the derivative of the step function before and after the origin be zero this will pose issue when we try to update our weight nothing much will happen now let s say we re in a classification context and we choose to use the sigmoid function which be of the follow equation as per usual we ll compute the derivative use differentiation rule as edit on the 2nd line the denominator should be raise to +2 not 2 thank to a reader for point this out sidenote relu activation function be also commonly use in classification context there be downside to use the sigmoid function — particularly the vanish gradient problem — which you can read more about here the sigmoid function be mathematically convenient there it be again because we can represent its derivative in term of the output of the function isn t that cool‽ we be now in a good place to perform backpropagation on a multilayer neural network let I introduce you to the net we be go to work with this net be still not as complex as one you may use in your programming but its architecture allow we to nevertheless get a good grasp on backprop in this net we have 3 input neuron and one output neuron there be four layer in total one input one output and two hidden layer there be 3 neuron in each hide layer too which by the way need not be the case the network be fully connect ; there be no miss connection each neuron node save the input which be usually pre process anyways be an activity ; it be the weighted sum of the previous neuron activity apply to the sigmoid activation function to perform backprop by hand we need to introduce the different variable state at each point layer wise in the neural network it be important to note that every variable you see here be a generalization on the entire layer at that point for example when I say x_i I be refer to the input to any input neuron arbitrary value of I I choose to place it in the middle of the layer for visibility purpose but that do not mean that x_i refer to the middle neuron I ll demonstrate and discuss the implication of this later on x refer to the input layer y refer to hide layer 1 z refer to hide layer 2 and p refer to the prediction output layer which fit in nicely with the notation use in our cost function if a variable have the subscript I it mean that the variable be the input to the relevant neuron at that layer if a variable have the subscript j it mean that the variable be the output of the relevant neuron at that layer for example x_i refer to any input value we enter into the network x_j be actually equal to x_i but this be only because we choose not to use an activation function — or rather we use the identity activation function — in the input layer s activity we only include these two separate variable to retain consistency y_i be the input to any neuron in the first hidden layer ; it be the weighted sum of all previous neuron each neuron in the input layer multiply by the corresponding connect weight y_j be the output of any neuron at the hide layer so it be equal to activation_function y_i = sigmoid y_i = sigmoid weighted_sum_of_x_j we can apply the same logic for z and p ultimately p_j be the sigmoid output of p_i and hence be the output of the entire neural network that we pass to the error cost function the weight be organize into three separate variable w1 w2 and w3 each w be a matrix if you be not comfortable with linear algebra think of a 2d array of all the weight at the give layer for example w1 be the weight that connect the input layer to the hidden layer 1 wlayer_ij refer to any arbitrary single weight at a give layer to get an intuition of ij which be really I j wlayer_i be all the weight that connect arbitrary neuron I at a give layer to the next layer wlayer_ij add the j component be the weight that connect arbitrary neuron I at a give layer to an arbitrary neuron j at the next layer essentially wlayer be a vector of wlayer_is which be a vector of real value wlayer_ijs note please note that the i s and j s in the weight and other variable be completely different these index do not correspond in any way in fact for x y z p i and j do not represent tensor index at all they simply represent the input and output of a neuron wlayer_ij represent an arbitrary weight at an index in a weight matrix and x_j y_j z_j p_j represent an arbitrary input output point of a neuron unit that last part about weight be tedious it s crucial to understand how we re separate the neural network here especially the notion of generalize on an entire layer before move forward to acquire a comprehensive intuition of backpropagation we re go to backprop this neural net as discuss before more specifically we re go to find the derivative of the error w r t an arbitrary weight in the input layer w1_ij we could find the derivative of the error w r t an arbitrary weight in the first or second hide layer but let s go as far back as we can ; the more backprop the well so mathematically we be try to obtain to perform our iterative optimization algorithm with we can express this graphically visually use the same principle as early chain rule like so in two layer we have three red line point in three different direction instead of just one this be a reinforcement of and why it be important to understand the fact that variable j be just a generalization represent any point in the layer so when we differentiate p_i with respect to the layer before that there be three different weight as I hope you can see in w3_ij that contribute to the value p_i there also happen to be three weight in w3 in total but this isn t the case for the layer before ; it be only the case because layer p have one neuron — the output — in it we stop backprop at the input layer and so we just point to the single weight we be look for wonderful now let s work out all this great stuff mathematically immediately we know we have already establish the left hand side so now we just need to use the chain rule to simplify it far the derivative of the error w r t the weight can be write as the derivative of the error w r t the output prediction multiply by the derivative of the output prediction w r t the weight at this point we ve traverse one red line back we know this because be reducible to a numerical value specifically the derivative of the error w r t the output prediction be hence go one more layer backwards we can determine that in other word the derivative of the output prediction w r t the weight be the derivative of the output w r t the input to the output layer p_i multiply by the derivative of that value w r t the weight this represent our second red line we can solve the first term like so this correspond with the derivative of the sigmoid function we solve early which be equal to the output multiply by one minus the output in this case p_j be the output of the sigmoid function now we have let s move on to the third red line s this one be interesting because we begin to spread out since there be multiple different weight that contribute to the value of p_i we need to take into account their individual pull factor into our derivative if you re a mathematician this notation may irk you slightly ; sorry if that s the case in computer science we tend to stray from the notion of completely legal mathematical expression this be yet again again another reason why it s key to understand the role of layer generalization ; z_j here be not just refer to the middle neuron it s refer to an arbitrary neuron the actual value of j in the summation be not change it s not even an index or a value in the first place and we don t really consider it it s less of a mathematical expression and more of a statement that we will iterate through each generalize neuron z_j and use it in other word we iterate over the derivative term and sum they up use z_1 z_2 and z_3 before we could write p_j as any single value because the output layer just contain one node ; there be just one p_j but we see here that this be no long the case we have multiple z_j value and p_i be functionally dependent on each of these z_j value so when we traverse from p_j to the precede layer we need to consider each contribution from layer z to p_j separately and add they up to create one total contribution there s no upper bind to the summation ; we just assume that we start at zero and end at our maximum value for the number of neuron in the layer please again note that the same change be not reflect in w1_ij where j refer to an entirely different thing instead we re just state that we will use the different z_j neuron in layer z since p_i be a summation of each weight multiply by each z_j weight sum if we be to take the derivative of p_i with any arbitrary z_j the result would be the connect weight since say weight would be the coefficient of the term derivative of m*x w r t x be just m w3_ij be loosely define here ij still refer to any arbitrary weight — where ij be still separate from the j use in p_i z_j — but again as computer scientist and not mathematician we need not be pedantic about the legality and intricacy of expression ; we just need an intuition of what the expression imply mean it s almost a succinct form of psuedo code so even though this define an arbitrary weight we know it mean the connect weight we can also see this from the diagram when we walk from p_j to an arbitrary z_j we walk along the connect weight so now we have at this point I like to continue play the reduction test the reduction test state that if we can far simplify a derivative term we still have more backprop to do since we can t yet quite put the derivative of z_j w r t w1_ij into a numerical term let s keep go and fast forward a bit use chain rule we follow the fourth line back to determine that since z_j be the sigmoid of z_i we use the same logic as the previous layer and apply the sigmoid derivative the derivative of z_i w r t w1_ij demonstrate by the fifth line s back require the same idea of spread out and summation of contribution briefly since z_i be the weighted sum of each y_j in y we sum over the derivative which similar to before simplifie to the relevant connect weight in the precede layer w2 in this case we re almost there let s go far ; there s still more reduction to do we have of course another sigmoid activation function to deal with this be the sixth red line notice now that we have just one line remain in fact our last derivative term here pass or rather fail the reduction test the last line traverse from the input at y_i to x_j walk along w1_ij wait a second — be this not what we be attempt to backprop to yes it be since we be for the first time directly derive y_i w r t the weight w1_ij we can think of the coefficient of w1_ij as be x_j in our weighted sum instead of the vice versa as use previously hence the simplification follow of course since each x_j in layer x contribute to the weighted sum y_i we sum over the effect and that s it we can t reduce any far from here now let s tie all these individual expression together edit the denominator on the left hand side should say dw1ij instead of layer with no more partial derivative term leave our work be complete this give we the derivative of the error w r t any arbitrary weight in the input layer w1 that be a lot of work — maybe now we can sympathize with the poor computer something you should notice be that value such as p_j a z_j y_j x_j etc be the value of the network at the different point but where do they come from actually we would need to perform a feed forward of the neural network first and then capture these variable our task be to now perform gradient descent to train the neural net we perform gradient descent on each weight in each layer notice that the result gradient should change each time because the weight itself change and as a result the performance and output of the entire net should change even if it s a small perturbation this mean that at each update we need to do a feed forward of the neural net not just once before but once each iteration these be then the step to train an entire neural network it s important to note that one must not initialize the weight to zero similar to what may be do in other machine learning algorithms if weight be initialize to zero after each update the outgoing weight of each neuron will be identical because the gradient will be identical which can be prove because of this the proceed hide unit will remain the same value and will continue to follow each other ultimately this mean that our training will become extremely constrained due to the symmetry and we win t be able to build interesting function also neural network may get stick at local optima place where the gradient be zero but be not the global minima so random weight initialization allow one to hopefully have a chance of circumvent that by start at many different random value 3 perform one feed forward use the training data 4 perform backpropagation to get the error derivative w r t each and every weight in the neural network 5 perform gradient descent to update each weight by the negative scalar reduction w r t some learning rate alpha of the respective error derivative increment the number of iteration 6 if we have converge in reality though we just stop when we have reach the number of maximum iteration training be complete else repeat start at step 3 if we initialize our weight randomly and not to zero and then perform gradient descent with derivative compute from backpropagation we should expect to train a neural network in no time I hope this example bring clarity to how backprop work and the intuition behind it if you didn t understand the intricacy of the example but understand and appreciate the concept of backprop as a whole you re still in a great place next we ll go ahead and explain backprop code that work on any generalize architecture of a neural network use the relu activation function now that we ve develop the math and intuition behind backpropagation let s try to implement it we ll divide our implementation into three distinct step let s start off by define what the api we re implement look like we ll define our network as a series of layer instance that our data pass through — this mean that instead of model each individual neuron we group neuron from a single layer together this make it a bit easy to reason about large network but also make the actual computation fast as we ll see shortly also — we re go to write the code in python each layer will have the follow api this isn t great api design — ideally we would decouple the backprop and weight update from the rest of the object so the specific algorithm we use for update weight isn t tie to the layer itself but that s not the point so we ll stick with this design for the purpose of explain how backpropagation work in a real life scenario also we ll be use numpy throughout the implementation it s an awesome tool for mathematical operation in python especially tensor base one but we don t have the time to get into how it work — if you want a good introduction here ya go we can start by implement the weight initialization as it turn out how you initialize your weight be actually kind of a big deal for both network performance and convergence rate here s how we ll initialize our weight this initialize a weight matrix of the appropriate dimension with random value sample from a normal distribution we then scale it rad 2 self size_in give we a variance of 2 self size_in derivation here and that s all we need for layer initialization let s move on to implement our first objective — feed forward this be actually pretty simple — a dot product of our input activation with the weight matrix follow by our activation function will give we the activation we need the dot product part should make intuitive sense ; if it doesn t you should sit down and try to work through it on a piece of paper this be where the performance gain of group neuron into layer come from instead of keep an individual weight vector for each neuron and perform a series of vector dot product we can just do a single matrix operation which thank to the wonder of modern processor be significantly fast in fact we can compute all of the activation from a layer in just two line simple enough let s move on to backpropagation this one s a bit more involved first we compute the derivative of the output w r t the weight then the derivative of the cost w r t the output follow by chain rule to get the derivative of the cost w r t the weight let s start with the first part — the derivative of the output w r t the weight that should be simple enough ; because you re multiply the weight by the corresponding input activation the derivative will just be the corresponding input activation except because we re use the relu activation function the weight have no effect if the corresponding output be < 0 because it get cap anyway this should take care of that hiccup more formally you re multiply by the derivative of the activation function which be 0 when the activation be < 0 and 1 elsewhere let s take a brief detour to talk about the out_grad parameter that our backward method get let s say we have a network with two layer the first have m neuron and the second have n each of the m neuron produce an activation and each of the n neuron look at each of the m activation the out_grad parameter be an m x n matrix of how each m affect each of the n neuron it feed into now we need the derivative of the cost w r t each of the output — which be essentially the out_grad parameter we re give we just need to sum up each row of the matrix we re give as per the backpropagation formula finally we end up with something like this now we need to compute the derivative of our input to pass along to the next layer we can perform a similar chain rule — derivative of the output w r t the inputs time the derivative of the cost w r t the output and that s it for the backpropagation step the final step be the weight update assume we re stick with gradient descent for this example this can be a simple one liner to actually train our network we take one of our training sample and call forward on each layer consecutively pass the output of the previous layer as the input of the follow layer we compute dj pass that as the out_grad parameter to the last layer s backward method we call backward on each of the layer in reverse order this time pass the output of the further layer as out_grad to the previous layer finally we call update on each of our layer and repeat there s one last detail that we should include which be the concept of a bias akin to that of a constant term in any give equation notice that with our current implementation the activation of a neuron be determine solely base on the activation of the previous layer there s no bias term that can shift the activation up or down independent of the input a bias term isn t strictly necessary — in fact if you train your network as be it would probably still work fine but if you do need a bias term the code stay almost the same — the only difference be that you need to add a column of 1s to the incoming activation and update your weight matrix accordingly so one of your weight get treat as a bias term the only other difference be that when return cost_wrt_input you can cut out the first row — nobody care about the gradient associate with the bias term because the previous layer have no say in the activation of the bias neuron implement backpropagation can be kind of tricky so it s often a good idea to check your implementation you can do so by compute the gradient numerically by literally perturb the weight and calculate the difference in your cost function and compare it to your backpropagation compute gradient this gradient check doesn t need to be run once you ve verify your implementation but it could save a lot of time track down potential problem with your network nowadays you often don t even need to implement a neural network on your own as library such as caffe torch or tensorflow will have implementation ready to go that be say it s often a good idea to try implement it on your own to get a well grasp of how everything work under the hood intrigue look to learn more about neural network here be some great online class to get you start stanford s cs231n although it s technically about convolutional neural network the class provide an excellent introduction to and survey of neural network in general class video note and assignment be all post here and if you have the patience for it I would strongly recommend walk through the assignment so you can really get to know what you re learn mit 6 034 this class teach by prof patrick henry winston explore many different algorithm and discipline in artificial intelligence there s a great lecture on backprop that I actually use as a stepping stone to get setup write this article I also learn genetic algorithm from prof winston — he s a great teacher we hope that if you visit this article without know how the backpropagation algorithm work you be read this with an at least rudimentary mathematical or conceptual intuition of it write and convey such a complex algorithm to a suppose beginner have prove to be an extremely difficult task for we but it s help we truly understand what we ve be learn about with great knowledge in a fundamental area of machine learning we be now excited to take a look at new interesting algorithm and discipline in the field we be look forward to continue document these endeavor together from a quick cheer to a stand ovation clap to show how much you enjoy this story rohankapur com our ongoing effort to make the mathematics science linguistic and philosophy of artificial intelligence fun and simple
Per Harald Borgen,1.3K,7,https://medium.com/learning-new-stuff/how-to-learn-neural-networks-758b78f2736e?source=tag_archive---------4----------------,learn how to code neural network learn new stuff medium,this be the second post in a series of I try to learn something new over a short period of time the first time consist of learn how to do machine learning in a week this time I ve try to learn neural network while I didn t manage to do it within a week due to various reason I do get a basic understanding of it throughout the summer and autumn of 2015 by basic understanding I mean that I finally know how to code simple neural network from scratch on my own in this post I ll give a few explanation and guide you to the resource I ve use in case you re interested in do this yourself so what be a neural network let s wait with the network part and start off with one single neuron the circle below illustrate an artificial neuron its input be 5 and its output be 1 the input be the sum of the three synapsis connect to the neuron the three arrow at the left at the far left we see two input value plus a bias value the input value be 1 and 0 the green number while the bias hold a value of 2 the brown number the two input be then multiply by their so call weight which be 7 and 3 the blue number finally we add it up with the bias and end up with a number in this case 5 the red number this be the input for our artificial neuron the neuron then perform some kind of computation on this number — in our case the sigmoid function and then spit out an output this happen to be 1 as sigmoid of 5 equal to 1 if we round the number up more info on the sigmoid function follow later if you connect a network of these neuron together you have a neural network which propagate forward — from input output via neuron which be connect to each other through synapsis like on the image to the left I can strongly recommend the welch labs video on youtube for get a well intuitive explanation of this process after you ve see the welch labs video its a good idea to spend some time watch week 4 of the coursera s machine learning course which cover neural network as it ll give you more intuition of how they work the course be fairly mathematical and its base around octave while I prefer python because of this I do not do the programming exercise instead I use the video to help I understand what I need to learn the first thing I realize I need to investigate far be the sigmoid function as this seem to be a critical part of many neural network I know a little bit about the function as it be also cover in week 3 of the same course so I go back and watch these video again but watch video win t get you all the way to really understand it I feel I need to code it from the ground up so I start to code a logistic regression algorithm from scratch which happen to use the sigmoid function it take a whole day and it s probably not a very good implementation of logistic regression but that doesn t matter as I finally understand how it work check the code here you don t need to perform this entire exercise yourself as it require some knowledge about and cost function and gradient descent which you might not have at this point but make sure you understand how the sigmoid function work understand how a neural network work from input to output isn t that difficult to understand at least conceptually more difficult though be understand how the neural network actually learn from look at a set of datum sample the concept be call backpropagation the weight be the blue number on our neuron in the beginning of the article this process happen backwards because you start at the end of the network observe how wrong the network guess be and then move backwards through the network while adjust the weight on the way until you finally reach the input to calculate this by hand require some calculus as it involve get some derivative of the network weight the kahn academy calculus course seem like a good way to start though I haven t use they myself as I take calculus on university the three good source I find for understand backpropagation be these you should definitely code along while you re read the article especially the two first one it ll give you some sample code to look back at when you re confused in the future plus I can t really emphasize this enough the third article be also fantastic but I ve use this more as a wiki than a plain tutorial as it s actually an entire book it contain thorough explanation all the important concept in neural network these article will also help you understand important concept as cost function and gradient descent which play equally important role in neural network in some article and tutorial you ll actually end up code small neural network as soon as you re comfortable with that I recommend you to go all in on this strategy it s both fun and an extremely effective way of learn one of the article I also learn a lot from be a neural network in 11 line of python by iamtrask it contain an extraordinary amount of compress knowledge and concept in just 11 line after you ve code along with this example you should do as the article state at the bottom which be to implement it once again without look at the tutorial this force you to really understand the concept and will likely reveal hole in your knowledge which isn t fun however when you finally manage it you ll feel like you ve just acquire a new superpower when you ve do this you can continue with this wild ml tutorial by denny britz which guide you through a little more robust neural network at this point you could either try and code your own neural network from scratch or start play around with some of the network you have code up already it s great fun to find a dataset that interest you and try to make some prediction with your neural net to get a hold of a dataset just visit my side project dataset co ← shameless self promotion and find one you like anyway the point be that you re now well off experiment with stuff that interest you rather than follow my advice personally I m currently learn how to use python library that make it easy to code up neural network like theano lasagne and nolearn I m use this to do challenge on kaggle which be both great fun and great learn good luck and don t forget to press the heart button if you like the article thank for read my name be per I m a co founder of scrimba — a well way to teach and learn code if you ve read this far I d recommend you to check out this demo from a quick cheer to a stand ovation clap to show how much you enjoy this story co founder of scrimba the next generation platform for teaching and learn code https scrimba com a publication about improve your technical skill
Shi Yan,4.4K,7,https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714?source=tag_archive---------5----------------,understand lstm and its diagram ml review medium,I just want to reiterate what s say here I m not well at explain lstm I want to write this down as a way to remember it myself I think the above blog post write by christopher olah be the good lstm material you would find please visit the original link if you want to learn lstm but I do create some nice diagram although we don t know how brain function yet we have the feeling that it must have a logic unit and a memory unit we make decision by reasoning and by experience so do computer we have the logic unit cpus and gpu and we also have memory but when you look at a neural network it function like a black box you feed in some input from one side you receive some output from the other side the decision it make be mostly base on the current input I think it s unfair to say that neural network have no memory at all after all those learn weight be some kind of memory of the training datum but this memory be more static sometimes we want to remember an input for later use there be many example of such a situation such as the stock market to make a good investment judgement we have to at least look at the stock datum from a time window the naive way to let neural network accept a time series datum be connect several neural network together each of the neural network handle one time step instead of feed the datum at each individual time step you provide datum at all time step within a window or a context to the neural network a lot of time you need to process datum that have periodic pattern as a silly example suppose you want to predict christmas tree sale this be a very seasonal thing and likely to peak only once a year so a good strategy to predict christmas tree sale be look at the datum from exactly a year back for this kind of problem you either need to have a big context to include ancient datum point or you have a good memory you know what datum be valuable to remember for later use and what need to be forget when it be useless theoretically the naively connect neural network so call recurrent neural network can work but in practice it suffer from two problem vanish gradient and explode gradient which make it unusable then later lstm long short term memory be invent to solve this issue by explicitly introduce a memory unit call the cell into the network this be the diagram of a lstm building block at a first sight this look intimidate let s ignore the internal but only look at the input and output of the unit the network take three input x_t be the input of the current time step h_t 1 be the output from the previous lstm unit and c_t 1 be the memory of the previous unit which I think be the most important input as for output h_t be the output of the current network c_t be the memory of the current unit therefore this single unit make decision by consider the current input previous output and previous memory and it generate a new output and alter its memory the way its internal memory c_t change be pretty similar to pipe water through a pipe assume the memory be water it flow into a pipe you want to change this memory flow along the way and this change be control by two valve the first valve be call the forget valve if you shut it no old memory will be keep if you fully open this valve all old memory will pass through the second valve be the new memory valve new memory will come in through a t shape joint like above and merge with the old memory exactly how much new memory should come in be control by the second valve on the lstm diagram the top pipe be the memory pipe the input be the old memory a vector the first cross ✖ it pass through be the forget valve it be actually an element wise multiplication operation so if you multiply the old memory c_t 1 with a vector that be close to 0 that mean you want to forget most of the old memory you let the old memory go through if your forget valve equal 1 then the second operation the memory flow will go through be this + operator this operator mean piece wise summation it resemble the t shape joint pipe new memory and the old memory will merge by this operation how much new memory should be add to the old memory be control by another valve the ✖ below the + sign after these two operation you have the old memory c_t 1 change to the new memory c_t now let look at the valve the first one be call the forget valve it be control by a simple one layer neural network the input of the neural network be h_t 1 the output of the previous lstm block x_t the input for the current lstm block c_t 1 the memory of the previous block and finally a bias vector b_0 this neural network have a sigmoid function as activation and it s output vector be the forget valve which will apply to the old memory c_t 1 by element wise multiplication now the second valve be call the new memory valve again it be a one layer simple neural network that take the same input as the forget valve this valve control how much the new memory should influence the old memory the new memory itself however be generate by another neural network it be also a one layer network but use tanh as the activation function the output of this network will element wise multiple the new memory valve and add to the old memory to form the new memory these two ✖ sign be the forget valve and the new memory valve and finally we need to generate the output for this lstm unit this step have an output valve that be control by the new memory the previous output h_t 1 the input x_t and a bias vector this valve control how much new memory should output to the next lstm unit the above diagram be inspire by christopher s blog post but most of the time you will see a diagram like below the major difference between the two variation be that the follow diagram doesn t treat the memory unit c as an input to the unit instead it treat it as an internal thing cell I like the christopher s diagram in that it explicitly show how this memory c get pass from the previous unit to the next but in the follow image you can t easily see that c_t 1 be actually from the previous unit and c_t be part of the output the second reason I don t like the follow diagram be that the computation you perform within the unit should be order but you can t see it clearly from the follow diagram for example to calculate the output of this unit you need to have c_t the new memory ready therefore the first step should be evaluate c_t the follow diagram try to represent this delay or order with dash line and solid line there be error in this picture dash line mean the old memory which be available at the beginning some solid line mean the new memory operation require the new memory have to wait until c_t be available but these two diagram be essentially the same here I want to use the same symbol and color of the first diagram to redraw the above diagram this be the forget gate valve that shut the old memory this be the new memory valve and the new memory these be the two valve and the element wise summation to merge the old memory and the new memory to form c_t in green flow back to the big cell this be the output valve and output of the lstm unit from a quick cheer to a stand ovation clap to show how much you enjoy this story software engineer & wantrepreneur interested in computer graphic bitcoin and deep learning highlight from machine learning research project and learn material from and for ml scientist engineer an enthusiast
Ross Goodwin,686,23,https://medium.com/artists-and-machine-intelligence/adventures-in-narrated-reality-6516ff395ba3?source=tag_archive---------6----------------,adventure in narrated reality artist and machine intelligence medium,by ross goodwin in may 2015 stanford phd student andrej karpathy write a blog post entitle the unreasonable effectiveness of recurrent neural network and release a code repository call char rnn both receive quite a lot of attention from the machine learn community in the month that follow spur commentary and a number of response post from other researcher I remember read these post early last summer initially I be somewhat underwhelmed — as at least one commentator point out much of the generate text that karpathy choose to highlight do not seem much well than result one might expect from high order character level markov chain here be a snippet of karpathy s char rnn generate shakespeare and here be a snippet of generate shakespeare from a high order character level markov chain via the post link above so I be discourage and without access to affordable gpu for train recurrent neural network I continue to experiment with markov chain generative grammar template system and other ml free solution for generating text in december new york university be kind enough to grant I access to their high performance compute facility I begin to train my own recurrent neural network use karpathy s code and I finally discover the quasi magical capacity of these machine since then I have be train a collection of recurrent neural network model for my thesis project at nyu and explore possibility for device that could enable such model to serve as expressive real time narrator in our everyday life at this point since this be my very first medium post perhaps I should introduce myself my name be ross goodwin I m a graduate student at nyu itp in my final semester and computational creative writing be my personal obsession before I begin my study at itp I be a political ghostwriter I graduate from mit in 2009 with a b s degree in economic and during my undergraduate year I have work on barack obama s 2008 presidential campaign at the time I want to be a political speechwriter and my first job after graduation be a presidential writer position at the white house in this role I write presidential proclamation which be statement of national day week and month of thing — everything from thanksgiving and african american history month to lesser know observance like safe boat week it be a very strange job but I thoroughly enjoy it I leave the white house in 2010 for a position at the u s department of the treasury where I work for two year mostly put together briefing binder for then secretary timothy geithner and deputy secretary neal wolin in the department s front office I didn t get many speechwrite opportunity and pursue a future in the financial world do not appeal to I so I leave to work as a freelance ghostwriter this be a rather dark time in my life as I rapidly find myself write for a variety of unsavory client and cause in order to pay my rent every month in complete these assignment I begin to integrate algorithm into my writing process to improve my productivity at the time I didn t think about these technique as algorithmic but it s obvious in retrospect for example if I have to write 12 letter I d write they in a spreadsheet with a paragraph in each cell each letter would exist in a column and I would write across the row — first I d write all the first paragraph as one group then all the second paragraph then all the third and so on if I have to write a similar group of letter the next day for the same client I would use an excel macro to randomly shuffle the cell then edit the paragraph for cohesion and turn the result in as an entirely new batch of letter write this way I find I could complete an 8 hour day of work in about 2 hour I use the rest of my time to work on a novel that s still not finish but that s a story for another time with help from some friend I turn the technique into a game we call the diagonalization argument after georg cantor s 1891 mathematical proof of the same name in early 2014 a client ask I to write review of all the guide available online to learn the python programming language one guide stand out above all other in the sheer number of time I see user reference it on various online forum and in the countless glow review it have earn across the internet learn python the hard way by zed shaw so to make my review well I decide I might as well try to learn python my past attempt at learn to code have fail due to lack of commitment lack of interest or lack of a good project to get start but this time be different somehow — ze s guide work for I and just like that I find myself completely and hopelessly addict to programming as a writer I gravitate immediately to the broad and expand world of natural language processing and generation my first few project be simple poetry generator and once I move to new york city and start itp I discover a local community of likeminde individual leverage computation to produce and enhance textual work I host a code poetry slam in november 2014 and begin attend todd anderson s monthly wordhack event at babycastle in early 2015 I develop and launch word camera a web app and set of physical device that use the clarifai api to tag image with noun conceptnet to find related word and a template system to stre the result together into descriptive though often bizarre prose poem relate to the capture photograph the project be about redefine the photographic experience and it earn more attention than I expect 1 2 3 in november I be invite to exhibit this work at idfa doclab in amsterdam at that point it become obvious that word camera or some extension thereof would become my itp thesis project and while search for way to improve its output I begin to experiment with train my own neural network rather than use those other have train via api as I mention above I start use nyu s high performance compute facility in december this supercompute cluster include a staggering array of computational resource — in particular at least 32 nvidia tesla k80 gpus each with 24 gb of gpu memory while gpus aren t strictly require to train deep neural network the massively parallel process involve make they all but a necessity for train a large model that will perform well in a reasonable amount of time use two of andrej karpathy s repository neuraltalk2 and char rnn respectively I train an image captioning model and a number of model for generating text as a result of have free access to the large gpu in the world I be able to start train very large model right away neuraltalk2 use a convolutional neural network to classify image then transfer that classification datum to a recurrent neural network that generate a brief caption for my first attempt at train a neuraltalk2 model I want to do something less traditional than simply captioning image in my opinion the idea of machine image captioning be problematic because it s so limited in scope fundamentally a machine that can caption image be a machine that can describe or relate to what it see in a highly intelligent way I do understand that image caption be an important benchmark for machine intelligence however I also believe that think such a machine s primary use case will be to replace human image captioning represent a highly restrictive and narrow point of view so I try train a model on frame and corresponding caption from every episode of the tv show the x file my idea be to create a model that if give an image would generate a plausible line of dialogue from what it see unfortunately it simply do not work — most likely due to the dialogue for a particular scene bear no direct relationship to that scene s imagery rather than generate a different line of dialogue for different image the machine seem to want to assign the same line to every image indiscriminately strangely these repetitive line tend to say thing like I don t know I m not sure what you want and I don t know what to do one of my faculty advisor patrick hebron jokingly suggest this may be a sign of metacognition — needless to say I be slightly creep out but excited to continue these exploration I try two other less than traditional approach with neuraltalk2 training on reddit image post and corresponding comment and training on picture of recreational drug and corresponding erowid experience report both work well than my x file experiment but neither produce particularly interesting result so I resign myself to train a traditional image captioning model use the microsoft common object in context mscoco caption set in term of object represent mscoco be far from exhaustive but it do contain over 120 000 image with 5 caption each which be more than I could ve expect to produce on my own from any source furthermore I figure I could always do something less traditional with such a model once train I make just one adjustment to karpathy s default training parameter decrease the word frequency threshold from five to three by default neuraltalk2 ignore any word that appear few than five time in the caption corpus it train on I guess that reduce this threshold would result in some extra verbosity in the generate caption possibly at the expense of accuracy as a more verbose model might describe detail that be not actually present in an image however after about five day of training I have produce a model that exceed 0 9 cider in test which be about as good as karpathy suggest the model could get in his documentation as oppose to neuraltalk2 which be design to caption image karpathy s char rnn employ a character level lstm recurrent neural network simply for generate text a recurrent neural network be fundamentally a linear pattern machine give a character or set of character as a seed a char rnn model will predict which character would come next base on what it have learn from its input corpus by do this again and again the model can generate text in the same manner as a markov chain though its internal process be far more sophisticated lstm stand for long short term memory which remain a popular architecture for recurrent neural network unlike a no frill vanilla rnn an lstm protect its fragile underlie neural net with gate that determine which connection will persist in the machine s weight matrix I ve be tell that other be use something call a gru but I have yet to investigate this architecture I train my first text generating lstm on the same prose corpus I use for word camera s literary epitaph after about 18 hour I be get result like this this paragraph strike I as highly poetic compare to what I d see in the past from a computer the language wasn t entirely sensical but it certainly conjure imagery and employ relatively solid grammar furthermore it be original originality have always be important to I in computer generate text — because what good be a generator if it just plagiarize your input corpus this be a major issue with high order markov chain but due to its more sophisticated internal mechanism the lstm didn t seem to have the same tendency unfortunately much of the prose train model output that contain less poetic language be also less interesting than the passage above but give that I could produce poetic language with a prose train model I wonder what result I could get from a poetry train model the output above come from the first model I train on poetry I use the most readily available book I could find mostly those of poet from the 19th century and early whose work have enter the public domain the consistent line break and capitalization scheme be encouraging but I still wasn t satisfied with the language — due to the predominant age of the corpus it seem too ornate and formal I want more modern sound poetic language and so I know I have to train a model on modern poetry I assemble a corpus of all the modern poetry book I could find online it wasn t nearly as easy as assemble the prior corpus — unfortunately I can t go into detail on how I get all the book for fear of be sue the result be much close to what I be look for in term of language but they be also inconsistent in quality at the time I believe this be because the corpus be too small so I begin to supplement my modern poetry corpus with select prose work to increase its size it remain likely that this be the case however I have not yet discover the seeding technique I would later learn can dramatically improve lstm output another idea occur to I I could seed a poetic language lstm model with a generate image caption to make a new more poetic version of word camera some of the initial result see leave be strike I show they to one of my mentor allison parrish who suggest that I find a way to integrate the caption throughout the poetic text rather than just at the beginning I have show she some long example where the language have stray quite far from the subject matter of the caption after a few line I think about how to accomplish this and settle on a technique of seed the poetic language lstm multiple time with the same image caption at different temperature temperature be a parameter a number between zero and one that control the riskiness of a recurrent neural network s character prediction a low temperature value will result in text that s repetitive but highly grammatical accordingly high temperature result will be more innovative and surprising the model may even invent its own word while contain more mistake by iterate through temperature value with the same seed the subject matter would remain consistent while the language vary result in long piece that seem more cohesive than anything I d ever produce with a computer as I refine the aforementioned technique I train more lstm model attempt to discover the good training parameter the performance of a neural network model be measure by its loss which drop during training and eventually should be as close to zero as possible a model s loss be a statistical measurement indicate how well a model can predict the character sequence in its own corpus during training there be two loss figure to monitor the training loss which be define by how well the model predict the part of the corpus it s actually train on and the validation loss which be define by how well the model predict an unknown validation sample that be remove from the corpus prior to train the goal of train a model be to reduce its validation loss as much as possible because we want a model that accurately predict unknown character sequence not just those it s already see to this end there be a number of parameter to adjust among which be the training process largely consist of monitor the validation loss as it drop across model checkpoint and monitor the difference between training loss and validation loss as karpathy write in his char rnn documentation in january I release my code on github along with a set of train neural network model an image captioning model and two poetic language lstm model in my github readme I highlight a few result I feel be particularly strong 1 2 3 4 5 unlike prior version of word camera that mostly rely on a strong connection between the image and the output I find that I could still enjoy the result when the image caption be totally incorrect and there often seem to be some other accidental or perhaps slightly less than accidental element connect the image to the word I then shift my focus to develop a new physical prototype with the prior version of word camera I believe one of the most important part of the experience be its portability that s why I develop a mobile web app first and why I ensure all the physical prototype I build be fully portable for the new version I start with a physical prototype rather than a mobile web application because develop an app initially seem infeasible due to computational requirement though I have since thought of some possible solution since this would be a rapid prototype I decide to use a very small messenger bag as the case rather than fabricate my own also my research suggest that some of karpathy s code may not run on the raspberry pi s arm architecture so I need a slightly large computer that would require a large power source I decide to use an intel nuc that I power with a backup battery for a laptop I mount an elp wide angle camera to the strap alongside a set of control a rotary potentiometer and a button that communicate with the main computer via an arduino originally I plan to dump the text output to a hack kindle but ultimately decide the tactile nature of thermal printer paper would provide for a superior experience and allow I to hand out the output on the street like i d do with prior word camera model I find a large format thermal printer model with build in battery that use 4 wide paper previous printer I d use have take paper half as wide and I be able to pick up a couple of they on ebay for less than $ 50 each base on a suggestion from my friend anthony kesich I decide to add an ascii image of the photo above the text in february I be invite to speak at an art and machine learning symposium at gray area in san francisco in amsterdam at idfa in november I have meet jessica brillhart who be a vr director on google s cardboard team in january I begin to collaborate with she and some other folk at google on deep dream vr experience with automate poetic voiceover if you re unfamiliar with deep dream check out this blog post from last summer along with the related github repo and wikipedia article we demonstrate these experience at the event which be also an auction to sell deep dream artwork to benefit the gray area foundation mike tyka an artist whose deep dream work be prominently feature in the auction have ask I to use my poetic language lstm to generate title for his artwork I have a lot of fun do this and I think the title come out well — they even earn a brief mention in the wire article about the show during my talk the day after the auction I demonstrate my prototype I walk onto the stage wear my messenger bag snap a quick photo before I start speak and reveal the output at the end I would have be more nervous about share the machine s poetic output in front of so many people but the poetry have already pass what be in my opinion a more genuine test of its integrity a small reading at a library in brooklyn alongside traditional poet early in february I be invite to share some work at the leonard library in williamsburg the theme of the evening s event be love and romance so I generate several poem 1 2 from image I consider romantic my reading be meet with overwhelming approval from the other poet at the event one of whom say that the poem I have generate from the iconic time square v j day kiss photograph by alfred eisenstaedt mess he up as it seem to contain a plausible description of a flashback from the man s perspective I have be worry because as I once hear allison parrish say so much commentary about computational creative writing focus on computer replace human — but as anyone who have work with computer and language know that perspective which allison summarize as now they re even take the poet s job be highly uninformed when we teach computer to write the computer don t replace we any more than piano replace pianist — in a certain way they become our pen and we become more than writer we become writer of writer nietzsche who be the first philosopher to use a typewriter famously write our writing tool be also work on our thought which media theorist friedrich kittler analyze in his book gramophone film typewriter p 200 if we employ machine intelligence to augment our writing activity it s worth ask how such technology would affect how we think about write as well as how we think in the general sense I m inclined to believe that such a transformation would be positive as it would enable we to reach beyond our native writing capacity and produce work that might well reflect our wordless internal thought and notion I hesitate to repeat the piano pianist analogy for fear of stomp out its impact but I think it apply here too in produce fully automate writing machine I be only attempt to demonstrate what be possible with a machine alone in my research I be ultimately strive to produce device that allow human to work in concert with machine to produce write work my ambition be to augment our creativity not to replace it another ambition of mine be to promote a new framework that I ve be call narrated reality we already have virtual reality vr and augmented reality ar so it only make sense to provide another option nr — perhaps one that s less visual and more about supplement exist experience with expressive narration that way we can enjoy our experience while we re have they then revisit they later in an augmented format for my itp thesis I have originally plan to produce one general purpose device that use photograph gps coordinate supplement with foursquare location and the time to narrate everyday experience however after receive some sage advice from taeyoon choi I have decide to split that project into three device a camera a compass and a clock that respectively use image location and time to realize narrated reality along with designing and build those device I be in the process of train a library of interchangeable lstm model in order to experience a variety of option with each device in this new space after train a number of model on fiction and poetry I decide to try something different I train a model on the oxford english dictionary the result be well than I ever could have anticipate an automate balderdash player that could generate plausible definition for make up word I make a twitter bot so that people could submit their linguistic invention and a tumblr blog for the complete unabridged definition I be amazed by the machine s ability to take in and parrot back string of arbitrary character it have never see before and how it often seem to understand they in the context of actual word the fictional definition it create for real word be also frequently entertain my favorite of these be its definition for love — although a prior version of the model have define love as past tense of leave which I find equally amusing one particularly fascinating discovery I make with this bot concern the importance of a certain seeding technique that kyle mcdonald teach I as discuss above when you generate text with a recurrent neural network you can provide a seed to get the machine start for example if you want to know the machine s feeling on the meaning of life you might seed your lstm with the follow text and the machine would logically complete your sentence base on the pattern it have absorb from its training corpus however to get well and more consistent result it make sense to prepend the seed with a pre seed another paragraph of text to push the lstm into a desire state in practice it s good to use a high quality sample of output from the model you re seed with length approximately equal to the sequence length see above you set during training this mean the seed will now look something like this and the raw output will look like this though usually I remove the pre seed when I present the output the difference be more than apparent when I begin use this technique with the dictionary model without the pre seed the bot would usually fail to repeat an unknown word within its generate definition with the pre seed it would reliably parrot back whatever gibberish it have receive in the end the oxford english dictionary model train to a significantly low final validation loss < 0 75 than any other model I have train or have train since one commenter on hacker news note after consider what to do next I decide to try integrate dictionary definition into the prose and poetry corpus I have be train before additionally another stanford phd student name justin johnson release a new and improved version of karpathy s char rnn torch rnn which promise to use 7x less memory which would in turn allow for I to train even large model than I have be train before on the same gpu it take I an evening to get torch rnn work on nyu s supercomputing cluster but once I have it run I be immediately able to start training model four time as large as those I d train on before my initial model have 20 25 million parameter and now I be train with 80 85 million with some extra room to increase batch size and sequence length parameter the result I get from the first model be stunning — the corpus be about 45 % poetry 45 % prose and 10 % dictionary definition and the output appear more prose like while remain somewhat cohesive and paint vivid imagery next I decide to train a model on noam chomsky s complete work most individual have not produce enough publicly available text 25 100 mb raw text or 50 200 novel to train an lstm this size noam chomsky be an exception and the corpus of his writing I be able to assemble weigh in at a hefty 41 2 mb this project be complicate by the fact that I work for noam chomsky as an undergraduate at mit but that s a story for another time here be a sample of the output from that model unfortunately I ve have trouble make it say anything interesting about language as it prefer to rattle on and on about the u s and israel and palestine perhaps I ll have to train the next model on academic paper alone and see what happen most recently I ve be train machine on movie screenplay and get some interesting result if you train an lstm on continuous dialogue you can ask the model question and receive plausible response I promise myself I wouldn t write more than 5000 word for this article and I ve already pass that threshold so rather than attempt some sort of eloquent conclusion I ll leave you with this brief video there s much more to come in the near future stay tune edit 6 9 16 check out part ii from a quick cheer to a stand ovation clap to show how much you enjoy this story not a poet | new form & interface for write language narrate reality & c ami be a program at google that bring together artist and engineer to realize project use machine intelligence work be develop together alongside artist current practice and show at gallery biennial festival or online
Eric Elliott,947,9,https://medium.com/javascript-scene/how-to-build-a-neuron-exploring-ai-in-javascript-pt-1-c2726f1f02b2?source=tag_archive---------7----------------,how to build a neuron explore ai in javascript pt 1,year ago I be work on a project that need to be adaptive essentially the software need to learn and get well at a frequently repeat task over time I d read about neural network and some early success people have achieve with they so I decide to try it out myself that mark the beginning of a life long fascination with ai ai be a really big deal there be a small handful of technology that will dramatically change the world over the course of the next 25 year three of the big disruptor rely deeply on ai self drive car alone will disrupt more than 10 million job in america radically improve transportation and shipping efficiency and may lead to a huge change in car ownership as we outsource transportation and the pain of car ownership and maintenance to app like uber you ve probably hear about google s self drive car but tesla mercede bmw and other car manufacturer be also make big bet on self drive technology regulation not technology be the primary obstacle for drone base commercial service such as amazon air and just a few day ago the faa relaxed restriction on commercial drone flight it s still not legal for amazon to deliver package to your door with drone but that will soon change and when that happen commerce will never be the same of course half a million consumer drone sale over the last holiday season imply that drone be go to change a lot more than commerce expect to see a lot more of they hover obnoxiously in every metro area in the world in the come year augmented and virtual reality will fundamentally transform what it mean to be human as our sense be augment by virtual construct mix seamlessly with the real world we ll find new way to work new way to play and new way to interact with each other include ar assist learn telepresence and radical new experience we haven t dream of yet all of these technology require our gadget to have an awareness of the surround environment and the ability to respond behaviorally to environmental input self drive car need to see obstacle and make correction to avoid they drone need to detect collision hazard wind and the ground to land on room scale vr need to alert you of the room boundary so you don t wander into wall and ar device need to detect table chair and desk and wall and allow virtual element and character to interact with they process sensory input and figure out what they mean be one of the most important job that our brain be responsible for how do the human brain deal with the complexity of that job with neuron take alone a single neuron doesn t do anything particularly interesting but when combine together neural network be responsible for our ability to recognize the world around we solve problem and interact with our environment and the people around we neural network be the mechanism that allow we to use language build tool catch ball type read this article remember thing and basically do all the thing we consider to be think recently scientist have be scan section of small animal brain on the road to whole brain emulation for example a molecular level model of the 302 neuron in the c elegan roundworm the blue brain project be an attempt to do the same thing with a human brain the research use microscope to scan slice of live human brain tissue it s an ambitious project that be still in its infancy a decade after it launch but nobody expect it to be finish tomorrow we be still a long way from whole brain emulation for anything but the simple organism but eventually we may be able to emulate a whole human brain on a computer at the molecular level before we try to emulate even basic neuron functionality ourselves we should learn more about how neuron work a neuron be a cell that collect input signal electrical potential from synaptic terminal typically from dendrite but sometimes directly on the cell membrane when those signal sum past a certain threshold potential at the axon hillock trigger zone it trigger an output signal call an action potential the action potential travel along the output nerve fiber call an axon the axon split into collateral branch which can carry the output signal to different part of the neural network each axon branch terminate by split into cluster of tiny terminal branch which interface with other neuron through synapsis synapse be the word use to describe the transmission mechanism from one neuron to the next there be two kind of synapse receptor on the postsynaptic terminal wall ion channel and metabolic channel ion channel be fast ten of millisecond and can either excite or inhibit the potential in the postsynaptic neuron by open channel for positively or negatively charge ion to enter the cell respectively in an ionotropic transmission the neurotransmitter be release from the presynaptic neuron into the synaptic cleft — a tiny gap between the terminal of the presynaptic neuron and the postsynaptic neuron it bind to receptor on the postsynaptic terminal wall which cause they to open allow electrically charge ion to flow into the postsynaptic cell cause a change to the cell s potential metabolic channel be slow and more control than ion channel in chemical transmission the action potential trigger the release of chemical transmitter from the presynaptic terminal into the synaptic cleft those chemical transmitter bind to metabolic receptor which do not have ion channel of their own that bind trigger chemical reaction on the inside of the cell wall to release g protein which can open ion channel connect to different receptor as the g protein must first diffuse and rebind to neighboring channel this process naturally take long the duration of metabolic effect can vary from about 100ms to several minute depend on how long it take for neurotransmitter to be absorb release diffused or recycle back into the presynaptic terminal like ion channel the signal can be either exciting or inhibitory to the postsynaptic neuron potential there be also another type of synapse call an electrical synapse unlike the chemical synapsis describe above which rely on chemical neurotransmitter and receptor at axon terminal an electrical synapse connect dendrite from one cell directly to dendrite of another cell by a gap junction which be a channel that allow ion and other small molecule to pass directly between the cell effectively create one large neuron with multiple axon cell connect by electrical synapsis almost always fire simultaneously when any connected cell fire all connect cell fire with it however some gap junction be one way among other thing electrical synapsis connect cell that control muscle group such as the heart where it s important that all related cell cooperate create simultaneous muscle contraction different synapsis can have different strength call weight a synapse weight can change over time through a process know as synaptic plasticity it be believe that change in synapse connection strength be how we form memory in other word in order to learn and form memory our brain literally rewire itself an increase in synaptic weight be call long term potentiation ltp a decrease in synaptic weight be call long term depression ltd if the postsynaptic neuron tend to fire a lot when the presynaptic neuron fire the synaptic weight increase if the cell don t tend to fire together often the connection weaken in other word the key to synaptic plasticity be hide in a pair of 20ms window if the presynaptic neuron fire before the postsynaptic neuron within 20ms the weight increase ltp if the presynaptic neuron fire after the postsynaptic neuron within 20ms the weight decrease ltd this process be call spike time dependent plasticity spike time dependent plasticity be discover in the 1990 s and be still be explore but it be believe that action potential backpropagation from the cell s axon to the dendrite be involve in the ltp process during a typical forward propagate event glutamate will be release from the presynaptic terminal which bind to ampa receptor in the postsynaptic terminal wall allow positively charge sodium ion na+ into the cell if a large enough depolarization event occur inside the cell perhaps a backpropagation potential from the axon trigger point electrostatic repulsion will open a magnesium block in nmda receptor allow even more sodium to flood the cell along with calcium ca2 + at the same time potassium k+ flow out of the cell these event themselves only last ten of millisecond but they have indirect last effect an influx of calcium cause extra ampa receptor to be insert into the cell membrane which will allow more sodium ion into the cell during future action potential event from the presynaptic neuron a similar process work in reverse to trigger ltd during ltp event a special class of protein call growth factor can also form which can cause new synapsis to grow strengthen the bond between the two cell the impact of new synapse growth can be permanent assume that the neuron continue to fire together frequently many artificial neuron act less like neuron and more like transistor with two simple state on or off if enough upstream neuron be on rather than off the neuron be on otherwise it s off other neural net use input value from 1 to +1 the basic math look a little like the follow this be a good idea if you want to conserve cpu power so you can emulate a lot more neuron and we ve be able to use these basic principle to accomplish very simple pattern recognition task such as optical character recognition ocr use pre train network however there s a problem as I ve describe above real neuron don t behave that way instead synapsis transmit fluctuate continuous value potential over time through the soma cell body to the axon hillock trigger zone where the sum of the signal may or may not trigger an action potential at any give moment in time if the potential in the soma remain high pulse may continue as the cell trigger at high frequency once every few millisecond lot of variable influence the process the trigger frequency and the pattern of action potential burst with the model present above how would you determine whether or not trigger occur within the ltp ltd window what critical element be our basic model miss time but that s a story for a different article stay tune for part 2 eric elliott be the author of programming javascript application o reilly and learn javascript with eric elliott he have contribute to software experience for adobe systems zumba fitness the wall street journal espn bbc and top recording artist include usher frank ocean metallica and many more he spend most of his time in the san francisco bay area with the most beautiful woman in the world from a quick cheer to a stand ovation clap to show how much you enjoy this story make some magic # javascript to submit dm your proposal to @js_cheerleader on twitter
Dhruv Parthasarathy,665,11,https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0?source=tag_archive---------8----------------,write an ai to win at pong from scratch with reinforcement learning,there s a huge difference between read about reinforcement learning and actually implement it in this post you ll implement a neural network for reinforcement learning and see it learn more and more as it finally become good enough to beat the computer in pong you can play around with other such atari game at the openai gym by the end of this post you ll be able to do the follow the code and the idea be all tightly base on andrej karpathy s blog post the code in me_pong py be intend to be a simple to follow version of pong py which be write by dr karpathy to follow along you ll need to know the following if you want a deep dive into the material at hand read the blog post on which all of this be base this post be mean to be a simple introduction to that material great let s get start we be give the following can we use these piece to train our agent to beat the computer moreover can we make our solution generic enough so it can be reuse to win in game that aren t pong indeed we can andrej do this by build a neural network that take in each image and output a command to our ai to move up or down we can break this down a bit more into the follow step our neural network base heavily on andrej s solution will do the follow ok now that we ve describe the problem and its solution let s get to write some code we re now go to follow the code in me_pong py please keep it open and read along the code start here first let s use openai gym to make a game environment and get our very first image of the game next we set a bunch of parameter base off of andrej s blog post we aren t go to worry about tune they but note that you can probably get well performance by do so the parameter we will use be then we set counter initial value and the initial weight in our neural network weight be store in matrix layer 1 of our neural network be a 200 x 6400 matrix represent the weight for our hide layer for layer 1 element w1_ij represent the weight of neuron I for input pixel j in layer 1 layer 2 be a 200 x 1 matrix represent the weight of the output of the hide layer on our final output for layer 2 element w2_i represent the weight we place on the activation of neuron I in the hidden layer we initialize each layer s weight with random number for now we divide by the square root of the number of the dimension size to normalize our weight next we set up the initial parameter for rmsprop a method for update weight that we will discuss later don t worry too much about understand what you see below I m mainly bring it up here so we can continue to follow along the main code block we ll need to collect a bunch of observation and intermediate value across the episode and use those to compute the gradient at the end base on the result the below set up the array where we ll collect all that information ok we re all do with the setup if you be follow it should look something like this phew now for the fun part the crux of our algorithm be go to live in a loop where we continually make a move and then learn base on the result of the move we ll put everything in a while block for now but in reality you might set up a break condition to stop the process the first step to our algorithm be process the image of the game that openai gym pass we we really don t care about the entire image just certain detail we do this below let s dive into preprocess_observation to see how we convert the image openai gym give we into something we can use to train our neural network the basic step be now that we ve preprocesse the observation let s move on to actually send the observation through our neural net to generate the probability of tell our ai to move up here be the step we ll take how exactly do apply_neural_nets take observation and weight and generate a probability of go up this be just the forward pass of the neural network let s look at the code below for more information as you can see it s not many step at all let s go step by step let s return to the main algorithm and continue on now that we have obtain a probability of go up we need to now record the result for later learning and choose an action to tell our ai to implement we choose an action by flip an imaginary coin that land up with probability up_probability and down with 1 up_probability if it land up we choose tell our ai to go up and if not we tell it to go down we also have do that we pass the action to openai gym via env step action ok we ve cover the first half of the solution we know what action to tell our ai to take if you ve be follow along your code should look like this now that we ve make our move it s time to start learn so we figure out the right weight in our neural network learning be all about see the result of the action I e whether or not we win the round and change our weight accordingly the first step to learning be ask the follow question mathematically this be just the derivative of our result with respect to the output of our final layer if l be the value of our result to we and f be the function that give we the activation of our final layer this derivative be just ∂l ∂f in a binary classification context I e we just have to tell the ai one of two action up or down this derivative turn out to be note that σ in the above equation represent the sigmoid function read the attribute classification section here for more information about how we get the above derivative we simplify this far below after one action move the paddle up or down we don t really have an idea of whether or not this be the right action so we re go to cheat and treat the action we end up sample from our probability as the correct action our predicion for this round be go to be the probability of go up we calculate use that we have that ∂l ∂f can be compute by awesome we have the gradient per action the next step be to figure out how we learn after the end of an episode I e when we or our opponent miss the ball and someone get a point we do this by compute the policy gradient of the network at the end of each episode the intuition here be that if we win the round we d like our network to generate more of the action that lead to we win alternatively if we lose we re go to try and generate less of these action openai gym provide we the handy do variable to tell we when an episode finish I e we miss the ball or our opponent miss the ball when we notice we be do the first thing we do be compile all our observation and gradient calculation for the episode this allow we to apply our learning over all the action in the episode next we want to learn in such a way that action take towards the end of an episode more heavily influence our learning than action take at the beginning this be call discount think about it this way if you move up at the first frame of the episode it probably have very little impact on whether or not you win however close to the end of the episode your action probably have a much large effect as they determine whether or not your paddle reach the ball and how your paddle hit the ball we re go to take this weighting into account by discount our reward such that reward from early frame be discount a lot more than reward for later frame after this we re go to finally use backpropagation to compute the gradient I e the direction we need to move our weight to improve let s dig in a bit into how the policy gradient for the episode be compute this be one of the most important part of reinforcement learning as it s how our agent figure out how to improve over time to begin with if you haven t already read this excerpt on backpropagation from michael nielsen s excellent free book on deep learning as you ll see in that excerpt there be four fundamental equation of backpropogation a technique for compute the gradient for our weight our goal be to find ∂c ∂w1 bp4 the derivative of the cost function with respect to the first layer s weight and ∂c ∂w2 the derivative of the cost function with respect to the second layer s weight these gradient will help we understand what direction to move our weight in for the great improvement to begin with let s start with ∂c ∂w2 if a^l2 be the activation of the hide layer layer 2 we see that the formula be indeed this be exactly what we do here next we need to calculate ∂c ∂w1 the formula for that is and we also know that a^l1 be just our observation_value so all we need now be δ^l2 once we have that we can calculate ∂c ∂w1 and return we do just that below if you ve be follow along your function should look like this with that we ve finish backpropagation and compute our gradient after we have finish batch_size episode we finally update our weight for our neural network and implement our learning to update the weight we simply apply rmsprop an algorithm for update weight describe by sebastian reuder here we implement this below this be the step that tweak our weight and allow we to get well over time this be basically it put it altogether it should look like this you just code a full neural network for play pong uncomment env render and run it for 3 4 day to see it finally beat the computer you ll need to do some pickling as do in andrej karpathy s solution to be able to visualize your result when you win accord to the blog post this algorithm should take around 3 day of training on a macbook to start beat the computer consider tweak the parameter or use convolutional neural net to boost the performance far if you want a further primer into neural network and reinforcement learning there be some great resource to learn more I work at udacity as the director of machine learning program from a quick cheer to a stand ovation clap to show how much you enjoy this story @dhruvp vp eng @athelas mit math and cs undergrad 13 mit cs master 14 previously director of ai program @ udacity
Waleed Abdulla,507,12,https://medium.com/@waleedka/traffic-sign-recognition-with-tensorflow-629dffc391a6?source=tag_archive---------9----------------,traffic sign recognition with tensorflow waleed abdulla medium,this be part 1 of a series about build a deep learning model to recognize traffic sign it s intend to be a learning experience for myself and for anyone else who like to follow along there be a lot of resource that cover the theory and math of neural network so I ll focus on the practical aspect instead I ll describe my own experience build this model and share the source code and relevant material this be suitable for those who know python and the basic of machine learning already but want hand on experience and to practice build a real application in this part I ll talk about image classification and I ll keep the model as simple as possible in later part I ll cover convolutional network data augmentation and object detection the source code be available in this jupyter notebook I m use python 3 5 and tensorflow 0 12 if you prefer to run the code in docker you can use my docker image that contain many popular deep learning tool run it with this command note that my project directory be in ~ traffic and I m map it to the traffic directory in the docker container modify this if you re use a different directory my first challenge be find a good training dataset traffic sign recognition be a well studied problem so I figure I ll find something online I start by google traffic sign dataset and find several option I pick the belgian traffic sign dataset because it be big enough to train on and yet small enough to be easy to work with you can download the dataset from http btsd ethz ch shareddata there be a lot of dataset on that page but you only need the two file list under belgiumts for classification crop image after expand the file this be my directory structure try to match it so you can run the code without have to change the path each of the two directory contain 62 subdirectory name sequentially from 00000 to 00061 the directory name represent the label and the image inside each directory be sample of each label or if you prefer to sound more formal do exploratory data analysis it s tempting to skip this part but I ve find that the code I write to examine the data end up be use a lot throughout the project I usually do this in jupyter notebook and share they with the team know your datum well from the start save you a lot of time later the image in this dataset be in an old ppm format so old in fact that most tool don t support it which mean that I couldn t casually browse the folder to take a look at the image luckily the scikit image library recognize this format this code will load the datum and return two list image and label this be a small dataset so I m load everything into ram to keep it simple for large dataset you d want to load the datum in batch after load the image into numpy array I display a sample image of each label see code in the notebook this be our dataset look like a good training set the image quality be great and there be a variety of angle and lighting condition more importantly the traffic sign occupy most of the area of each image which allow I to focus on object classification and not have to worry about find the location of the traffic sign in the image object detection I ll get to object detection in a future post the first thing I notice from the sample above be that image be square ish but have different aspect ratio my neural network will take a fix size input so I have some preprocessing to do I ll get to that soon but first let s pick one label and see more of its image here be an example of label 32 it look like the dataset consider all speed limit sign to be of the same class regardless of the number on they that s fine as long as we know about it beforehand and know what to expect that s why understand your dataset be so important and can save you a lot of pain and confusion later I ll leave explore the other label to you label 26 and 27 be interesting to check they also have number in red circle so the model will have to get really good to differentiate between they most image classification network expect image of a fix size and our first model will do as well so we need to resize all the image to the same size but since the image have different aspect ratio then some of they will be stretch vertically or horizontally be that a problem I think it s not in this case because the difference in aspect ratio be not that large my own criterion be that if a person can recognize the image when they re stretch then the model should be able to do so as well what be the size of the image anyway let s print a few example the size seem to hover around 128x128 I could use that size to preserve as much information as possible but in early development I prefer to use a small size because it lead to fast training which allow I to iterate fast I experiment with 16x16 and 20x20 but they be too small I end up pick 32x32 which be easy to recognize see below and reduce the size of the model and training datum by a factor of 16 compare to 128x128 I m also in the habit of print the min and max value often it s a simple way to verify the range of the datum and catch bug early this tell I that the image color be the standard range of 0 255 we re get to the interesting part continue the theme of keep it simple I start with the simple possible model a one layer network that consist of one neuron per label this network have 62 neuron and each neuron take the rgb value of all pixel as input effectively each neuron receive 32 * 32 * 3=3072 input this be a fully connect layer because every neuron connect to every input value you re probably familiar with its equation I start with a simple model because it s easy to explain easy to debug and fast to train once this work end to end expand on it be much easy than build something complex from the start tensorflow encapsulate the architecture of a neural network in an execution graph the graph consist of operation op for short such as add multiply reshape etc these op perform action on datum in tensor multidimensional array I ll go through the code to build the graph step by step below but here be the full code if you prefer to scan it first first I create the graph object tensorflow have a default global graph but I don t recommend use it global variable be bad in general because they make it too easy to introduce bug I prefer to create the graph explicitly then I define placeholder for the image and label the placeholder be tensorflow s way of receive input from the main program notice that I create the placeholder and all other op inside the block of with graph as_default this be so they become part of my graph object rather than the global graph the shape of the images_ph placeholder be none 32 32 3 it stand for batch size height width channel often shorten as nhwc the none for batch size mean that the batch size be flexible which mean that we can feed different batch size to the model without have to change the code pay attention to the order of your input because some model and framework might use a different arrangement such as nchw next I define the fully connect layer rather than implement the raw equation y = xw + b I use a handy function that do that in one line and also apply the activation function it expect input as a one dimensional vector though so I flatten the image first I m use the relu activation function here it simply convert all negative value to zero it s be show to work well in classification task and train fast than sigmoid or tanh for more background check here and here the output of the fully connect layer be a logit vector of length 62 technically it s none 62 because we re deal with a batch of logit vector a row in the logit tensor might look like this 0 3 0 0 1 2 2 1 01 0 4 0 0 the high the value the more likely that the image represent that label logit be not probability though — they can have any value and they don t add up to 1 the actual absolute value of the logit be not important just their value relative to each other it s easy to convert logit to probability use the softmax function if need it s not need here in this application we just need the index of the large value which correspond to the i d of the label the argmax op do that the argmax output will be integer in the range 0 to 61 choose the right loss function be an area of research in and of itself which I win t delve into it here other than to say that cross entropy be the most common function for classification task if you re not familiar with it there be a really good explanation here and here cross entropy be a measure of difference between two vector of probability so we need to convert label and the logit to probability vector the function sparse_softmax_cross_entropy_with_logit simplifie that it take the generate logit and the groundtruth label and do three thing convert the label index of shape none to logit of shape none 62 one hot vector then it run softmax to convert both prediction logit and label logit to probability and finally calculate the cross entropy between the two this generate a loss vector of shape none 1d of length = batch size which we pass through reduce_mean to get one single number that represent the loss value choose the optimization algorithm be another decision to make I usually use the adam optimizer because it s be show to converge fast than simple gradient descent this post do a great job compare different gradient descent optimizer the last node in the graph be the initialization op which simply set the value of all variable to zero or to random value or whatever the variable be set to initialize to notice that the code above doesn t execute any of the op yet it s just build the graph and describe its input the variable we define above such as init loss predicted_label don t contain numerical value they be reference to op that we ll execute next this be where we iteratively train the model to minimize the loss function before we start train though we need to create a session object I mention the graph object early and how it hold all the op of the model the session on the other hand hold the value of all the variable if a graph hold the equation y = xw+b then the session hold the actual value of these variable usually the first thing to run after start a session be the initialization op init to initialize the variable then we start the training loop and run the train op repeatedly while not necessary it s useful to run the loss op as well to print its value and monitor the progress of the training in case you re wonder I set the loop to 201 so that the i % 10 condition be satisfied in the last round and print the last loss value the output should look something like this now we have a train model in memory in the session object to use it we call session run just like in the training code the predicted_label op return the output of the argmax function so that s what we need to run here I classify 10 random image and print both the prediction and the groundtruth label for comparison in the notebook I include a function to visualize the result as well it generate something like this the visualization show that the model be work but doesn t quantify how accurate it be and you might ve notice that it s classify the training image so we don t know yet if the model generalize to image that it hasn t see before next we calculate a well evaluation metric to properly measure how the model generalize to datum it hasn t see I do the evaluation on test datum that I didn t use in train the belgiumts dataset make this easy by provide two separate set one for training and one for testing in the notebook I load the test set resize the image to 32x32 and then calculate the accuracy this be the relevant part of the code that calculate the accuracy the accuracy I get in each run range between 0 40 and 0 70 depend on whether the model land on a local minimum or a global minimum this be expect when run a simple model like this one in a future post I ll talk about way to improve the consistency of the result congratulation we have a work simple neural network give how simple this neural network be training take just a minute on my laptop so I didn t bother save the train model in the next part I ll add code to save and load train model and expand to use multiple layer convolutional network and datum augmentation stay tune from a quick cheer to a stand ovation clap to show how much you enjoy this story startup deep learn computer vision
Stefan Kojouharov,14.2K,7,https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?source=tag_archive---------0----------------,cheat sheet for ai neural network machine learn deep learning & big datum,over the past few month I have be collect ai cheat sheet from time to time I share they with friend and colleague and recently I have be get ask a lot so I decide to organize and share the entire collection to make thing more interesting and give context I add description and or excerpt for each major topic this be the most complete list and the big o be at the very end enjoy this machine learn cheat sheet will help you find the right estimator for the job which be the most difficult part the flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problem and how to solve it scikit learn formerly scikit learn be a free software machine learning library for the python programming language it feature various classification regression and clustering algorithm include support vector machine random forest gradient boost k mean and dbscan and be design to interoperate with the python numerical and scientific library numpy and scipy in may 2017 google announce the second generation of the tpu as well as the availability of the tpus in google compute engine 12 the second generation tpus deliver up to 180 teraflop of performance and when organize into cluster of 64 tpu provide up to 11 5 petaflop in 2017 google s tensorflow team decide to support keras in tensorflow s core library chollet explain that keras be conceive to be an interface rather than an end to end machine learning framework it present a high level more intuitive set of abstraction that make it easy to configure neural network regardless of the backend scientific computing library numpy target the cpython reference implementation of python which be a non optimize bytecode interpreter mathematical algorithm write for this version of python often run much slow than compile equivalent numpy address the slowness problem partly by provide multidimensional array and function and operator that operate efficiently on array require rewrite some code mostly inner loop use numpy the name panda be derive from the term panel datum an econometric term for multidimensional structured data set the term datum wrangler be start to infiltrate pop culture in the 2017 movie kong skull island one of the character play by actor marc evan jackson be introduce as steve woodward our data wrangler scipy build on the numpy array object and be part of the numpy stack which include tool like matplotlib panda and sympy and an expand set of scientific computing librarie this numpy stack have similar user to other application such as matlab gnu octave and scilab the numpy stack be also sometimes refer to as the scipy stack 3 matplotlib be a plotting library for the python programming language and its numerical mathematics extension numpy it provide an object orient api for embed plot into application use general purpose gui toolkit like tkinter wxpython qt or gtk+ there be also a procedural pylab interface base on a state machine like opengl design to closely resemble that of matlab though its use be discourage 2 scipy make use of matplotlib pyplot be a matplotlib module which provide a matlab like interface 6 matplotlib be design to be as usable as matlab with the ability to use python with the advantage that it be free > > > if you like this list you can let I know here < < < stefan be the founder of chatbot s life a chatbot medium and consult firm chatbot s life have grow to over 150k view per month and have become the premium place to learn about bot & ai online chatbot s life have also consult many of the top bot company like swelly instav outbrain neargroup and a number of enterprise big o algorithm cheat sheet http bigocheatsheet com bokeh cheat sheet https s3 amazonaw com asset datacamp com blog_asset python_bokeh_cheat_sheet pdf datum science cheat sheet https www datacamp com community tutorial python data science cheat sheet basic datum wrangle cheat sheet https www rstudio com wp content upload 2015 02 datum wrangle cheatsheet pdf datum wrangle https en wikipedia org wiki data_wrangle ggplot cheat sheet https www rstudio com wp content upload 2015 03 ggplot2 cheatsheet pdf keras cheat sheet https www datacamp com community blog keras cheat sheet#gs drkenms keras https en wikipedia org wiki keras machine learn cheat sheet https ai icymi email new machinelearning cheat sheet by emily barry abdsc machine learning cheat sheet https doc microsoft com en in azure machine learning machine learn algorithm cheat sheet ml cheat sheet http peekaboo vision blogspot com 2013 01 machine learn cheat sheet for scikit html matplotlib cheat sheet https www datacamp com community blog python matplotlib cheat sheet#gs uekyspy matpotlib https en wikipedia org wiki matplotlib neural network cheat sheet http www asimovinstitute org neural network zoo neural network graph cheat sheet http www asimovinstitute org blog neural network https www quora com where can find a cheat sheet for neural network numpy cheat sheet https www datacamp com community blog python numpy cheat sheet#gs ak5zbge numpy https en wikipedia org wiki numpy panda cheat sheet https www datacamp com community blog python panda cheat sheet#gs oundfxm panda https en wikipedia org wiki panda _ software panda cheat sheet https www datacamp com community blog panda cheat sheet python#gs hpforic pyspark cheat sheet https www datacamp com community blog pyspark cheat sheet python#gs l = j1zxq scikit cheat sheet https www datacamp com community blog scikit learn cheat sheet scikit learn https en wikipedia org wiki scikit learn scikit learn cheat sheet http peekaboo vision blogspot com 2013 01 machine learn cheat sheet for scikit html scipy cheat sheet https www datacamp com community blog python scipy cheat sheet#gs jdsg3oi scipy https en wikipedia org wiki scipy tesorflow cheat sheet https www altoros com tensorflow cheat sheet html tensor flow https en wikipedia org wiki tensorflow from a quick cheer to a stand ovation clap to show how much you enjoy this story founder of chatbot life I help company create great chatbot & ai system and share my insight along the way late news info and tutorial on artificial intelligence machine learn deep learn big datum and what it mean for humanity
Avinash Sharma V,6.9K,10,https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0?source=tag_archive---------1----------------,understand activation function in neural network,recently a colleague of mine ask I a few question like why do we have so many activation function why be that one work well than the other how do we know which one to use be it hardcore math and so on so I think why not write an article on it for those who be familiar with neural network only at a basic level and be therefore wonder about activation function and their why how mathematic note this article assume that you have a basic knowledge of an artificial neuron I would recommend read up on the basic of neural network before read this article for well understanding so what do an artificial neuron do simply put it calculate a weighted sum of its input add a bias and then decide whether it should be fire or not yeah right an activation function do this but let s go with the flow for a moment so consider a neuron now the value of y can be anything range from inf to + inf the neuron really doesn t know the bound of the value so how do we decide whether the neuron should fire or not why this firing pattern because we learn it from biology that s the way brain work and brain be a work testimony of an awesome and intelligent system we decide to add activation function for this purpose to check the y value produce by a neuron and decide whether outside connection should consider this neuron as fire or not or rather let s say — activate or not the first thing that come to our mind be how about a threshold base activation function if the value of y be above a certain value declare it activate if it s less than the threshold then say it s not hmm great this could work activation function a = activate if y > threshold else not alternatively a = 1 if y > threshold 0 otherwise well what we just do be a step function see the below figure its output be 1 activate when value > 0 threshold and output a 0 not activate otherwise great so this make an activation function for a neuron no confusion however there be certain drawback with this to understand it well think about the following suppose you be create a binary classifier something which should say a yes or no activate or not activate a step function could do that for you that s exactly what it do say a 1 or 0 now think about the use case where you would want multiple such neuron to be connect to bring in more class class1 class2 class3 etc what will happen if more than 1 neuron be activate all neuron will output a 1 from step function now what would you decide which class be it hmm hard complicated you would want the network to activate only 1 neuron and other should be 0 only then would you be able to say it classify properly identify the class ah this be hard to train and converge this way it would have be well if the activation be not binary and it instead would say 50 % activate or 20 % activate and so on and then if more than 1 neuron activate you could find which neuron have the high activation and so on well than max a softmax but let s leave that for now in this case as well if more than 1 neuron say 100 % activate the problem still persist I know but since there be intermediate activation value for the output learning can be smooth and easy less wiggly and chance of more than 1 neuron be 100 % activate be less when compare to step function while training also depend on what you be train and the datum ok so we want something to give we intermediate analog activation value rather than say activate or not binary the first thing that come to our mind would be linear function a = cx a straight line function where activation be proportional to input which be the weighted sum from neuron this way it give a range of activation so it be not binary activation we can definitely connect a few neuron together and if more than 1 fire we could take the max or softmax and decide base on that so that be ok too then what be the problem with this if you be familiar with gradient descent for training you would notice that for this function derivative be a constant a = cx derivative with respect to x be c that mean the gradient have no relationship with x it be a constant gradient and the descent be go to be on constant gradient if there be an error in prediction the change make by back propagation be constant and not depend on the change in input delta x this be not that good not always but bear with I there be another problem too think about connect layer each layer be activate by a linear function that activation in turn go into the next level as input and the second layer calculate weight sum on that input and it in turn fire base on another linear activation function no matter how many layer we have if all be linear in nature the final activation function of last layer be nothing but just a linear function of the input of first layer pause for a bit and think about it that mean these two layer or n layer can be replace by a single layer ah we just lose the ability of stack layer this way no matter how we stack the whole network be still equivalent to a single layer with linear activation a combination of linear function in a linear manner be still another linear function let s move on shall we well this look smooth and step function like what be the benefit of this think about it for a moment first thing first it be nonlinear in nature combination of this function be also nonlinear great now we can stack layer what about non binary activation yes that too it will give an analog activation unlike step function it have a smooth gradient too and if you notice between x value 2 to 2 y value be very steep which mean any small change in the value of x in that region will cause value of y to change significantly ah that mean this function have a tendency to bring the y value to either end of the curve look like it s good for a classifier consider its property yes it indeed be it tend to bring the activation to either side of the curve above x = 2 and below x = 2 for example make clear distinction on prediction another advantage of this activation function be unlike linear function the output of the activation function be always go to be in range 0 1 compare to inf inf of linear function so we have our activation bind in a range nice it win t blow up the activation then this be great sigmoid function be one of the most widely use activation function today then what be the problem with this if you notice towards either end of the sigmoid function the y value tend to respond very less to change in x what do that mean the gradient at that region be go to be small it give rise to a problem of vanish gradient hmm so what happen when the activation reach near the near horizontal part of the curve on either side gradient be small or have vanish can not make significant change because of the extremely small value the network refuse to learn far or be drastically slow depend on use case and until gradient computation get hit by float point value limit there be way to work around this problem and sigmoid be still very popular in classification problem another activation function that be use be the tanh function hm this look very similar to sigmoid in fact it be a scaled sigmoid function ok now this have characteristic similar to sigmoid that we discuss above it be nonlinear in nature so great we can stack layer it be bind to range 1 1 so no worry of activation blow up one point to mention be that the gradient be strong for tanh than sigmoid derivative be steep decide between the sigmoid or tanh will depend on your requirement of gradient strength like sigmoid tanh also have the vanish gradient problem tanh be also a very popular and widely use activation function later come the relu function a x = max 0 x the relu function be as show above it give an output x if x be positive and 0 otherwise at first look this would look like have the same problem of linear function as it be linear in positive axis first of all relu be nonlinear in nature and combination of relu be also non linear in fact it be a good approximator any function can be approximate with combination of relu great so this mean we can stack layer it be not bind though the range of relu be 0 inf this mean it can blow up the activation another point that I would like to discuss here be the sparsity of the activation imagine a big neural network with a lot of neuron use a sigmoid or tanh will cause almost all neuron to fire in an analog way remember that mean almost all activation will be process to describe the output of a network in other word the activation be dense this be costly we would ideally want a few neuron in the network to not activate and thereby make the activation sparse and efficient relu give we this benefit imagine a network with random initialize weight or normalised and almost 50 % of the network yield 0 activation because of the characteristic of relu output 0 for negative value of x this mean a few neuron be fire sparse activation and the network be light woah nice relu seem to be awesome yes it be but nothing be flawless not even relu because of the horizontal line in relu for negative x the gradient can go towards 0 for activation in that region of relu gradient will be 0 because of which the weight will not get adjust during descent that mean those neuron which go into that state will stop respond to variation in error input simply because gradient be 0 nothing change this be call die relu problem this problem can cause several neuron to just die and not respond make a substantial part of the network passive there be variation in relu to mitigate this issue by simply make the horizontal line into non horizontal component for example y = 0 01x for x<0 will make it a slightly inclined line rather than horizontal line this be leaky relu there be other variation too the main idea be to let the gradient be non zero and recover during training eventually relu be less computationally expensive than tanh and sigmoid because it involve simple mathematical operation that be a good point to consider when we be design deep neural net now which activation function to use do that mean we just use relu for everything we do or sigmoid or tanh well yes and no when you know the function you be try to approximate have certain characteristic you can choose an activation function which will approximate the function fast lead to fast training process for example a sigmoid work well for a classifier see the graph of sigmoid doesn t it show the property of an ideal classifier because approximate a classifier function as combination of sigmoid be easy than maybe relu for example which will lead to fast training process and convergence you can use your own custom function too if you don t know the nature of the function you be try to learn then maybe I would suggest start with relu and then work backwards relu work most of the time as a general approximator in this article I try to describe a few activation function use commonly there be other activation function too but the general idea remain the same research for well activation function be still ongoing hope you get the idea behind activation function why they be use and how do we decide which one to use from a quick cheer to a stand ovation clap to show how much you enjoy this story musing of an ai deep learning mathematics addict
Elle O'Brien,2.3K,6,https://towardsdatascience.com/romance-novels-generated-by-artificial-intelligence-1b31d9c872b2?source=tag_archive---------2----------------,romance novel generate by artificial intelligence,I ve always be fascinate with romance novel — the kind they sell at the drugstore for a couple of dollar usually with some attractive soft light couple on the cover so when I start futze around with text generate neural network a few week ago I develop an urgent curiosity to discover what artificial intelligence could contribute to the ever popular genre maybe one day there will be entire book write by computer for now let s start with title I gather over 20 000 harlequin romance novel title and give they to a neural network a type of artificial intelligence that learn the structure of text it s powerful enough to string together word in a way that seem almost human 90 % human the other 10 % be all wackiness I be not disappoint with what come out I even photoshoppe some of my favorite into existence the author name be synthesize from machine learning too let s have a look by theme a common theme in romance novel be pregnancy and the word baby have a strong showing in the title I train the neural network on naturally the neural network come up with a lot of baby theme title there s an unusually high concentration of sheikh viking and billionaire in the harlequin world likewise the neural network generate some colorful new bachelor type I have so many question how be the prince pregnant what sort of consulting do the count do who be butterfly earl and what make the sheikh s desire so convenient although there be exception most romance novel end in happily ever after a lot of they even start with an unexpected wedding — a marriage of convenience or a stipulation of a business contract or a sham that turn into real love the neural network seem to have internalize something about matrimony doctor and surgeon be common paramour for mistress head towards the marriage valley christmas be a magical time for surgeon sheikh playboy dad consultant and the woman who love they what or where be knith I just like mission christmas this neural network have never see the big montana sky but it have some questionable idea about cowboy the neural network generate some decidedly pg 13 title they can t all live happily ever after some of the generate title sound like m night shyamalan be a collaborator how do the word fear get in there it s possible the network generate it without have fear in the training set but a subset of the harlequin empire be gear towards paranormal and gothic romance that might have include the word * note I check and there be veil of fear publish in 2012 to wrap it up some of the adorable failure and near miss generate by the neural network I hope you ve enjoy computer generate romance novel title half as much as I have maybe someone out there can write about the virgin viking or the consultant count or the baby surgeon seduction I d buy it I build a webscraper in python thank beautiful soup that grab about 20 000 romance novel title publish under the harlequin brand off of fictiondb com harlequin be to I synonymous with the romance genre although it comprise only a fraction albeit a healthy one of the entire market I feed this list of book title into a recurrent neural network use software I get from github and wait a few hour for the magic to happen the model I fit be a 3 layer 256 node recurrent neural network I also train the network on the author list in to create some new pen name for more about the neural network I use have a look at the fabulous work of andrej karpathy I discover that surgery by the sea be actually a real novel write by sheila dougla and publish in 1979 so this one isn t an original neural network creation because the training set be rather small only about 1 mb of text datum it s to be expect that sometimes the machine will spit out one of the title it be train on one of the more challenging aspect of this project be discern when that happen since the real publish title can be more surprising than anything bear out of artificial intelligence for example the $ 4 98 daddy and 6 1 grinch be both real in fact the very first romance novel publish by harlequin be call the manatee from a quick cheer to a stand ovation clap to show how much you enjoy this story computational scientist software developer science writer sharing concept idea and code
Slav Ivanov,4.4K,10,https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607?source=tag_archive---------3----------------,37 reason why your neural network be not work slav,the network have be train for the last 12 hour it all look good the gradient be flow and the loss be decrease but then come the prediction all zero all background nothing detect what do I do wrong — I ask my computer who didn t answer where do you start check if your model be output garbage for example predict the mean of all output or it have really poor accuracy a network might not be train for a number of reason over the course of many debug session I would often find myself do the same check I ve compile my experience along with the good idea around in this handy list I hope they would be of use to you too a lot of thing can go wrong but some of they be more likely to be break than other I usually start with this short list as an emergency first response if the step above don t do it start go down the follow big list and verify thing one by one check if the input datum you be feed the network make sense for example I ve more than once mix the width and the height of an image sometimes I would feed all zero by mistake or I would use the same batch over and over so print display a couple of batch of input and target output and make sure they be ok try pass random number instead of actual datum and see if the error behave the same way if it do it s a sure sign that your net be turn datum into garbage at some point try debug layer by layer op by op and see where thing go wrong your datum might be fine but the code that pass the input to the net might be break print the input of the first layer before any operation and check it check if a few input sample have the correct label also make sure shuffle input sample work the same way for output label maybe the non random part of the relationship between the input and output be too small compare to the random part one could argue that stock price be like this I e the input be not sufficiently relate to the output there isn t an universal way to detect this as it depend on the nature of the datum this happen to I once when I scrape an image dataset off a food site there be so many bad label that the network couldn t learn check a bunch of input sample manually and see if label seem off the cutoff point be up for debate as this paper get above 50 % accuracy on mnist use 50 % corrupt label if your dataset hasn t be shuffle and have a particular order to it order by label this could negatively impact the learn shuffle your dataset to avoid this make sure you be shuffle input and label together be there a 1000 class a image for every class b image then you might need to balance your loss function or try other class imbalance approach if you be train a net from scratch I e not finetune you probably need lot of datum for image classification people say you need a 1000 image per class or more this can happen in a sorted dataset I e the first 10k sample contain the same class easily fixable by shuffle the dataset this paper point out that have a very large batch can reduce the generalization ability of the model thank to @hengcherkeng for this one do you standardize your input to have zero mean and unit variance augmentation have a regularize effect too much of this combine with other form of regularization weight l2 dropout etc can cause the net to underfit if you be use a pretraine model make sure you be use the same normalization and preprocessing as the model be when train for example should an image pixel be in the range 0 1 1 1 or 0 255 cs231n point out a common pitfall also check for different preprocessing in each sample or batch this will help with find where the issue be for example if the target output be an object class and coordinate try limit the prediction to object class only again from the excellent cs231n initialize with small parameter without regularization for example if we have 10 class at chance mean we will get the correct class 10 % of the time and the softmax loss be the negative log probability of the correct class so ln 0 1 = 2 302 after this try increase the regularization strength which should increase the loss if you implement your own loss function check it for bug and add unit test often my loss would be slightly incorrect and hurt the performance of the network in a subtle way if you be use a loss function provide by your framework make sure you be pass to it what it expect for example in pytorch I would mix up the nllloss and crossentropyloss as the former require a softmax input and the latter doesn t if your loss be compose of several small loss function make sure their magnitude relative to each be correct this might involve test different combination of loss weight sometimes the loss be not the good predictor of whether your network be train properly if you can use other metric like accuracy do you implement any of the layer in the network yourself check and double check to make sure they be work as intend check if you unintentionally disabled gradient update for some layer variable that should be learnable maybe the expressive power of your network be not enough to capture the target function try add more layer or more hide unit in fully connect layer if your input look like k h w = 64 64 64 it s easy to miss error relate to wrong dimension use weird number for input dimension for example different prime number for each dimension and check how they propagate through the network if you implement gradient descent by hand gradient checking make sure that your backpropagation work like it should more info 1 2 3 overfit a small subset of the datum and make sure it work for example train with just 1 or 2 example and see if your network can learn to differentiate these move on to more sample per class if unsure use xavier or he initialization also your initialization might be lead you to a bad local minimum so try a different initialization and see if it help maybe you use a particularly bad set of hyperparameter if feasible try a grid search too much regularization can cause the network to underfit badly reduce regularization such as dropout batch norm weight bias l2 regularization etc in the excellent practical deep learning for coder course jeremy howard advise get rid of underfitte first this mean you overfit the training datum sufficiently and only then address overfitte maybe your network need more time to train before it start make meaningful prediction if your loss be steadily decrease let it train some more some framework have layer like batch norm dropout and other layer behave differently during training and testing switching to the appropriate mode might help your network to predict properly your choice of optimizer shouldn t prevent your network from training unless you have select particularly bad hyperparameter however the proper optimizer for a task can be helpful in get the most training in the short amount of time the paper which describe the algorithm you be use should specify the optimizer if not I tend to use adam or plain sgd with momentum check this excellent post by sebastian ruder to learn more about gradient descent optimizer a low learning rate will cause your model to converge very slowly a high learning rate will quickly decrease the loss in the beginning but might have a hard time find a good solution play around with your current learning rate by multiply it by 0 1 or 10 get a nan non a number be a much big issue when training rnn from what I hear some approach to fix it do I miss anything be anything wrong let I know by leave a reply below from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur hacker machine learn deep learning and other type of learn
Slav Ivanov,2.9K,9,https://blog.slavv.com/picking-a-gpu-for-deep-learning-3d4795c273b9?source=tag_archive---------4----------------,pick a gpu for deep learning slav,quite a few people have ask I recently about choose a gpu for machine learning as it stand success with deep learning heavily dependent on have the right hardware to work with when I be build my personal deep learning box I review all the gpu on the market in this article I m go to share my insight about choose the right graphic processor also we ll go over deep learning dl be part of the field of machine learn ml dl work by approximate a solution to a problem use neural network one of the nice property of about neural network be that they find pattern in the datum feature by themselves this be oppose to have to tell your algorithm what to look for as in the olde time however often this mean the model start with a blank state unless we be transfer learn to capture the nature of the datum from scratch the neural net need to process a lot of information there be two way to do so — with a cpu or a gpu the main computational module in a computer be the central processing unit well know as cpu it be design to do computation rapidly on a small amount of datum for example multiply a few number on a cpu be blazingly fast but it struggle when operate on a large amount of data e g multiply matrix of ten or hundred thousand number behind the scene dl be mostly comprise of operation like matrix multiplication amusingly 3d computer game rely on these same operation to render that beautiful landscape you see in rise of the tomb raider thus gpus be develop to handle lot of parallel computation use thousand of core also they have a large memory bandwidth to deal with the datum for these computation this make they the ideal commodity hardware to do dl on or at least until asic for machine learning like google s tpu make their way to market for I the most important reason for pick a powerful graphic processor be save time while prototyping model if the network train fast the feedback time will be short thus it would be easy for my brain to connect the dot between the assumption I have for the model and its result see tim dettmer answer to why be gpu well suited to deep learning on quora for a well explanation also for an in depth albeit slightly outdated gpu comparison see his article which gpu s to get for deep learning there be main characteristic of a gpu relate to dl be there be two reason for have multiple gpu you want to train several model at once or you want to do distribute training of a single model we ll go over each one train several model at once be a great technique to test different prototype and hyperparameter it also shorten your feedback cycle and let you try out many thing at once distribute training or train a single network on several video card be slowly but surely gain traction nowadays there be easy to use approach to this for tensorflow and keras via horovod cntk and pytorch the distribute training library offer almost linear speed up to the number of card for example with 2 gpu you get 1 8x fast training pcie lane update the caveat to use multiple video card be that you need to be able to feed they with datum for this purpose each gpu should have 16 pcie lane available for data transfer tim dettmer point out that have 8 pcie lane per card should only decrease performance by 0 10 % for two gpu for a single card any desktop processor and chipset like intel i5 7500 and asus tuf z270 will use 16 lane however for two gpu you can go 8x 8x lane or get a processor and a motherboard that support 32 pcie lane 32 lane be outside the realm of desktop cpu an intel xeon with a msi — x99a sli plus will do the job for 3 or 4 gpu go with 8x lane per card with a xeon with 24 to 32 pcie lane to have 16 pcie lane available for 3 or 4 gpu you need a monstrous processor something in the class of or amd threadripper 64 lane with a corresponding motherboard also for more gpu you need a fast processor and hard disk to be able to feed they datum quickly enough so they don t sit idle nvidia have be focus on deep learning for a while now and the head start be pay off their cuda toolkit be deeply entrench it work with all major dl framework — tensoflow pytorch caffe cntk etc as of now none of these work out of the box with opencl cuda alternative which run on amd gpu I hope support for opencl come soon as there be great inexpensive gpu from amd on the market also some amd card support half precision computation which double their performance and vram size currently if you want to do dl and want to avoid major headache choose nvidia your gpu need a computer around it hard disk first you need to read the datum off the disk an ssd be recommend here but an hdd can work as well cpu that datum might have to be decode by the cpu e g jpeg fortunately any mid range modern processor will do just fine motherboard the data pass via the motherboard to reach the gpu for a single video card almost any chipset will work if you be plan on work with multiple graphic card read this section ram it be recommend to have 2 gigabyte of memory for every gigabyte of video card ram have more certainly help in some situation like when you want to keep an entire dataset in memory power supply it should provide enough power for the cpu and the gpu plus 100 watt extra you can get all of this for $ 500 to $ 1000 or even less if you buy a use workstation here be performance comparison between all card check the individual card profile below notably the performance of titan xp and gtx 1080 ti be very close despite the huge price gap between they the price comparison reveal that gtx 1080 ti gtx 1070 and gtx 1060 have great value for the compute performance they provide all the card be in the same league value wise except titan xp the king of the hill when every gb of vram matter this card have more than any other on the consumer market it s only a recommend buy if you know why you want it for the price of titan x you could get two gtx 1080 which be a lot of power and 16 gbs of vram this card be what I currently use it s a great high end option with lot of ram and high throughput very good value I recommend this gpu if you can afford it it work great for computer vision or kaggle competition quite capable mid to high end card the price be reduce from $ 700 to $ 550 when 1080 ti be introduce 8 gb be enough for most computer vision task people regularly compete on kaggle with these the new card in nvidia s lineup if 1080 be over budget this will get you the same amount of vram 8 gb also 80 % of the performance for 80 % of the price pretty sweet deal it s hard to get these nowadays because they be use for cryptocurrency mining with a considerable amount of vram for this price but somewhat slow if you can get it or a couple second hand at a good price go for it it s quite cheap but 6 gb vram be limit that s probably the minimum you want to have if you be do computer vision it will be okay for nlp and categorical datum model also available as p106 100 for cryptocurrency mining but it s the same card without a display output the entry level card which will get you start but not much more still if you be unsure about get in deep learning this might be a cheap way to get your foot wet titan x pascal it use to be the good consumer gpu nvidia have to offer make obsolete by 1080 ti which have the same spec and be 40 % cheap tesla gpusthis include k40 k80 which be 2x k40 in one p100 and other you might already be use these via amazon web service google cloud platform or another cloud provider in my previous article I do some benchmark on gtx 1080 ti vs k40 the 1080 perform five time fast than the tesla card and 2 5x fast than k80 k40 have 12 gb vram and k80 a whopping 24 gbs in theory the p100 and gtx 1080 ti should be in the same league performance wise however this cryptocurrency comparison have p100 lagging in every benchmark it be worth note that you can do half precision on p100 effectively double the performance and vram size on top of all this k40 go for over $ 2000 k80 for over $ 3000 and p100 be about $ 4500 and they get still get eat alive by a desktop grade card obviously as it stand I don t recommend get they all the spec in the world win t help you if you don t know what you be look for here be my gpu recommendation depend on your budget I have over $ 1000 get as many gtx 1080 ti or gtx 1080 as you can if you have 3 or 4 gpu run in the same box beware of issue with feed they with datum also keep in mind the airflow in the case and the space on the motherboard I have $ 700 to $ 900 gtx 1080 ti be highly recommend if you want to go multi gpu get 2x gtx 1070 if you can find they or 2x gtx 1070 ti kaggle here I come I have $ 400 to $ 700 get the gtx 1080 or gtx 1070 ti maybe 2x gtx 1060 if you really want 2 gpu however know that 6 gb per model can be limit I have $ 300 to $ 400 gtx 1060 will get you start unless you can find a use gtx 1070 I have less than $ 300 get gtx 1050 ti or save for gtx 1060 if you be serious about deep learn deep learning have the great promise of transform many area of our life unfortunately learn to wield this powerful tool require good hardware hopefully I ve give you some clarity on where to start in this quest disclosure the above be affiliate link to help I pay for well more gpu from a quick cheer to a stand ovation clap to show how much you enjoy this story entrepreneur hacker machine learn deep learning and other type of learn
gk_,1.8K,6,https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6?source=tag_archive---------5----------------,text classification use neural network machine learning,understand how chatbots work be important a fundamental piece of machinery inside a chat bot be the text classifier let s look at the inner working of an artificial neural network ann for text classification we ll use 2 layer of neuron 1 hide layer and a bag of word approach to organize our training data text classification come in 3 flavor pattern matching algorithm neural net while the algorithmic approach use multinomial naive baye be surprisingly effective it suffer from 3 fundamental flaw as with its naive counterpart this classifier isn t attempt to understand the meaning of a sentence it s try to classify it in fact so call ai chat bot do not understand language but that s another story let s examine our text classifier one section at a time we will take the follow step the code be here we re use ipython notebook which be a super productive way of work on data science project the code syntax be python we begin by import our natural language toolkit we need a way to reliably tokenize sentence into word and a way to stem word and our training data 12 sentence belong to 3 class intent we can now organize our data structure for document class and word notice that each word be stem and lower cased stemming help the machine equate word like have and have we don t care about case our training datum be transform into bag of word for each sentence the above step be a classic in text classification each training sentence be reduce to an array of 0 s and 1 s against the array of unique word in the corpus be stem then transform to input a 1 for each word in the bag the be ignore and output the first class note that a sentence could be give multiple class or none make sure the above make sense and play with the code until you grok it next we have our core function for our 2 layer neural network if you be new to artificial neural network here be how they work we use numpy because we want our matrix multiplication to be fast we use a sigmoid function to normalize value and its derivative to measure the error rate iterating and adjust until our error rate be acceptably low also below we implement our bag of word function transform an input sentence into an array of 0 s and 1 s this match precisely with our transform for training datum always crucial to get this right and now we code our neural network training function to create synaptic weight don t get too excited this be mostly matrix multiplication — from middle school math class we be now ready to build our neural network model we will save this as a json structure to represent our synaptic weight you should experiment with different alpha gradient descent parameter and see how it affect the error rate this parameter help our error adjustment find the low error rate synapse_0 + = alpha * synapse_0_weight_update we use 20 neuron in our hidden layer you can adjust this easily these parameter will vary depend on the dimension and shape of your training data tune they down to ~10^ 3 as a reasonable error rate the synapse json file contain all of our synaptic weight this be our model this classify function be all that s need for the classification once synapse weight have be calculate ~15 line of code the catch if there s a change to the training datum our model will need to be re calculate for a very large dataset this could take a non insignificant amount of time we can now generate the probability of a sentence belong to one or more of our class this be super fast because it s dot product calculation in our previously define think function experiment with other sentence and different probability you can then add training datum and improve expand the model notice the solid prediction with scant training datum some sentence will produce multiple prediction above a threshold you will need to establish the right threshold level for your application not all text classification scenario be the same some predictive situation require more confidence than other the last classification show some internal detail notice the bag of word bow for the sentence 2 word match our corpus the neural net also learn from the 0 s the non match word a low probability classification be easily show by provide a sentence where a common word be the only match for example here you have a fundamental piece of machinery for build a chat bot capable of handle a large # of class intent and suitable for class with limited or extensive training datum pattern add one or more response to an intent be trivial from a quick cheer to a stand ovation clap to show how much you enjoy this story philosopher entrepreneur investor understand how machine learning and artificial intelligence will change your work & life
nafrondel,1.7K,5,https://medium.com/@nafrondel/you-requested-someone-with-a-degree-in-this-holds-up-hand-d4bf18e96ff?source=tag_archive---------6----------------,you request someone with a degree in this * hold up hand *,you request someone with a degree in this * hold up hand * so there be two main school of artificial intelligence — symbolic and non symbolic symbolic say the good way to make ai be to make an expert ai — e g if you want a doctor ai you feed it medical text book and it answer question by look it up in the text book non symbolic say the good way to make ai be to decide that computer be well at understanding in computer so give the information to the ai and let it turn that in to something it understand as a bit of an apt aside — consider the chinese room think experiment imagine you put someone in a room with shelf full of book the book be fill with symbol and look up table and the person inside be tell you will be give a sheet of paper with symbol on use the book in the room to look up the symbol to write in reply then a person outside the room post message in to the room in mandarin and get message back in mandarin the person inside the room doesn t understand mandarin the knowledge be all in the book but to the person outside the room it look like they understand mandarin that be how symbolic ai work it have no inate knowledge of the subject mater it just follow instruction even if some if those instruction be to update the book non symbolic ai say that it d be well if the ai write the book itself so look back at the chinese room this be like teach the person in the room mandarin and the book be their study note the trouble be teach someone mandarin take time and effort as we re start with a blank slate here but consider that it take decade to teach a child their first language yet it take only a little more effort to teach they a second language so back to the ai — once we teach it one language we want it to be like the child we want it to be easy for it to learn a second language this be where artificial neural network come in these be our blank slate child they re make up of three part input neurone output the neurone be where the magic happen — they re model on brain they re a blob of neurone that can connect up to one another or cut link so they can join one bit of the brain up to another and let a signal go from one place to another this be what join the input up to the output and in the pavlovian way when something good happen the brain remember by strengthen the link between neurone but just like a baby these start out pretty much random so all you get out be baby babble but we don t want baby babble we have to teach it how to get from dog to chien not dog to goobababaa when teach the ann you give it an input and if the output be wrong give it a tap on the nose and the neurone remember whatever we just do be wrong don t do it again by decrease the value it have on the link between the neurone that lead to the wrong answer and of it get it right give it a rub on the head and it do the opposite it increase the number mean it ll be more likely to take that path next time this mean that over time it ll join up the input dog to the output chien so how do this explain the article well anns work in both direction we can give it output and it ll give we back input by follow the path of neurone back in the opposite direction so by teach it dog mean chien it also know chien could mean dog that also mean we can teach it that perro mean dog when we re speak spanish so when we teach it the fast way for it to go from perro to dog be to follow the same path that take chien to dog mean over time it will pull the neurone link chien and dog close to perro as well which link perro to chien as well this three way link in the middle of perro dog and chien be the language the google ai be create for itself back up a bit to our imaginary child learn a new language when they learn their first language e g english they don t write an english dictionary in their head they hear the word and map they to an idea that the word represent this be why people frequently misquote film they remember what the quote mean not what the word be so when the child learn a second language they hear chien as be french but map it to the idea of dog then when they hear perro they hear it as spanish but map that to the idea of dog too this mean the child only have to learn about the idea of a dog once but can then link that idea up to many language or synonym for dog and this be what the google ai be do instead of think if dog = chien and chien = perro perro must = dog it think dog=0x3b chien = 0x3b perro=0x3b where 0x3b be the idea of dog mean it can then turn 0x3b in to whichever language you ask for tl;dr it wasn t big news because artificial neural network have be do this since they be invent in the 40 and the entire non symbolic branch of ai be all about have computer invent their own language to understand and learn thing p s it really be smart enough to warrant that excitement most people have no idea how much they rely on ai from the relatively simple ai that run their washing machine to the ai that read the address hand write on mail and then figure out the good way to deliver it these be real everyday machine make decision for we even your computer mouse have ai in it to determine what you want to point at rather than what you actually point at on a 1080p screen there be 2 million point you could click on it s not by accident that it s pretty easy to pick the correct one mobile phone constantly run ai to decide which phone tower to connect to while the backbone of the internet be a huge interconnected ai decide the fast way to get datum from one computer to another thinking decision making ai be in our hand beneath our foot in our car and almost every electronic device we have the robot have already take over ; from a quick cheer to a stand ovation clap to show how much you enjoy this story
Neelabh Pant,2K,11,https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f?source=tag_archive---------7----------------,a guide for time series prediction use recurrent neural network lstms,the statsbot team have already publish the article about use time series analysis for anomaly detection today we d like to discuss time series prediction with a long short term memory model lstms we ask a data scientist neelabh pant to tell you about his experience of forecasting exchange rate use recurrent neural network as an indian guy live in the us I have a constant flow of money from home to I and vice versa if the usd be strong in the market then the indian rupee inr go down hence a person from india buy a dollar for more rupee if the dollar be weak you spend less rupee to buy the same dollar if one can predict how much a dollar will cost tomorrow then this can guide one s decision making and can be very important in minimize risk and maximize return look at the strength of a neural network especially a recurrent neural network I come up with the idea of predict the exchange rate between the usd and the inr there be a lot of method of forecasting exchange rate such as in this article we ll tell you how to predict the future exchange rate behavior use time series analysis and by make use of machine learning with time series let we begin by talk about sequence problem the simple machine learning problem involve a sequence be a one to one problem in this case we have one datum input or tensor to the model and the model generate a prediction with the give input linear regression classification and even image classification with convolutional network fall into this category we can extend this formulation to allow for the model to make use of the pass value of the input and the output it be know as the one to many problem the one to many problem start like the one to one problem where we have an input to the model and the model generate one output however the output of the model be now feed back to the model as a new input the model now can generate a new output and we can continue like this indefinitely you can now see why these be know as recurrent neural network a recurrent neural network deal with sequence problem because their connection form a direct cycle in other word they can retain state from one iteration to the next by use their own output as input for the next step in programming term this be like run a fix program with certain input and some internal variable the simple recurrent neural network can be view as a fully connect neural network if we unroll the time axis in this univariate case only two weight be involve the weight multiply the current input xt which be u and the weight multiply the previous output yt 1 which be w this formula be like the exponential weight move average ewma by make its pass value of the output with the current value of the input one can build a deep recurrent neural network by simply stack unit to one another a simple recurrent neural network work well only for a short term memory we will see that it suffer from a fundamental problem if we have a long time dependency as we have talk about a simple recurrent network suffer from a fundamental problem of not be able to capture long term dependency in a sequence this be a problem because we want our rnn to analyze text and answer question which involve keep track of long sequence of word in late 90 lstm be propose by sepp hochreiter and jurgen schmidhuber which be relatively insensitive to gap length over alternative rnn hide markov model and other sequence learning method in numerous application this model be organize in cell which include several operation lstm have an internal state variable which be pass from one cell to another and modify by operation gate 1 forget gate it be a sigmoid layer that take the output at t 1 and the current input at time t and concatenate they into a single tensor and apply a linear transformation follow by a sigmoid because of the sigmoid the output of this gate be between 0 and 1 this number be multiply with the internal state and that be why the gate be call a forget gate if ft=0 then the previous internal state be completely forget while if ft=1 it will be pass through unaltered 2 input gate the input gate take the previous output and the new input and pass they through another sigmoid layer this gate return a value between 0 and 1 the value of the input gate be multiply with the output of the candidate layer this layer apply a hyperbolic tangent to the mix of input and previous output return a candidate vector to be add to the internal state the internal state be update with this rule the previous state be multiply by the forget gate and then add to the fraction of the new candidate allow by the output gate 3 output gate this gate control how much of the internal state be pass to the output and it work in a similar way to the other gate these three gate describe above have independent weight and bias hence the network will learn how much of the past output to keep how much of the current input to keep and how much of the internal state to send out to the output in a recurrent neural network you not only give the network the datum but also the state of the network one moment before for example if I say hey something crazy happen to I when I be drive there be a part of your brain that be flip a switch that s say oh this be a story neelabh be tell I it be a story where the main character be neelabh and something happen on the road now you carry a little part of that one sentence I just tell you as you listen to all my other sentence you have to keep a bit of information from all past sentence around in order to understand the entire story another example be video processing where you would again need a recurrent neural network what happen in the current frame be heavily dependent upon what be in the last frame of the movie most of the time over a period of time a recurrent neural network try to learn what to keep and how much to keep from the past and how much information to keep from the present state which make it so powerful as compare to a simple feed forward neural network I be impressed with the strength of a recurrent neural network and decide to use they to predict the exchange rate between the usd and the inr the dataset use in this project be the exchange rate datum between january 2 1980 and august 10 2017 later I ll give you a link to download this dataset and experiment with it the dataset display the value of $ 1 in rupee we have a total of 13 730 record start from january 2 1980 to august 10 2017 over the period the price to buy $ 1 in rupee have be rise one can see that there be a huge dip in the american economy during 2007 2008 which be hugely cause by the great recession during that period it be a period of general economic decline observe in world market during the late 2000 and early 2010s this period be not very good for the world s develop economy particularly in north america and europe include russia which fall into a definitive recession many of the new develop economy suffer far less impact particularly china and india whose economy grow substantially during this period now to train the machine we need to divide the dataset into test and training set it be very important when you do time series to split train and test with respect to a certain date so you don t want your test datum to come before your training datum in our experiment we will define a date say january 1 2010 as our split date the training data be the datum between january 2 1980 and december 31 2009 which be about 11 000 training data point the test dataset be between january 1 2010 and august 10 2017 which be about 2 700 point the next thing to do be normalize the dataset you only need to fit and transform your training datum and just transform your test datum the reason you do that be you don t want to assume that you know the scale of your test datum normalize or transform the data mean that the new scale variable will be between zero and one a fully connect model be a simple neural network model which be build as a simple regression model that will take one input and will spit out one output this basically take the price from the previous day and forecast the price of the next day as a loss function we use mean square error and stochastic gradient descent as an optimizer which after enough number of epoch will try to look for a good local optimum below be the summary of the fully connect layer after train this model for 200 epoch or early_callback whichever come first the model try to learn the pattern and the behavior of the datum since we split the datum into training and testing set we can now predict the value of testing datum and compare they with the ground truth as you can see the model be not good it essentially be repeat the previous value and there be a slight shift the fully connect model be not able to predict the future from the single previous value let we now try use a recurrent neural network and see how well it do the recurrent model we have use be a one layer sequential model we use 6 lstm node in the layer to which we give input of shape 1 1 which be one input give to the network with one value the last layer be a dense layer where the loss be mean squared error with stochastic gradient descent as an optimizer we train this model for 200 epoch with early_stoppe callback the summary of the model be show above this model have learn to reproduce the yearly shape of the datum and doesn t have the lag it use to have with a simple feed forward neural network it be still underestimate some observation by certain amount and there be definitely room for improvement in this model there can be a lot of change to be make in this model to make it well one can always try to change the configuration by change the optimizer another important change I see be by use the slide time window method which come from the field of stream datum management system this approach come from the idea that only the most recent datum be important one can show the model datum from a year and try to make a prediction for the first day of the next year slide time window method be very useful in term of fetch important pattern in the dataset that be highly dependent on the past bulk of observation try to make change to this model as you like and see how the model react to those change I make the dataset available on my github account under deep learning in python repository feel free to download the dataset and play with it I personally follow some of my favorite data scientist like kirill eremenko jose portilla dan van boxel well know as dan do datum and many more most of they be available on different podcast station where they talk about different current subject like rnn convolutional neural network lstm and even the most recent technology neural ture machine try to keep up with the news of different artificial intelligence conference by the way if you be interested then kirill eremenko be come to san diego this november with his amazing team to give talk on machine learn neural network and datum science lstm model be powerful enough to learn the most important past behavior and understand whether or not those past behavior be important feature in make future prediction there be several application where lstms be highly use application like speech recognition music composition handwriting recognition and even in my current research of human mobility and travel prediction accord to I lstm be like a model which have its own memory and which can behave like an intelligent human in make decision thank you again and happy machine learning from a quick cheer to a stand ovation clap to show how much you enjoy this story I love data science let s build some intelligent bot together ; data story on machine learning and analytic from statsbot s maker
Eugenio Culurciello,2.2K,15,https://towardsdatascience.com/neural-network-architectures-156e5bad51ba?source=tag_archive---------8----------------,neural network architecture towards data science,deep neural network and deep learning be powerful and popular algorithm and a lot of their success lay in the careful design of the neural network architecture I want to revisit the history of neural network design in the last few year and in the context of deep learning for a more in depth analysis and comparison of all the network report here please see our recent article one representative figure from this article be here report top 1 one crop accuracy versus amount of operation require for a single forward pass in multiple popular neural network architecture it be the year 1994 and this be one of the very first convolutional neural network and what propel the field of deep learning this pioneering work by yann lecun be name lenet5 after many previous successful iteration since the year 1988 the lenet5 architecture be fundamental in particular the insight that image feature be distribute across the entire image and convolution with learnable parameter be an effective way to extract similar feature at multiple location with few parameter at the time there be no gpu to help training and even cpus be slow therefore be able to save parameter and computation be a key advantage this be in contrast to use each pixel as a separate input of a large multi layer neural network lenet5 explain that those should not be use in the first layer because image be highly spatially correlated and use individual pixel of the image as separate input feature would not take advantage of these correlation lenet5 feature can be summarize as in overall this network be the origin of much of the recent architecture and a true inspiration for many people in the field in the year from 1998 to 2010 neural network be in incubation most people do not notice their increase power while many other researcher slowly progress more and more datum be available because of the rise of cell phone camera and cheap digital camera and computing power be on the rise cpus be become fast and gpu become a general purpose computing tool both of these trend make neural network progress albeit at a slow rate both datum and computing power make the task that neural network tackle more and more interesting and then it become clear in 2010 dan claudiu ciresan and jurgen schmidhuber publish one of the very fist implementation of gpu neural net this implementation have both forward and backward implement on a a nvidia gtx 280 graphic processor of an up to 9 layer neural network in 2012 alex krizhevsky release alexnet which be a deep and much wide version of the lenet and win by a large margin the difficult imagenet competition alexnet scale the insight of lenet into a much large neural network that could be use to learn much more complex object and object hierarchy the contribution of this work be at the time gpu offer a much large number of core than cpus and allow 10x fast training time which in turn allow to use large dataset and also big image the success of alexnet start a small revolution convolutional neural network be now the workhorse of deep learning which become the new name for large neural network that can now solve useful task in december 2013 the nyu lab from yann lecun come up with overfeat which be a derivative of alexnet the article also propose learn bounding box which later give rise to many other paper on the same topic I believe it be well to learn to segment object rather than learn artificial bounding box the vgg network from oxford be the first to use much small 3×3 filter in each convolutional layer and also combine they as a sequence of convolution this seem to be contrary to the principle of lenet where large convolution be use to capture similar feature in an image instead of the 9×9 or 11×11 filter of alexnet filter start to become small too dangerously close to the infamous 1×1 convolution that lenet want to avoid at least on the first layer of the network but the great advantage of vgg be the insight that multiple 3×3 convolution in sequence can emulate the effect of large receptive field for example 5×5 and 7×7 these idea will be also use in more recent network architecture as inception and resnet the vgg network use multiple 3x3 convolutional layer to represent complex feature notice block 3 4 5 of vgg e 256×256 and 512×512 3×3 filter be use multiple time in sequence to extract more complex feature and the combination of such feature this be effectively like have large 512×512 classifier with 3 layer which be convolutional this obviously amount to a massive number of parameter and also learn power but training of these network be difficult and have to be split into small network with layer add one by one all this because of the lack of strong way to regularize the model or to somehow restrict the massive search space promote by the large amount of parameter vgg use large feature size in many layer and thus inference be quite costly at run time reduce the number of feature as do in inception bottleneck will save some of the computational cost network in network nin have the great and simple insight of use 1x1 convolution to provide more combinational power to the feature of a convolutional layer the nin architecture use spatial mlp layer after each convolution in order to well combine feature before another layer again one can think the 1x1 convolution be against the original principle of lenet but really they instead help to combine convolutional feature in a well way which be not possible by simply stack more convolutional layer this be different from use raw pixel as input to the next layer here 1×1 convolution be use to spatially combine feature across feature map after convolution so they effectively use very few parameter share across all pixel of these feature the power of mlp can greatly increase the effectiveness of individual convolutional feature by combine they into more complex group this idea will be later use in most recent architecture as resnet and inception and derivative nin also use an average pooling layer as part of the last classifier another practice that will become common this be do to average the response of the network to multiple be of the input image before classification christian szegedy from google begin a quest aim at reduce the computational burden of deep neural network and devise the googlenet the first inception architecture by now fall 2014 deep learning model be become extermely useful in categorize the content of image and video frame most skeptic have give in that deep learning and neural net come back to stay this time give the usefulness of these technique the internet giant like google be very interested in efficient and large deployment of architecture on their server farm christian think a lot about way to reduce the computational burden of deep neural net while obtain state of art performance on imagenet for example or be able to keep the computational cost the same while offer improve performance he and his team come up with the inception module which at a first glance be basically the parallel combination of 1×1 3×3 and 5×5 convolutional filter but the great insight of the inception module be the use of 1×1 convolutional block nin to reduce the number of feature before the expensive parallel block this be commonly refer as bottleneck this deserve its own section to explain see bottleneck layer section below googlenet use a stem without inception module as initial layer and an average pooling plus softmax classifier similar to nin this classifier be also extremely low number of operation compare to the one of alexnet and vgg this also contribute to a very efficient network design inspire by nin the bottleneck layer of inception be reduce the number of feature and thus operation at each layer so the inference time could be keep low before pass datum to the expensive convolution module the number of feature be reduce by say 4 time this lead to large saving in computational cost and the success of this architecture let s examine this in detail let s say you have 256 feature come in and 256 come out and let s say the inception layer only perform 3x3 convolution that be 256x256 x 3x3 convolution that have to be perform 589 000s multiply accumulate or mac operation that may be more than the computational budget we have say to run this layer in 0 5 milli second on a google server instead of do this we decide to reduce the number of feature that will have to be convolve say to 64 or 256 4 in this case we first perform 256 > 64 1×1 convolution then 64 convolution on all inception branch and then we use again a 1x1 convolution from 64 > 256 feature back again the operation be now for a total of about 70 000 versus the almost 600 000 we have before almost 10x less operation and although we be do less operation we be not lose generality in this layer in fact the bottleneck layer have be prove to perform at state of art on the imagenet dataset for example and will be also use in later architecture such as resnet the reason for the success be that the input feature be correlate and thus redundancy can be remove by combine they appropriately with the 1x1 convolution then after convolution with a small number of feature they can be expand again into meaningful combination for the next layer christian and his team be very efficient researcher in february 2015 batch normalize inception be introduce as inception v2 batch normalization compute the mean and standard deviation of all feature map at the output of a layer and normalize their response with these value this correspond to whiten the datum and thus make all the neural map have response in the same range and with zero mean this help training as the next layer do not have to learn offset in the input datum and can focus on how to well combine feature in december 2015 they release a new version of the inception module and the corresponding architecture this article well explain the original googlenet architecture give a lot more detail on the design choice a list of the original idea be inception still use a pooling layer plus softmax as final classifier the revolution then come in december 2015 at about the same time as inception v3 resnet have a simple idea feed the output of two successive convolutional layer and also bypass the input to the next layer this be similar to old idea like this one but here they bypass two layer and be apply to large scale bypass after 2 layer be a key intuition as bypass a single layer do not give much improvement by 2 layer can be think as a small classifier or a network in network this be also the very first time that a network of > hundred even 1000 layer be train resnet with a large number of layer start to use a bottleneck layer similar to the inception bottleneck this layer reduce the number of feature at each layer by first use a 1x1 convolution with a small output usually 1 4 of the input and then a 3x3 layer and then again a 1x1 convolution to a large number of feature like in the case of inception module this allow to keep the computation low while provide rich combination of feature see bottleneck layer section after googlenet and inception resnet use a fairly simple initial layer at the input stem a 7x7 conv layer follow with a pool of 2 contrast this to more complex and less intuitive stem as in inception v3 v4 resnet also use a pooling layer plus softmax as final classifier additional insight about the resnet architecture be appear every day and christian and team be at it again with a new version of inception the inception module after the stem be rather similar to inception v3 they also combine the inception module with the resnet module this time though the solution be in my opinion less elegant and more complex but also full of less transparent heuristic it be hard to understand the choice and it be also hard for the author to justify they in this regard the prize for a clean and simple network that can be easily understand and modify now go to resnet squeezenet have be recently release it be a re hash of many concept from resnet and inception and show that after all a well design of architecture will deliver small network size and parameter without need complex compression algorithm our team set up to combine all the feature of the recent architecture into a very efficient and light weight network that use very few parameter and computation to achieve state of the art result this network architecture be dub enet and be design by adam paszke we have use it to perform pixel wise labeling and scene parse here be some video of enet in action these video be not part of the training dataset the technical report on enet be available here enet be a encoder plus decoder network the encoder be a regular cnn design for categorization while the decoder be a upsampling network design to propagate the category back into the original image size for segmentation this work use only neural network and no other algorithm to perform image segmentation as you can see in this figure enet have the high accuracy per parameter use of any neural network out there enet be design to use the minimum number of resource possible from the start as such it achieve such a small footprint that both encoder and decoder network together only occupy 0 7 mb with fp16 precision even at this small size enet be similar or above other pure neural network solution in accuracy of segmentation a systematic evaluation of cnn module have be present the find out that be advantageous to use • use elu non linearity without batchnorm or relu with it • apply a learn colorspace transformation of rgb • use the linear learning rate decay policy • use a sum of the average and max pooling layer • use mini batch size around 128 or 256 if this be too big for your gpu decrease the learning rate proportionally to the batch size • use fully connect layer as convolutional and average the prediction for the final decision • when invest in increase training set size check if a plateau have not be reach • cleanliness of the data be more important then the size • if you can not increase the input image size reduce the stride in the con sequent layer it have roughly the same effect • if your network have a complex and highly optimize architecture like e g googlenet be careful with modification xception improve on the inception module and architecture with a simple and more elegant architecture that be as effective as resnet and inception v4 the xception module be present here this network can be anyone s favorite give the simplicity and elegance of the architecture present here the architecture have 36 convolutional stage make it close in similarity to a resnet 34 but the model and code be as simple as resnet and much more comprehensible than inception v4 a torch7 implementation of this network be available here an implementation in keras tf be availble here it be interesting to note that the recent xception architecture be also inspire by our work on separable convolutional filter a new mobilenet architecture be also available since april 2017 this architecture use separable convolution to reduce the number of parameter the separate convolution be the same as xception above now the claim of the paper be that there be a great reduction in parameter — about 1 2 in case of facenet as report in the paper here be the complete model architecture unfortunately we have test this network in actual application and find it to be abysmally slow on a batch of 1 on a titan xp gpu look at a comparison here of inference time per image clearly this be not a contender in fast inference it may reduce the parameter and size of network on disk but be not usable fractalnet use a recursive architecture that be not test on imagenet and be a derivative or the more general resnet we believe that craft neural network architecture be of paramount importance for the progress of the deep learning field our group highly recommend read carefully and understand all the paper in this post but one could now wonder why we have to spend so much time in craft architecture and why instead we do not use datum to tell we what to use and how to combine module this would be nice but now it be work in progress some initial interesting result be here note also that here we mostly talk about architecture for computer vision similarly neural network architecture develop in other area and it be interesting to study the evolution of architecture for all other task also if you be interested in a comparison of neural network architecture and computational performance see our recent paper this post be inspire by discussion with abhishek chaurasia adam paszke sangpil kim alfredo canziani and other in our e lab at purdue university I have almost 20 year of experience in neural network in both hardware and software a rare combination see about I here medium webpage scholar linkedin and more if you find this article useful please consider a donation to support more tutorial and blog any contribution can make a difference from a quick cheer to a stand ovation clap to show how much you enjoy this story I dream and build new technology sharing concept idea and code
Gary Marcus,1.3K,27,https://medium.com/@GaryMarcus/in-defense-of-skepticism-about-deep-learning-6e8bfd5ae0f1?source=tag_archive---------0----------------,in defense of skepticism about deep learning gary marcus medium,in a recent appraisal of deep learning marcus 2018 I outline ten challenge for deep learning and suggest that deep learning by itself although useful be unlikely to lead on its own to artificial general intelligence I suggest instead the deep learning be view not as a universal solvent but simply as one tool among many in place of pure deep learning I call for hybrid model that would incorporate not just supervised form of deep learning but also other technique as well such as symbol manipulation and unsupervised learn itself possibly reconceptualize I also urge the community to consider incorporate more innate structure into ai system within a few day thousand of people have weigh in over twitter some enthusiastic e g the good discussion of # deeplearne and # ai I ve read in many year some not thoughtful but mostly wrong nevertheless because I think clarity around these issue be so important I ve compile a list of fourteen commonly ask query where do unsupervise learning fit in why didn t I say more nice thing about deep learn what give I the right to talk about this stuff in the first place what s up with ask a neural network to generalize from even number to odd number hint that s the most important one and lot more I haven t address literally every question I have see but I have try to be representative 1 what be general intelligence thomas dietterich an eminent professor of machine learning and my most thorough and explicit critic thus far give a nice answer that I be very comfortable with 2 marcus wasn t very nice to deep learning he should have say more nice thing about all of its vast accomplishment and he minimize other dietterich mention above make both of these point write on the first part of that true I could have say more positive thing but it s not like I didn t say any or even like I forgot to mention dietterich s good example ; I mention it on the first page more generally later in the article I cite a couple of great text and excellent blog that have pointer to numerous example a lot of they though would not really count as agi which be the main focus of my paper google translate for example be extremely impressive but it s not general ; it can t for example answer question about what it have translate the way a human translator could the second part be more substantive be 1 000 category really very finite well yes compare to the flexibility of cognition cognitive scientist generally place the number of atomic concept know by an individual as be on the order of 50 000 and we can easily compose those into a vastly great number of complex thought pet and fish be probably count in those 50 000 ; pet fish which be something different probably isn t count and I can easily entertain the concept of a pet fish that be suffer from ick or note that it be always disappointing to buy a pet fish only to discover that it be infect with ick an experience that I have as a child and evidently still resent how many idea like that I can express it s a lot more than 1 000 I be not precisely sure how many visual category a person can recognize but suspect the math be roughly similar try google image on pet fish and you do ok ; try it on pet fish wear goggle and you mostly find dog wear goggle with a false alarm rate of over 80 % machine win over nonexpert human on distinguish similar dog breed but people win by a wide margin on interpret complex scene like what would happen to a skydiver who be wear a backpack rather than a parachute in focus on 1 000 category chunk the machine learning field be in my view do itself a disservice trade a short term feeling of success for a denial of hard more open end problem like scene and sentence comprehension that must eventually be address compare to the essentially infinite range of sentence and scene we can see and comprehend 1000 of anything really be small see also note 2 at bottom 3 marcus say deep learning be useless but it s great for many thing of course it be useful ; I never say otherwise only that a in its current supervised form deep learning might be approach its limit and b that those limit would stop short from full artificial general intelligence — unless maybe we start incorporate a bunch of other stuff like symbol manipulation and innateness the core of my conclusion be this 4 one thing that I don t understand — @garymarcus say that dl be not good for hierarchical structure but in @ylecun nature review paper say that that dl be particularly suit for exploit such hierarchy this be an astute question from ram shankar and I should have be a lot clear about the answer there be many different type of hierarchy one could think about deep learning be really good probably the good ever at the sort of feature wise hierarchy lecun talk about which I typically refer to as hierarchical feature detection ; you build line out of pixel letter out of line word out of letter and so forth kurzweil and hawkin have emphasize this sort of thing too and it really go back to hubel and wiesel 1959 in neuroscience experiment and to fukushima fukushima miyake & ito 1983 in ai fukushima in his neocognitron model hand wire his hierarchy of successively more abstract feature ; lecun and many other after show that at least in some case you don t have to hand engineer they but you don t have to keep track of the subcomponent you encounter along the way ; the top level system need not explicitly encode the structure of the overall output in term of which part be see along the way ; this be part of why a deep learning system can be fool into think a pattern of a black and yellow stripe be a school bus nguyen yosinski & clune 2014 that stripe pattern be strongly correlate with activation of the school bus output unit which be in turn correlate with a bunch of low level feature but in a typical image recognition deep network there be no fully realize representation of a school bus as be make up of wheel a chassis window etc virtually the whole spoof literature can be think of in these term note 3 the structural sense of hierarchy which I be discuss be different and focus around system that can make explicit reference to the part of large whole the classic illustration would be chomsky s sense of hierarchy in which a sentence be compose of increasingly complex grammatical unit e g use a novel phrase like the man who mistook his hamburger for a hot dog with a large sentence like the actress insist that she would not be outdo by the man who mistook his hamburger for a hot dog I don t think deep learning do well here e g in discern the relation between the actress the man and the misidentified hot dog though attempt have certainly be make even in vision the problem be not entirely licked ; hinton s recent capsule work sabour frosst & hinton 2017 for example be an attempt to build in more robust part whole direction for image recognition by use more structured network I see this as a good trend and one potential way to begin to address the spoof problem but also as a reflection of trouble with the standard deep learning approach 5 it s weird to discuss deep learning in the context of general ai general ai be not the goal of deep learning good twitter response to this come from university of quebec professor daniel lemire oh come on hinton bengio be openly go for a model of human intelligence second prize go to a math phd at google jeremy kun who counter the dubious claim that general ai be not the goal of deep learning with if that s true then deep learning expert sure let everyone believe it be without correct they andrew ng s recent harvard business review article which I cite imply that deep learning can do anything a person can do in a second thomas dietterich s tweet that say in part it be hard to argue that there be limit to dl jeremy howard worry that the idea that deep learning be overhype might itself be overhype and then suggest that every know limit have be counter deepmind s recent alphago paper see note 4 be position somewhat similarly with silver et al silver et al 2017 enthusiastically report that in that paper s conclude discussion not one of the 10 challenge to deep learning that I review be mention as I will discuss in a paper come out soon it s not actually a pure deep learning system but that s a story for another day the main reason people keep benchmarke their ai system against human be precisely because agi be the goal 6 what marcus say be a problem with supervised learning not deep learning yann lecun present a version of this in a comment on my facebook page the part about my allegedly not recognize lecun s recent work be well odd it s true that I couldn t find a good summary article to cite when I ask lecun he tell I by email that there wasn t one yet but I do mention his interest explicitly I also note that my conclusion be positive too although I express reservation about current approach to build unsupervised system I end optimistically what lecun s remark do get right be that many of the problem I address be a general problem with supervised learning not something unique to deep learning ; I could have be more clear about this many other supervised learning technique face similar challenge such as problem in generalization and dependence on massive datum set ; relatively little of what I say be unique to deep learning in my focus on assess deep learning at the five year resurgence mark I neglect to say that but it doesn t really help deep learning that other supervised learning technique be in the same boat if someone could come up with a truly impressive way of use deep learning in an unsupervised way a reassessment might be require but I don t see that unsupervised learning at least as it currently pursue particularly remedy the challenge I raise e g with respect to reason hierarchical representation transfer robustness and interpretability it s simply a promissory note note 5 as portland state and santa fe institute professor melanie mitchell s put it in a thus far unanswered tweet I would too in the meantime I see no principled reason to believe that unsupervised learning can solve the problem I raise unless we add in more abstract symbolic representation first 7 deep learning be not just convolutional network of the sort marcus critique it s essentially a new style of programming — differentiable programming — and the field be try to work out the reusable construct in this style we have some convolution pool lstm gan vae memory unit route unit etc — tom dietterich this seem in the context of dietterich s long series of tweet to have be propose as a criticism but I be puzzle by that as I be a fan of differentiable programming and say so perhaps the point be that deep learning can be take in a broad way in any event I would not equate deep learning and differentiable programming e g approach that I cite like neural ture machine and neural programming deep learning be a component of many differentiable system but such system also build in exactly the sort of element draw from symbol manipulation that I be and have be urge the field to integrate marcus 2001 ; marcus marblestone & dean 2014a ; marcus marblestone & dean 2014b include memory unit and operation over variable and other system like route unit stress in the more recent two essay if integrate all this stuff into deep learning be what get we to agi my conclusion quote below will have turn out to be dead on 8 now vs the future maybe deep learning doesn t work now but it s offspring will get we to agi possibly I do think that deep learning might play an important role in get we to agi if some key thing many not yet discover be add in first but what we add matter and whether it be reasonable to call some future system an instance of deep learning per se or more sensible to call the ultimate system a such and such that use deep learning depend on where deep learning fit into the ultimate solution maybe for example in truly adequate natural language understanding system symbol manipulation will play an equally large role as deep learning or an even large one part of the issue here be of course terminological a very good friend recently ask I why can t we just call anything that include deep learning deep learning even if it include symbol manipulation some enhancement to deep learning ought to work to which I respond why not call anything that include symbol manipulation symbol manipulation even if it include deep learning gradient base optimization should get its due but so should symbol manipulation which as yet be the only know tool for systematically represent and achieve high level abstraction bedrock to virtually all of the world s complex computer system from spreadsheet to programming environment to operating system eventually I conjecture credit will also be due to the inevitable marriage between the two hybrid system that bring together the two great idea of 20th century ai symbol processing and neural network both initially develop in the 1950 other new tool yet to be invent may be critical as well to a true acolyte of deep learning anything be deep learn no matter what it s incorporate and no matter how different it might be from current technique viva imperialism if you replace every transistor in a classic symbolic microprocessor with a neuron but keep the chip s logic entirely unchanged a true deep learning acolyte would still declare victory but we win t understand the principle drive eventual success if we lump everything together note 6 9 no machine can extrapolate it s not fair to expect a neural network to generalize from even number to odd number here s a function express over binary digit f 110 = 011 ; f 100 = 001 ; f 010 = 010 what s f 111 if you be an ordinary human you be probably go to guess 111 if you be neural network of the sort I discuss you probably win t if you have be tell many time that hide layer in neural network abstract function you should be a little bit surprised by this if you be a human you might think of the function as something like reversal easily express in a line of computer code if you be a neural network of a certain sort it s very hard to learn the abstraction of reversal in a way that extend from even in that context to odd but be that impossible certainly not if you have a prior notion of an integer try another this time in decimal f 4 = 8 ; f 6 = 12 what s f 5 none of my human reader would care that question happen to require you to extrapolate from even number to odd ; a lot of neural network would be flummox sure the function be undetermined by the sparse number of example like all function but it be interesting and important that most people would amid the infinite range of a priori possible induction would alight on f 5 = 10 and just as interesting that most standard multilayer perceptron represent the number as binary digit wouldn t that s tell we something but many people in the neural network community françois chollet be one very salient exception don t want to listen importantly recognize that a rule apply to any integer be roughly the same kind of generalization that allow one to recognize that a novel noun that can be use in one context can be use in a huge variety of other context from the first time I hear the word blicket use as an object I can guess that it will fit into a wide range of frame like I think I see a blicket I have a close encounter with a blicket and exceptionally large blicket frighten I etc and I can both generate and interpret such sentence without specific further training it doesn t matter whether blicket be or not similar in for example phonology to other word I have hear nor whether I pile on the adjective or use the word as a subject or an object if most machine learning ml paradigm have a problem with this we should have problem with most ml paradigm be I be fair well yes and no it s true that I be ask neural network to do something that violate their assumption a neural network advocate might for example say hey wait a minute in your reversal example there be three dimension in your input space represent the left binary digit the middle binary digit and rightmost binary digit the rightmost binary digit have only be a zero in the training ; there be no way a network can know what to do when you get to one in that position for example vincent lostenlan a postdoc at cornell say dietterich make essentially the same point more concisely but although both be right about why odd and even be in this context hard for deep learning they be both wrong about the large issue for three reason first it can t be that people can t extrapolate you just do in two different example at the top of this section paraphrase chico marx who be you go to believe I or your own eye to someone immerse deeply — perhaps too deeply — in contemporary machine learn my odd and even problem seem unfair because a certain dimension the one which contain the value of 1 in the rightmost digit hasn t be illustrate in the training regime but when you a human look at my example above you will not be stymie by this particular gap in the training datum you win t even notice it because your attention be on high level regularity people routinely extrapolate in exactly the fashion that I have be describe like recognize string reversal from the three training example I give above in a technical sense that be extrapolation and you just do it in the algebraic mind I refer to this specific kind of extrapolation as generalize universally quantify one to one mapping outside of a space of training example as a field we desperately need a solution to this challenge if we be ever to catch up to human learning — even if it mean shake up our assumption now it might reasonably be object that it s not a fair fight human manifestly depend on prior knowledge when they generalize such mapping in some sense dieterrich propose this objection later in his tweet stream true enough but in a way that s the point neural network of a certain sort don t have a good way of incorporate the right sort of prior knowledge in the place it be precisely because those network don t have a way of incorporate prior knowledge like many generalization hold for all element of unbounded class or odd number leave a remainder of one when divide by two that neural network that lack operation over variable fail the right sort of prior knowledge that would allow neural network to acquire and represent universally quantify one to one mapping standard neural network can t represent such mapping except in certain limited way convolution be a way of building in one particular such mapping prior to learn second say that no current system deep learning or otherwise can extrapolate in the way that I have describe be no excuse ; once again other architecture may be in the choppy water but that doesn t mean we shouldn t be try to swim to shore if we want to get to agi we have to solve the problem put differently yes one could certainly hack together solution to get deep learning to solve my specific number series problem by for example play game with the input encoding scheme ; the real question if we want to get to agi be how to have a system learn the sort of generalization I be describe in a general way third the claim that no current system can extrapolate turn out to be well false ; there be already ml system that can extrapolate at least some function of exactly the sort I describe and you probably own one microsoft excel its flash fill function in particular gulwani 2011 power by a very different approach to machine learning it can do certain kind of extrapolation albeit in a narrow context by the bushel e g try type the decimal digit 1 11 21 in a series of row and see if the system can extrapolate via flash fill to the eleventh item in the sequence 101 spoiler alert it can in exactly the same way as you probably would even though there be no positive example in the training dimension of the hundred digit the system learn from example the function you want and extrapolate it piece of cake can any deep learning system do that with three training example even with a range of experience on other small counting function like 1 3 5 and 2 4 6 well maybe but only the one that be likely do so be likely to be hybrid that build in operation over variable which be quite different from the sort of typical convolutional neural network that most people associate with deep learning put all this very differently one crude way to think about where we be with most ml system that we have today note 7 be that they just aren t design to think outside the box ; they be design to be awesome interpolator inside the box that s fine for some purpose but not other human be well at think outside box than contemporary ai ; I don t think anyone can seriously doubt that but that kind of extrapolation that microsoft can do in a narrow context but that no machine can do with human like breadth be precisely what machine learn engineer really ought to be work on if they want to get to agi 10 everybody in the field already know this there be nothing new here well certainly not everybody ; as note there be many critic who think we still don t know the limit of deep learning and other who believe that there might be some but none yet discover that say I never say that any of my point be entirely new ; for virtually all I cite other scholar who have independently reach similar conclusion 11 marcus fail to cite x definitely true ; the literature review be incomplete one favorite among the paper I fail to cite be shanahan s deep symbolic reinforcement garnelo arulkumaran & shanahan 2016 ; I also can t believe I forget richardson and domingo 2006 markov logic network I also wish I have cite evans and edward grefenstette 2017 a great paper from deepmind and smolensky s tensor calculus work smolensky et al 2016 and work on inductive programming in various form gulwani et al 2015 and probabilistic programming too by noah goodman goodman mansinghka roy bonawitz & tenenbaum 2012 all seek to bring rule and network close to together and old stuff by pioneer like jordan pollack smolensky et al 2016 and forbus and gentner s falkenhainer forbus & gentner 1989 and hofstadter and mitchell s 1994 work on analogy ; and many other I be sure there be a lot more I could and should have cite overall I try to be representative rather than fully comprehensive but I still could have do well # chagrin 12 marcus have no standing in the field ; he isn t a practitioner ; he be just a critic hesitant to raise this one but it come up in all kind of different response even from the mouth of certain well know professional as ram shankar note as a community we must circumscribe our criticism to science and merit base argument what really matter be not my credential which I believe do in fact qualify I to write but the validity of the argument either my argument be correct or they be not still for those who be curious I supply an optional mini history of some of my relevant credential in note 8 at the end 13 re hierarchy what about socher s tree rnn I have write to he in hope of have a well understanding of its current status I ve also privately push several other team towards try out task like lake and baroni 2017 present pengfei et al 2017 offer some interesting discussion 14 you could have be more critical of deep learning nobody quite say that not in exactly those word but a few come close generally privately one colleague for example point out that there may be some serious error of future forecasting around the same colleague add another colleague ml researcher and author pedro domingo point out still other shortcoming of current deep learning method that I didn t mention like other flexible supervised learning method deep learning system can be unstable in the sense that slightly change the training datum may result in large change in the result model as domingos note there s no guarantee this sort of rise and decline win t repeat itself neural network have rise and fall several time before all the way back to rosenblatt s first perceptron in 1957 we shouldn t mistake cyclical enthusiasm for a complete solution to intelligence which still seem to I anyway to be decade away if we want to reach agi we owe it to ourselves to be as keenly aware of challenge we face as we be of our success 2 there be other problem too in rely on these 1 000 image set for example in read a draft of this paper melanie mitchell point I to important recent work by loghmani and colleague 2017 on assess how deep learning do in the real world quote from the abstract the paper analyze the transferability of deep representation from web image to robotic datum in the wild despite the promising result obtain with representation develop from web image the experiment demonstrate that object classification with real life robotic datum be far from be solve 3 and that literature be grow fast in late december there be a paper about fool deep net into mistake a pair of skier for a dog https arxiv org pdf 1712 09665 pdf and another on a general purpose tool for build real world adversarial patch https arxiv org pdf 1712 09665 pdf see also https arxiv org ab 1801 00634 it s frightening to think how vulnerable deep learning can be real world context and for that matter consider filip pieknewski s blog on why photo train deep learning system have trouble transfer what they have learn to line drawing https blog piekniewski info 2016 12 29 can a deep net see a cat vision be not as solve as many people seem to think 4 as I will explain in the forthcoming paper alphago be not actually a pure deep reinforcement learning system although the quote passage present it as such it s really more of a hybrid with important component that be drive by symbol manipulating algorithm along with a well engineer deep learning component 5 alphazero by the way isn t unsupervise it s self supervise use self play and simulation as a way of generate supervised datum ; I will have a lot more to say about that system in a forthcoming paper 6 consider for example google search and how one might understand it google have recently add in a deep learning algorithm rankbrain to the wide array of algorithms it use for search and google search certainly take in datum and knowledge and process they hierarchically which accord to maher ibrahim be all you need to count as be deep learning but realistically deep learning be just one cue among many ; the knowledge graph component for example be base instead primarily on classical ai notion of traverse ontology by any reasonable measure google search be a hybrid with deep learning as just one strand among many call google search as a whole a deep learning system would be grossly misleading akin to relabele carpentry screwdrivery just because screwdriver happen to be involve 7 important exception include inductive logic programming inductive function program the brain behind microsoft s flash fill and neural programming all be make some progress here ; some of these even include deep learning but they also all include structured representation and operation over variable among their primitive operation ; that s all I be ask for 8 my ai experiment begin in adolescence with among other thing a latin english translator that I code in the programming language logo in graduate school study with steven pinker I explore the relation between language acquisition symbolic rule and neural network I also owe a debt to my undergraduate mentor neil stilling the child language datum I gather marcus et al 1992 for my dissertation have be cite hundred of time and be the most frequently model datum in the 90 s debate about neural network and how child learn language in the late 1990 s I discover some specific replicable problem with multilayer perceptron marcus 1998b ; marcus 1998a ; base on those observation I design a widely cite experiment publish in science marcus vijayan bandi rao & vishton 1999 that show that young infant could extract algebraic rule contra jeff elman s 1990 then popular neural network all of this culminate in a 2001 mit press book marcus 2001 which lobby for a variety of representational primitive some of which have begin to pop up in recent neural network ; in particular that the use of operation over variable in the new field of differentiable programming daniluk rocktäschel welbl & riedel 2017 ; grave et al 2016 owe something to the position outline in that book there be a strong emphasis on have memory record as well which can be see in the memory network be develop e g at facebook bordes usunier chopra & weston 2015 the next decade see I work on other problem include innateness marcus 2004 which I will discuss at length in the forthcoming piece about alphago and evolution marcus 2004 ; marcus 2008 I eventually return to ai and cognitive modeling publish a 2014 article on cortical computation in science marcus marblestone & dean 2014 that also anticipate some of what be now happen in differentiable programming more recently I take a leave from academia to find and lead a machine learn company in 2014 ; by any reasonable measure that company be successful acquire by uber roughly two year after found as co founder and ceo I put together a team of some of the very good machine learn talent in the world include zoubin ghahramani jeff clune noah goodman ken stanley and jason yosinski and play a pivotal role in develop our core intellectual property and shape our intellectual mission a patent be pende co write by zoubin ghahramani and myself although much of what we do there remain confidential now own by uber and not by I I can say that a large part of our effort be address towards integrate deep learning with our own technique which give I a great deal of familiarity with joy and tribulation of tensorflow and vanish and explode gradient we aim for state of the art result sometimes successfully sometimes not with sparse datum use hybridize deep learning system on a daily basis borde a usuni n chopra s & weston j 2015 large scale simple question answer with memory network arxiv daniluk m rocktäschel t welbl j & riedel s 2017 frustratingly short attention span in neural language modeling arxiv elman j l 1990 finding structure in time cognitive science 14 2 2 179 211 evans r & grefenstette e 2017 learn explanatory rule from noisy datum arxiv cs ne falkenhainer b forbus k d & gentner d 1989 the structure mapping engine algorithm and example artificial intelligence 41 1 1 1 63 fukushima k miyake s & ito t 1983 neocognitron a neural network model for a mechanism of visual pattern recognition ieee transaction on system man and cybernetic 5 826 834 garnelo m arulkumaran k & shanahan m 2016 towards deep symbolic reinforcement learning arxiv cs ai goodman n mansinghka v roy d m bonawitz k & tenenbaum j b 2012 church a language for generative model arxiv preprint arxiv 1206 3255 grave a wayne g reynolds m harley t danihelka I grabska barwińska a et al 2016 hybrid computing use a neural network with dynamic external memory nature 538 7626 7626 471 476 gulwani s 2011 automate string processing in spreadsheet use input output example dl acm org 46 1 1 317 330 gulwani s hernández orallo j kitzelmann e muggleton s h schmid u & zorn b 2015 inductive programming meet the real world communication of the acm 58 11 11 90 99 hofstadter d r & mitchell m 1994 the copycat project a model of mental fluidity and analogy make advance in connectionist and neural computation theory 2 31 112 31 112 29 30 hosseini h xiao b jaiswal m & poovendran r 2017 on the limitation of convolutional neural network in recognize negative image arxiv cs cv hubel d h & wiesel t n 1959 receptive field of single neurone in the cat s striate cortex the journal of physiology 148 3 3 574 591 lake b m & baroni m 2017 still not systematic after all these year on the compositional skill of sequence to sequence recurrent network arxiv loghmani m r caputo b & vincze m 2017 recognize object in the wild where do we stand arxiv cs ro marcus g f 1998a rethink eliminative connectionism cogn psychol 37 3 3 243 — 282 marcus g f 1998b can connectionism save constructivism cognition 66 2 2 153 — 182 marcus g f 2001 the algebraic mind integrate connectionism and cognitive science cambridge mass mit press marcus g f 2004 the birth of the mind how a tiny number of gene create the complexity of human thought basic book marcus g f 2008 kluge the haphazard construction of the human mind boston houghton mifflin marcus g 2018 deep learn a critical appraisal arxiv marcus g f marblestone a & dean t 2014a the atom of neural computation science 346 6209 6209 551 — 552 marcus g f marblestone a h & dean t l 2014b frequently ask question for the atom of neural computation biorxiv arxiv q bio nc marcus g f 2001 the algebraic mind integrate connectionism and cognitive science cambridge mass mit press marcus g f pinker s ullman m hollander m rosen t j & xu f 1992 overregularization in language acquisition monogr soc re child dev 57 4 4 1 182 marcus g f vijayan s bandi rao s & vishton p m 1999 rule learn by seven month old infant science 283 5398 5398 77 80 nguyen a yosinski j & clune j 2014 deep neural network be easily fool high confidence prediction for unrecognizable image arxiv cs cv pengfei l xipeng q & xuanje h 2017 dynamic compositional neural network over tree structure ijcai proceeding from proceeding of the twenty sixth international joint conference on artificial intelligence ijcai 17 ribeiro m t singh s & guestrin c 2016 why should I trust you explain the prediction of any classifier arxiv cs lg richardson m & domingo p 2006 markov logic network machine learn 62 1 1 107 136 sabour s dffsdfdsf n & hinton g e 2017 dynamic routing between capsule arxiv cs cv silver d schrittwieser j simonyan k antonoglou I huang a guez a et al 2017 master the game of go without human knowledge nature 550 7676 7676 354 359 smolensky p lee m he x yih w t gao j & deng l 2016 basic reasoning with tensor product representation arxiv cs ai from a quick cheer to a stand ovation clap to show how much you enjoy this story ceo & founder geometric intelligence acquire by uber professor of psychology and neural science nyu freelancer for the new yorker & new york times
Sarthak Jain,3.9K,10,https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74?source=tag_archive---------1----------------,how to easily detect object with deep learning on raspberry pi,disclaimer I m build nanonet com to help build ml with less datum and no hardware the raspberry pi be a neat piece of hardware that have capture the heart of a generation with ~15 m device sell with hacker build even cool project on it give the popularity of deep learning and the raspberry pi camera we think it would be nice if we could detect any object use deep learning on the pi now you will be able to detect a photobomber in your selfie someone enter harambe s cage where someone keep the sriracha or an amazon delivery guy enter your house 20 m year of evolution have make human vision fairly evolve the human brain have 30 % of it s neuron work on processing vision as compare with 8 percent for touch and just 3 percent for hear human have two major advantage when compare with machine one be stereoscopic vision the second be an almost infinite supply of training datum an infant of 5 year have have approximately 2 7b image sample at 30fps to mimic human level performance scientist break down the visual perception task into four different category object detection have be good enough for a variety of application even though image segmentation be a much more precise result it suffer from the complexity of create training datum it typically take a human annotator 12x more time to segment an image than draw bounding box ; this be more anecdotal and lack a source also after detect object it be separately possible to segment the object from the bounding box object detection be of significant practical importance and have be use across a variety of industry some of the example be mention below object detection can be use to answer a variety of question these be the broad category there be a variety of model architecture that be use for object detection each with trade off between speed size and accuracy we pick one of the most popular one yolo you only look once and have show how it work below in under 20 line of code if you ignore the comment note this be pseudo code not intend to be a work example it have a black box which be the cnn part of it which be fairly standard and show in the image below you can read the full paper here https pjreddie com media file paper yolo_1 pdf for this task you probably need a few 100 image per object try to capture datum as close to the datum you re go to finally make prediction on draw bounding box on the image you can use a tool like labelimg you will typically need a few people who will be work on annotate your image this be a fairly intensive and time consume task you can read more about this at medium com nanonet nanonet how to use deep learning when you have limit datum f68c0b512cab you need a pretraine model so you can reduce the amount of datum require to train without it you might need a few 100k image to train the model you can find a bunch of pretraine model here the process of train a model be unnecessarily difficult to simplify the process we create a docker image would make it easy to train to start train the model you can run the docker image have a run sh script that can be call with the follow parameter you can find more detail at to train a model you need to select the right hyper parameter find the right parameter the art of deep learning involve a little bit of hit and try to figure out which be the good parameter to get the high accuracy for your model there be some level of black magic associate with this along with a little bit of theory this be a great resource for find the right parameter quantize model make it small to fit on a small device like the raspberry pi or mobile small device like mobile phone and rasberry pi have very little memory and computation power training neural network be do by apply many tiny nudge to the weight and these small increment typically need float point precision to work though there be research effort to use quantize representation here too take a pre train model and running inference be very different one of the magical quality of deep neural network be that they tend to cope very well with high level of noise in their input why quantize neural network model can take up a lot of space on disk with the original alexnet be over 200 mb in float format for example almost all of that size be take up with the weight for the neural connection since there be often many million of these in a single model the node and weight of a neural network be originally store as 32 bit float point number the simple motivation for quantization be to shrink file size by store the min and max for each layer and then compress each float value to an eight bit integer the size of the file be reduce by 75 % code for quantization you need the raspberry pi camera live and work then capture a new image for instruction on how to install checkout this link download model once your do train the model you can download it on to your pi to export the model run then download the model onto the raspberry pi install tensorflow on the raspberry pi depend on your device you might need to change the installation a little run model for predict on the new image the raspberry pi have constraint on both memory and compute a version of tensorflow compatible with the raspberry pi gpu be still not available therefore it be important to benchmark how much time do each of the model take to make a prediction on a new image we have remove the need to annotate image we have expert annotator who will annotate your image for you we automatically train the good model for you to achieve this we run a battery of model with different parameter to select the good for your data nanonet be entirely in the cloud and run without use any of your hardware which make it much easy to use since device like the raspberry pi and mobile phone be not build to run complex compute heavy task you can outsource the workload to our cloud which do all of the compute for you get your free api key from http app nanonet com user api_key collect the image of object you want to detect you can annotate they either use our web ui https app nanonet com objectannotation appid = your_model_id or use open source tool like labelimg once you have dataset ready in folder image image file and annotation annotation for the image file start upload the dataset once the image have be upload begin train the model the model take ~2 hour to train you will get an email once the model be train in the meanwhile you check the state of the model once the model be train you can make prediction use the model from a quick cheer to a stand ovation clap to show how much you enjoy this story founder & ceo @ nanonet com nanonet machine learn api
Favio Vázquez,3.3K,14,https://towardsdatascience.com/a-weird-introduction-to-deep-learning-7828803693b0?source=tag_archive---------2----------------,a weird introduction to deep learning towards data science,there be amazing introduction course and blog post on deep learning I will name some of they in the resource section but this be a different kind of introduction but why weird maybe because it win t follow the normal structure of a deep learning post where you start with the math then go into the paper the implementation and then to application it will be more close to the post I do before about my journey into deep learning I think tell a story can be much more helpful than just throw information and formula everywhere so let s begin note there s a companion webinar to this article find it here sometimes be important to have a write backup of your thought I tend to talk a lot and be present in several presentation and conference and this be my way of contribute with a little knowledge to everyone deep learning dl be such an important field for data science ai technology and our life right now and it deserve all of the attention be get please don t say that deep learning be just add a layer to a neural net and that s it magic nope I m hope that after read this you have a different perspective of what dl be I just create this timeline base on several paper and other timeline with the purpose of everyone see that deep learning be much more than just neural network there have be really theoretical advance software and hardware improvement that be necessary for we to get to this day if you want it just pe I and I ll send it to you find my contact in the end of the article deep learning have be around for quite a while now so why it become so relevant so fast the last 5 7 year as I say before until the late 2000 we be still miss a reliable way to train very deep neural network nowadays with the development of several simple but important theoretical and algorithmic improvement the advance in hardware mostly gpus now tpus and the exponential generation and accumulation of data dl come naturally to fit this miss spot to transform the way we do machine learn deep learning be an active field of research too nothing be settle or close we be still search for the good model topology of the network good way to optimize their hyperparameter and more be very hard as any other active field on science to keep up to date with the investigation but it s not impossible a side note on topology and machine learn deep learning with topological signature by hofer et al luckily for we there be lot of people help understand and digest all of this information through course like the andrew ng one blog post and much more this for I be weird or uncommon because normally you have to wait for sometime sometime year to be able to digest difficult and advance information in paper or research journal of course most area of science be now really fast too to get from a paper to a blog post that tell you what yo need to know but in my opinion dl have a different feel we be work with something that be very exciting most people in the field be say that the last idea in the paper of deep learn specifically new topology and configuration for nn or algorithm to improve their usage be the good idea in machine learning in decade remember that dl be inside of ml I ve use the word learn a lot in this article so far but what be learn in the context of machine learn the word learn describe an automatic search process for well representation of the datum you be analyze and study please have this in mind be not make a computer learn this be a very important word for this field rep re sen ta tion don t forget about it what be a representation it s a way to look at datum let I give you an example let s say I tell you I want you to drive a line that separate the blue circle from the green triangle for this plot so if you want to use a line this be what the author say this be impossible if we remember the concept of a line so be the case lose actually no if we find a way of represent this datum in a different way in a way we can draw a straight line to separate the type of datum this be somethinkg that math teach we hundred of year ago in this case what we need be a coordinate transformation so we can plot or represent this datum in a way we can draw this line if we look the polar coordinate transformation we have the solution and that s it now we can draw a line so in this simple example we find and choose the transformation to get a well representation by hand but if we create a system a program that can search for different representation in this case a coordinate change and then find a way of calculate the percentage of category be classify correctly with this new approach in that moment we be do machine learn this be something very important to have in mind deep learning be representation learning use different kind of neural network and optimize the hyperparameter of the net to get learn the good representation for our datum this wouldn t be possible without the amazing breakthrough that lead we to the current state of deep learning here I name some of they learn representation by back propagate error by david e rumelhart geoffrey e hinton & ronald j williams a theoretical framework for back propagation by yann lecun 2 idea well initialization of the parameter of the net something to remember the initialization strategy should be select accord to the activation function use next 3 idea well activation function this mean well way of approximate the function fast lead to fast training process 4 idea dropout well way of prevent overfitting and more dropout a simple way to prevent neural network from overfitte a great paper by srivastava hinton and other 5 idea convolutional neural net cnns gradient base learning apply to document recognition by lecun and other imagenet classification with deep convolutional neural network by krizhevsky and other 6 idea residual net resnet 7 idea region base cnn use for object detection and more 8 idea recurrent neural network rnn and lstms btw it be show by liao and poggio 2016 that resnet = = rnn arxiv 1604 03640v1 9 idea generative adversarial network gan 10 idea capsule network and there be many other but I think those be really important theoretical and algorithmic breakthrough that be change the world and that give momentum for the dl revolution it s not easy to get start but I ll try my good to guide you through this process check out this resource but remember this be not only watch video and read paper it s about understand programming code fail and then make it happen 1 learn python and r ; 0 andrew ng and coursera you know he doesn t need an intro siraj raval he s amazing he have the power to explain hard concept in a fun and easy way follow he on his youtube channel specifically this playlist — the math of intelligence — intro to deep learn 3 françois chollet s book deep learning with python and r 3 ibm cognitive class 5 datacamp deep learning be one of the most important tool and theory a data scientist should learn we be so lucky to see amazing people create both research software tool and hardware specific for dl task dl be computationally expensive and even though there s be advance in theory software and hardware we need the development in big datum and distribute machine learning to improve performance and efficiency great people and company be make amazing effort to join the distribute framework spark and dl library tf and keras here s an overview 2 elepha distribute dl with keras & pyspark 3 yahoo inc tensorflowonspark 4 cern distribute keras keras + spark 5 qubole tutorial kera + spark 6 intel corporation bigdl distribute deep learning library for apache spark 7 tensorflow and spark on google cloud as I ve say before one of the most important moment for this field be the creation and open source of tensorflow tensorflow be an open source software library for numerical computation use data flow graph node in the graph represent mathematical operation while the graph edge represent the multidimensional datum array tensor communicate between they the thing you be see in the image above be tensor manipulation work with the riemann tensor in general relativity tensor define mathematically be simply array of number or function that transform accord to certain rule under a change of coordinate but in the scope of machine learning and deep learn a tensor be a generalization of vector and matrix to potentially high dimension internally tensorflow represent tensor as n dimensional array of base datatype we use heavily tensor all the time in dl but you don t need to be an expert in they to use it you may need to understand a little bit about they so here I list some good resource after you check that out the breakthrough I mention before and the programming framework like tensorflow or kera for more on keras go here now I think you have an idea of what you need to understand and work with deep learning but what have we achieve so far with dl to name a few from françois chollet book on dl and much more here s a list of 30 great and funny application of dl thinking about the future of deep learning for programming or building application I ll repeat what I say in other post I really think guis and automl be the near future of get thing do with deep learning don t get I wrong I love code but I think the amount of code we will be write next year will decay we can not spend so many hour worldwide program the same stuff over and over again so I think these two feature guis and automl will help data scientist on get more productive and solve more problem on of the well free platform for do these task in a simple gui be deep cognition their simple drag & drop interface help you design deep learning model with ease deep learning studio can automatically design a deep learning model for your custom dataset thank to their advance automl feature with nearly one click here you can learn more about they take a look at the price o it s freeeee I mean it s amazing how fast the development in the area be right now that we can have simple guis to interact with all the hard and interesting concept I talk about in this post one of the thing I like about that platform be that you can still code interact with tensorflow keras caffe mxnet an much more with the command line or their notebook without instal anything you have both the notebook and the cli I take my hat off to they and their contribution to society other interesting application of deep learning that you can try for free or for little cost be some of they be on private beta thank for read this weird introduction to deep learning I hope it help you get start in this amazing area or maybe just discover something new if you have question just add I on linkedin and we ll chat there from a quick cheer to a stand ovation clap to show how much you enjoy this story data scientist physicist and computer engineer love share idea thought and contribute to open source in machine learning and deep learning ; share concept idea and code
Oleksandr Savsunenko,5.5K,4,https://hackernoon.com/the-new-neural-internet-is-coming-dda85b876adf?source=tag_archive---------3----------------,the new neural internet be come hacker noon,how it all begin the landscape think of the typical and well study neural network such as image classifier as a left hemisphere of the neural network technology with this in mind it be easy to understand what be generative adversarial network it be a kind of right hemisphere — the one that be claim to be responsible for creativity the generative adversarial network gan be the first step of neural network technology learning creativity typical gan be a neural network train to generate image on the certain topic use an image dataset and some random noise as a seed up until now image create by gan be of low quality and limited in resolution recent advance by nvidia show that it be within a reach to generate photorealistic image in high resolution and they publish the technology itself in open access there be a plethora of gan type of various complexity architecture and strange acronym we be mostly interested here in conditional gan and variational autoencoder conditional gan be capable of not just mimic the broad type of image as bedroom face dog but also dive into more specific category for example the text2image network be capable of translation textual image description into the image itself by vary random seed that be concatenate to the meaning vector we be able to produce an infinite number of bird image matching description let s just close your eye and see the world in 2 year company like nvidia will push gan technology to industry ready level same as they do with celebrity face generation this mean that a gan will be able to generate any image on demand on the fly base on textual for example description this will render obsolete a number of photography and design relate industry here s how this will work again the network be able to generate an infinite number of image by vary random seed and here s the scary part such a network can receive not only description of the target object it need to generate but also a vector describe you — the ad consumer this ad can have a very deep description of your personality web browse history recent transaction and geolocation so the gan will generate one time unique and that fit you perfectly ctr be go sky high by measure your reaction the network will adapt and make ad target you more and more precisely hit your soft spot so at the end of the day we be go to see a fully personalize content everywhere on the internet everyone will see fully custom version of all content that be adapt to the consumer base on his lifestyle opinion and history we all witness arousal of this bubble pattern after late usa election and it s go to be get bad gan will able to target content precisely to you with no limitation of the medium — start from image ad and up to complex opinion tread and publication generate by machine this will create a constant feedback loop improve base on your interaction and there be go to be a competition of different gan between each other kind of a fully automate war of phycological manipulation have humanity as a battlefield the drive force behind this trend be extremely simple — profit and this be not a scary doomsday scenario this actually be happen today I have no idea but surely we need few thing broad public discussion about this technology inevitable arrival and a backup plan to stop it so it s well to start think now — how we can fight this process and benefit from it at the same time we be not there yet due to some technical limitation up until recently image generate by gan be just of bad quality and easily spot as fake nvidia show that it be actually doable to generate 1024x1024 extremely real face to move thing forward we would need fast and big gpu more theoretical study on gan more smart hack around gan training more label dataset etc please notice — we don t need new power source quantum processor but they can help general ai to reach this point or some other purely theoretical new cool thing all we need be within a reach of few year and likely big corp already have this kind of resource available also we will need smart neural network I be definitely look for progress in capsule approach by hinton et al and of course we will be the first to implement this in super resolution technology that should heavily benefit from gan progress let I know what you think from a quick cheer to a stand ovation clap to show how much you enjoy this story machine learning engineer doer maker dreamer father how hacker start their afternoon
Max Pechyonkin,3.4K,8,https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a?source=tag_archive---------4----------------,stochastic weight averaging — a new way to get state of the art result in deep learning,in this article I will discuss two interesting recent paper that provide an easy way to improve performance of any give neural network by use a smart way to ensemble they be additional prerequisite reading that will make context of this post much more easy to understand traditional ensemble combine several different model and make they predict on the same input then some way of averaging be use to determine the final prediction of the ensemble it can be simple voting an average or even another model that learn to predict correct value or label base on the input of model in the ensemble ridge regression be one particular way of combine several prediction which be use by kaggle win machine learning practitioner when apply in deep learning ensemble can be use to combine prediction of several neural network to produce one final prediction usually it be a good idea to use neural network of different architecture in an ensemble because they will likely make mistake on different training sample and therefore the benefit of ensemble will be large however you can also ensemble model with the same architecture and it will give surprisingly good result one very cool trick exploit this approach be propose in the snapshot ensemble paper the author take weight snapshot while train the same network and then after training create an ensemble of net with the same architecture but different weight this allow to improve test performance and it be a very cheap way too because you just train one model once just save weight from time to time you can refer to this awesome post for more detail if you aren t yet use cyclical learning rate then you definitely should as it become the standard state of the art training technique that be very simple not computationally heavy and provide significant gain at almost no additional cost all of the example above be ensemble in the model space because they combine several model and then use model prediction to produce the final prediction in the paper that I be discuss in this post however the author propose to use a novel ensemble in the weight space this method produce an ensemble by combine weight of the same network at different stage of training and then use this model with combine weight to make prediction there be 2 benefit from this approach let s see how it work but first we need to understand some important fact about loss surface and generalizable solution the first important insight be that a train network be a point in multidimensional weight space for a give architecture each distinct combination of network weight produce a separate model since there be infinitely many combination of weight for any give architecture there will be infinitely many solution the goal of training of a neural network be to find a particular solution point in the weight space that will provide low value of the loss function both on training and testing data set during training by change weight training algorithm change the network and travel in the weight space gradient descent algorithm travel on a loss plane in this space where plane elevation be give by the value of the loss function it be very hard to visualize and understand the geometry of multidimensional weight space at the same time it be very important to understand it because stochastic gradient descent essentially traverse a loss surface in this highly multidimensional space during training and try to find a good solution — a point on the loss surface where loss value be low it be know that such surface have many local optima but it turn out that not all of they be equally good one metric that can distinguish a good solution from a bad one be its flatness the idea be that training datum set and testing datum set will produce similar but not exactly the same loss surface you can imagine that a test surface will be shift a bit relative to the train surface for a narrow solution during test time a point that give low loss can have a large loss because of this shift this mean that this narrow solution do not generalize well — training loss be low while test loss be large on the other hand for a wide and flat solution this shift will lead to training and testing loss be close to each other I explain the difference between narrow and wide solution because the new method which be the focus of this post lead to nice and wide solution initially sgd will make a big jump in the weight space then as the learning rate get small due to cosine annealing sgd will converge to some local solution and the algorithm will take a snapshot of the model by add it to the ensemble then the rate be reset to high value again and sgd take a large jump again before converge to some different local solution cycle length in the snapshot ensembling approach be 20 to 40 epoch the idea of long learning rate cycle be to be able to find sufficiently different model in the weight space if the model be too similar then prediction of the separate network in the ensemble will be too close and the benefit of ensemble will be negligible snapshot ensemble work really well and improve model performance but fast geometric ensemble work even well fast geometric ensembling be very similar to snapshot ensembling but be have some distinguish feature it use linear piecewise cyclical learning rate schedule instead of cosine secondly the cycle length in fge be much short — only 2 to 4 epoch per cycle at first intuition the short cycle be wrong because the model at the end of each cycle will be close to each other and therefore ensemble they will not give any benefit however as the author discover because there exist connected path of low loss between sufficiently different model it be possible to travel along those path in small step and the model encounter along will be different enough to allow ensemble they with good result thus fge show improvement compare to snapshot ensemble and it take small step to find the model which make it fast to train to benefit from both snapshot ensembling or fge one need to store multiple model and then make prediction for all of they before average for the final prediction thus for additional performance of the ensemble one need to pay with high amount of computation so there be no free lunch there or be there this be where the new paper with stochastic weight averaging come in stochastic weight averaging closely approximate fast geometric ensembling but at a fraction of computational loss swa can be apply to any architecture and datum set and show good result in all of they the paper suggest that swa lead to wide minima the benefit of which I discuss above swa be not an ensemble in its classical understanding at the end of training you get one model but it s performance beat snapshot ensemble and approach fge intuition for swa come from empirical observation that local minima at the end of each learning rate cycle tend to accumulate at the border of area on loss surface where loss value be low point w1 w2 and w3 be at the border of the red area of low loss in the left panel of figure above by take the average of several such point it be possible to achieve a wide generalizable solution with even low loss wswa in the left panel of the figure above here be how it work instead of an ensemble of many model you only need two model at the end of each learning rate cycle the current weight of the second model will be use to update the weight of the run average model by take weight mean between the old run average weight and the new set of weight from the second model formula provide in the figure on the leave by follow this approach you only need to train one model and store only two model in memory during training for prediction you only need the run average model and predict on it be much fast than use ensemble describe above where you use many model to predict and then average result author of the paper provide their own implementation in pytorch also swa be implement in the awesome fast ai library that everyone should be use and if you haven t yet see their course then follow the link you can follow I on twitter let s also connect on linkedin from a quick cheer to a stand ovation clap to show how much you enjoy this story deep learning sharing concept idea and code
Daniel Simmons,3.4K,8,https://itnext.io/you-can-build-a-neural-network-in-javascript-even-if-you-dont-really-understand-neural-networks-e63e12713a3?source=tag_archive---------5----------------,you can build a neural network in javascript even if you don t really understand neural network,click here to share this article on linkedin » skip this part if you just want to get on with it I should really start by admit that I m no expert in neural network or machine learning to be perfectly honest most of it still completely baffle I but hopefully that s encouraging to any fellow non expert who might be read this eager to get their foot wet in m l machine learning be one of those thing that would come up from time to time and I d think to myself yeah that would be pretty cool but I m not sure that I want to spend the next few month learn linear algebra and calculus like a lot of developer however I m pretty handy with javascript and would occasionally look for example of machine learning implement in js only to find heap of article and stackoverflow post about how js be a terrible language for m l which admittedly it be then I d get distract and move on figure that they be right and I should just get back to validate form input and wait for css grid to take off but then I find brain js and I be blow away where have this be hide the documentation be well write and easy to follow and within about 30 minute of get start I d set up and train a neural network in fact if you want to just skip this whole article and just read the readme on github be my guest it s really great that say what follow be not an in depth tutorial about neural network that delve into hide input layer activation function or how to use tensorflow instead this be a dead simple beginner level explanation of how to implement brain js that go a bit beyond the documentation here s a general outline of what we ll be do if you d prefer to just download a work version of this project rather than follow along with the article then you can clone the github repository here create a new directory and plop a good ol index html boilerplate file in there then create three js file brain js training datum js and script js or whatever generic term you use for your default js file and of course import all of these at the bottom of your index html file easy enough so far now go here to get the source code for brain js copy & paste the whole thing into your empty brain js file hit save and bam 2 out of 4 file be finish next be the fun part decide what your machine will learn there be countless practical problem that you can solve with something like this ; sentiment analysis or image classification for example I happen to think that application of m l that process text as input be particularly interesting because you can find training datum virtually everywhere and they have a huge variety of potential use case so the example that we ll be use here will be one that deal with classify text we ll be determine whether a tweet be write by donald trump or kim kardashian ok so this might not be the most useful application but twitter be a treasure trove of machine learning fodder and useless though it may be our tweet author identifier will nevertheless illustrate a pretty powerful point once it s be train our neural network will be able to look at a tweet that it have never see before and then be able to determine whether it be write by donald trump or by kim kardashian just by recognize pattern in the thing they write in order to do that we ll need to feed it as much training datum as we can bear to copy paste into our training datum js file and then we can see if we can identify ourselves some tweet author now all that s leave to do be set up brain js in our script js file and feed it some training datum in our training datum js file but before we do any of that let s start with a 30 000 foot view of how all of this will work set up brain js be extremely easy so we win t spend too much time on that but there be a few detail about how it s go to expect its input datum to be format that we should go over first let s start by look at the setup example that s include in the documentation which I ve slightly modify here that illustrate all this pretty well first of all the example above be actually a work a I it look at a give color and tell you whether black text or white text would be more legible on it which hopefully illustrate how easy brain js be to use just instantiate it train it and run it that s it I mean if you inline the training datum that would be 3 line of code pretty cool now let s talk about train datum for a minute there be two important thing to note in the above example other than the overall input { } output { } format of the training datum first the datum do not need to be all the same length as you can see on line 11 above only an r and a b value get pass whereas the other two input pass an r g and b value also even though the example above show the input as object it s worth mention that you could also use array I mention this largely because we ll be pass array of vary length in our project second those be not valid rgb value every one of they would come out as black if you be to actually use it that s because input value have to be between 0 and 1 in order for brain js to work with they so in the above example each color have to be process probably just feed through a function that divide it by 255 — the max value for rgb in order to make it work and we ll be do the same thing so if we want out neural network to accept tweet I e string as an input we ll need to run they through an similar function call encode below that will turn every character in a string into a value between 0 and 1 and store it in an array fortunately javascript have a native method for convert any character into ascii code call charcodeat so we ll use that and divide the outcome by the max value for extended ascii character 255 we re use extended ascii just in case we encounter any fringe case like é or 1⁄2 which will ensure that we get a value < 1 also we ll be store our training datum as plain text not as the encode datum that we ll ultimately be feed into our a I you ll thank I for this later so we ll need another function call processtrainingdata below that will apply the previously mention encode function to our training datum selectively convert the text into encode character and return an array of training datum that will play nicely with brain j so here s what all of that code will look like this go into your script js file something that you ll notice here that wasn t present in the example from the documentation show early other than the two helper function that we ve already go over be on line 20 in the train function which save the train neural network to a global variable call trainednet this prevent we from have to re train our neural network every time we use it once the network be train and save to the variable we can just call it like a function and pass in our encode input as show on line 25 in the execute function to use our a i alright so now your index html brain js and script js file be finish now all we need be to put something into training datum js and we ll be ready to go last but not least our training datum like I mention we re store all our tweet as text and encode they into numeric value on the fly which will make your life a whole lot easy when you actually need to copy paste training datum no format necessary just paste in the text and add a new row add that to your training data js file and you re do note although the above example only show 3 sample from each person I use 10 of each ; I just didn t want this sample to take up too much space of course your neural network s accuracy will increase proportionally to the amount of training datum that you give it so feel free to use more or less than I and see how it affect your outcome now to run your newly train neural network just throw an extra line at the bottom of your script js file that call the execute function and pass in a tweet from trump or kardashian ; make sure to console log it because we haven t build a ui here s a tweet from kim kardashian that be not in my training datum I e the network have never encounter this tweet before then pull up your index html page on localhost check the console aaand there it be the network correctly identify a tweet that it have never see before as originate from kim kardashian with a certainty of 86 % now let s try it again with a trump tweet and the result again a never before see tweet and again correctly identify this time with 97 % certainty now you have a neural network that can be train on any text that you want you could easily adapt this to identify the sentiment of an email or your company s online review identify spam classify blog post determine whether a message be urgent or not or any of a thousand different application and as useless as our tweet identifier be it still illustrate a really interesting point that a neural network like this can perform task as nuance as identify someone base on the way they write so even if you don t go out and create an innovative or useful tool that s power by machine learning this be still a great bit of experience to have in your developer tool belt you never know when it might come in handy or even open up new opportunity down the road once again all of this be available in a github repo here from a quick cheer to a stand ovation clap to show how much you enjoy this story web developer javascript enthusiast box fan itnext be a platform for it developer & software engineer to share knowledge connect collaborate learn and experience next gen technology
Eugenio Culurciello,2.8K,13,https://towardsdatascience.com/artificial-intelligence-ai-in-2018-and-beyond-e06f05167f9c?source=tag_archive---------6----------------,artificial intelligence ai in 2018 and beyond towards data science,these be my opinion on where deep neural network and machine learning be head in the large field of artificial intelligence and how we can get more and more sophisticated machine that can help we in our daily routine please note that these be not prediction of forecast but more a detailed analysis of the trajectory of the field the trend and the technical need we have to achieve useful artificial intelligence not all machine learning be target artificial intelligence and there be low hang fruit which we will examine here also the goal of the field be to achieve human and super human ability in machine that can help we in every day live autonomous vehicle smart home artificial assistant security camera be a first target home cooking and cleaning robot be a second target together with surveillance drone and robot another one be assistant on mobile device or always on assistant another be full time companion assistant that can hear and see what we experience in our life one ultimate goal be a fully autonomous synthetic entity that can behave at or beyond human level performance in everyday task see more about these goal here and here and here software be define here as neural network architecture train with an optimization algorithm to solve a specific task today neural network be the de facto tool for learn to solve task that involve learn supervise to categorize from a large dataset but this be not artificial intelligence which require act in the real world often learn without supervision and from experience never see before often combine previous knowledge in disparate circumstance to solve the current challenge neural network architecture — when the field boom a few year back we often say it have the advantage to learn the parameter of an algorithms automatically from datum and as such be superior to hand craft feature but we conveniently forget to mention one little detail the neural network architecture that be at the foundation of training to solve a specific task be not learn from datum in fact it be still design by hand hand craft from experience and it be currently one of the major limitation of the field there be research in this direction here and here for example but much more be need neural network architecture be the fundamental core of learn algorithm even if our learning algorithm be capable of master a new task if the neural network be not correct they will not be able to the problem on learn neural network architecture from datum be that it currently take too long to experiment with multiple architecture on a large dataset one have to try train multiple architecture from scratch and see which one work well well this be exactly the time consume trial and error procedure we be use today we ought to overcome this limitation and put more brain power on this very important issue unsupervised learning — we can not always be there for our neural network guide they at every stop of their life and every experience we can not afford to correct they at every instance and provide feedback on their performance we have our life to live but that be exactly what we do today with supervised neural network we offer help at every instance to make they perform correctly instead human learn from just a handful of example and can self correct and learn more complex datum in a continuous fashion we have talk about unsupervised learning extensively here predictive neural network — a major limitation of current neural network be that they do not possess one of the most important feature of human brain their predictive power one major theory about how the human brain work be by constantly make prediction predictive coding if you think about it we experience it every day as you lift an object that you think be light but turn out heavy it surprise you because as you approach to pick it up you have predict how it be go to affect you and your body or your environment in overall prediction allow not only to understand the world but also to know when we do not and when we should learn in fact we save information about thing we do not know and surprise we so next time they will not and cognitive ability be clearly link to our attention mechanism in the brain our innate ability to forego of 99 9 % of our sensory input only to focus on the very important datum for our survival — where be the threat and where do we run to to avoid it or in the modern world where be my cell phone as we walk out the door in a rush building predictive neural network be at the core of interact with the real world and act in a complex environment as such this be the core network for any work in reinforcement learning see more below we have talk extensively about the topic of predictive neural network and be one of the pioneering group to study they and create they for more detail on predictive neural network see here and here and here limitation of current neural network — we have talk about before on the limitation of neural network as they be today can not predict reason on content and have temporal instability — we need a new kind of neural network that you can about read here neural network capsule be one approach to solve the limitation of current neural network we review they here we argue here that capsule have to be extend with a few additional feature continuous learning — this be important because neural network need to continue to learn new datum point continuously for their life current neural network be not able to learn new datum without be re train from scratch at every instance neural network need to be able to self assess the need of new training and the fact that they do know something this be also need to perform in real life and for reinforcement learning task where we want to teach machine to do new task without forget old one for more detail see this excellent blog post by vincenzo lomonaco transfer learning — or how do we have these algorithm learn on their own by watch video just like we do when we want to learn how to cook something new that be an ability that require all the component we list above and also be important for reinforcement learning now you can really train your machine to do what you want by just give an example the same way we human do every reinforcement learning — this be the holy grail of deep neural network research teach machine how to learn to act in an environment the real world this require self learn continuous learn predictive power and a lot more we do not know there be much work in the field of reinforcement learning but to the author it be really only scratch the surface of the problem still million of mile away from it we already talk about this here reinforcement learning be often refer as the cherry on the cake mean that it be just minor training on top of a plastic synthetic brain but how can we get a generic brain that then solve all problem easily it be a chicken in the egg problem today to solve reinforcement learning problem one by one we use standard neural network both these component be obvious solution to the problem and currently be clearly wrong but that be what everyone use because they be some of the available building block as such result be unimpressive yes we can learn to play video game from scratch and master fully observable game like chess and go but I do not need to tell you that be nothing compare to solve problem in a complex world imagine an ai that can play horizon zero dawn well than human I want to see that but this be what we want machine that can operate like we our proposal for reinforcement learning work be detail here it use a predictive neural network that can operate continuously and an associative memory to store recent experience no more recurrent neural network — recurrent neural network rnn have their day count rnn be particularly bad at parallelize for training and also slow even on special custom machine due to their very high memory bandwidth usage — as such they be memory bandwidth bind rather than computation bind see here for more detail attention base neural network be more efficient and fast to train and deploy and they suffer much less from scalability in training and deployment attention in neural network have the potential to really revolutionize a lot of architecture yet it have not be as recognize as it should the combination of associative memory and attention be at the heart of the next wave of neural network advancement attention have already show to be able to learn sequence as well as rnns and at up to 100x less computation who can ignore that we recognize that attention base neural network be go to slowly supplant speech recognition base on rnn and also find their way in reinforcement learning architecture and ai in general localization of information in categorization neural network — we have talk about how we can localize and detect key point in image and video extensively here this be practically a solved problem that will be embed in future neural network architecture hardware for deep learning be at the core of progress let we now forget that the rapid expansion of deep learning in 2008 2012 and in the recent year be mainly due to hardware and we have talk about hardware extensively before but we need to give you a recent update last 1 2 year see a boom in the are of machine learning hardware and in particular on the one target deep neural network we have significant experience here and we be fwdnxt the maker of snowflake deep neural network accelerator there be several company work in this space nvidia obviously intel nervana movidius bitmain cambricon cerebras deephi google graphcore groq huawei arm wave computing all be develop custom high performance micro chip that will be able to train and run deep neural network the key be to provide the low power and the highest measured performance while compute recent useful neural network operation not raw theoretical operation per second — as many claim to do but few people in the field understand how hardware can really change machine learn neural network and ai in general and few understand what be important in micro chip and how to develop they here be our list about neuromorphic neural network hardware please see here we talk briefly about application in the goal section above but we really need to go into detail here how be ai and neural network go to get into our daily life here be our list I have almost 20 year of experience in neural network in both hardware and software a rare combination see about I here medium webpage scholar linkedin and more if you find this article useful please consider a donation to support more tutorial and blog any contribution can make a difference for interesting additional reading please see from a quick cheer to a stand ovation clap to show how much you enjoy this story I dream and build new technology sharing concept idea and code
Devin Soni,5.8K,4,https://towardsdatascience.com/spiking-neural-networks-the-next-generation-of-machine-learning-84e167f4eb2b?source=tag_archive---------7----------------,spike neural network the next generation of machine learning,everyone who have be remotely tune in to recent progress in machine learning have hear of the current 2nd generation artificial neural network use for machine learn these be generally fully connect take in continuous value and output continuous value although they have allow we to make breakthrough progress in many field they be biologically inn accurate and do not actually mimic the actual mechanism of our brain s neuron the 3rd generation of neural network spike neural network aim to bridge the gap between neuroscience and machine learning use biologically realistic model of neuron to carry out computation a spike neural network snn be fundamentally different from the neural network that the machine learn community know snns operate use spike which be discrete event that take place at point in time rather than continuous value the occurrence of a spike be determine by differential equation that represent various biological process the most important of which be the membrane potential of the neuron essentially once a neuron reach a certain potential it spike and the potential of that neuron be reset the most common model for this be the leaky integrate and fire lif model additionally snn be often sparsely connect and take advantage of specialized network topology at first glance this may seem like a step backwards we have move from continuous output to binary and these spike train be not very interpretable however spike train offer we enhanced ability to process spatio temporal datum or in other word real world sensory datum the spatial aspect refer to the fact that neuron be only connect to neuron local to they so these inherently process chunk of the input separately similar to how a cnn would use a filter the temporal aspect refer to the fact that spike train occur over time so what we lose in binary encoding we gain in the temporal information of the spike this allow we to naturally process temporal datum without the extra complexity that rnn add it have be prove in fact that spike neuron be fundamentally more powerful computational unit than traditional artificial neuron give that these snn be more powerful in theory than 2nd generation network it be natural to wonder why we do not see widespread use of they the main issue that currently lie in practical use of snn be that of training although we have unsupervise biological learning method such as hebbian learning and stdp there be no know effective supervised training method for snn that offer high performance than 2nd generation network since spike train be not differentiable we can not train snn use gradient descent without lose the precise temporal information in spike train therefore in order to properly use snn for real world task we would need to develop an effective supervised learning method this be a very difficult task as do so would involve determine how the human brain actually learn give the biological realism in these network another issue that we be much close to solve be that simulate snns on normal hardware be very computationally intensive since it require simulate differential equation however neuromorphic hardware such as ibm s truenorth aim to solve this by simulate neuron use specialized hardware that can take advantage of the discrete and sparse nature of neuronal spike behavior the future of snn therefore remain unclear on one hand they be the natural successor of our current neural network but on the other they be quite far from be practical tool for most task there be some current real world application of snn in real time image and audio processing but the literature on practical application remain sparse most paper on snn be either theoretical or show performance under that of a simple fully connect 2nd generation network however there be many team work on develop snn supervised learning rule and I remain optimistic for the future of snn make sure you give this post 50 clap and my blog a follow if you enjoy this post and want to see more from a quick cheer to a stand ovation clap to show how much you enjoy this story crypto market data science ☞ twitter @devin_soni ☞ website https 100 github io share concept idea and code
Carlos E. Perez,3.9K,7,https://medium.com/intuitionmachine/neurons-are-more-complex-than-what-we-have-imagined-b3dd00a1dcd3?source=tag_archive---------8----------------,surprise neuron be now more complex than we think,one of the big misconception around be the idea that deep learning dl or artificial neural network ann mimics biological neuron at good ann mimic a cartoonish version of a 1957 model of a neuron anyone claim deep learning be biologically inspire be do so for marketing purpose or have never bother to read biological literature neuron in deep learning be essentially mathematical function that perform a similarity function of its input against internal weight the close a match be make the more likely an action be perform I e not send a signal to zero there be exception to this model see autoregressive network however it be general enough to include the perceptron convolution network and rnns neuron be very different from dl construct they don t maintain continuous signal but rather exhibit spiking or event drive behavior so when you hear about neuromorphic hardware then these be inspire on integrate and spike neuron these kind of system at good get a lot of press see ibm truenorth but have never be show to be effective however there have be some research work that have show some progress see https arxiv org ab 1802 02627v1 if you ask I if you truly want to build biologically inspire cognition then you should at the very least explore system be not continuous like dl biological system by nature will use the least amount of energy to survive dl system in stark contrast be power hungry that s because dl be a brute force method to achieve cognition we know it work we just don t know how to scale it down jeff hawkins of numenta have always lament that a more biologically inspire approach be need so in his research in build cognitive machinery he have architecte system that try to more closely mirror the structure of the neo cortex numenta s model of a neuron be considerably more elaborate than the deep learning model of a neuron as you can see in this graphic the team at numenta be bet on this approach in the hope of create something that be more capable than deep learning it hasn t be prove to be anywhere near successful they ve be do this long enough that the odd of they succeed be diminish overtime bycontrast deep learning despite its model of a cartoon neuron have be show to be unexpectedly effective in perform all kind of mind boggle feat of cognition deep learning be do something that be extraordinarily correct we just don t know exactly what that be unfortunately we have to throw in a new monkey wrench on all these research new experiment on the nature of neuron have reveal that biological neuron be even more complex than we have imagine they to be in short there be a lot more go on inside a single neuron than the simple idea of integrate and fire neuron may not be pure function dependent on a single parameter I e weight but rather they be stateful machine alternatively perhaps the weight may not even be single value and instead require complex value or maybe high dimension this be all behavior that research have yet to explore and thus we have little understanding to date if you think this throw a monkey wrench on our understanding there s an even new discovery that reveal even great complexity what this research reveal be that there be a mechanism for neuron to communicate with each other by send package of rna code to clarify these be package of instruction and not package of datum there be a profound difference between send code and send datum this imply that behavior from one neuron can change the behavior of another neuron ; not through observation but rather through injection of behavior this code exchange mechanism hint at the validity of my early conjecture be biological brain make of only discrete logic experimental evidence reveal a new reality even at the small unit of our cognition there be a kind of conversational cognition that be go on between individual neuron that modify each other s behavior thus not only be neuron machine with state they be also machine with an instruction set and a way to send code to each other I m sorry but this be just another level of complexity there be two obvious ramification of these experimental discovery the first be that our estimate of the computational capability of the human brain be likely to be at least an order of magnitude off the second be that research will begin in earnest to explore dl architecture with more complex internal node or neuron structure if we be to make the rough argument that a single neuron perform a single operation the total capacity of the human brain be measure at 38 peta operation per second if be then to assume a dl model of operation be equal to float point operation then a 38 petaflop system would be equivalent in capability the top rank supercomputer sunway taihulight from china be estimate at 125 petaflop however let s say the new result reveal 10x more computation then the number should be 380 petaflop and we perhaps have breathing room until 2019 what be obvious however be that biological brain actually perform much more cognition with less computation the second consequence it that it s now time to get back to the drawing board and begin to explore more complex kind of neuron the more complex kind we ve see to date be the one derive from lstm here be the result of a brute force architectural search for lstm like neuron it s not clear why these more complex lstm be more effective only the architectural search algorithm know but it can t explain itself there be newly release paper that explore more complex hand engineer lstms that reveal measurable improvement over standard lstms in summary a research plan that explore more complex kind of neuron may bear promise fruit this be not unlike the research that explore the use of complex value in neural network in these complex value network performance improvement be notice only on rnn network this should indicate that these internal neuron complexity may be necessary for capability beyond simple perception I suspect that these complexity be necessary for advanced cognition that seem to evade current deep learning system these include robustness to adversarial feature learn to forget learn what to ignore learn abstraction and recognize contextual switching I predict in the near future that we shall see more aggressive research in this area after all nature be already unequivocally tell we that neuron be individually more complex and therefore our own neuron model may also need to be more complex perhaps we need something as complicated as a grassmann algebra to make progress ; from a quick cheer to a stand ovation clap to show how much you enjoy this story author of artificial intuition and the deep learning playbook — intuition machine inc deep learning pattern methodology and strategy
Nityesh Agarwal,2.4K,13,https://towardsdatascience.com/wth-does-a-neural-network-even-learn-a-newcomers-dilemma-bd8d1bbbed89?source=tag_archive---------9----------------,wth do a neural network even learn — a newcomer s dilemma,I believe we all have that psychologist philosopher in our brain that like to ponder upon how thinking happen there a simple clear bird s eye view of what neural network learn — they learn increasingly more complex concept doesn t that feel familiar isn t that how we learn anything at all for instance let s consider how we as kid probably learn to recognise object and animal — see so neural network learn like we do it almost ease the mind to believe that we have this intangible sort of man make thing that be analogous to the mind itself it be especially appeal to someone who have just begin his her deep learning journey but no a neural network s learning be not analogous to our own almost all the credible guide and starter pack on the subject of deep learning come with a warning something along the line of and that s where all the confusion begin I think this be mostly because of the way in which most of the tutorial and beginner level book approach the subject let s see how michael nielsen describe what the hide neuron be do in his book — neural network and deep learning he like many other use the analogy between neural network and the human mind to try to explain a neural network the way line and edge make loop which then help in recognise some digit be what we would think of do many other tutorial try to use a similar analogy to explain what it mean to build a hierarchy of knowledge I have to say that because of this analogy I understand neural net well but it be one of the paradox that the very analogy that make a difficult concept intelligible to the masse can also create an illusion of knowledge among they reader need to understand that it be just an analogy nothing more nothing less they need to understand that every simple analogy need to be follow by more rigorous seemingly difficult explanation now don t get I wrong I be deeply thankful to michael nielsen for write this book it be one of the good book on the subject out there he be careful in mention that this be just for the sake of argument but I take it to mean this — maybe the network win t use the same exact piece maybe it will figure out some other piece and join they in some other way to recognise the digit but the essence will be the same right I mean each of those piece have to be some kind of an edge or a line or some loopy structure after all it doesn t seem like there be other possibility if you want to build a hierarchical structure to solve the problem of recognise digit as I gain a well intuition about they and how they work I understand that this view be obviously wrong it hit I let s consider loop — be able to identify a loop be essential for we human to write digit an 8 be two loop join end to end a 9 is loop with a tail under it and a 6 be loop with a tail up top but when it come to recognise digit in an image feature like loop seem difficult and infeasible for a neural network remember I m talk about your vanilla neutral network or mlp here I know its just a lot of hand wavy reasoning but I think it be enough to convince probably the edge and all the other hand engineer feature will face similar problem and there s the dilemma I have no clue about the answer or how to find it until 3blue1brown release a set of video about neural network it be grant sanderson s take at explain the subject to newcomer maybe even he feel that there be some miss piece in the explanation by other people and that he could address they in his tutorial and boy do he grant sanderson of 3blue1brown who use a structure with 2 hide layer say — the very loop and edge that we rule out above they be not look for loop or edge or anything even remotely close they be look for well something inexplicable some strange pattern that can be confuse for random noise I find those weight matrix image in the above screenshot really fascinating I think of they as a lego puzzle the weight matrix image be like the elementary lego block and my task be to figure out a way to arrange they together so that I could create all 10 digit this idea be inspire from the excerpt of neural network and deep learning that I post above there we see how we could assemble a 0 use hand make feature like edge and curve so I think that maybe we could do the same with the feature that the neural network actually find good all I need be those weight matrix image that be use in 3blue1brown s video now the problem be that grant have put only 7 image in the video so I be go to have to generate they on my own and create my very own set of lego block I import the code use in michael nielsen s book to a jupyter notebook then I extend the network class in there to include the method that would help I visualise the weight matrice one pixel for every connection in the network one image for each neuron show how much it like colour blue or dislike colour red the previous layer neuron so if I be to look at the image belong to one of the neuron in the hidden layer it would be like a heat map show one feature one basic lego block that will be use to recognise digit blue pixel would represent connection that it like whereas red one would represent the connection that it dislike I train a neural network that have notice that we will have 30 different type of basic lego block for our lego puzzle here because that s the size of our hidden layer and here s what they look like — these be the feature that we be look for the one that be well than loop and edge accord to the network and here s how it classify all 10 digit and guess what none of they make any sense none of the feature seem to capture any isolated distinguishable feature in the input image all of they can be mistake to be just randomly shape blob at randomly choose place I mean just look at how it identify a 0 this be the weight matrix image for the output neuron that recognize 0 s to be clear the pixel in this image represent the weight connect the hide layer to the output neuron that recognise 0 s we shall take only a handful of the most useful feature for each digit into account to do that we can visually select the most intense blue pixel and the most intense red pixel here the blue one should give we the most useful feature and the red one should give we the most dreaded one think of it as the neuron say — the image will absolutely * not * match this prototype if it be a 0 index of the three most intense blue pixel 3 6 26indice of the three most intense red pixel 5 18 22 matrix 6 and 26 seem to capture something like a blue boundary of sort that be surround inner red pixel — exactly what could actually help in identify a 0 but what about matrix 3 it do not capture any feature we can even explain in word the same go for matrix 18 why would the neuron not like it it seem quite similar to matrix 3 and let s not even go into the weird blue s in 22 nonsensical see let s do it for 1 index of the three most intense blue pixel 0 11 16indice of the top two most intense red pixel 7 20 I have no word for this one I win t even try to comment in what world can those be use to identify 1 s now the much anticipated 8 how will it represent the 2 loop in it top 3 most intense blue pixel 1 6 14top 3 most intense red pixel 7 24 27 nope this isn t any good either there seem to be no loop like we be expect it to have but there be another interesting thing to notice in here — a majority of the pixel in the output layer neuron image the one above the collage be red it seem like the network have figure out a way to recognise 8 use feature that it do not like so no I couldn t put digit together use those feature as lego block I fail real bad at the task but to be fair to myself those feature weren t so much lego blocky either here s why — so there it be neural network can be say to learn like we if you consider the way they build hierarchy of feature just like we do but when you see the feature themselves they be nothing like what we would use the network give you almost no explanation for the feature that they learn neural network be good function approximator when we build and train one we mostly just care about its accuracy — on what percentage of the test sample do it give positive result this work incredibly well for a lot of purpose because modern neural net can have remarkably high accuracy — upward of 98 % be not uncommon meaning that the chance of failure be just 1 in a 100 but here s the catch — when they be wrong there s no easy way to understand the reason why they be they can t be debug in the traditional sense for example here s an embarrassing incident that happen with google because of this understanding what neural network learn be a subject of great importance it be crucial to unleash the true power of deep learning it will help we in a few week ago the new york times magazine run a story about how neural network be train to predict the death of cancer patient with a remarkable accuracy here s what the writer an oncologist say I think I can strongly relate to this because of my little project during the little project that I describe early I stumble upon a few other result that I find really cool and worth share so here they be — small network I want to see how low I could make the hidden layer size while still get a considerable accuracy across my test set it turn out that with 10 neuron the network be able to classify 9343 out of 10000 test image correctly that s 93 43 % accuracy at classify image that it have never see with just 10 hide neuron just 10 different type of lego block to recognise 10 digit I find this incredibly fascinating of course these weight don t make much sense either in case you be curious I try it with 5 neuron too and I get an accuracy of 86 65 % ; 4 neuron accuracy 83 73 % ; below that it drop very steeply — 3 neuron 58 75 % 2 neuron 22 80 % weight initialisation + regularisation make a lot of difference just regularise your network and use good initialisation for the weight can have a huge effect on what your network learn let I demonstrate I use the same network architecture mean same no of layer and same no of neuron in the layer I then train 2 network object one without regularisation and use the same old np random randn whereas in the other one I use regularisation along with np random randn sqrt n this be what I observe yeah I be shock too note I have show the weight matrix associate with different index neuron in the above collage this be because due to different initialisation even the one at the same index learn different feature so I choose the one that appear to make the effect most starke to know more about weight initialisation technique in neural network I recommend that you start here if you want to discuss this article or any other project that you have in mind or really anything ai please feel free to comment below or drop I a message on linkedin facebook or twitter I have learn a lot more about deep learning since I do the project in this article like complete the deep learning specialisation at coursera 😄 don t hesitate to reach out if you think I could be of any help thank you for read 😄 you can follow I on twitter — https twitter com nityeshaga ; I win t spam your feed 😉 originally publish on the zeolearn blog from a quick cheer to a stand ovation clap to show how much you enjoy this story reader writer and a programmer sharing concept idea and code
